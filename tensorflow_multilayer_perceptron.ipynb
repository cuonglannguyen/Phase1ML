{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tensorflow_multilayer_perceptron.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SNa7o6S82aaB"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "import tensorflow.compat.v1 as tf\n",
        "import numpy as np\n",
        "import random\n",
        "tf.disable_v2_behavior()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def save_parameters(name,value,epoch):\n",
        "  filename = name.replace(':','-colon-') + '-epoch-{}.txt'.format(epoch)\n",
        "  if len(value.shape) == 1:\n",
        "    string_form = ','.join([str(number) for number in value])\n",
        "  else:\n",
        "    string_form = '\\n'.join([','.join([str(number) for number in value[row]]) for row in range(value.shape[0])])\n",
        "    with open('D:/E/CUONG/machine learning/20news-bydate/saved-paras' + filename,'w') as f:\n",
        "      f.write(string_form)\n",
        "def restore_parameters(name, epoch):\n",
        "  filename = name.replace(':','-colon-') + '-epoch-{}.txt'.format(epoch)\n",
        "  with open('D:/E/CUONG/machine learning/20news-bydate/saved-paras' + filename,'w') as f:\n",
        "    lines = f.read().splitlines()\n",
        "    if len(lines) == 1:\n",
        "      value = [float(number) for number in lines[0].split(',')]\n",
        "    else:\n",
        "      value = [[float(number) for number in lines[row].split(',')] for row in range(len(lines))]\n",
        "  return value\n",
        "class MLP:\n",
        "  def __init__(self, vocab_size, hidden_size):\n",
        "    self._vocab_size = vocab_size\n",
        "    self._hidden_size = hidden_size\n",
        "  def build_graph(self):\n",
        "    NUM_CLASSES = 100\n",
        "    self._X = tf.placeholder(tf.float32, shape = [None, self._vocab_size])\n",
        "    self._real_Y = tf.placeholder(tf.int32, shape = [None, ])\n",
        "\n",
        "    weights_1 = tf.get_variable(name = 'weights_input_hidden', shape = (self._vocab_size, self._hidden_size), initial_value = tf.random_uniform_initializer(seed = 2018))\n",
        "    biases_1 = tf.get_variable(name = 'biases_input_hidden', shape = (self._hidden_size), initial_value = tf.random_uniform_initializer(seed = 2018))\n",
        "    weights_2 = tf.get_variable(name = 'weights_hidden_output', shape = (self._hidden_size, NUM_CLASSES), initial_value = tf.random_uniform_initializer(seed = 2018))\n",
        "    biases_2 = tf.get_variable(name = 'biases_hidden_output', shape = (NUM_CLASSES), initial_value = tf.random_uniform_initializer(seed = 2018))\n",
        "    hidden = tf.matmul(self._X, weights_1) + biases_1\n",
        "    hidden = tf.sigmoid(hidden)\n",
        "    logits = tf.matmul(hidden, weights_2) + biases_2\n",
        "    labels_one_hot = tf.one_hot(indices=self._real_Y, depth=NUM_CLASSES, dtype=tf.float32)\n",
        "    loss = tf.nn.softmax_cross_entropy_with_logits(labels=labels_one_hot, logits=logits)\n",
        "    loss = tf.reduce_mean(loss)\n",
        "    probs = tf.nn.softmax(logits)\n",
        "    predicted_labels = tf.argmax(probs, axis = 1)\n",
        "    predicted_labels = tf.squeeze(predicted_labels)\n",
        "    return predicted_labels, loss\n",
        "  def trainer(self, loss, learning_rate):\n",
        "    train_op = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
        "    return train_op\n",
        "with open('D:/E/CUONG/machine learning/20news-bydate/words_idfs.txt') as f:\n",
        "    vocab_size = len(f.read().splitlines())\n",
        "    mlp = MLP(vocab_size = vocab_size, hidden_size = 50)\n",
        "    predicted_labels, loss = mlp.build_graph()\n",
        "    train_op = mlp.trainer(loss = loss, learning_rate = 0.1)\n",
        "\n",
        "\n",
        "class DataReader:\n",
        "  def __init__(self, data_path, batch_size, vocab_size):\n",
        "    self._batch_size = batch_size\n",
        "    with open(data_path) as f:\n",
        "      d_lines = f.read().splitlines()\n",
        "    self._data = []\n",
        "    self._labels =[]\n",
        "    for data_id, line in enumerate(d_lines):\n",
        "      vector = [0.0 for _ in range(vocab_size)]\n",
        "      features = line.split('<fff>')\n",
        "      label, doc_id = int(features[0]), int(features[1])\n",
        "      tokens = features[2].split()\n",
        "      for token in tokens:\n",
        "        index, value = int(token.split(':')[0]). float(token.split(':')[1])\n",
        "        vector[index] = value\n",
        "      self._data.append(vector)\n",
        "      self._labels.append(label)\n",
        "      self._data = np.array(self._data)\n",
        "      self.labels = np.array(self._labels)\n",
        "      self._num_epoch = 0\n",
        "      self._batch_id = 0\n",
        "  def next_batch(self):\n",
        "    start = self._batch_id + self._batch_size\n",
        "    end = start + self._batch_size\n",
        "    self._batch_id += 1\n",
        "    if end + self._batch_size > len(self._data):\n",
        "      end = len(self._data)\n",
        "      self._num_epoch += 1\n",
        "      self._batch_id = 0\n",
        "      indices = range(len(self._data))\n",
        "      random.seed(2018)\n",
        "      random.shuffle(indices)\n",
        "      self._data, self._labels = self.data[indices], self._labels[indices]\n",
        "    return self._data[start:end], self._labels[start:end]\n",
        "def load_dataset():\n",
        "    train_data_reader = DataReader(data_path = 'D:/E/CUONG/machine learning/20news-bydate/20news-train-tfidf.txt',batch_size = 50, vocab_size = vocab_size)\n",
        "    test_data_reader = DataReader(data_path = 'D:/E/CUONG/machine learning/20news-bydate/20news-test-tfidf.txt',batch_size = 50, vocab_size = vocab_size)\n",
        "    return train_data_reader, test_data_reader    \n",
        "with tf.Session() as sess:\n",
        "    train_data_reader, test_data_reader = load_dataset()\n",
        "    step, MAX_STEP = 0, 1000**2\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    while step < MAX_STEP:\n",
        "      train_data, train_labels = train_data_reader.next_batch()\n",
        "      plabels_eval, loss_eval, _ = sess.run([predicted_labels, loss, train_op], feed_dict={mlp._X: train_data, mlp._real_Y: train_labels})\n",
        "      trainable_variables = tf.trainable_variables()\n",
        "      for variable in trainable_variables:\n",
        "        save_parameters(name = variable.name, value = variable.eval(), epoch = train_data_reader._num_epoch)\n",
        "\n",
        "\n",
        "      step += 1\n",
        "      print('step: {}, loss: {}'.format(step, loss_eval))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "abU0NzF_2kYb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}