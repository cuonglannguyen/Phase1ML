{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "SNa7o6S82aaB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Admin\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:101: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import tensorflow.compat.v1 as tf\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from os import listdir\n",
    "from os.path import isfile\n",
    "import re\n",
    "import keras \n",
    "tf.disable_v2_behavior()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "abU0NzF_2kYb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 1, loss: 4.169332027435303\n",
      "step: 2, loss: 0.003972718492150307\n",
      "step: 3, loss: 2.6313202397432178e-05\n",
      "step: 4, loss: 5.36440438736463e-07\n",
      "step: 5, loss: 3.0994396382766354e-08\n",
      "step: 6, loss: 2.38418573772492e-09\n",
      "step: 7, loss: 0.0\n",
      "step: 8, loss: 0.0\n",
      "step: 9, loss: 0.0\n",
      "step: 10, loss: 14.665328025817871\n",
      "step: 11, loss: 32.89753341674805\n",
      "step: 12, loss: 29.00861167907715\n",
      "step: 13, loss: 23.246826171875\n",
      "step: 14, loss: 16.935144424438477\n",
      "step: 15, loss: 10.153888702392578\n",
      "step: 16, loss: 3.7774713039398193\n",
      "step: 17, loss: 0.38595789670944214\n",
      "step: 18, loss: 0.006771399173885584\n",
      "step: 19, loss: 2.5346902475575916e-05\n",
      "step: 20, loss: 2.431865482321882e-07\n",
      "step: 21, loss: 3.8146950487316644e-08\n",
      "step: 22, loss: 17.639118194580078\n",
      "step: 23, loss: 23.644428253173828\n",
      "step: 24, loss: 21.5782470703125\n",
      "step: 25, loss: 18.51752471923828\n",
      "step: 26, loss: 15.378205299377441\n",
      "step: 27, loss: 11.579537391662598\n",
      "step: 28, loss: 7.943426609039307\n",
      "step: 29, loss: 3.9362692832946777\n",
      "step: 30, loss: 1.240530014038086\n",
      "step: 31, loss: 0.08726412802934647\n",
      "step: 32, loss: 0.003877706127241254\n",
      "step: 33, loss: 0.0006380208069458604\n",
      "step: 34, loss: 24.260648727416992\n",
      "step: 35, loss: 26.577783584594727\n",
      "step: 36, loss: 25.079017639160156\n",
      "step: 37, loss: 22.864574432373047\n",
      "step: 38, loss: 20.264162063598633\n",
      "step: 39, loss: 17.408418655395508\n",
      "step: 40, loss: 14.412736892700195\n",
      "step: 41, loss: 12.038142204284668\n",
      "step: 42, loss: 9.27933120727539\n",
      "step: 43, loss: 6.6038689613342285\n",
      "step: 44, loss: 4.175597190856934\n",
      "step: 45, loss: 2.8398516178131104\n",
      "step: 46, loss: 12.096085548400879\n",
      "step: 47, loss: 10.940234184265137\n",
      "step: 48, loss: 10.913595199584961\n",
      "step: 49, loss: 9.89670181274414\n",
      "step: 50, loss: 8.967114448547363\n",
      "step: 51, loss: 7.622203350067139\n",
      "step: 52, loss: 6.298313140869141\n",
      "step: 53, loss: 4.823845386505127\n",
      "step: 54, loss: 3.4764797687530518\n",
      "step: 55, loss: 2.5795657634735107\n",
      "step: 56, loss: 1.5543923377990723\n",
      "step: 57, loss: 6.975457668304443\n",
      "step: 58, loss: 11.321131706237793\n",
      "step: 59, loss: 11.164556503295898\n",
      "step: 60, loss: 10.861947059631348\n",
      "step: 61, loss: 8.868509292602539\n",
      "step: 62, loss: 7.6939544677734375\n",
      "step: 63, loss: 6.328889846801758\n",
      "step: 64, loss: 4.951040744781494\n",
      "step: 65, loss: 3.728076457977295\n",
      "step: 66, loss: 2.918224573135376\n",
      "step: 67, loss: 1.9129431247711182\n",
      "step: 68, loss: 1.1919785737991333\n",
      "step: 69, loss: 9.27685832977295\n",
      "step: 70, loss: 13.136346817016602\n",
      "step: 71, loss: 12.10223388671875\n",
      "step: 72, loss: 11.718879699707031\n",
      "step: 73, loss: 10.975625038146973\n",
      "step: 74, loss: 10.284649848937988\n",
      "step: 75, loss: 9.76731014251709\n",
      "step: 76, loss: 8.980903625488281\n",
      "step: 77, loss: 8.203845977783203\n",
      "step: 78, loss: 7.8436808586120605\n",
      "step: 79, loss: 7.104228496551514\n",
      "step: 80, loss: 6.40826416015625\n",
      "step: 81, loss: 9.305932998657227\n",
      "step: 82, loss: 8.884113311767578\n",
      "step: 83, loss: 7.970940113067627\n",
      "step: 84, loss: 7.385651111602783\n",
      "step: 85, loss: 6.470230579376221\n",
      "step: 86, loss: 5.385858058929443\n",
      "step: 87, loss: 5.083709239959717\n",
      "step: 88, loss: 4.465716361999512\n",
      "step: 89, loss: 4.304250717163086\n",
      "step: 90, loss: 3.8879599571228027\n",
      "step: 91, loss: 3.598484754562378\n",
      "step: 92, loss: 3.9230217933654785\n",
      "step: 93, loss: 8.779236793518066\n",
      "step: 94, loss: 8.471531867980957\n",
      "step: 95, loss: 8.305649757385254\n",
      "step: 96, loss: 8.023138046264648\n",
      "step: 97, loss: 7.7796220779418945\n",
      "step: 98, loss: 7.550410747528076\n",
      "step: 99, loss: 7.298307418823242\n",
      "step: 100, loss: 7.025364875793457\n",
      "step: 101, loss: 6.788293361663818\n",
      "step: 102, loss: 6.529361724853516\n",
      "step: 103, loss: 6.2565531730651855\n",
      "step: 104, loss: 6.728816509246826\n",
      "step: 105, loss: 9.907530784606934\n",
      "step: 106, loss: 9.719426155090332\n",
      "step: 107, loss: 9.511829376220703\n",
      "step: 108, loss: 9.371912002563477\n",
      "step: 109, loss: 9.022883415222168\n",
      "step: 110, loss: 8.877514839172363\n",
      "step: 111, loss: 8.498735427856445\n",
      "step: 112, loss: 8.272740364074707\n",
      "step: 113, loss: 7.926090240478516\n",
      "step: 114, loss: 7.670245170593262\n",
      "step: 115, loss: 7.3354973793029785\n",
      "step: 116, loss: 6.847085952758789\n",
      "step: 117, loss: 6.206813812255859\n",
      "step: 118, loss: 6.010122299194336\n",
      "step: 119, loss: 5.806330680847168\n",
      "step: 120, loss: 5.621518135070801\n",
      "step: 121, loss: 5.340761184692383\n",
      "step: 122, loss: 5.0809831619262695\n",
      "step: 123, loss: 4.66374397277832\n",
      "step: 124, loss: 4.1860432624816895\n",
      "step: 125, loss: 3.8888092041015625\n",
      "step: 126, loss: 2.8337295055389404\n",
      "step: 127, loss: 2.4048643112182617\n",
      "step: 128, loss: 3.3901374340057373\n",
      "step: 129, loss: 11.571773529052734\n",
      "step: 130, loss: 11.282127380371094\n",
      "step: 131, loss: 11.186262130737305\n",
      "step: 132, loss: 10.86430549621582\n",
      "step: 133, loss: 10.651005744934082\n",
      "step: 134, loss: 10.338764190673828\n",
      "step: 135, loss: 10.051501274108887\n",
      "step: 136, loss: 9.8234281539917\n",
      "step: 137, loss: 9.519400596618652\n",
      "step: 138, loss: 9.422018051147461\n",
      "step: 139, loss: 9.014477729797363\n",
      "step: 140, loss: 9.057242393493652\n",
      "step: 141, loss: 9.092239379882812\n",
      "step: 142, loss: 8.977439880371094\n",
      "step: 143, loss: 8.536998748779297\n",
      "step: 144, loss: 8.317687034606934\n",
      "step: 145, loss: 8.070128440856934\n",
      "step: 146, loss: 7.728003025054932\n",
      "step: 147, loss: 7.444190502166748\n",
      "step: 148, loss: 7.14705753326416\n",
      "step: 149, loss: 6.859687328338623\n",
      "step: 150, loss: 6.553656101226807\n",
      "step: 151, loss: 6.293325424194336\n",
      "step: 152, loss: 8.139175415039062\n",
      "step: 153, loss: 10.169407844543457\n",
      "step: 154, loss: 10.045778274536133\n",
      "step: 155, loss: 9.784278869628906\n",
      "step: 156, loss: 9.444413185119629\n",
      "step: 157, loss: 9.147244453430176\n",
      "step: 158, loss: 8.86630916595459\n",
      "step: 159, loss: 8.495833396911621\n",
      "step: 160, loss: 8.150286674499512\n",
      "step: 161, loss: 7.6418046951293945\n",
      "step: 162, loss: 7.21558952331543\n",
      "step: 163, loss: 6.8465681076049805\n",
      "step: 164, loss: 8.961642265319824\n",
      "step: 165, loss: 11.459117889404297\n",
      "step: 166, loss: 11.251823425292969\n",
      "step: 167, loss: 10.671496391296387\n",
      "step: 168, loss: 10.680035591125488\n",
      "step: 169, loss: 10.03891372680664\n",
      "step: 170, loss: 9.674336433410645\n",
      "step: 171, loss: 9.43503189086914\n",
      "step: 172, loss: 9.231925964355469\n",
      "step: 173, loss: 8.767475128173828\n",
      "step: 174, loss: 8.433647155761719\n",
      "step: 175, loss: 8.21205997467041\n",
      "step: 176, loss: 9.600187301635742\n",
      "step: 177, loss: 9.445042610168457\n",
      "step: 178, loss: 9.109524726867676\n",
      "step: 179, loss: 8.880338668823242\n",
      "step: 180, loss: 8.52146053314209\n",
      "step: 181, loss: 8.298190116882324\n",
      "step: 182, loss: 7.868602275848389\n",
      "step: 183, loss: 7.565743446350098\n",
      "step: 184, loss: 7.176080703735352\n",
      "step: 185, loss: 6.771085739135742\n",
      "step: 186, loss: 6.388812065124512\n",
      "step: 187, loss: 5.85945987701416\n",
      "step: 188, loss: 7.355052947998047\n",
      "step: 189, loss: 7.7996602058410645\n",
      "step: 190, loss: 7.379268646240234\n",
      "step: 191, loss: 6.94102668762207\n",
      "step: 192, loss: 6.534597396850586\n",
      "step: 193, loss: 5.9595770835876465\n",
      "step: 194, loss: 5.378840446472168\n",
      "step: 195, loss: 4.658461093902588\n",
      "step: 196, loss: 4.027560234069824\n",
      "step: 197, loss: 3.33433198928833\n",
      "step: 198, loss: 2.5905182361602783\n",
      "step: 199, loss: 9.636220932006836\n",
      "step: 200, loss: 10.950763702392578\n",
      "step: 201, loss: 10.608525276184082\n",
      "step: 202, loss: 10.164990425109863\n",
      "step: 203, loss: 9.744253158569336\n",
      "step: 204, loss: 9.201881408691406\n",
      "step: 205, loss: 8.654847145080566\n",
      "step: 206, loss: 7.9546284675598145\n",
      "step: 207, loss: 7.452751636505127\n",
      "step: 208, loss: 6.738702774047852\n",
      "step: 209, loss: 6.173401355743408\n",
      "step: 210, loss: 9.916186332702637\n",
      "step: 211, loss: 12.791049003601074\n",
      "step: 212, loss: 11.81534194946289\n",
      "step: 213, loss: 11.247071266174316\n",
      "step: 214, loss: 10.592166900634766\n",
      "step: 215, loss: 9.872942924499512\n",
      "step: 216, loss: 9.060388565063477\n",
      "step: 217, loss: 8.460882186889648\n",
      "step: 218, loss: 7.954982757568359\n",
      "step: 219, loss: 7.37430477142334\n",
      "step: 220, loss: 6.4221649169921875\n",
      "step: 221, loss: 6.014517307281494\n",
      "step: 222, loss: 5.876890182495117\n",
      "step: 223, loss: 5.383292198181152\n",
      "step: 224, loss: 4.972426891326904\n",
      "step: 225, loss: 4.8136305809021\n",
      "step: 226, loss: 3.2128076553344727\n",
      "step: 227, loss: 2.872735023498535\n",
      "step: 228, loss: 3.3272900581359863\n",
      "step: 229, loss: 2.6693308353424072\n",
      "step: 230, loss: 2.773500919342041\n",
      "step: 231, loss: 2.892747163772583\n",
      "step: 232, loss: 2.891200542449951\n",
      "step: 233, loss: 2.816871404647827\n",
      "step: 234, loss: 2.753798246383667\n",
      "step: 235, loss: 2.929081678390503\n",
      "step: 236, loss: 2.5989022254943848\n",
      "step: 237, loss: 2.886836528778076\n",
      "step: 238, loss: 2.670102596282959\n",
      "step: 239, loss: 2.3454322814941406\n",
      "step: 240, loss: 2.468430280685425\n",
      "step: 241, loss: 2.163947343826294\n",
      "step: 242, loss: 2.589646100997925\n",
      "step: 243, loss: 2.66811466217041\n",
      "step: 244, loss: 2.5572757720947266\n",
      "step: 245, loss: 2.3395581245422363\n",
      "step: 246, loss: 2.2907042503356934\n",
      "step: 247, loss: 2.540308713912964\n",
      "step: 248, loss: 2.506709337234497\n",
      "step: 249, loss: 2.26059889793396\n",
      "step: 250, loss: 2.4576447010040283\n",
      "step: 251, loss: 1.9617518186569214\n",
      "step: 252, loss: 2.2560861110687256\n",
      "step: 253, loss: 2.5726654529571533\n",
      "step: 254, loss: 2.011075973510742\n",
      "step: 255, loss: 2.247976779937744\n",
      "step: 256, loss: 2.199815034866333\n",
      "step: 257, loss: 2.1020584106445312\n",
      "step: 258, loss: 2.2033987045288086\n",
      "step: 259, loss: 1.8448718786239624\n",
      "step: 260, loss: 2.2331736087799072\n",
      "step: 261, loss: 2.004544258117676\n",
      "step: 262, loss: 2.2177727222442627\n",
      "step: 263, loss: 1.7195504903793335\n",
      "step: 264, loss: 1.7298260927200317\n",
      "step: 265, loss: 1.8869630098342896\n",
      "step: 266, loss: 2.133913278579712\n",
      "step: 267, loss: 1.8136439323425293\n",
      "step: 268, loss: 1.7573946714401245\n",
      "step: 269, loss: 1.5185657739639282\n",
      "step: 270, loss: 2.1225368976593018\n",
      "step: 271, loss: 1.6856290102005005\n",
      "step: 272, loss: 1.5598336458206177\n",
      "step: 273, loss: 1.6905821561813354\n",
      "step: 274, loss: 1.3791064023971558\n",
      "step: 275, loss: 1.7733858823776245\n",
      "step: 276, loss: 1.3458365201950073\n",
      "step: 277, loss: 1.9990838766098022\n",
      "step: 278, loss: 1.6774846315383911\n",
      "step: 279, loss: 1.4763967990875244\n",
      "step: 280, loss: 1.4111428260803223\n",
      "step: 281, loss: 1.2392528057098389\n",
      "step: 282, loss: 1.726141095161438\n",
      "step: 283, loss: 1.2294436693191528\n",
      "step: 284, loss: 1.494025707244873\n",
      "step: 285, loss: 1.8678550720214844\n",
      "step: 286, loss: 1.28466796875\n",
      "step: 287, loss: 1.4791946411132812\n",
      "step: 288, loss: 1.205939531326294\n",
      "step: 289, loss: 1.6369149684906006\n",
      "step: 290, loss: 1.3938390016555786\n",
      "step: 291, loss: 1.478629469871521\n",
      "step: 292, loss: 1.453474998474121\n",
      "step: 293, loss: 1.2692523002624512\n",
      "step: 294, loss: 1.085870623588562\n",
      "step: 295, loss: 1.233863115310669\n",
      "step: 296, loss: 1.147369384765625\n",
      "step: 297, loss: 1.0759028196334839\n",
      "step: 298, loss: 1.181443452835083\n",
      "step: 299, loss: 1.358737587928772\n",
      "step: 300, loss: 0.9339325428009033\n",
      "step: 301, loss: 1.121250867843628\n",
      "step: 302, loss: 1.1447676420211792\n",
      "step: 303, loss: 0.9873183965682983\n",
      "step: 304, loss: 0.8526483774185181\n",
      "step: 305, loss: 1.1392641067504883\n",
      "step: 306, loss: 0.8954209685325623\n",
      "step: 307, loss: 0.8160046935081482\n",
      "step: 308, loss: 1.0522658824920654\n",
      "step: 309, loss: 0.9059846997261047\n",
      "step: 310, loss: 0.8768994808197021\n",
      "step: 311, loss: 0.6942446231842041\n",
      "step: 312, loss: 0.4663536548614502\n",
      "step: 313, loss: 1.189063310623169\n",
      "step: 314, loss: 0.747963011264801\n",
      "step: 315, loss: 1.3700412511825562\n",
      "step: 316, loss: 0.8868150115013123\n",
      "step: 317, loss: 0.8741060495376587\n",
      "step: 318, loss: 0.6624619364738464\n",
      "step: 319, loss: 1.0252418518066406\n",
      "step: 320, loss: 1.1726980209350586\n",
      "step: 321, loss: 0.9101687073707581\n",
      "step: 322, loss: 0.9415735602378845\n",
      "step: 323, loss: 1.17558753490448\n",
      "step: 324, loss: 0.7878057956695557\n",
      "step: 325, loss: 1.0757015943527222\n",
      "step: 326, loss: 0.9785303473472595\n",
      "step: 327, loss: 1.0810551643371582\n",
      "step: 328, loss: 1.1033945083618164\n",
      "step: 329, loss: 0.7346938848495483\n",
      "step: 330, loss: 1.0670138597488403\n",
      "step: 331, loss: 0.7367107272148132\n",
      "step: 332, loss: 1.0180186033248901\n",
      "step: 333, loss: 0.7833643555641174\n",
      "step: 334, loss: 1.0805492401123047\n",
      "step: 335, loss: 0.5511258840560913\n",
      "step: 336, loss: 0.8299250602722168\n",
      "step: 337, loss: 0.7420907616615295\n",
      "step: 338, loss: 0.9653344750404358\n",
      "step: 339, loss: 0.5358394980430603\n",
      "step: 340, loss: 0.543293833732605\n",
      "step: 341, loss: 0.9403750896453857\n",
      "step: 342, loss: 0.5859229564666748\n",
      "step: 343, loss: 0.6336272358894348\n",
      "step: 344, loss: 0.5911414623260498\n",
      "step: 345, loss: 0.8913561105728149\n",
      "step: 346, loss: 0.557338297367096\n",
      "step: 347, loss: 0.7292177677154541\n",
      "step: 348, loss: 0.873920202255249\n",
      "step: 349, loss: 0.8060828447341919\n",
      "step: 350, loss: 0.9034742116928101\n",
      "step: 351, loss: 0.5558139085769653\n",
      "step: 352, loss: 0.930716335773468\n",
      "step: 353, loss: 0.9276570677757263\n",
      "step: 354, loss: 0.5524325370788574\n",
      "step: 355, loss: 1.0488852262496948\n",
      "step: 356, loss: 0.5942704677581787\n",
      "step: 357, loss: 0.8822731971740723\n",
      "step: 358, loss: 0.7157345414161682\n",
      "step: 359, loss: 0.5638856291770935\n",
      "step: 360, loss: 0.1324552744626999\n",
      "step: 361, loss: 0.7527462840080261\n",
      "step: 362, loss: 1.4953200817108154\n",
      "step: 363, loss: 0.5568783283233643\n",
      "step: 364, loss: 0.4771800935268402\n",
      "step: 365, loss: 0.5385317802429199\n",
      "step: 366, loss: 0.6891361474990845\n",
      "step: 367, loss: 0.479862779378891\n",
      "step: 368, loss: 0.37386879324913025\n",
      "step: 369, loss: 0.5087493062019348\n",
      "step: 370, loss: 0.6374149918556213\n",
      "step: 371, loss: 0.5229929685592651\n",
      "step: 372, loss: 0.31665536761283875\n",
      "step: 373, loss: 0.8304445743560791\n",
      "step: 374, loss: 0.8334361910820007\n",
      "step: 375, loss: 0.5193323493003845\n",
      "step: 376, loss: 0.6547977328300476\n",
      "step: 377, loss: 0.7053964138031006\n",
      "step: 378, loss: 0.3949121832847595\n",
      "step: 379, loss: 0.7535426616668701\n",
      "step: 380, loss: 0.5729674696922302\n",
      "step: 381, loss: 0.5808787941932678\n",
      "step: 382, loss: 0.5033532977104187\n",
      "step: 383, loss: 0.3088165521621704\n",
      "step: 384, loss: 0.42298203706741333\n",
      "step: 385, loss: 0.7152956128120422\n",
      "step: 386, loss: 0.5813792943954468\n",
      "step: 387, loss: 0.7882159948348999\n",
      "step: 388, loss: 0.8115475177764893\n",
      "step: 389, loss: 0.4268612265586853\n",
      "step: 390, loss: 0.8357892632484436\n",
      "step: 391, loss: 0.3700653314590454\n",
      "step: 392, loss: 0.5190916657447815\n",
      "step: 393, loss: 0.461749792098999\n",
      "step: 394, loss: 0.3687552511692047\n",
      "step: 395, loss: 0.9403220415115356\n",
      "step: 396, loss: 0.6137529611587524\n",
      "step: 397, loss: 0.5012747049331665\n",
      "step: 398, loss: 0.5654170513153076\n",
      "step: 399, loss: 0.27866408228874207\n",
      "step: 400, loss: 0.6933067440986633\n",
      "step: 401, loss: 0.3905895948410034\n",
      "step: 402, loss: 0.5485243797302246\n",
      "step: 403, loss: 0.3746282160282135\n",
      "step: 404, loss: 0.5749775171279907\n",
      "step: 405, loss: 0.9146633744239807\n",
      "step: 406, loss: 0.6823229193687439\n",
      "step: 407, loss: 0.501340389251709\n",
      "step: 408, loss: 0.6166727542877197\n",
      "step: 409, loss: 0.5663242340087891\n",
      "step: 410, loss: 0.3879072666168213\n",
      "step: 411, loss: 0.3297816514968872\n",
      "step: 412, loss: 0.582902193069458\n",
      "step: 413, loss: 0.6307244300842285\n",
      "step: 414, loss: 0.28392350673675537\n",
      "step: 415, loss: 0.7361611723899841\n",
      "step: 416, loss: 0.7341876029968262\n",
      "step: 417, loss: 0.6916618347167969\n",
      "step: 418, loss: 0.34898653626441956\n",
      "step: 419, loss: 0.2366618514060974\n",
      "step: 420, loss: 0.7594109177589417\n",
      "step: 421, loss: 0.4113941192626953\n",
      "step: 422, loss: 0.5287506580352783\n",
      "step: 423, loss: 0.4282011091709137\n",
      "step: 424, loss: 0.6620246767997742\n",
      "step: 425, loss: 0.5557898879051208\n",
      "step: 426, loss: 0.5350356101989746\n",
      "step: 427, loss: 0.41995808482170105\n",
      "step: 428, loss: 0.6861245036125183\n",
      "step: 429, loss: 0.24971546232700348\n",
      "step: 430, loss: 0.5381785035133362\n",
      "step: 431, loss: 0.34086790680885315\n",
      "step: 432, loss: 0.6233281493186951\n",
      "step: 433, loss: 0.29448387026786804\n",
      "step: 434, loss: 0.5544373989105225\n",
      "step: 435, loss: 0.6723675727844238\n",
      "step: 436, loss: 0.13987989723682404\n",
      "step: 437, loss: 0.6150248050689697\n",
      "step: 438, loss: 1.0291765928268433\n",
      "step: 439, loss: 0.4278120696544647\n",
      "step: 440, loss: 0.299796462059021\n",
      "step: 441, loss: 0.5871509313583374\n",
      "step: 442, loss: 0.5209832191467285\n",
      "step: 443, loss: 0.4744304418563843\n",
      "step: 444, loss: 0.5651471018791199\n",
      "step: 445, loss: 0.4746443033218384\n",
      "step: 446, loss: 0.18607617914676666\n",
      "step: 447, loss: 0.5219147205352783\n",
      "step: 448, loss: 0.24827031791210175\n",
      "step: 449, loss: 0.4773402512073517\n",
      "step: 450, loss: 0.791765570640564\n",
      "step: 451, loss: 0.706271767616272\n",
      "step: 452, loss: 0.10283169150352478\n",
      "step: 453, loss: 0.17605793476104736\n",
      "step: 454, loss: 0.04008977860212326\n",
      "step: 455, loss: 0.17635759711265564\n",
      "step: 456, loss: 0.149473175406456\n",
      "step: 457, loss: 0.23850677907466888\n",
      "step: 458, loss: 0.2765817940235138\n",
      "step: 459, loss: 0.16440030932426453\n",
      "step: 460, loss: 0.13920460641384125\n",
      "step: 461, loss: 0.1473352015018463\n",
      "step: 462, loss: 0.1674773097038269\n",
      "step: 463, loss: 0.24202074110507965\n",
      "step: 464, loss: 0.18017299473285675\n",
      "step: 465, loss: 0.17764240503311157\n",
      "step: 466, loss: 0.15591402351856232\n",
      "step: 467, loss: 0.09749863296747208\n",
      "step: 468, loss: 0.32996535301208496\n",
      "step: 469, loss: 0.266404926776886\n",
      "step: 470, loss: 0.23337934911251068\n",
      "step: 471, loss: 0.28339532017707825\n",
      "step: 472, loss: 0.24628537893295288\n",
      "step: 473, loss: 0.1306474655866623\n",
      "step: 474, loss: 0.16159819066524506\n",
      "step: 475, loss: 0.09427445381879807\n",
      "step: 476, loss: 0.20263433456420898\n",
      "step: 477, loss: 0.1320425271987915\n",
      "step: 478, loss: 0.2885294258594513\n",
      "step: 479, loss: 0.2415349930524826\n",
      "step: 480, loss: 0.20892231166362762\n",
      "step: 481, loss: 0.2712002396583557\n",
      "step: 482, loss: 0.03119306080043316\n",
      "step: 483, loss: 0.11918357014656067\n",
      "step: 484, loss: 0.10845065861940384\n",
      "step: 485, loss: 0.19229325652122498\n",
      "step: 486, loss: 0.10500174760818481\n",
      "step: 487, loss: 0.44082173705101013\n",
      "step: 488, loss: 0.1432305872440338\n",
      "step: 489, loss: 0.32982271909713745\n",
      "step: 490, loss: 0.29549989104270935\n",
      "step: 491, loss: 0.15286070108413696\n",
      "step: 492, loss: 0.1273857057094574\n",
      "step: 493, loss: 0.0633729100227356\n",
      "step: 494, loss: 0.07507992535829544\n",
      "step: 495, loss: 0.1432422697544098\n",
      "step: 496, loss: 0.3476755917072296\n",
      "step: 497, loss: 0.619087278842926\n",
      "step: 498, loss: 0.2689308822154999\n",
      "step: 499, loss: 0.09031350165605545\n",
      "step: 500, loss: 0.16603849828243256\n",
      "step: 501, loss: 0.21924692392349243\n",
      "step: 502, loss: 0.3845328390598297\n",
      "step: 503, loss: 0.17794038355350494\n",
      "step: 504, loss: 0.3651546537876129\n",
      "step: 505, loss: 0.16350461542606354\n",
      "step: 506, loss: 0.04664981737732887\n",
      "step: 507, loss: 0.044373732060194016\n",
      "step: 508, loss: 0.10414785146713257\n",
      "step: 509, loss: 0.08446165919303894\n",
      "step: 510, loss: 0.2501467168331146\n",
      "step: 511, loss: 0.32837626338005066\n",
      "step: 512, loss: 0.0842762440443039\n",
      "step: 513, loss: 0.3393210172653198\n",
      "step: 514, loss: 0.1359541267156601\n",
      "step: 515, loss: 0.2229108363389969\n",
      "step: 516, loss: 0.3170102536678314\n",
      "step: 517, loss: 0.1971266120672226\n",
      "step: 518, loss: 0.226862832903862\n",
      "step: 519, loss: 0.23001785576343536\n",
      "step: 520, loss: 0.1892070770263672\n",
      "step: 521, loss: 0.1812477558851242\n",
      "step: 522, loss: 0.11233790218830109\n",
      "step: 523, loss: 0.10553914308547974\n",
      "step: 524, loss: 0.2504565119743347\n",
      "step: 525, loss: 0.2790924906730652\n",
      "step: 526, loss: 0.06818167120218277\n",
      "step: 527, loss: 0.11269208788871765\n",
      "step: 528, loss: 0.18809381127357483\n",
      "step: 529, loss: 0.26603007316589355\n",
      "step: 530, loss: 0.1798323094844818\n",
      "step: 531, loss: 0.29770466685295105\n",
      "step: 532, loss: 0.08695162087678909\n",
      "step: 533, loss: 0.14716121554374695\n",
      "step: 534, loss: 0.3648810088634491\n",
      "step: 535, loss: 0.2864856421947479\n",
      "step: 536, loss: 0.16060785949230194\n",
      "step: 537, loss: 0.08689527213573456\n",
      "step: 538, loss: 0.08213526010513306\n",
      "step: 539, loss: 0.13954545557498932\n",
      "step: 540, loss: 0.4352000951766968\n",
      "step: 541, loss: 0.05206206813454628\n",
      "step: 542, loss: 0.17072148621082306\n",
      "step: 543, loss: 0.12450855225324631\n",
      "step: 544, loss: 0.22346052527427673\n",
      "step: 545, loss: 0.41614243388175964\n",
      "step: 546, loss: 0.21433059871196747\n",
      "step: 547, loss: 0.36195969581604004\n",
      "step: 548, loss: 0.19578339159488678\n",
      "step: 549, loss: 0.27302446961402893\n",
      "step: 550, loss: 0.10697990655899048\n",
      "step: 551, loss: 0.29507675766944885\n",
      "step: 552, loss: 0.24995356798171997\n",
      "step: 553, loss: 0.3146907091140747\n",
      "step: 554, loss: 0.214248389005661\n",
      "step: 555, loss: 0.36368516087532043\n",
      "step: 556, loss: 0.0947667583823204\n",
      "step: 557, loss: 0.13519583642482758\n",
      "step: 558, loss: 0.13449932634830475\n",
      "step: 559, loss: 0.1742447316646576\n",
      "step: 560, loss: 0.25730961561203003\n",
      "step: 561, loss: 0.23130497336387634\n",
      "step: 562, loss: 0.06752311438322067\n",
      "step: 563, loss: 0.3139401078224182\n",
      "step: 564, loss: 0.20751436054706573\n",
      "step: 565, loss: 0.17438684403896332\n",
      "step: 566, loss: 0.3267074525356293\n",
      "step: 567, loss: 0.34057340025901794\n",
      "step: 568, loss: 0.19596408307552338\n",
      "step: 569, loss: 0.2210272252559662\n",
      "step: 570, loss: 0.14581915736198425\n",
      "step: 571, loss: 0.18789537250995636\n",
      "step: 572, loss: 0.03247315064072609\n",
      "step: 573, loss: 0.12449973076581955\n",
      "step: 574, loss: 0.21971094608306885\n",
      "step: 575, loss: 0.039317745715379715\n",
      "step: 576, loss: 0.12122828513383865\n",
      "step: 577, loss: 0.20904283225536346\n",
      "step: 578, loss: 0.1440928876399994\n",
      "step: 579, loss: 0.2355363667011261\n",
      "step: 580, loss: 0.18475668132305145\n",
      "step: 581, loss: 0.24317803978919983\n",
      "step: 582, loss: 0.23125223815441132\n",
      "step: 583, loss: 0.15204890072345734\n",
      "step: 584, loss: 0.22290143370628357\n",
      "step: 585, loss: 0.2083224356174469\n",
      "step: 586, loss: 0.10515458881855011\n",
      "step: 587, loss: 0.22309936583042145\n",
      "step: 588, loss: 0.22126762568950653\n",
      "step: 589, loss: 0.2524960935115814\n",
      "step: 590, loss: 0.16435249149799347\n",
      "step: 591, loss: 0.5410997867584229\n",
      "step: 592, loss: 0.09592502564191818\n",
      "step: 593, loss: 0.057488251477479935\n",
      "step: 594, loss: 0.14724940061569214\n",
      "step: 595, loss: 0.2221776247024536\n",
      "step: 596, loss: 0.09490390866994858\n",
      "step: 597, loss: 0.0926794707775116\n",
      "step: 598, loss: 0.278881311416626\n",
      "step: 599, loss: 0.18016719818115234\n",
      "step: 600, loss: 0.09717283397912979\n",
      "step: 601, loss: 0.10807780176401138\n",
      "step: 602, loss: 0.17751480638980865\n",
      "step: 603, loss: 0.1816210001707077\n",
      "step: 604, loss: 0.10217195749282837\n",
      "step: 605, loss: 0.06083551049232483\n",
      "step: 606, loss: 0.17743098735809326\n",
      "step: 607, loss: 0.19583982229232788\n",
      "step: 608, loss: 0.060120340436697006\n",
      "step: 609, loss: 0.1593012660741806\n",
      "step: 610, loss: 0.27244722843170166\n",
      "step: 611, loss: 0.12976974248886108\n",
      "step: 612, loss: 0.07554817199707031\n",
      "step: 613, loss: 0.24956649541854858\n",
      "step: 614, loss: 0.30905479192733765\n",
      "step: 615, loss: 0.22693213820457458\n",
      "step: 616, loss: 0.31829240918159485\n",
      "step: 617, loss: 0.5199491381645203\n",
      "step: 618, loss: 0.20393186807632446\n",
      "step: 619, loss: 0.12501002848148346\n",
      "step: 620, loss: 0.035600945353507996\n",
      "step: 621, loss: 0.1304071694612503\n",
      "step: 622, loss: 0.29577428102493286\n",
      "step: 623, loss: 0.06472443789243698\n",
      "step: 624, loss: 0.3831543028354645\n",
      "step: 625, loss: 0.2570835053920746\n",
      "step: 626, loss: 0.17364583909511566\n",
      "step: 627, loss: 0.17555423080921173\n",
      "step: 628, loss: 0.2993703782558441\n",
      "step: 629, loss: 0.1465689241886139\n",
      "step: 630, loss: 0.14153975248336792\n",
      "step: 631, loss: 0.25184401869773865\n",
      "step: 632, loss: 0.09150085598230362\n",
      "step: 633, loss: 0.127767875790596\n",
      "step: 634, loss: 0.06958480179309845\n",
      "step: 635, loss: 0.14496871829032898\n",
      "step: 636, loss: 0.32655638456344604\n",
      "step: 637, loss: 0.0896114706993103\n",
      "step: 638, loss: 0.14250721037387848\n",
      "step: 639, loss: 0.04803182929754257\n",
      "step: 640, loss: 0.42713919281959534\n",
      "step: 641, loss: 0.007658977061510086\n",
      "step: 642, loss: 0.22831295430660248\n",
      "step: 643, loss: 0.19755616784095764\n",
      "step: 644, loss: 0.04306427016854286\n",
      "step: 645, loss: 0.2637541592121124\n",
      "step: 646, loss: 0.0266774483025074\n",
      "step: 647, loss: 0.06281080096960068\n",
      "step: 648, loss: 0.039146311581134796\n",
      "step: 649, loss: 0.18628425896167755\n",
      "step: 650, loss: 0.23098917305469513\n",
      "step: 651, loss: 0.2773168683052063\n",
      "step: 652, loss: 0.09950917214155197\n",
      "step: 653, loss: 0.09656824171543121\n",
      "step: 654, loss: 0.07551377266645432\n",
      "step: 655, loss: 0.14352987706661224\n",
      "step: 656, loss: 0.23759618401527405\n",
      "step: 657, loss: 0.07588330656290054\n",
      "step: 658, loss: 0.27484169602394104\n",
      "step: 659, loss: 0.19183681905269623\n",
      "step: 660, loss: 0.2735103666782379\n",
      "step: 661, loss: 0.4185478687286377\n",
      "step: 662, loss: 0.29306161403656006\n",
      "step: 663, loss: 0.16518642008304596\n",
      "step: 664, loss: 0.12526322901248932\n",
      "step: 665, loss: 0.15405108034610748\n",
      "step: 666, loss: 0.13630175590515137\n",
      "step: 667, loss: 0.23861335217952728\n",
      "step: 668, loss: 0.22847460210323334\n",
      "step: 669, loss: 0.11380699276924133\n",
      "step: 670, loss: 0.16163310408592224\n",
      "step: 671, loss: 0.23809562623500824\n",
      "step: 672, loss: 0.039103858172893524\n",
      "step: 673, loss: 0.4632784128189087\n",
      "step: 674, loss: 0.15634319186210632\n",
      "step: 675, loss: 0.3871379792690277\n",
      "step: 676, loss: 0.14024682343006134\n",
      "step: 677, loss: 0.25948071479797363\n",
      "step: 678, loss: 0.03910045325756073\n",
      "step: 679, loss: 0.03087523765861988\n",
      "step: 680, loss: 0.06019698083400726\n",
      "step: 681, loss: 0.10556836426258087\n",
      "step: 682, loss: 0.018812282010912895\n",
      "step: 683, loss: 0.012146210297942162\n",
      "step: 684, loss: 0.07436657696962357\n",
      "step: 685, loss: 0.08882614225149155\n",
      "step: 686, loss: 0.018613498657941818\n",
      "step: 687, loss: 0.01257534883916378\n",
      "step: 688, loss: 0.009339183568954468\n",
      "step: 689, loss: 0.01280956994742155\n",
      "step: 690, loss: 0.00819216389209032\n",
      "step: 691, loss: 0.004443593788892031\n",
      "step: 692, loss: 0.011481818743050098\n",
      "step: 693, loss: 0.05926094949245453\n",
      "step: 694, loss: 0.010999910533428192\n",
      "step: 695, loss: 0.04894554615020752\n",
      "step: 696, loss: 0.01956760324537754\n",
      "step: 697, loss: 0.06754674762487411\n",
      "step: 698, loss: 0.0642843022942543\n",
      "step: 699, loss: 0.007126223295927048\n",
      "step: 700, loss: 0.007347981445491314\n",
      "step: 701, loss: 0.010758128948509693\n",
      "step: 702, loss: 0.02165229804813862\n",
      "step: 703, loss: 0.003953704610466957\n",
      "step: 704, loss: 0.015382674522697926\n",
      "step: 705, loss: 0.08316536992788315\n",
      "step: 706, loss: 0.015388059429824352\n",
      "step: 707, loss: 0.022640762850642204\n",
      "step: 708, loss: 0.04515908285975456\n",
      "step: 709, loss: 0.011442173272371292\n",
      "step: 710, loss: 0.040158238261938095\n",
      "step: 711, loss: 0.007348220329731703\n",
      "step: 712, loss: 0.046955786645412445\n",
      "step: 713, loss: 0.009995902888476849\n",
      "step: 714, loss: 0.02228960022330284\n",
      "step: 715, loss: 0.003779653226956725\n",
      "step: 716, loss: 0.020935801789164543\n",
      "step: 717, loss: 0.07116694748401642\n",
      "step: 718, loss: 0.12259363383054733\n",
      "step: 719, loss: 0.02247471548616886\n",
      "step: 720, loss: 0.009107726626098156\n",
      "step: 721, loss: 0.14655619859695435\n",
      "step: 722, loss: 0.038765452802181244\n",
      "step: 723, loss: 0.01836266741156578\n",
      "step: 724, loss: 0.014290745370090008\n",
      "step: 725, loss: 0.03375982865691185\n",
      "step: 726, loss: 0.013008816167712212\n",
      "step: 727, loss: 0.19528687000274658\n",
      "step: 728, loss: 0.015714997425675392\n",
      "step: 729, loss: 0.07991819083690643\n",
      "step: 730, loss: 0.0027147617656737566\n",
      "step: 731, loss: 0.0286400243639946\n",
      "step: 732, loss: 0.008368736132979393\n",
      "step: 733, loss: 0.005065554287284613\n",
      "step: 734, loss: 0.05101260542869568\n",
      "step: 735, loss: 0.008759994059801102\n",
      "step: 736, loss: 0.006611811928451061\n",
      "step: 737, loss: 0.09879855811595917\n",
      "step: 738, loss: 0.021427426487207413\n",
      "step: 739, loss: 0.006807784549891949\n",
      "step: 740, loss: 0.006647486239671707\n",
      "step: 741, loss: 0.019245034083724022\n",
      "step: 742, loss: 0.05063995346426964\n",
      "step: 743, loss: 0.02288723737001419\n",
      "step: 744, loss: 0.009938823990523815\n",
      "step: 745, loss: 0.061686281114816666\n",
      "step: 746, loss: 0.018380528315901756\n",
      "step: 747, loss: 0.09403879940509796\n",
      "step: 748, loss: 0.04259755462408066\n",
      "step: 749, loss: 0.005553322844207287\n",
      "step: 750, loss: 0.009266017936170101\n",
      "step: 751, loss: 0.006040662527084351\n",
      "step: 752, loss: 0.013424551114439964\n",
      "step: 753, loss: 0.43715712428092957\n",
      "step: 754, loss: 0.12045370042324066\n",
      "step: 755, loss: 0.014259993098676205\n",
      "step: 756, loss: 0.024152273312211037\n",
      "step: 757, loss: 0.1420833319425583\n",
      "step: 758, loss: 0.197983056306839\n",
      "step: 759, loss: 0.0051140692085027695\n",
      "step: 760, loss: 0.020611567422747612\n",
      "step: 761, loss: 0.03538424149155617\n",
      "step: 762, loss: 0.04552440345287323\n",
      "step: 763, loss: 0.07164933532476425\n",
      "step: 764, loss: 0.09652242064476013\n",
      "step: 765, loss: 0.040419138967990875\n",
      "step: 766, loss: 0.018764683976769447\n",
      "step: 767, loss: 0.004869329743087292\n",
      "step: 768, loss: 0.03401396423578262\n",
      "step: 769, loss: 0.1250024288892746\n",
      "step: 770, loss: 0.040744148194789886\n",
      "step: 771, loss: 0.09199017286300659\n",
      "step: 772, loss: 0.005788119044154882\n",
      "step: 773, loss: 0.06606344878673553\n",
      "step: 774, loss: 0.005677471868693829\n",
      "step: 775, loss: 0.010555987246334553\n",
      "step: 776, loss: 0.016237594187259674\n",
      "step: 777, loss: 0.11360478401184082\n",
      "step: 778, loss: 0.005381579976528883\n",
      "step: 779, loss: 0.010969890281558037\n",
      "step: 780, loss: 0.005228826310485601\n",
      "step: 781, loss: 0.016983728855848312\n",
      "step: 782, loss: 0.04968593642115593\n",
      "step: 783, loss: 0.13125860691070557\n",
      "step: 784, loss: 0.05179012194275856\n",
      "step: 785, loss: 0.0378510057926178\n",
      "step: 786, loss: 0.008191291242837906\n",
      "step: 787, loss: 0.0029755013529211283\n",
      "step: 788, loss: 0.05868770182132721\n",
      "step: 789, loss: 0.17238113284111023\n",
      "step: 790, loss: 0.006849060766398907\n",
      "step: 791, loss: 0.0045873019844293594\n",
      "step: 792, loss: 0.03744640201330185\n",
      "step: 793, loss: 0.029076216742396355\n",
      "step: 794, loss: 0.06320852041244507\n",
      "step: 795, loss: 0.03473375365138054\n",
      "step: 796, loss: 0.008298766799271107\n",
      "step: 797, loss: 0.037996694445610046\n",
      "step: 798, loss: 0.016340255737304688\n",
      "step: 799, loss: 0.11147794872522354\n",
      "step: 800, loss: 0.06528811901807785\n",
      "step: 801, loss: 0.03243676573038101\n",
      "step: 802, loss: 0.17263060808181763\n",
      "step: 803, loss: 0.04397967830300331\n",
      "step: 804, loss: 0.16168054938316345\n",
      "step: 805, loss: 0.02018950879573822\n",
      "step: 806, loss: 0.13675594329833984\n",
      "step: 807, loss: 0.016262628138065338\n",
      "step: 808, loss: 0.015193738043308258\n",
      "step: 809, loss: 0.009308595210313797\n",
      "step: 810, loss: 0.0468171127140522\n",
      "step: 811, loss: 0.031010687351226807\n",
      "step: 812, loss: 0.37270230054855347\n",
      "step: 813, loss: 0.047458820044994354\n",
      "step: 814, loss: 0.02191101759672165\n",
      "step: 815, loss: 0.06031166389584541\n",
      "step: 816, loss: 0.02127099223434925\n",
      "step: 817, loss: 0.02627948299050331\n",
      "step: 818, loss: 0.010676265694200993\n",
      "step: 819, loss: 0.04539695754647255\n",
      "step: 820, loss: 0.007730936631560326\n",
      "step: 821, loss: 0.03547811135649681\n",
      "step: 822, loss: 0.003918752074241638\n",
      "step: 823, loss: 0.013184205628931522\n",
      "step: 824, loss: 0.009049490094184875\n",
      "step: 825, loss: 0.007176786195486784\n",
      "step: 826, loss: 0.008827495388686657\n",
      "step: 827, loss: 0.03772934526205063\n",
      "step: 828, loss: 0.012008419260382652\n",
      "step: 829, loss: 0.015218960121273994\n",
      "step: 830, loss: 0.00621045520529151\n",
      "step: 831, loss: 0.030698146671056747\n",
      "step: 832, loss: 0.004581280518323183\n",
      "step: 833, loss: 0.004768412560224533\n",
      "step: 834, loss: 0.016575738787651062\n",
      "step: 835, loss: 0.018528539687395096\n",
      "step: 836, loss: 0.019630366936326027\n",
      "step: 837, loss: 0.008560793474316597\n",
      "step: 838, loss: 0.0022365516051650047\n",
      "step: 839, loss: 0.004573123995214701\n",
      "step: 840, loss: 0.020028967410326004\n",
      "step: 841, loss: 0.003294718451797962\n",
      "step: 842, loss: 0.024537254124879837\n",
      "step: 843, loss: 0.14263373613357544\n",
      "step: 844, loss: 0.07189632207155228\n",
      "step: 845, loss: 0.024215878918766975\n",
      "step: 846, loss: 0.08729366958141327\n",
      "step: 847, loss: 0.049750715494155884\n",
      "step: 848, loss: 0.15470609068870544\n",
      "step: 849, loss: 0.01698690466582775\n",
      "step: 850, loss: 0.014778755605220795\n",
      "step: 851, loss: 0.018141983076930046\n",
      "step: 852, loss: 0.017490774393081665\n",
      "step: 853, loss: 0.02780669927597046\n",
      "step: 854, loss: 0.0350811704993248\n",
      "step: 855, loss: 0.024201245978474617\n",
      "step: 856, loss: 0.0707688182592392\n",
      "step: 857, loss: 0.12275713682174683\n",
      "step: 858, loss: 0.0025623259134590626\n",
      "step: 859, loss: 0.016911840066313744\n",
      "step: 860, loss: 0.020115051418542862\n",
      "step: 861, loss: 0.027280230075120926\n",
      "step: 862, loss: 0.1494918316602707\n",
      "step: 863, loss: 0.013781284913420677\n",
      "step: 864, loss: 0.008233782835304737\n",
      "step: 865, loss: 0.055696144700050354\n",
      "step: 866, loss: 0.0042744893580675125\n",
      "step: 867, loss: 0.004511065781116486\n",
      "step: 868, loss: 0.03926309943199158\n",
      "step: 869, loss: 0.018529348075389862\n",
      "step: 870, loss: 0.05159341171383858\n",
      "step: 871, loss: 0.08804450929164886\n",
      "step: 872, loss: 0.00835459679365158\n",
      "step: 873, loss: 0.048543721437454224\n",
      "step: 874, loss: 0.021851154044270515\n",
      "step: 875, loss: 0.03783007711172104\n",
      "step: 876, loss: 0.04202788695693016\n",
      "step: 877, loss: 0.05978763476014137\n",
      "step: 878, loss: 0.0033611194230616093\n",
      "step: 879, loss: 0.005746740847826004\n",
      "step: 880, loss: 0.0710047036409378\n",
      "step: 881, loss: 0.0024777420330792665\n",
      "step: 882, loss: 0.02396925911307335\n",
      "step: 883, loss: 0.007199219428002834\n",
      "step: 884, loss: 0.0022882295306771994\n",
      "step: 885, loss: 0.012834415771067142\n",
      "step: 886, loss: 0.03908051922917366\n",
      "step: 887, loss: 0.04818165302276611\n",
      "step: 888, loss: 0.06064120680093765\n",
      "step: 889, loss: 0.007499114144593477\n",
      "step: 890, loss: 0.03805173933506012\n",
      "step: 891, loss: 0.008576138876378536\n",
      "step: 892, loss: 0.06327623873949051\n",
      "step: 893, loss: 0.07971323281526566\n",
      "step: 894, loss: 0.020741600543260574\n",
      "step: 895, loss: 0.023167872801423073\n",
      "step: 896, loss: 0.027426622807979584\n",
      "step: 897, loss: 0.07998718321323395\n",
      "step: 898, loss: 0.05336623638868332\n",
      "step: 899, loss: 0.022735916078090668\n",
      "step: 900, loss: 0.08324331045150757\n",
      "step: 901, loss: 0.004693031311035156\n",
      "step: 902, loss: 0.04646066203713417\n",
      "step: 903, loss: 0.07602798193693161\n",
      "step: 904, loss: 0.026599599048495293\n",
      "step: 905, loss: 0.0031546426471322775\n",
      "step: 906, loss: 0.0020806463435292244\n",
      "step: 907, loss: 0.06938158720731735\n",
      "step: 908, loss: 0.023211505264043808\n",
      "step: 909, loss: 0.0020918070804327726\n",
      "step: 910, loss: 0.002484050812199712\n",
      "step: 911, loss: 0.006141275633126497\n",
      "step: 912, loss: 0.048186443746089935\n",
      "step: 913, loss: 0.05118798837065697\n",
      "step: 914, loss: 0.004072730429470539\n",
      "step: 915, loss: 0.004321934189647436\n",
      "step: 916, loss: 0.003826693631708622\n",
      "step: 917, loss: 0.0016003453638404608\n",
      "step: 918, loss: 0.004573649261146784\n",
      "step: 919, loss: 0.0022317618131637573\n",
      "step: 920, loss: 0.002706158207729459\n",
      "step: 921, loss: 0.005047389771789312\n",
      "step: 922, loss: 0.002476034453138709\n",
      "step: 923, loss: 0.007194391451776028\n",
      "step: 924, loss: 0.005214439705014229\n",
      "step: 925, loss: 0.01007480826228857\n",
      "step: 926, loss: 0.019214065745472908\n",
      "step: 927, loss: 0.002930948743596673\n",
      "step: 928, loss: 0.006952719297260046\n",
      "step: 929, loss: 0.023540593683719635\n",
      "step: 930, loss: 0.00580851873382926\n",
      "step: 931, loss: 0.0013469388941302896\n",
      "step: 932, loss: 0.009552289731800556\n",
      "step: 933, loss: 0.020343532785773277\n",
      "step: 934, loss: 0.0017775753512978554\n",
      "step: 935, loss: 0.0021156473085284233\n",
      "step: 936, loss: 0.002970729488879442\n",
      "step: 937, loss: 0.0009154098224826157\n",
      "step: 938, loss: 0.0022308737970888615\n",
      "step: 939, loss: 0.003014372196048498\n",
      "step: 940, loss: 0.0022269808687269688\n",
      "step: 941, loss: 0.0025912367273122072\n",
      "step: 942, loss: 0.0065923635847866535\n",
      "step: 943, loss: 0.0031615642365068197\n",
      "step: 944, loss: 0.0033194117713719606\n",
      "step: 945, loss: 0.004184462130069733\n",
      "step: 946, loss: 0.022229090332984924\n",
      "step: 947, loss: 0.0023637644480913877\n",
      "step: 948, loss: 0.05596962571144104\n",
      "step: 949, loss: 0.0013244444271549582\n",
      "step: 950, loss: 0.003728716168552637\n",
      "step: 951, loss: 0.008991808630526066\n",
      "step: 952, loss: 0.003027186496183276\n",
      "step: 953, loss: 0.10980666428804398\n",
      "step: 954, loss: 0.0028843358159065247\n",
      "step: 955, loss: 0.002645337488502264\n",
      "step: 956, loss: 0.051430776715278625\n",
      "step: 957, loss: 0.0033499186392873526\n",
      "step: 958, loss: 0.002845791867002845\n",
      "step: 959, loss: 0.001351919723674655\n",
      "step: 960, loss: 0.0024735545739531517\n",
      "step: 961, loss: 0.002297637052834034\n",
      "step: 962, loss: 0.010855468921363354\n",
      "step: 963, loss: 0.003339581424370408\n",
      "step: 964, loss: 0.0012390855699777603\n",
      "step: 965, loss: 0.005521052051335573\n",
      "step: 966, loss: 0.07477805763483047\n",
      "step: 967, loss: 0.0374874472618103\n",
      "step: 968, loss: 0.015581229701638222\n",
      "step: 969, loss: 0.0035915488842874765\n",
      "step: 970, loss: 0.0014577992260456085\n",
      "step: 971, loss: 0.02355707809329033\n",
      "step: 972, loss: 0.013910515233874321\n",
      "step: 973, loss: 0.02375447191298008\n",
      "step: 974, loss: 0.0035559791140258312\n",
      "step: 975, loss: 0.0025806650519371033\n",
      "step: 976, loss: 0.0006377378012984991\n",
      "step: 977, loss: 0.014660793356597424\n",
      "step: 978, loss: 0.15836139023303986\n",
      "step: 979, loss: 0.0015575737925246358\n",
      "step: 980, loss: 0.13005536794662476\n",
      "step: 981, loss: 0.012443332932889462\n",
      "step: 982, loss: 0.007332229986786842\n",
      "step: 983, loss: 0.003991472534835339\n",
      "step: 984, loss: 0.0020611106883734465\n",
      "step: 985, loss: 0.003223547013476491\n",
      "step: 986, loss: 0.12123489379882812\n",
      "step: 987, loss: 0.040389515459537506\n",
      "step: 988, loss: 0.000999433221295476\n",
      "step: 989, loss: 0.0010485207894816995\n",
      "step: 990, loss: 0.0034648829605430365\n",
      "step: 991, loss: 0.07058248668909073\n",
      "step: 992, loss: 0.0037598214112222195\n",
      "step: 993, loss: 0.0013349467189982533\n",
      "step: 994, loss: 0.0020113668870180845\n",
      "step: 995, loss: 0.002647852525115013\n",
      "step: 996, loss: 0.058873239904642105\n",
      "step: 997, loss: 0.003921727184206247\n",
      "step: 998, loss: 0.0036475686356425285\n",
      "step: 999, loss: 0.00807791855186224\n",
      "step: 1000, loss: 0.01134083978831768\n",
      "step: 1001, loss: 0.004220155533403158\n",
      "step: 1002, loss: 0.0007383325137197971\n",
      "step: 1003, loss: 0.0043477932922542095\n",
      "step: 1004, loss: 0.00185986771248281\n",
      "step: 1005, loss: 0.003082750365138054\n",
      "step: 1006, loss: 0.006502332165837288\n",
      "step: 1007, loss: 0.03633753955364227\n",
      "step: 1008, loss: 0.008365548215806484\n",
      "step: 1009, loss: 0.012606250122189522\n",
      "step: 1010, loss: 0.005466895177960396\n",
      "step: 1011, loss: 0.004610965494066477\n",
      "step: 1012, loss: 0.08278897404670715\n",
      "step: 1013, loss: 0.0056852190755307674\n",
      "step: 1014, loss: 0.0048408168368041515\n",
      "step: 1015, loss: 0.03782518580555916\n",
      "step: 1016, loss: 0.0017234721453860402\n",
      "step: 1017, loss: 0.0019140647491440177\n",
      "step: 1018, loss: 0.027456514537334442\n",
      "step: 1019, loss: 0.00227383803576231\n",
      "step: 1020, loss: 0.002090643858537078\n",
      "step: 1021, loss: 0.0023196779657155275\n",
      "step: 1022, loss: 0.08240210264921188\n",
      "step: 1023, loss: 0.0008267047232948244\n",
      "step: 1024, loss: 0.004467567894607782\n",
      "step: 1025, loss: 0.0007809719536453485\n",
      "step: 1026, loss: 0.011254037730395794\n",
      "step: 1027, loss: 0.015032036229968071\n",
      "step: 1028, loss: 0.006396337877959013\n",
      "step: 1029, loss: 0.003475592704489827\n",
      "step: 1030, loss: 0.0009953002445399761\n",
      "step: 1031, loss: 0.019351568073034286\n",
      "step: 1032, loss: 0.008626758120954037\n",
      "step: 1033, loss: 0.0054424419067800045\n",
      "step: 1034, loss: 0.0012912324164062738\n",
      "step: 1035, loss: 0.007835223339498043\n",
      "step: 1036, loss: 0.04890953376889229\n",
      "step: 1037, loss: 0.009672621265053749\n",
      "step: 1038, loss: 0.019189726561307907\n",
      "step: 1039, loss: 0.002097619231790304\n",
      "step: 1040, loss: 0.0028517479076981544\n",
      "step: 1041, loss: 0.003139863722026348\n",
      "step: 1042, loss: 0.0015327759319916368\n",
      "step: 1043, loss: 0.007657803129404783\n",
      "step: 1044, loss: 0.0011847870191559196\n",
      "step: 1045, loss: 0.0010441290214657784\n",
      "step: 1046, loss: 0.010906105861067772\n",
      "step: 1047, loss: 0.001061114831827581\n",
      "step: 1048, loss: 0.009182528592646122\n",
      "step: 1049, loss: 0.003308157669380307\n",
      "step: 1050, loss: 0.0040229326114058495\n",
      "step: 1051, loss: 0.0047063278034329414\n",
      "step: 1052, loss: 0.005225658882409334\n",
      "step: 1053, loss: 0.11472197622060776\n",
      "step: 1054, loss: 0.0015612351708114147\n",
      "step: 1055, loss: 0.0012766036670655012\n",
      "step: 1056, loss: 0.008031264878809452\n",
      "step: 1057, loss: 0.005063819233328104\n",
      "step: 1058, loss: 0.0018784516723826528\n",
      "step: 1059, loss: 0.008490929380059242\n",
      "step: 1060, loss: 0.03426577150821686\n",
      "step: 1061, loss: 0.0021687925327569246\n",
      "step: 1062, loss: 0.0066477213986217976\n",
      "step: 1063, loss: 0.005078373476862907\n",
      "step: 1064, loss: 0.0034589089918881655\n",
      "step: 1065, loss: 0.004941869061440229\n",
      "step: 1066, loss: 0.002089251298457384\n",
      "step: 1067, loss: 0.0030720550566911697\n",
      "step: 1068, loss: 0.2105172723531723\n",
      "step: 1069, loss: 0.09876466542482376\n",
      "step: 1070, loss: 0.005787144880741835\n",
      "step: 1071, loss: 0.007822140119969845\n",
      "step: 1072, loss: 0.0017494871281087399\n",
      "step: 1073, loss: 0.001424827380105853\n",
      "step: 1074, loss: 0.004949131980538368\n",
      "step: 1075, loss: 0.0004712250956799835\n",
      "step: 1076, loss: 0.003417000873014331\n",
      "step: 1077, loss: 0.0006993918213993311\n",
      "step: 1078, loss: 0.000991431181319058\n",
      "step: 1079, loss: 0.00547855393961072\n",
      "step: 1080, loss: 0.0014482426922768354\n",
      "step: 1081, loss: 0.0019400386372581124\n",
      "step: 1082, loss: 0.0036472815554589033\n",
      "step: 1083, loss: 0.01022662129253149\n",
      "step: 1084, loss: 0.17617812752723694\n",
      "step: 1085, loss: 0.0013559128856286407\n",
      "step: 1086, loss: 0.0006145417573861778\n",
      "step: 1087, loss: 0.0015991488471627235\n",
      "step: 1088, loss: 0.008349294774234295\n",
      "step: 1089, loss: 0.00115649972576648\n",
      "step: 1090, loss: 0.0024223122745752335\n",
      "step: 1091, loss: 0.008697031065821648\n",
      "step: 1092, loss: 0.0008709242683835328\n",
      "step: 1093, loss: 0.002881300402805209\n",
      "step: 1094, loss: 0.0018570121610537171\n",
      "step: 1095, loss: 0.0011621169978752732\n",
      "step: 1096, loss: 0.0016388185322284698\n",
      "step: 1097, loss: 0.0016491933492943645\n",
      "step: 1098, loss: 0.003863105084747076\n",
      "step: 1099, loss: 0.00540314381942153\n",
      "step: 1100, loss: 0.002842215821146965\n",
      "step: 1101, loss: 0.017379777505993843\n",
      "step: 1102, loss: 0.003224888350814581\n",
      "step: 1103, loss: 0.0016658027889207006\n",
      "step: 1104, loss: 0.0007608889718540013\n",
      "step: 1105, loss: 0.019244389608502388\n",
      "step: 1106, loss: 0.042608361691236496\n",
      "step: 1107, loss: 0.05150846391916275\n",
      "step: 1108, loss: 0.0018097475403919816\n",
      "step: 1109, loss: 0.015568230301141739\n",
      "step: 1110, loss: 0.008236217312514782\n",
      "step: 1111, loss: 0.004086397122591734\n",
      "step: 1112, loss: 0.02730235643684864\n",
      "step: 1113, loss: 0.03970189392566681\n",
      "step: 1114, loss: 0.0011973793152719736\n",
      "step: 1115, loss: 0.021525172516703606\n",
      "step: 1116, loss: 0.001616218825802207\n",
      "step: 1117, loss: 0.004601239692419767\n",
      "step: 1118, loss: 0.0033351450692862272\n",
      "step: 1119, loss: 0.04269709438085556\n",
      "step: 1120, loss: 0.0013100227806717157\n",
      "step: 1121, loss: 0.002304765162989497\n",
      "step: 1122, loss: 0.00341346045024693\n",
      "step: 1123, loss: 0.0057123214937746525\n",
      "step: 1124, loss: 0.05025754123926163\n",
      "step: 1125, loss: 0.001157892867922783\n",
      "step: 1126, loss: 0.028634831309318542\n",
      "step: 1127, loss: 0.0012588856043294072\n",
      "step: 1128, loss: 0.0005047996528446674\n",
      "step: 1129, loss: 0.04934599995613098\n",
      "step: 1130, loss: 0.0005027097649872303\n",
      "step: 1131, loss: 0.0019393163966014981\n",
      "step: 1132, loss: 0.00128653843421489\n",
      "step: 1133, loss: 0.003263860708102584\n",
      "step: 1134, loss: 0.0036546236369758844\n",
      "step: 1135, loss: 0.0017626441549509764\n",
      "step: 1136, loss: 0.013048503547906876\n",
      "step: 1137, loss: 0.03806178644299507\n",
      "step: 1138, loss: 0.03482925891876221\n",
      "step: 1139, loss: 0.0013034824514761567\n",
      "step: 1140, loss: 0.0005815189797431231\n",
      "step: 1141, loss: 0.002296649618074298\n",
      "step: 1142, loss: 0.0011484842980280519\n",
      "step: 1143, loss: 0.001243777689523995\n",
      "step: 1144, loss: 0.05374811589717865\n",
      "step: 1145, loss: 0.0031488484237343073\n",
      "step: 1146, loss: 0.001998880412429571\n",
      "step: 1147, loss: 0.0012297281064093113\n",
      "step: 1148, loss: 0.0011919934768229723\n",
      "step: 1149, loss: 0.0011655052658170462\n",
      "step: 1150, loss: 0.03753532096743584\n",
      "step: 1151, loss: 0.0019637446384876966\n",
      "step: 1152, loss: 0.000869960174895823\n",
      "step: 1153, loss: 0.05554190278053284\n",
      "step: 1154, loss: 0.021752581000328064\n",
      "step: 1155, loss: 0.05090852081775665\n",
      "step: 1156, loss: 0.0015699322102591395\n",
      "step: 1157, loss: 0.007641317788511515\n",
      "step: 1158, loss: 0.002646156819537282\n",
      "step: 1159, loss: 0.0005539940902963281\n",
      "step: 1160, loss: 0.0016351011581718922\n",
      "step: 1161, loss: 0.0009785398142412305\n",
      "step: 1162, loss: 0.002816507127135992\n",
      "step: 1163, loss: 0.01271171122789383\n",
      "step: 1164, loss: 0.04726244509220123\n",
      "step: 1165, loss: 0.00052849599160254\n",
      "step: 1166, loss: 0.0007540235528722405\n",
      "step: 1167, loss: 0.0004995788913220167\n",
      "step: 1168, loss: 0.0006054038531146944\n",
      "step: 1169, loss: 0.0014627769123762846\n",
      "step: 1170, loss: 0.10311790555715561\n",
      "step: 1171, loss: 0.0005502201966010034\n",
      "step: 1172, loss: 0.001070812693797052\n",
      "step: 1173, loss: 0.0008209728403016925\n",
      "step: 1174, loss: 0.0007411236292682588\n",
      "step: 1175, loss: 0.012193161062896252\n",
      "step: 1176, loss: 0.001505946391262114\n",
      "step: 1177, loss: 0.0012119548628106713\n",
      "step: 1178, loss: 0.0005042849807068706\n",
      "step: 1179, loss: 0.0038401249330490828\n",
      "step: 1180, loss: 0.0019031224073842168\n",
      "step: 1181, loss: 0.0017616072436794639\n",
      "step: 1182, loss: 0.0005100363632664084\n",
      "step: 1183, loss: 0.0005224189371801913\n",
      "step: 1184, loss: 0.007012779824435711\n",
      "step: 1185, loss: 0.0025076724123209715\n",
      "step: 1186, loss: 0.0018758188234642148\n",
      "step: 1187, loss: 0.0006028115749359131\n",
      "step: 1188, loss: 0.0010038255713880062\n",
      "step: 1189, loss: 0.000885318499058485\n",
      "step: 1190, loss: 0.00045754204620607197\n",
      "step: 1191, loss: 0.032186754047870636\n",
      "step: 1192, loss: 0.0019892153795808554\n",
      "step: 1193, loss: 0.0005955258966423571\n",
      "step: 1194, loss: 0.0010409215465188026\n",
      "step: 1195, loss: 0.0005621335003525019\n",
      "step: 1196, loss: 0.002075503347441554\n",
      "step: 1197, loss: 0.0011093348730355501\n",
      "step: 1198, loss: 0.008491751737892628\n",
      "step: 1199, loss: 0.0025283026043325663\n",
      "step: 1200, loss: 0.003231100970879197\n",
      "step: 1201, loss: 0.0011530560441315174\n",
      "step: 1202, loss: 0.004660394042730331\n",
      "step: 1203, loss: 0.0019232368795201182\n",
      "step: 1204, loss: 0.0024280366487801075\n",
      "step: 1205, loss: 0.0009955831337720156\n",
      "step: 1206, loss: 0.0025037683080881834\n",
      "step: 1207, loss: 0.0004772222018800676\n",
      "step: 1208, loss: 0.0006195732858031988\n",
      "step: 1209, loss: 0.03574667498469353\n",
      "step: 1210, loss: 0.00447496585547924\n",
      "step: 1211, loss: 0.0019490084378048778\n",
      "step: 1212, loss: 0.0014675111742690206\n",
      "step: 1213, loss: 0.0016004948411136866\n",
      "step: 1214, loss: 0.0021297025959938765\n",
      "step: 1215, loss: 0.11877444386482239\n",
      "step: 1216, loss: 0.0006377453100867569\n",
      "step: 1217, loss: 0.0008072100463323295\n",
      "step: 1218, loss: 0.0008347497205249965\n",
      "step: 1219, loss: 0.0005164998583495617\n",
      "step: 1220, loss: 0.0007746044429950416\n",
      "step: 1221, loss: 0.0023632764350622892\n",
      "step: 1222, loss: 0.0013275607489049435\n",
      "step: 1223, loss: 0.0005727505194954574\n",
      "step: 1224, loss: 0.0014677796280011535\n",
      "step: 1225, loss: 0.0481751449406147\n",
      "step: 1226, loss: 0.0007185055292211473\n",
      "step: 1227, loss: 0.0007883667713031173\n",
      "step: 1228, loss: 0.0017772398423403502\n",
      "step: 1229, loss: 0.0019717239774763584\n",
      "step: 1230, loss: 0.0008697964367456734\n",
      "step: 1231, loss: 0.03035176917910576\n",
      "step: 1232, loss: 0.06451284140348434\n",
      "step: 1233, loss: 0.0007924200617708266\n",
      "step: 1234, loss: 0.0016363002359867096\n",
      "step: 1235, loss: 0.001091723213903606\n",
      "step: 1236, loss: 0.0012540105963125825\n",
      "step: 1237, loss: 0.03149022161960602\n",
      "step: 1238, loss: 0.013236462138593197\n",
      "step: 1239, loss: 0.0013456029118970037\n",
      "step: 1240, loss: 0.0025614129845052958\n",
      "step: 1241, loss: 0.0024484978057444096\n",
      "step: 1242, loss: 0.002130068140104413\n",
      "step: 1243, loss: 0.004037924110889435\n",
      "step: 1244, loss: 0.0015914242248982191\n",
      "step: 1245, loss: 0.005005058832466602\n",
      "step: 1246, loss: 0.0011110862251371145\n",
      "step: 1247, loss: 0.0010631809709593654\n",
      "step: 1248, loss: 0.0011656142305582762\n",
      "step: 1249, loss: 0.0011861110106110573\n",
      "step: 1250, loss: 0.0010769774671643972\n",
      "step: 1251, loss: 0.0005803043022751808\n",
      "step: 1252, loss: 0.0010550625156611204\n",
      "step: 1253, loss: 0.10346385836601257\n",
      "step: 1254, loss: 0.0010877574095502496\n",
      "step: 1255, loss: 0.0002613522228784859\n",
      "step: 1256, loss: 0.01018060464411974\n",
      "step: 1257, loss: 0.02685873955488205\n",
      "step: 1258, loss: 0.0009382575517520308\n",
      "step: 1259, loss: 0.000977985910139978\n",
      "step: 1260, loss: 0.001219900674186647\n",
      "step: 1261, loss: 0.0013753464445471764\n",
      "step: 1262, loss: 0.001074648811481893\n",
      "step: 1263, loss: 0.0014936598017811775\n",
      "step: 1264, loss: 0.0007652401691302657\n",
      "step: 1265, loss: 0.00686328299343586\n",
      "step: 1266, loss: 0.001067918143235147\n",
      "step: 1267, loss: 0.00035593542270362377\n",
      "step: 1268, loss: 0.0005446416325867176\n",
      "step: 1269, loss: 0.001142877503298223\n",
      "step: 1270, loss: 0.001441937405616045\n",
      "step: 1271, loss: 0.0006805402226746082\n",
      "step: 1272, loss: 0.02324608527123928\n",
      "step: 1273, loss: 0.0007766105700284243\n",
      "step: 1274, loss: 0.0016853052657097578\n",
      "step: 1275, loss: 0.000868852948769927\n",
      "step: 1276, loss: 0.003954677376896143\n",
      "step: 1277, loss: 0.0022273657377809286\n",
      "step: 1278, loss: 0.004697337746620178\n",
      "step: 1279, loss: 0.08546505868434906\n",
      "step: 1280, loss: 0.00184494664426893\n",
      "step: 1281, loss: 0.05653780326247215\n",
      "step: 1282, loss: 0.0008281632326543331\n",
      "step: 1283, loss: 0.0007780093001201749\n",
      "step: 1284, loss: 0.000583068816922605\n",
      "step: 1285, loss: 0.053611911833286285\n",
      "step: 1286, loss: 0.04729507118463516\n",
      "step: 1287, loss: 0.0009768872987478971\n",
      "step: 1288, loss: 0.0010664797155186534\n",
      "step: 1289, loss: 0.0016104067908599973\n",
      "step: 1290, loss: 0.0009138144087046385\n",
      "step: 1291, loss: 0.004178079776465893\n",
      "step: 1292, loss: 0.0016329388599842787\n",
      "step: 1293, loss: 0.000597582315094769\n",
      "step: 1294, loss: 0.0013574743643403053\n",
      "step: 1295, loss: 0.0005443253903649747\n",
      "step: 1296, loss: 0.0012486833147704601\n",
      "step: 1297, loss: 0.0025897398591041565\n",
      "step: 1298, loss: 0.0009718207875266671\n",
      "step: 1299, loss: 0.0005650546518154442\n",
      "step: 1300, loss: 0.005659507121890783\n",
      "step: 1301, loss: 0.0012703133979812264\n",
      "step: 1302, loss: 0.00037446164060384035\n",
      "step: 1303, loss: 0.01445079781115055\n",
      "step: 1304, loss: 0.0008741183555684984\n",
      "step: 1305, loss: 0.0024762784596532583\n",
      "step: 1306, loss: 0.0008933363715186715\n",
      "step: 1307, loss: 0.003176220692694187\n",
      "step: 1308, loss: 0.0008745425730012357\n",
      "step: 1309, loss: 0.0005719680921174586\n",
      "step: 1310, loss: 0.004273526836186647\n",
      "step: 1311, loss: 0.0037625229451805353\n",
      "step: 1312, loss: 0.0014054898638278246\n",
      "step: 1313, loss: 0.0030436022207140923\n",
      "step: 1314, loss: 0.0018771731993183494\n",
      "step: 1315, loss: 0.0020804787054657936\n",
      "step: 1316, loss: 0.0008711490081623197\n",
      "step: 1317, loss: 0.0005524466396309435\n",
      "step: 1318, loss: 0.004673055838793516\n",
      "step: 1319, loss: 0.0009766973089426756\n",
      "step: 1320, loss: 0.0005458714440464973\n",
      "step: 1321, loss: 0.0013876646989956498\n",
      "step: 1322, loss: 0.0016057273605838418\n",
      "step: 1323, loss: 0.14884443581104279\n",
      "step: 1324, loss: 0.0022246281150728464\n",
      "step: 1325, loss: 0.04012579843401909\n",
      "step: 1326, loss: 0.0020690602250397205\n",
      "step: 1327, loss: 0.002997794421389699\n",
      "step: 1328, loss: 0.0016115466132760048\n",
      "step: 1329, loss: 0.0020758777391165495\n",
      "step: 1330, loss: 0.0029897636268287897\n",
      "step: 1331, loss: 0.0009681836236268282\n",
      "step: 1332, loss: 0.0011980459094047546\n",
      "step: 1333, loss: 0.00038905226392671466\n",
      "step: 1334, loss: 0.06576859951019287\n",
      "step: 1335, loss: 0.0027783799450844526\n",
      "step: 1336, loss: 0.05678385868668556\n",
      "step: 1337, loss: 0.027700597420334816\n",
      "step: 1338, loss: 0.0012959135929122567\n",
      "step: 1339, loss: 0.0003463234461378306\n",
      "step: 1340, loss: 0.0015451567014679313\n",
      "step: 1341, loss: 0.0015115663409233093\n",
      "step: 1342, loss: 0.0008710360270924866\n",
      "step: 1343, loss: 0.0023654347751289606\n",
      "step: 1344, loss: 0.023517217487096786\n",
      "step: 1345, loss: 0.022615455090999603\n",
      "step: 1346, loss: 0.00244486378505826\n",
      "step: 1347, loss: 0.0011799265630543232\n",
      "step: 1348, loss: 0.015048818662762642\n",
      "step: 1349, loss: 0.0037311178166419268\n",
      "step: 1350, loss: 0.0023347469978034496\n",
      "step: 1351, loss: 0.0015760287642478943\n",
      "step: 1352, loss: 0.0017944967839866877\n",
      "step: 1353, loss: 0.0012142738560214639\n",
      "step: 1354, loss: 0.0008478817180730402\n",
      "step: 1355, loss: 0.031009793281555176\n",
      "step: 1356, loss: 0.0005965302116237581\n",
      "step: 1357, loss: 0.0018752035684883595\n",
      "step: 1358, loss: 0.0003837771655526012\n",
      "step: 1359, loss: 0.000478599889902398\n",
      "step: 1360, loss: 0.0009023097809404135\n",
      "step: 1361, loss: 0.0012971614487469196\n",
      "step: 1362, loss: 0.0009210974094457924\n",
      "step: 1363, loss: 0.0009071063832379878\n",
      "step: 1364, loss: 0.0016142353415489197\n",
      "step: 1365, loss: 0.37819400429725647\n",
      "step: 1366, loss: 0.0005125123425386846\n",
      "step: 1367, loss: 0.000744402117561549\n",
      "step: 1368, loss: 0.016166552901268005\n",
      "step: 1369, loss: 0.0005049595492891967\n",
      "step: 1370, loss: 0.00025343013112433255\n",
      "step: 1371, loss: 0.0004082621890120208\n",
      "step: 1372, loss: 0.026792114600539207\n",
      "step: 1373, loss: 0.0007791672833263874\n",
      "step: 1374, loss: 0.23645633459091187\n",
      "step: 1375, loss: 0.0006375155644491315\n",
      "step: 1376, loss: 0.0017448541475459933\n",
      "step: 1377, loss: 0.0009720213711261749\n",
      "step: 1378, loss: 0.0006846748292446136\n",
      "step: 1379, loss: 0.0023260442540049553\n",
      "step: 1380, loss: 0.0008006042335182428\n",
      "step: 1381, loss: 0.0008907419396564364\n",
      "step: 1382, loss: 0.0007790957461111248\n",
      "step: 1383, loss: 0.0007453791331499815\n",
      "step: 1384, loss: 0.0008343987865373492\n",
      "step: 1385, loss: 0.0007120717200450599\n",
      "step: 1386, loss: 0.0033455761149525642\n",
      "step: 1387, loss: 0.038523346185684204\n",
      "step: 1388, loss: 0.08300437778234482\n",
      "step: 1389, loss: 0.009031794033944607\n",
      "step: 1390, loss: 0.0002583487657830119\n",
      "step: 1391, loss: 0.0035588296595960855\n",
      "step: 1392, loss: 0.0007781633175909519\n",
      "step: 1393, loss: 0.001048400765284896\n",
      "step: 1394, loss: 0.0006741031538695097\n",
      "step: 1395, loss: 0.14217619597911835\n",
      "step: 1396, loss: 0.0028598534408956766\n",
      "step: 1397, loss: 0.005223305430263281\n",
      "step: 1398, loss: 0.001003491342999041\n",
      "step: 1399, loss: 0.0014464337145909667\n",
      "step: 1400, loss: 0.0006538925226777792\n",
      "step: 1401, loss: 0.001174636883661151\n",
      "step: 1402, loss: 0.0004197927482891828\n",
      "step: 1403, loss: 0.0007667773752473295\n",
      "step: 1404, loss: 0.04468948766589165\n",
      "step: 1405, loss: 0.0011488193413242698\n",
      "step: 1406, loss: 0.001850923290476203\n",
      "step: 1407, loss: 0.0023030599113553762\n",
      "step: 1408, loss: 0.001216658391058445\n",
      "step: 1409, loss: 0.005632036831229925\n",
      "step: 1410, loss: 0.0005862129619345069\n",
      "step: 1411, loss: 0.0021603144705295563\n",
      "step: 1412, loss: 0.0018152950797230005\n",
      "step: 1413, loss: 0.003832210088148713\n",
      "step: 1414, loss: 0.0029161537531763315\n",
      "step: 1415, loss: 0.0007560727535746992\n",
      "step: 1416, loss: 0.0025821616873145103\n",
      "step: 1417, loss: 0.0011909131426364183\n",
      "step: 1418, loss: 0.0007040641503408551\n",
      "step: 1419, loss: 0.0015920060686767101\n",
      "step: 1420, loss: 0.0018212322611361742\n",
      "step: 1421, loss: 0.0006185377133078873\n",
      "step: 1422, loss: 0.0007616357761435211\n",
      "step: 1423, loss: 0.0003328736056573689\n",
      "step: 1424, loss: 0.0007299903663806617\n",
      "step: 1425, loss: 0.00045922864228487015\n",
      "step: 1426, loss: 0.001212103758007288\n",
      "step: 1427, loss: 0.002163122408092022\n",
      "step: 1428, loss: 0.0005137109546922147\n",
      "step: 1429, loss: 0.046796493232250214\n",
      "step: 1430, loss: 0.0013085417449474335\n",
      "step: 1431, loss: 0.0010601517278701067\n",
      "step: 1432, loss: 0.0023144034203141928\n",
      "step: 1433, loss: 0.0017592571675777435\n",
      "step: 1434, loss: 0.000727386970538646\n",
      "step: 1435, loss: 0.0009104936034418643\n",
      "step: 1436, loss: 0.00220622681081295\n",
      "step: 1437, loss: 0.02203270047903061\n",
      "step: 1438, loss: 0.0012260624207556248\n",
      "step: 1439, loss: 0.0007159616216085851\n",
      "step: 1440, loss: 0.0007807831279933453\n",
      "step: 1441, loss: 0.0003031468659173697\n",
      "step: 1442, loss: 0.0016893312567844987\n",
      "step: 1443, loss: 0.0011383328819647431\n",
      "step: 1444, loss: 0.0002811559534166008\n",
      "step: 1445, loss: 0.0007406867225654423\n",
      "step: 1446, loss: 0.0005107477190904319\n",
      "step: 1447, loss: 0.0014176737749949098\n",
      "step: 1448, loss: 0.0007314862450584769\n",
      "step: 1449, loss: 0.0017143019940704107\n",
      "step: 1450, loss: 0.0003705399576574564\n",
      "step: 1451, loss: 0.0008829068392515182\n",
      "step: 1452, loss: 0.050313256680965424\n",
      "step: 1453, loss: 0.02133292146027088\n",
      "step: 1454, loss: 0.0019105002284049988\n",
      "step: 1455, loss: 0.0007023604121059179\n",
      "step: 1456, loss: 0.0010352727258577943\n",
      "step: 1457, loss: 0.039649397134780884\n",
      "step: 1458, loss: 0.0023149687331169844\n",
      "step: 1459, loss: 0.07594257593154907\n",
      "step: 1460, loss: 0.05889260396361351\n",
      "step: 1461, loss: 0.0003399267152417451\n",
      "step: 1462, loss: 0.001018755603581667\n",
      "step: 1463, loss: 0.03965607285499573\n",
      "step: 1464, loss: 0.0003145745722576976\n",
      "step: 1465, loss: 0.009119090624153614\n",
      "step: 1466, loss: 0.0017386536346748471\n",
      "step: 1467, loss: 0.001187797635793686\n",
      "step: 1468, loss: 0.0005197701975703239\n",
      "step: 1469, loss: 0.0003054109402000904\n",
      "step: 1470, loss: 0.0010522027732804418\n",
      "step: 1471, loss: 0.004109456203877926\n",
      "step: 1472, loss: 0.0027031900826841593\n",
      "step: 1473, loss: 0.00030606816289946437\n",
      "step: 1474, loss: 0.0005931759369559586\n",
      "step: 1475, loss: 0.0010050598066300154\n",
      "step: 1476, loss: 0.00024273832968901843\n",
      "step: 1477, loss: 0.028050974011421204\n",
      "step: 1478, loss: 0.0006611038115806878\n",
      "step: 1479, loss: 0.001957522938027978\n",
      "step: 1480, loss: 0.0005409006262198091\n",
      "step: 1481, loss: 0.00041636114474385977\n",
      "step: 1482, loss: 0.000536796695087105\n",
      "step: 1483, loss: 0.0011255424469709396\n",
      "step: 1484, loss: 0.0012703090906143188\n",
      "step: 1485, loss: 0.0013990327715873718\n",
      "step: 1486, loss: 0.0013987066922709346\n",
      "step: 1487, loss: 0.0003089096280746162\n",
      "step: 1488, loss: 0.0014575235545635223\n",
      "step: 1489, loss: 0.0029450689908117056\n",
      "step: 1490, loss: 0.01814250834286213\n",
      "step: 1491, loss: 0.0005493190255947411\n",
      "step: 1492, loss: 0.0005366364493966103\n",
      "step: 1493, loss: 0.003822037950158119\n",
      "step: 1494, loss: 0.0007007570820860565\n",
      "step: 1495, loss: 0.0010690324706956744\n",
      "step: 1496, loss: 0.0007562845130451024\n",
      "step: 1497, loss: 0.00043775621452368796\n",
      "step: 1498, loss: 0.001093259546905756\n",
      "step: 1499, loss: 0.0005360549548640847\n",
      "step: 1500, loss: 0.0003833595255855471\n",
      "step: 1501, loss: 0.00033339858055114746\n",
      "step: 1502, loss: 0.0006233233725652099\n",
      "step: 1503, loss: 0.0004647739988286048\n",
      "step: 1504, loss: 0.0012913781683892012\n",
      "step: 1505, loss: 0.0029879161156713963\n",
      "step: 1506, loss: 0.0004463213263079524\n",
      "step: 1507, loss: 0.0037014742847532034\n",
      "step: 1508, loss: 0.001717982580885291\n",
      "step: 1509, loss: 0.0006665316759608686\n",
      "step: 1510, loss: 0.0008891965262591839\n",
      "step: 1511, loss: 0.00032424990786239505\n",
      "step: 1512, loss: 0.0020321737974882126\n",
      "step: 1513, loss: 0.0010469069238752127\n",
      "step: 1514, loss: 0.00044014357263222337\n",
      "step: 1515, loss: 0.0004991650930605829\n",
      "step: 1516, loss: 0.07915017753839493\n",
      "step: 1517, loss: 0.0005018285010010004\n",
      "step: 1518, loss: 0.000895339937414974\n",
      "step: 1519, loss: 0.002030509989708662\n",
      "step: 1520, loss: 0.0005645359051413834\n",
      "step: 1521, loss: 0.010082271881401539\n",
      "step: 1522, loss: 0.0017858871724456549\n",
      "step: 1523, loss: 0.000875541940331459\n",
      "step: 1524, loss: 0.0013919553020969033\n",
      "step: 1525, loss: 0.001233852468430996\n",
      "step: 1526, loss: 0.00032059941440820694\n",
      "step: 1527, loss: 0.0005009404267184436\n",
      "step: 1528, loss: 0.0019988263957202435\n",
      "step: 1529, loss: 0.0006095702410675585\n",
      "step: 1530, loss: 0.0008483584970235825\n",
      "step: 1531, loss: 0.0006797704845666885\n",
      "step: 1532, loss: 0.01858700066804886\n",
      "step: 1533, loss: 0.0006292947218753397\n",
      "step: 1534, loss: 0.000787783763371408\n",
      "step: 1535, loss: 0.0003389291523490101\n",
      "step: 1536, loss: 0.0025822946336120367\n",
      "step: 1537, loss: 0.0004072602605447173\n",
      "step: 1538, loss: 0.0006593452417291701\n",
      "step: 1539, loss: 0.0006386712775565684\n",
      "step: 1540, loss: 0.0003742404514923692\n",
      "step: 1541, loss: 0.03945300728082657\n",
      "step: 1542, loss: 0.0008103612344712019\n",
      "step: 1543, loss: 0.0009204485104419291\n",
      "step: 1544, loss: 0.0016952892765402794\n",
      "step: 1545, loss: 0.0010986579582095146\n",
      "step: 1546, loss: 0.0013097840128466487\n",
      "step: 1547, loss: 0.02039123699069023\n",
      "step: 1548, loss: 0.012825612910091877\n",
      "step: 1549, loss: 0.01816069334745407\n",
      "step: 1550, loss: 0.0010620724642649293\n",
      "step: 1551, loss: 0.00031640983070246875\n",
      "step: 1552, loss: 0.0018839731346815825\n",
      "step: 1553, loss: 0.00024413198116235435\n",
      "step: 1554, loss: 0.0005188594223000109\n",
      "step: 1555, loss: 0.002764743519946933\n",
      "step: 1556, loss: 0.0018169591203331947\n",
      "step: 1557, loss: 0.0019522220827639103\n",
      "step: 1558, loss: 0.005693776533007622\n",
      "step: 1559, loss: 0.0007295259274542332\n",
      "step: 1560, loss: 0.0007158350199460983\n",
      "step: 1561, loss: 0.0007073218584991992\n",
      "step: 1562, loss: 0.0006722815451212227\n",
      "step: 1563, loss: 0.001126106595620513\n",
      "step: 1564, loss: 0.0013945704558864236\n",
      "step: 1565, loss: 0.001208735746331513\n",
      "step: 1566, loss: 0.0027428746689110994\n",
      "step: 1567, loss: 0.0015480481088161469\n",
      "step: 1568, loss: 0.003991070203483105\n",
      "step: 1569, loss: 0.0006625343230552971\n",
      "step: 1570, loss: 0.0005689411773346364\n",
      "step: 1571, loss: 0.0013481548521667719\n",
      "step: 1572, loss: 0.002790322294458747\n",
      "step: 1573, loss: 0.0006505082710646093\n",
      "step: 1574, loss: 0.0003730098542291671\n",
      "step: 1575, loss: 0.0017841788940131664\n",
      "step: 1576, loss: 0.0009231844451278448\n",
      "step: 1577, loss: 0.0033062933944165707\n",
      "step: 1578, loss: 0.002531202044337988\n",
      "step: 1579, loss: 0.02979425899684429\n",
      "step: 1580, loss: 0.0015106403734534979\n",
      "step: 1581, loss: 0.000920337566640228\n",
      "step: 1582, loss: 0.0010528052225708961\n",
      "step: 1583, loss: 0.000395751470932737\n",
      "step: 1584, loss: 0.00046377311809919775\n",
      "step: 1585, loss: 0.0008096509263850749\n",
      "step: 1586, loss: 0.0012160151964053512\n",
      "step: 1587, loss: 0.0006855152314528823\n",
      "step: 1588, loss: 0.08725718408823013\n",
      "step: 1589, loss: 0.000375878851627931\n",
      "step: 1590, loss: 0.0012293454492464662\n",
      "step: 1591, loss: 0.00022396183339878917\n",
      "step: 1592, loss: 0.0005279468605294824\n",
      "step: 1593, loss: 0.0010639217216521502\n",
      "step: 1594, loss: 0.00034133336157537997\n",
      "step: 1595, loss: 0.0003439230495132506\n",
      "step: 1596, loss: 0.0019557862542569637\n",
      "step: 1597, loss: 0.00029442395316436887\n",
      "step: 1598, loss: 0.00021431986533571035\n",
      "step: 1599, loss: 0.0009366528829559684\n",
      "step: 1600, loss: 0.0010611072648316622\n",
      "step: 1601, loss: 0.0008504344732500613\n",
      "step: 1602, loss: 0.0004182741104159504\n",
      "step: 1603, loss: 0.00042631648830138147\n",
      "step: 1604, loss: 0.0005361957591958344\n",
      "step: 1605, loss: 0.03328891098499298\n",
      "step: 1606, loss: 0.0005343972006812692\n",
      "step: 1607, loss: 0.046364255249500275\n",
      "step: 1608, loss: 0.00032302775071002543\n",
      "step: 1609, loss: 0.0003510874230414629\n",
      "step: 1610, loss: 0.0007426294614560902\n",
      "step: 1611, loss: 0.0003843893064185977\n",
      "step: 1612, loss: 0.0004504715034272522\n",
      "step: 1613, loss: 0.0004951361333951354\n",
      "step: 1614, loss: 0.0004101768136024475\n",
      "step: 1615, loss: 0.0005358271882869303\n",
      "step: 1616, loss: 0.0011991540668532252\n",
      "step: 1617, loss: 0.0006184612866491079\n",
      "step: 1618, loss: 0.0006237268098630011\n",
      "step: 1619, loss: 0.0003582361387088895\n",
      "step: 1620, loss: 0.00107766583096236\n",
      "step: 1621, loss: 0.0007482924847863615\n",
      "step: 1622, loss: 0.0005286617670208216\n",
      "step: 1623, loss: 0.00024374184431508183\n",
      "step: 1624, loss: 0.00041227779001928866\n",
      "step: 1625, loss: 0.0003008648636750877\n",
      "step: 1626, loss: 0.0017299759201705456\n",
      "step: 1627, loss: 0.0013269674964249134\n",
      "step: 1628, loss: 0.0015460964059457183\n",
      "step: 1629, loss: 0.034108154475688934\n",
      "step: 1630, loss: 0.0005898153758607805\n",
      "step: 1631, loss: 0.00079915183596313\n",
      "step: 1632, loss: 0.0005433186306618154\n",
      "step: 1633, loss: 0.0008624504553154111\n",
      "step: 1634, loss: 0.0006942985928617418\n",
      "step: 1635, loss: 0.0009725449490360916\n",
      "step: 1636, loss: 0.00034096429590135813\n",
      "step: 1637, loss: 0.0007167431176640093\n",
      "step: 1638, loss: 0.1207098588347435\n",
      "step: 1639, loss: 0.0012690341100096703\n",
      "step: 1640, loss: 0.0010020651388913393\n",
      "step: 1641, loss: 0.0007712768856436014\n",
      "step: 1642, loss: 0.0011247389484196901\n",
      "step: 1643, loss: 0.06344281136989594\n",
      "step: 1644, loss: 0.00037505358341149986\n",
      "step: 1645, loss: 0.0008809196879155934\n",
      "step: 1646, loss: 0.001143361907452345\n",
      "step: 1647, loss: 0.0010458972537890077\n",
      "step: 1648, loss: 0.01667153649032116\n",
      "step: 1649, loss: 0.0007103492971509695\n",
      "step: 1650, loss: 0.0008062324486672878\n",
      "step: 1651, loss: 0.0006674169562757015\n",
      "step: 1652, loss: 0.011967744678258896\n",
      "step: 1653, loss: 0.0005909373867325485\n",
      "step: 1654, loss: 0.0009548034286126494\n",
      "step: 1655, loss: 0.0028452365659177303\n",
      "step: 1656, loss: 0.001327252946794033\n",
      "step: 1657, loss: 0.0015711517771705985\n",
      "step: 1658, loss: 0.0011683921329677105\n",
      "step: 1659, loss: 0.0018859374104067683\n",
      "step: 1660, loss: 0.0009446428157389164\n",
      "step: 1661, loss: 0.00981803797185421\n",
      "step: 1662, loss: 0.0007160994573496282\n",
      "step: 1663, loss: 0.0005193948745727539\n",
      "step: 1664, loss: 0.0004162634431850165\n",
      "step: 1665, loss: 0.0008663872140459716\n",
      "step: 1666, loss: 0.0005437504150904715\n",
      "step: 1667, loss: 0.0002604841720312834\n",
      "step: 1668, loss: 0.000621879065874964\n",
      "step: 1669, loss: 0.0007491819560527802\n",
      "step: 1670, loss: 0.00034675028291530907\n",
      "step: 1671, loss: 0.0025289070326834917\n",
      "step: 1672, loss: 0.0006736604846082628\n",
      "step: 1673, loss: 0.00033053927472792566\n",
      "step: 1674, loss: 0.04189414158463478\n",
      "step: 1675, loss: 0.0004074839816894382\n",
      "step: 1676, loss: 0.00046067938092164695\n",
      "step: 1677, loss: 0.0003107069351244718\n",
      "step: 1678, loss: 0.0011384765384718776\n",
      "step: 1679, loss: 0.045071378350257874\n",
      "step: 1680, loss: 0.0007146410061977804\n",
      "step: 1681, loss: 0.008645182475447655\n",
      "step: 1682, loss: 0.0006152659188956022\n",
      "step: 1683, loss: 0.0002389303408563137\n",
      "step: 1684, loss: 0.0003143188077956438\n",
      "step: 1685, loss: 0.0030158725567162037\n",
      "step: 1686, loss: 0.00031643352122046053\n",
      "step: 1687, loss: 0.00028023766935802996\n",
      "step: 1688, loss: 0.0004568991425912827\n",
      "step: 1689, loss: 0.0006904482142999768\n",
      "step: 1690, loss: 0.0005620219744741917\n",
      "step: 1691, loss: 0.0004487466358114034\n",
      "step: 1692, loss: 0.0006725461571477354\n",
      "step: 1693, loss: 0.019577620550990105\n",
      "step: 1694, loss: 0.0008608727366663516\n",
      "step: 1695, loss: 0.008205454796552658\n",
      "step: 1696, loss: 0.002217908389866352\n",
      "step: 1697, loss: 0.0006895035621710122\n",
      "step: 1698, loss: 0.0007613205234520137\n",
      "step: 1699, loss: 0.03114486299455166\n",
      "step: 1700, loss: 0.0005101787392050028\n",
      "step: 1701, loss: 0.0006668904679827392\n",
      "step: 1702, loss: 0.0009183724178001285\n",
      "step: 1703, loss: 0.0005655704298987985\n",
      "step: 1704, loss: 0.0012751766480505466\n",
      "step: 1705, loss: 0.02266039326786995\n",
      "step: 1706, loss: 0.0009605176164768636\n",
      "step: 1707, loss: 0.0002943991275969893\n",
      "step: 1708, loss: 0.0005558796692639589\n",
      "step: 1709, loss: 0.0005734916194342077\n",
      "step: 1710, loss: 0.0012798564275726676\n",
      "step: 1711, loss: 0.00042760735959745944\n",
      "step: 1712, loss: 0.0019829608500003815\n",
      "step: 1713, loss: 0.0007174685597419739\n",
      "step: 1714, loss: 0.0021135821007192135\n",
      "step: 1715, loss: 0.00032066195853985846\n",
      "step: 1716, loss: 0.0006507770740427077\n",
      "step: 1717, loss: 0.000675341347232461\n",
      "step: 1718, loss: 0.006494607776403427\n",
      "step: 1719, loss: 0.01417970098555088\n",
      "step: 1720, loss: 0.0006554680876433849\n",
      "step: 1721, loss: 0.0012764069251716137\n",
      "step: 1722, loss: 0.0033552830573171377\n",
      "step: 1723, loss: 0.00041848470573313534\n",
      "step: 1724, loss: 0.00017726000805851072\n",
      "step: 1725, loss: 0.0004472570726647973\n",
      "step: 1726, loss: 0.00021392118651419878\n",
      "step: 1727, loss: 0.003112257458269596\n",
      "step: 1728, loss: 0.0007647189777344465\n",
      "step: 1729, loss: 0.00023702233738731593\n",
      "step: 1730, loss: 0.0006380914128385484\n",
      "step: 1731, loss: 0.0003461266169324517\n",
      "step: 1732, loss: 0.02180873416364193\n",
      "step: 1733, loss: 0.0004509795398917049\n",
      "step: 1734, loss: 0.0011755230370908976\n",
      "step: 1735, loss: 0.00032146729063242674\n",
      "step: 1736, loss: 0.0011377850314602256\n",
      "step: 1737, loss: 0.0008413599571213126\n",
      "step: 1738, loss: 0.0010241629788652062\n",
      "step: 1739, loss: 0.0005168685456737876\n",
      "step: 1740, loss: 0.0005888055311515927\n",
      "step: 1741, loss: 0.017333831638097763\n",
      "step: 1742, loss: 0.000326702487654984\n",
      "step: 1743, loss: 0.00015001186693552881\n",
      "step: 1744, loss: 0.003934036940336227\n",
      "step: 1745, loss: 0.0001968800788745284\n",
      "step: 1746, loss: 0.0004654802323784679\n",
      "step: 1747, loss: 0.0004871153214480728\n",
      "step: 1748, loss: 0.0012847745092585683\n",
      "step: 1749, loss: 0.00046696982462890446\n",
      "step: 1750, loss: 0.001572441658936441\n",
      "step: 1751, loss: 0.00010106995614478365\n",
      "step: 1752, loss: 0.0003893804969266057\n",
      "step: 1753, loss: 0.00036153121618553996\n",
      "step: 1754, loss: 0.00013808747462462634\n",
      "step: 1755, loss: 0.0009491433156654239\n",
      "step: 1756, loss: 0.000848347379360348\n",
      "step: 1757, loss: 0.08531823754310608\n",
      "step: 1758, loss: 0.0005241971812210977\n",
      "step: 1759, loss: 0.05107736214995384\n",
      "step: 1760, loss: 0.0006096581346355379\n",
      "step: 1761, loss: 0.0005365757388062775\n",
      "step: 1762, loss: 0.002001718617975712\n",
      "step: 1763, loss: 0.0017785258824005723\n",
      "step: 1764, loss: 0.0008183437748812139\n",
      "step: 1765, loss: 0.17064379155635834\n",
      "step: 1766, loss: 0.0004911065916530788\n",
      "step: 1767, loss: 0.00043217389611527324\n",
      "step: 1768, loss: 0.000714306952431798\n",
      "step: 1769, loss: 0.0005882022669538856\n",
      "step: 1770, loss: 0.0010862852213904262\n",
      "step: 1771, loss: 0.00036728777922689915\n",
      "step: 1772, loss: 0.018546801060438156\n",
      "step: 1773, loss: 0.006955252494663\n",
      "step: 1774, loss: 0.0002764370001386851\n",
      "step: 1775, loss: 0.001108518335968256\n",
      "step: 1776, loss: 0.0010984110413119197\n",
      "step: 1777, loss: 0.0016259800177067518\n",
      "step: 1778, loss: 0.00029621346038766205\n",
      "step: 1779, loss: 0.0004103535902686417\n",
      "step: 1780, loss: 0.0008196218404918909\n",
      "step: 1781, loss: 0.0006809743354097009\n",
      "step: 1782, loss: 0.0008580642752349377\n",
      "step: 1783, loss: 0.0009528868831694126\n",
      "step: 1784, loss: 0.00017485061835031956\n",
      "step: 1785, loss: 0.0007678360561840236\n",
      "step: 1786, loss: 0.002252618782222271\n",
      "step: 1787, loss: 0.0006734153721481562\n",
      "step: 1788, loss: 0.0004742394667118788\n",
      "step: 1789, loss: 0.00014748735702596605\n",
      "step: 1790, loss: 0.001034873304888606\n",
      "step: 1791, loss: 0.0004429861146491021\n",
      "step: 1792, loss: 0.16190145909786224\n",
      "step: 1793, loss: 0.0018340477254241705\n",
      "step: 1794, loss: 0.000476029294077307\n",
      "step: 1795, loss: 0.03707270324230194\n",
      "step: 1796, loss: 0.0010905948001891375\n",
      "step: 1797, loss: 0.0007992336177267134\n",
      "step: 1798, loss: 0.00033503174199722707\n",
      "step: 1799, loss: 0.02305157110095024\n",
      "step: 1800, loss: 0.0010116832563653588\n",
      "step: 1801, loss: 0.0014614042593166232\n",
      "step: 1802, loss: 0.0007797864964231849\n",
      "step: 1803, loss: 0.0008653852273710072\n",
      "step: 1804, loss: 0.0003609058912843466\n",
      "step: 1805, loss: 0.0010994530748575926\n",
      "step: 1806, loss: 0.0009987674420699477\n",
      "step: 1807, loss: 0.0005103533621877432\n",
      "step: 1808, loss: 0.00042719440534710884\n",
      "step: 1809, loss: 0.0003271271416451782\n",
      "step: 1810, loss: 0.0003568251559045166\n",
      "step: 1811, loss: 0.0003331658663228154\n",
      "step: 1812, loss: 0.09767948091030121\n",
      "step: 1813, loss: 0.0003110032994300127\n",
      "step: 1814, loss: 0.0001628874451853335\n",
      "step: 1815, loss: 0.00038981635589152575\n",
      "step: 1816, loss: 0.00010705543536460027\n",
      "step: 1817, loss: 0.00047708116471767426\n",
      "step: 1818, loss: 0.0003883669269271195\n",
      "step: 1819, loss: 0.0007717348635196686\n",
      "step: 1820, loss: 0.00017653957183938473\n",
      "step: 1821, loss: 0.0005842337268404663\n",
      "step: 1822, loss: 0.0007393280393444002\n",
      "step: 1823, loss: 0.00025764922611415386\n",
      "step: 1824, loss: 0.0003984827490057796\n",
      "step: 1825, loss: 0.0008123868610709906\n",
      "step: 1826, loss: 0.000553970574401319\n",
      "step: 1827, loss: 0.010334903374314308\n",
      "step: 1828, loss: 0.000310870906105265\n",
      "step: 1829, loss: 0.004119783639907837\n",
      "step: 1830, loss: 0.00031645316630601883\n",
      "step: 1831, loss: 0.00033353589242324233\n",
      "step: 1832, loss: 0.00019746992620639503\n",
      "step: 1833, loss: 0.0020529001485556364\n",
      "step: 1834, loss: 0.0007586244028061628\n",
      "step: 1835, loss: 0.0007137981592677534\n",
      "step: 1836, loss: 0.00044003018410876393\n",
      "step: 1837, loss: 0.0003464195178821683\n",
      "step: 1838, loss: 0.015157383866608143\n",
      "step: 1839, loss: 0.056581735610961914\n",
      "step: 1840, loss: 0.0003868867934215814\n",
      "step: 1841, loss: 0.0003128165262751281\n",
      "step: 1842, loss: 0.0002971695503219962\n",
      "step: 1843, loss: 0.0012947069481015205\n",
      "step: 1844, loss: 0.001958392793312669\n",
      "step: 1845, loss: 0.00023585771850775927\n",
      "step: 1846, loss: 0.0007988555589690804\n",
      "step: 1847, loss: 0.00018102274043485522\n",
      "step: 1848, loss: 0.00039098848355934024\n",
      "step: 1849, loss: 0.025949738919734955\n",
      "step: 1850, loss: 0.0011680109892040491\n",
      "step: 1851, loss: 0.00013157533248886466\n",
      "step: 1852, loss: 0.029947493225336075\n",
      "step: 1853, loss: 0.0004012321587651968\n",
      "step: 1854, loss: 0.00019778564455918968\n",
      "step: 1855, loss: 0.0006376674864441156\n",
      "step: 1856, loss: 0.08264186978340149\n",
      "step: 1857, loss: 0.0002115350216627121\n",
      "step: 1858, loss: 0.00018549582455307245\n",
      "step: 1859, loss: 0.00104011211078614\n",
      "step: 1860, loss: 0.0006785379955545068\n",
      "step: 1861, loss: 0.000721681397408247\n",
      "step: 1862, loss: 0.0003603102231863886\n",
      "step: 1863, loss: 0.00021181657211855054\n",
      "step: 1864, loss: 0.00029282228206284344\n",
      "step: 1865, loss: 0.0003147730603814125\n",
      "step: 1866, loss: 0.00034069432877004147\n",
      "step: 1867, loss: 0.00024192400451283902\n",
      "step: 1868, loss: 0.006707014981657267\n",
      "step: 1869, loss: 0.0002197597932536155\n",
      "step: 1870, loss: 0.00070071971276775\n",
      "step: 1871, loss: 0.022143175825476646\n",
      "step: 1872, loss: 0.0004333753022365272\n",
      "step: 1873, loss: 0.000813365273643285\n",
      "step: 1874, loss: 0.0001782302715582773\n",
      "step: 1875, loss: 0.0007446780218742788\n",
      "step: 1876, loss: 0.0002060199185507372\n",
      "step: 1877, loss: 0.0004647962632589042\n",
      "step: 1878, loss: 0.0003298383962828666\n",
      "step: 1879, loss: 0.00024122251488734037\n",
      "step: 1880, loss: 0.043788693845272064\n",
      "step: 1881, loss: 0.0464346706867218\n",
      "step: 1882, loss: 0.0006489612860605121\n",
      "step: 1883, loss: 0.00034433731343597174\n",
      "step: 1884, loss: 0.0013671568594872952\n",
      "step: 1885, loss: 0.0002511042694095522\n",
      "step: 1886, loss: 0.0002088812179863453\n",
      "step: 1887, loss: 0.0002829525910783559\n",
      "step: 1888, loss: 0.00083484424976632\n",
      "step: 1889, loss: 0.0003123533970210701\n",
      "step: 1890, loss: 0.0008006468415260315\n",
      "step: 1891, loss: 0.032379768788814545\n",
      "step: 1892, loss: 0.00038109932211227715\n",
      "step: 1893, loss: 0.0002867838484235108\n",
      "step: 1894, loss: 0.04748275876045227\n",
      "step: 1895, loss: 0.0013138605281710625\n",
      "step: 1896, loss: 0.26264306902885437\n",
      "step: 1897, loss: 0.0005978663102723658\n",
      "step: 1898, loss: 0.0006378011894412339\n",
      "step: 1899, loss: 0.0006077932193875313\n",
      "step: 1900, loss: 0.0006791774067096412\n",
      "step: 1901, loss: 0.0005179292056709528\n",
      "step: 1902, loss: 0.0004352172836661339\n",
      "step: 1903, loss: 0.0009056704584509134\n",
      "step: 1904, loss: 0.00025578957865945995\n",
      "step: 1905, loss: 0.0008156233816407621\n",
      "step: 1906, loss: 0.0010911617428064346\n",
      "step: 1907, loss: 0.00019535221508704126\n",
      "step: 1908, loss: 0.0007512127631343901\n",
      "step: 1909, loss: 0.0003327248268760741\n",
      "step: 1910, loss: 0.00023556611267849803\n",
      "step: 1911, loss: 0.0003595374582801014\n",
      "step: 1912, loss: 0.0004850563418585807\n",
      "step: 1913, loss: 0.0003870069340337068\n",
      "step: 1914, loss: 0.0005953146028332412\n",
      "step: 1915, loss: 0.00036405256832949817\n",
      "step: 1916, loss: 0.0008283374481834471\n",
      "step: 1917, loss: 0.0006979071185924113\n",
      "step: 1918, loss: 0.0007634919602423906\n",
      "step: 1919, loss: 0.0003962251648772508\n",
      "step: 1920, loss: 0.011851208284497261\n",
      "step: 1921, loss: 0.00062484823865816\n",
      "step: 1922, loss: 0.0002150160144083202\n",
      "step: 1923, loss: 0.0004373948904685676\n",
      "step: 1924, loss: 0.004753615707159042\n",
      "step: 1925, loss: 0.0006001639412716031\n",
      "step: 1926, loss: 0.00021340855164453387\n",
      "step: 1927, loss: 0.0005016783834435046\n",
      "step: 1928, loss: 0.00020228275388944894\n",
      "step: 1929, loss: 0.00021112382819410414\n",
      "step: 1930, loss: 0.00044964917469769716\n",
      "step: 1931, loss: 0.00019580604566726834\n",
      "step: 1932, loss: 0.00040213341708295047\n",
      "step: 1933, loss: 0.0002858513907995075\n",
      "step: 1934, loss: 0.047000493854284286\n",
      "step: 1935, loss: 0.0006829268531873822\n",
      "step: 1936, loss: 0.0005864700069651008\n",
      "step: 1937, loss: 0.0012517551658675075\n",
      "step: 1938, loss: 0.0003200818900950253\n",
      "step: 1939, loss: 0.00016014189168345183\n",
      "step: 1940, loss: 0.00050755386473611\n",
      "step: 1941, loss: 0.0006365326116792858\n",
      "step: 1942, loss: 0.0014252264518290758\n",
      "step: 1943, loss: 0.00030714477179571986\n",
      "step: 1944, loss: 0.00045058055547997355\n",
      "step: 1945, loss: 0.0034674035850912333\n",
      "step: 1946, loss: 0.0008482171106152236\n",
      "step: 1947, loss: 0.0001711655204417184\n",
      "step: 1948, loss: 0.00034099037293344736\n",
      "step: 1949, loss: 0.00041843188228085637\n",
      "step: 1950, loss: 0.000254349026363343\n",
      "step: 1951, loss: 0.00018237683980260044\n",
      "step: 1952, loss: 0.0005554118542931974\n",
      "step: 1953, loss: 0.00044121683458797634\n",
      "step: 1954, loss: 0.0008338144980370998\n",
      "step: 1955, loss: 0.0005245559732429683\n",
      "step: 1956, loss: 0.00028740603011101484\n",
      "step: 1957, loss: 0.019385719671845436\n",
      "step: 1958, loss: 0.00029213636298663914\n",
      "step: 1959, loss: 0.0002472549967933446\n",
      "step: 1960, loss: 0.0008350199204869568\n",
      "step: 1961, loss: 0.0003285360289737582\n",
      "step: 1962, loss: 0.00043886699131689966\n",
      "step: 1963, loss: 0.000623410742264241\n",
      "step: 1964, loss: 0.00034586084075272083\n",
      "step: 1965, loss: 0.0003450085932854563\n",
      "step: 1966, loss: 0.0002292075369041413\n",
      "step: 1967, loss: 0.023601720109581947\n",
      "step: 1968, loss: 0.0016203118721023202\n",
      "step: 1969, loss: 0.00012868214980699122\n",
      "step: 1970, loss: 0.0013867754023522139\n",
      "step: 1971, loss: 0.0008569564670324326\n",
      "step: 1972, loss: 0.0016862557968124747\n",
      "step: 1973, loss: 0.0005579824210144579\n",
      "step: 1974, loss: 0.0008584788301959634\n",
      "step: 1975, loss: 0.00024375304928980768\n",
      "step: 1976, loss: 0.051455069333314896\n",
      "step: 1977, loss: 0.00035620201379060745\n",
      "step: 1978, loss: 0.00037556103779934347\n",
      "step: 1979, loss: 0.00027795371715910733\n",
      "step: 1980, loss: 0.0003905981429852545\n",
      "step: 1981, loss: 0.0008888787124305964\n",
      "step: 1982, loss: 0.07328412681818008\n",
      "step: 1983, loss: 0.000913407071493566\n",
      "step: 1984, loss: 0.0012044444447383285\n",
      "step: 1985, loss: 0.0002433848421787843\n",
      "step: 1986, loss: 0.00034537893952801824\n",
      "step: 1987, loss: 0.004303158726543188\n",
      "step: 1988, loss: 0.00043321988778188825\n",
      "step: 1989, loss: 0.00031010189559310675\n",
      "step: 1990, loss: 0.02735712379217148\n",
      "step: 1991, loss: 0.0011710802791640162\n",
      "step: 1992, loss: 0.00096200464759022\n",
      "step: 1993, loss: 0.00021236768225207925\n",
      "step: 1994, loss: 0.0002462163392920047\n",
      "step: 1995, loss: 0.005753092933446169\n",
      "step: 1996, loss: 0.0007816903525963426\n",
      "step: 1997, loss: 0.0010728518245741725\n",
      "step: 1998, loss: 0.00041625238372944295\n",
      "step: 1999, loss: 0.0010735034011304379\n",
      "step: 2000, loss: 0.0007223509019240737\n",
      "step: 2001, loss: 0.0004605789144989103\n",
      "step: 2002, loss: 0.00045616496936418116\n",
      "step: 2003, loss: 0.036770034581422806\n",
      "step: 2004, loss: 0.0008340846979990602\n",
      "step: 2005, loss: 0.0005997318075969815\n",
      "step: 2006, loss: 0.00021565286442637444\n",
      "step: 2007, loss: 0.00048601924208924174\n",
      "step: 2008, loss: 0.001503265229985118\n",
      "step: 2009, loss: 0.027044499292969704\n",
      "step: 2010, loss: 0.00035510468296706676\n",
      "step: 2011, loss: 0.03969825431704521\n",
      "step: 2012, loss: 0.0007275827811099589\n",
      "step: 2013, loss: 0.0008280319161713123\n",
      "step: 2014, loss: 0.0004076999321114272\n",
      "step: 2015, loss: 0.00013380777090787888\n",
      "step: 2016, loss: 0.00019164723926223814\n",
      "step: 2017, loss: 0.05696869269013405\n",
      "step: 2018, loss: 0.00035208009649068117\n",
      "step: 2019, loss: 0.1897091120481491\n",
      "step: 2020, loss: 0.0007418736931867898\n",
      "step: 2021, loss: 0.0007359699229709804\n",
      "step: 2022, loss: 0.00047259495477192104\n",
      "step: 2023, loss: 0.00027020342531614006\n",
      "step: 2024, loss: 0.00033660203916952014\n",
      "step: 2025, loss: 0.00043204386020079255\n",
      "step: 2026, loss: 0.000721862306818366\n",
      "step: 2027, loss: 0.00046527705853804946\n",
      "step: 2028, loss: 0.0003707611467689276\n",
      "step: 2029, loss: 0.05826747044920921\n",
      "step: 2030, loss: 0.001024049473926425\n",
      "step: 2031, loss: 0.0005885869613848627\n",
      "step: 2032, loss: 0.00042347409180365503\n",
      "step: 2033, loss: 0.00038999615935608745\n",
      "step: 2034, loss: 0.024099312722682953\n",
      "step: 2035, loss: 0.0011160705471411347\n",
      "step: 2036, loss: 0.0005717978929169476\n",
      "step: 2037, loss: 0.0004004116344731301\n",
      "step: 2038, loss: 0.00021288375137373805\n",
      "step: 2039, loss: 0.00012027975026285276\n",
      "step: 2040, loss: 0.00046869792276993394\n",
      "step: 2041, loss: 0.0007158253574743867\n",
      "step: 2042, loss: 0.0017026483546942472\n",
      "step: 2043, loss: 0.0006787612219341099\n",
      "step: 2044, loss: 0.0005039388779550791\n",
      "step: 2045, loss: 0.018721014261245728\n",
      "step: 2046, loss: 0.0007448063115589321\n",
      "step: 2047, loss: 0.0003044857585337013\n",
      "step: 2048, loss: 0.024799659848213196\n",
      "step: 2049, loss: 0.0009450806537643075\n",
      "step: 2050, loss: 0.000341082108207047\n",
      "step: 2051, loss: 0.01790713705122471\n",
      "step: 2052, loss: 0.0007873206050135195\n",
      "step: 2053, loss: 0.00042650915565900505\n",
      "step: 2054, loss: 0.00040934179560281336\n",
      "step: 2055, loss: 0.00042487672180868685\n",
      "step: 2056, loss: 0.00040873594116419554\n",
      "step: 2057, loss: 0.00016450202383566648\n",
      "step: 2058, loss: 0.0004377407894935459\n",
      "step: 2059, loss: 0.0004986824351362884\n",
      "step: 2060, loss: 0.0002377943746978417\n",
      "step: 2061, loss: 8.009906014194712e-05\n",
      "step: 2062, loss: 0.0006330489413812757\n",
      "step: 2063, loss: 0.000615151016972959\n",
      "step: 2064, loss: 0.0002896575315389782\n",
      "step: 2065, loss: 0.0005093934596516192\n",
      "step: 2066, loss: 0.0003866851329803467\n",
      "step: 2067, loss: 0.00013278756523504853\n",
      "step: 2068, loss: 0.0002632402756717056\n",
      "step: 2069, loss: 0.0015385248698294163\n",
      "step: 2070, loss: 0.00011149816418765113\n",
      "step: 2071, loss: 0.0004674106603488326\n",
      "step: 2072, loss: 0.0023599721025675535\n",
      "step: 2073, loss: 0.0004973927279934287\n",
      "step: 2074, loss: 0.0004884717054665089\n",
      "step: 2075, loss: 0.00041417882312089205\n",
      "step: 2076, loss: 0.0001996001083170995\n",
      "step: 2077, loss: 0.0002461821713950485\n",
      "step: 2078, loss: 0.0001635818334762007\n",
      "step: 2079, loss: 0.0001916903565870598\n",
      "step: 2080, loss: 0.0001932555460371077\n",
      "step: 2081, loss: 0.0002563835005275905\n",
      "step: 2082, loss: 0.00018821601406671107\n",
      "step: 2083, loss: 0.028743363916873932\n",
      "step: 2084, loss: 0.0006716910284012556\n",
      "step: 2085, loss: 0.000889974064193666\n",
      "step: 2086, loss: 0.002914230339229107\n",
      "step: 2087, loss: 0.00038189574843272567\n",
      "step: 2088, loss: 0.0004151914035901427\n",
      "step: 2089, loss: 0.000679629622027278\n",
      "step: 2090, loss: 0.00018700220971368253\n",
      "step: 2091, loss: 0.0003561022749636322\n",
      "step: 2092, loss: 0.00027330408920533955\n",
      "step: 2093, loss: 0.00014685967471450567\n",
      "step: 2094, loss: 0.000286064634565264\n",
      "step: 2095, loss: 0.00042975449468940496\n",
      "step: 2096, loss: 0.0003108847013209015\n",
      "step: 2097, loss: 0.00017580534040462226\n",
      "step: 2098, loss: 0.00036419372190721333\n",
      "step: 2099, loss: 0.0005782793741673231\n",
      "step: 2100, loss: 0.22197483479976654\n",
      "step: 2101, loss: 0.00028486759401857853\n",
      "step: 2102, loss: 0.0007147379219532013\n",
      "step: 2103, loss: 0.0002714087313506752\n",
      "step: 2104, loss: 0.00033492236980237067\n",
      "step: 2105, loss: 0.00036549405194818974\n",
      "step: 2106, loss: 0.0149915162473917\n",
      "step: 2107, loss: 0.02457968145608902\n",
      "step: 2108, loss: 0.0011440609814599156\n",
      "step: 2109, loss: 0.0015938974684104323\n",
      "step: 2110, loss: 0.0003118289459962398\n",
      "step: 2111, loss: 0.003718959167599678\n",
      "step: 2112, loss: 0.000553063815459609\n",
      "step: 2113, loss: 0.00010119768558070064\n",
      "step: 2114, loss: 0.00029695811099372804\n",
      "step: 2115, loss: 0.0006668971618637443\n",
      "step: 2116, loss: 0.0005849662702530622\n",
      "step: 2117, loss: 0.0005644155899062753\n",
      "step: 2118, loss: 0.0003187699185218662\n",
      "step: 2119, loss: 0.0004402899357955903\n",
      "step: 2120, loss: 0.0003758826351258904\n",
      "step: 2121, loss: 0.00014344940427690744\n",
      "step: 2122, loss: 0.00035907496931031346\n",
      "step: 2123, loss: 0.00012318490189500153\n",
      "step: 2124, loss: 0.00042429749737493694\n",
      "step: 2125, loss: 0.00042311352444812655\n",
      "step: 2126, loss: 0.001113344100303948\n",
      "step: 2127, loss: 0.004056979902088642\n",
      "step: 2128, loss: 0.00020534996292553842\n",
      "step: 2129, loss: 0.0001444238005205989\n",
      "step: 2130, loss: 0.00018300865485798568\n",
      "step: 2131, loss: 0.0006254839827306569\n",
      "step: 2132, loss: 0.0003898067807313055\n",
      "step: 2133, loss: 0.00022201538376975805\n",
      "step: 2134, loss: 0.03238363564014435\n",
      "step: 2135, loss: 0.00025368150090798736\n",
      "step: 2136, loss: 0.017680257558822632\n",
      "step: 2137, loss: 0.0002464406134095043\n",
      "step: 2138, loss: 0.0001055704997270368\n",
      "step: 2139, loss: 0.00046597421169281006\n",
      "step: 2140, loss: 0.00035888724960386753\n",
      "step: 2141, loss: 0.013146525248885155\n",
      "step: 2142, loss: 0.0010689556365832686\n",
      "step: 2143, loss: 0.0003214494790881872\n",
      "step: 2144, loss: 0.0005124043091200292\n",
      "step: 2145, loss: 0.0009240516810677946\n",
      "step: 2146, loss: 0.0007426172378472984\n",
      "step: 2147, loss: 0.0003907409554813057\n",
      "step: 2148, loss: 0.0005074945511296391\n",
      "step: 2149, loss: 0.00030693828011862934\n",
      "step: 2150, loss: 0.0006355078658089042\n",
      "step: 2151, loss: 0.00024839825346134603\n",
      "step: 2152, loss: 0.0004731736262328923\n",
      "step: 2153, loss: 0.0008086875313892961\n",
      "step: 2154, loss: 0.00015417637769132853\n",
      "step: 2155, loss: 0.00025353056844323874\n",
      "step: 2156, loss: 0.0006751660257577896\n",
      "step: 2157, loss: 0.00033682785579003394\n",
      "step: 2158, loss: 0.0005097006796859205\n",
      "step: 2159, loss: 8.07858377811499e-05\n",
      "step: 2160, loss: 0.00031180036603473127\n",
      "step: 2161, loss: 0.0007663529249839485\n",
      "step: 2162, loss: 0.0005021704128012061\n",
      "step: 2163, loss: 0.00014573750377167016\n",
      "step: 2164, loss: 0.009975598193705082\n",
      "step: 2165, loss: 0.0004922092775814235\n",
      "step: 2166, loss: 0.00011843781976494938\n",
      "step: 2167, loss: 0.00017874709737952799\n",
      "step: 2168, loss: 0.001051986706443131\n",
      "step: 2169, loss: 0.0003294754133094102\n",
      "step: 2170, loss: 0.0004523179668467492\n",
      "step: 2171, loss: 0.00014649001241195947\n",
      "step: 2172, loss: 0.0003374235238879919\n",
      "step: 2173, loss: 0.00026491167955100536\n",
      "step: 2174, loss: 0.00015322746185120195\n",
      "step: 2175, loss: 0.0005031208856962621\n",
      "step: 2176, loss: 0.001458105631172657\n",
      "step: 2177, loss: 0.0002458911039866507\n",
      "step: 2178, loss: 0.00018955701671075076\n",
      "step: 2179, loss: 0.0005698701133951545\n",
      "step: 2180, loss: 0.0003564906946849078\n",
      "step: 2181, loss: 0.0004957730416208506\n",
      "step: 2182, loss: 0.0002121453726431355\n",
      "step: 2183, loss: 0.00036673370050266385\n",
      "step: 2184, loss: 0.0009380565606988966\n",
      "step: 2185, loss: 0.021613599732518196\n",
      "step: 2186, loss: 0.00015317996439989656\n",
      "step: 2187, loss: 0.00019923456420656294\n",
      "step: 2188, loss: 0.0006920903106220067\n",
      "step: 2189, loss: 0.010752473026514053\n",
      "step: 2190, loss: 0.018996525555849075\n",
      "step: 2191, loss: 0.0007249937043525279\n",
      "step: 2192, loss: 0.0002228512748843059\n",
      "step: 2193, loss: 0.0006036716513335705\n",
      "step: 2194, loss: 8.89504881342873e-05\n",
      "step: 2195, loss: 0.00045383430551737547\n",
      "step: 2196, loss: 0.00031283203861676157\n",
      "step: 2197, loss: 0.00013340126315597445\n",
      "step: 2198, loss: 0.0004626262525562197\n",
      "step: 2199, loss: 0.0003477430436760187\n",
      "step: 2200, loss: 0.00038457420305348933\n",
      "step: 2201, loss: 0.0002493166830390692\n",
      "step: 2202, loss: 6.488100916612893e-05\n",
      "step: 2203, loss: 0.00022304084268398583\n",
      "step: 2204, loss: 0.00022547566913999617\n",
      "step: 2205, loss: 0.0005287728272378445\n",
      "step: 2206, loss: 0.0005360178183764219\n",
      "step: 2207, loss: 0.00030100467847660184\n",
      "step: 2208, loss: 0.00032745153293944895\n",
      "step: 2209, loss: 0.0001266081671928987\n",
      "step: 2210, loss: 0.00017784158990252763\n",
      "step: 2211, loss: 0.06114186719059944\n",
      "step: 2212, loss: 0.06454863399267197\n",
      "step: 2213, loss: 0.0003726510622072965\n",
      "step: 2214, loss: 0.0004537798813544214\n",
      "step: 2215, loss: 0.00013468392717186362\n",
      "step: 2216, loss: 0.0001312568929279223\n",
      "step: 2217, loss: 0.00027904054149985313\n",
      "step: 2218, loss: 0.0014652145328000188\n",
      "step: 2219, loss: 0.00013992736057844013\n",
      "step: 2220, loss: 0.00021826141164638102\n",
      "step: 2221, loss: 0.022212887182831764\n",
      "step: 2222, loss: 0.007913495413959026\n",
      "step: 2223, loss: 0.0002586674818303436\n",
      "step: 2224, loss: 0.0002424279082333669\n",
      "step: 2225, loss: 0.00032105352147482336\n",
      "step: 2226, loss: 0.01654624007642269\n",
      "step: 2227, loss: 0.0003886158810928464\n",
      "step: 2228, loss: 0.0016714605735614896\n",
      "step: 2229, loss: 0.00039015381480567157\n",
      "step: 2230, loss: 0.0002214460982941091\n",
      "step: 2231, loss: 0.00021295597252901644\n",
      "step: 2232, loss: 0.00022985458781477064\n",
      "step: 2233, loss: 0.0004646552260965109\n",
      "step: 2234, loss: 0.0011138289701193571\n",
      "step: 2235, loss: 0.0001926637050928548\n",
      "step: 2236, loss: 0.002494462765753269\n",
      "step: 2237, loss: 0.00023325617075897753\n",
      "step: 2238, loss: 0.00028302561258897185\n",
      "step: 2239, loss: 0.0002188250800827518\n",
      "step: 2240, loss: 0.04882130026817322\n",
      "step: 2241, loss: 0.0004884966183453798\n",
      "step: 2242, loss: 0.0003526056243572384\n",
      "step: 2243, loss: 0.0005122119328007102\n",
      "step: 2244, loss: 0.0003371365019120276\n",
      "step: 2245, loss: 0.00014333715080283582\n",
      "step: 2246, loss: 0.0007637700182385743\n",
      "step: 2247, loss: 0.0007877174066379666\n",
      "step: 2248, loss: 0.00031859250157140195\n",
      "step: 2249, loss: 0.0004953077295795083\n",
      "step: 2250, loss: 0.0003105680225417018\n",
      "step: 2251, loss: 0.00022416582214646041\n",
      "step: 2252, loss: 0.00012646867253351957\n",
      "step: 2253, loss: 0.0005400319350883365\n",
      "step: 2254, loss: 0.00024698831839486957\n",
      "step: 2255, loss: 0.0003834518720395863\n",
      "step: 2256, loss: 0.00047987085417844355\n",
      "step: 2257, loss: 0.00022781472944188863\n",
      "step: 2258, loss: 0.00045543990563601255\n",
      "step: 2259, loss: 0.05897720158100128\n",
      "step: 2260, loss: 0.0001460647617932409\n",
      "step: 2261, loss: 0.0002621641324367374\n",
      "step: 2262, loss: 0.0002318671758985147\n",
      "step: 2263, loss: 0.0006309832679107785\n",
      "step: 2264, loss: 0.00011029996676370502\n",
      "step: 2265, loss: 0.0003533773997332901\n",
      "step: 2266, loss: 0.006003556773066521\n",
      "step: 2267, loss: 0.0004815389693249017\n",
      "step: 2268, loss: 0.0002780018839985132\n",
      "step: 2269, loss: 0.00033175080898217857\n",
      "step: 2270, loss: 0.00012656048056669533\n",
      "step: 2271, loss: 0.00024884112644940615\n",
      "step: 2272, loss: 0.00027097403653897345\n",
      "step: 2273, loss: 8.112141949823126e-05\n",
      "step: 2274, loss: 0.0001229546614922583\n",
      "step: 2275, loss: 0.0003470739466138184\n",
      "step: 2276, loss: 7.81351700425148e-05\n",
      "step: 2277, loss: 0.000269690208369866\n",
      "step: 2278, loss: 7.497639307985082e-05\n",
      "step: 2279, loss: 0.00020550648332573473\n",
      "step: 2280, loss: 0.008886437863111496\n",
      "step: 2281, loss: 0.020626837387681007\n",
      "step: 2282, loss: 0.0002120430435752496\n",
      "step: 2283, loss: 0.00027145721833221614\n",
      "step: 2284, loss: 0.00015319985686801374\n",
      "step: 2285, loss: 0.0015666693216189742\n",
      "step: 2286, loss: 0.00027071370277553797\n",
      "step: 2287, loss: 0.033504124730825424\n",
      "step: 2288, loss: 0.00015491504746023566\n",
      "step: 2289, loss: 0.000755090790335089\n",
      "step: 2290, loss: 0.0007386694778688252\n",
      "step: 2291, loss: 0.0002704018261283636\n",
      "step: 2292, loss: 0.00029892809106968343\n",
      "step: 2293, loss: 0.026026306673884392\n",
      "step: 2294, loss: 0.00015258387429639697\n",
      "step: 2295, loss: 0.00034289283212274313\n",
      "step: 2296, loss: 0.0004299212887417525\n",
      "step: 2297, loss: 0.00016121575026772916\n",
      "step: 2298, loss: 0.0002901531697716564\n",
      "step: 2299, loss: 0.008036069571971893\n",
      "step: 2300, loss: 0.00019551994046196342\n",
      "step: 2301, loss: 0.0010119621874764562\n",
      "step: 2302, loss: 0.010738070122897625\n",
      "step: 2303, loss: 0.00015296219498850405\n",
      "step: 2304, loss: 0.0015809796750545502\n",
      "step: 2305, loss: 0.0026526362635195255\n",
      "step: 2306, loss: 0.0006086644716560841\n",
      "step: 2307, loss: 0.00033069911296479404\n",
      "step: 2308, loss: 0.00013173198385629803\n",
      "step: 2309, loss: 0.00044046060065738857\n",
      "step: 2310, loss: 0.00016009235696401447\n",
      "step: 2311, loss: 0.00042005450814031065\n",
      "step: 2312, loss: 0.00020616613619495183\n",
      "step: 2313, loss: 0.0001174664794234559\n",
      "step: 2314, loss: 0.00013855673023499548\n",
      "step: 2315, loss: 0.0005419942899607122\n",
      "step: 2316, loss: 0.0001290514483116567\n",
      "step: 2317, loss: 0.00020283561025280505\n",
      "step: 2318, loss: 0.036452766507864\n",
      "step: 2319, loss: 0.0005106730968691409\n",
      "step: 2320, loss: 0.00013720263086725026\n",
      "step: 2321, loss: 0.0003511756658554077\n",
      "step: 2322, loss: 0.00011886841821251437\n",
      "step: 2323, loss: 0.00014816064503975213\n",
      "step: 2324, loss: 0.0004643547290470451\n",
      "step: 2325, loss: 0.03376171737909317\n",
      "step: 2326, loss: 0.00037694713682867587\n",
      "step: 2327, loss: 0.00017612161173019558\n",
      "step: 2328, loss: 0.00013565804692916572\n",
      "step: 2329, loss: 0.0003054167900700122\n",
      "step: 2330, loss: 0.0003083547344431281\n",
      "step: 2331, loss: 0.00046874539111740887\n",
      "step: 2332, loss: 0.0004338949511293322\n",
      "step: 2333, loss: 0.00022551316942553967\n",
      "step: 2334, loss: 0.0005438149091787636\n",
      "step: 2335, loss: 0.00016586270066909492\n",
      "step: 2336, loss: 0.0001192460476886481\n",
      "step: 2337, loss: 0.0002870856551453471\n",
      "step: 2338, loss: 0.0003878124407492578\n",
      "step: 2339, loss: 0.0002495615044608712\n",
      "step: 2340, loss: 0.0001398493768647313\n",
      "step: 2341, loss: 7.936648762552068e-05\n",
      "step: 2342, loss: 0.00015366740990430117\n",
      "step: 2343, loss: 0.0003960389003623277\n",
      "step: 2344, loss: 0.00021700718207284808\n",
      "step: 2345, loss: 0.00029777808231301606\n",
      "step: 2346, loss: 0.00021508987993001938\n",
      "step: 2347, loss: 0.00035631400533020496\n",
      "step: 2348, loss: 0.0001524932449683547\n",
      "step: 2349, loss: 0.00028371528605930507\n",
      "step: 2350, loss: 0.0002782180090434849\n",
      "step: 2351, loss: 0.0002025528228841722\n",
      "step: 2352, loss: 0.0001700692664599046\n",
      "step: 2353, loss: 0.00020359449263196439\n",
      "step: 2354, loss: 0.00012939161388203502\n",
      "step: 2355, loss: 0.0002903291315305978\n",
      "step: 2356, loss: 0.0014092072378844023\n",
      "step: 2357, loss: 0.0003180626663379371\n",
      "step: 2358, loss: 0.00026803105720318854\n",
      "step: 2359, loss: 0.00015387983876280487\n",
      "step: 2360, loss: 0.0751919150352478\n",
      "step: 2361, loss: 0.00020220417354721576\n",
      "step: 2362, loss: 0.00021138746524229646\n",
      "step: 2363, loss: 0.0002950590569525957\n",
      "step: 2364, loss: 0.0003444952890276909\n",
      "step: 2365, loss: 0.0002318631304660812\n",
      "step: 2366, loss: 7.658419053768739e-05\n",
      "step: 2367, loss: 0.00015076735871843994\n",
      "step: 2368, loss: 0.0001438001199858263\n",
      "step: 2369, loss: 0.00020485417917370796\n",
      "step: 2370, loss: 0.00015352641639765352\n",
      "step: 2371, loss: 0.00012470378715079278\n",
      "step: 2372, loss: 7.953005842864513e-05\n",
      "step: 2373, loss: 0.0003557245363481343\n",
      "step: 2374, loss: 0.0004895030870102346\n",
      "step: 2375, loss: 0.0002427647850709036\n",
      "step: 2376, loss: 0.00016764597967267036\n",
      "step: 2377, loss: 0.0004114882030989975\n",
      "step: 2378, loss: 0.00017905904678627849\n",
      "step: 2379, loss: 0.00032750528771430254\n",
      "step: 2380, loss: 0.00025237788213416934\n",
      "step: 2381, loss: 0.00012748505105264485\n",
      "step: 2382, loss: 0.00022234076459426433\n",
      "step: 2383, loss: 0.00022682010603602976\n",
      "step: 2384, loss: 0.0007531256997026503\n",
      "step: 2385, loss: 0.00015845078451093286\n",
      "step: 2386, loss: 0.0005536245298571885\n",
      "step: 2387, loss: 0.00022198513033799827\n",
      "step: 2388, loss: 0.0002512106439098716\n",
      "step: 2389, loss: 0.0003252886235713959\n",
      "step: 2390, loss: 0.0002994260867126286\n",
      "step: 2391, loss: 0.00021682254737243056\n",
      "step: 2392, loss: 5.771860378445126e-05\n",
      "step: 2393, loss: 0.0004875428567174822\n",
      "step: 2394, loss: 0.0006642094813287258\n",
      "step: 2395, loss: 0.07107514888048172\n",
      "step: 2396, loss: 0.05616107955574989\n",
      "step: 2397, loss: 0.00038476608460769057\n",
      "step: 2398, loss: 0.0002526997704990208\n",
      "step: 2399, loss: 0.00014252617256715894\n",
      "step: 2400, loss: 0.0003396200481802225\n",
      "step: 2401, loss: 0.001480379607528448\n",
      "step: 2402, loss: 0.00011036153591703624\n",
      "step: 2403, loss: 0.00038186911842785776\n",
      "step: 2404, loss: 0.11608376353979111\n",
      "step: 2405, loss: 0.003929837606847286\n",
      "step: 2406, loss: 0.00012165890802862123\n",
      "step: 2407, loss: 0.000443177210399881\n",
      "step: 2408, loss: 0.0002350857830606401\n",
      "step: 2409, loss: 0.00022254003852140158\n",
      "step: 2410, loss: 0.00039312682929448783\n",
      "step: 2411, loss: 0.0005747848190367222\n",
      "step: 2412, loss: 0.00018632775754667819\n",
      "step: 2413, loss: 0.00025455362629145384\n",
      "step: 2414, loss: 0.00022256668307818472\n",
      "step: 2415, loss: 8.655616693431512e-05\n",
      "step: 2416, loss: 0.00010660955013008788\n",
      "step: 2417, loss: 0.00014726145309396088\n",
      "step: 2418, loss: 0.00025832682149484754\n",
      "step: 2419, loss: 0.00013046643289271742\n",
      "step: 2420, loss: 0.0001553720940137282\n",
      "step: 2421, loss: 0.00030452662031166255\n",
      "step: 2422, loss: 0.0002839314693119377\n",
      "step: 2423, loss: 0.0002968661137856543\n",
      "step: 2424, loss: 0.0003388668119441718\n",
      "step: 2425, loss: 0.0005136015824973583\n",
      "step: 2426, loss: 0.00013721446157433093\n",
      "step: 2427, loss: 0.00026152527425438166\n",
      "step: 2428, loss: 0.00015341895050369203\n",
      "step: 2429, loss: 0.0009250231669284403\n",
      "step: 2430, loss: 0.027490317821502686\n",
      "step: 2431, loss: 0.0007221558480523527\n",
      "step: 2432, loss: 0.00035704977926798165\n",
      "step: 2433, loss: 0.004524733871221542\n",
      "step: 2434, loss: 0.00022207631263881922\n",
      "step: 2435, loss: 0.00029263831675052643\n",
      "step: 2436, loss: 0.0002144851314369589\n",
      "step: 2437, loss: 0.0009519421146251261\n",
      "step: 2438, loss: 0.00020575437520164996\n",
      "step: 2439, loss: 0.0002922440180554986\n",
      "step: 2440, loss: 0.020441023632884026\n",
      "step: 2441, loss: 0.0001905750104924664\n",
      "step: 2442, loss: 0.0003779382968787104\n",
      "step: 2443, loss: 0.0003348979807924479\n",
      "step: 2444, loss: 0.0001908305857796222\n",
      "step: 2445, loss: 0.00015532871475443244\n",
      "step: 2446, loss: 0.00021393921633716673\n",
      "step: 2447, loss: 0.00039497314719483256\n",
      "step: 2448, loss: 0.0014615736436098814\n",
      "step: 2449, loss: 0.010034996084868908\n",
      "step: 2450, loss: 7.232274219859391e-05\n",
      "step: 2451, loss: 0.0206418689340353\n",
      "step: 2452, loss: 0.00046158480108715594\n",
      "step: 2453, loss: 0.00019020080799236894\n",
      "step: 2454, loss: 0.00016507998225279152\n",
      "step: 2455, loss: 8.361700020032004e-05\n",
      "step: 2456, loss: 0.0001769734953995794\n",
      "step: 2457, loss: 0.037848178297281265\n",
      "step: 2458, loss: 0.0008562374860048294\n",
      "step: 2459, loss: 0.000737687514629215\n",
      "step: 2460, loss: 0.000152781794895418\n",
      "step: 2461, loss: 0.0001227402244694531\n",
      "step: 2462, loss: 0.00021464609017129987\n",
      "step: 2463, loss: 0.0008668657974340022\n",
      "step: 2464, loss: 0.00020071300968993455\n",
      "step: 2465, loss: 8.207512291846797e-05\n",
      "step: 2466, loss: 0.00027820238028652966\n",
      "step: 2467, loss: 0.0001512922317488119\n",
      "step: 2468, loss: 0.00012234364112373441\n",
      "step: 2469, loss: 0.0003421271685510874\n",
      "step: 2470, loss: 0.00014724064385518432\n",
      "step: 2471, loss: 0.00022610115411225706\n",
      "step: 2472, loss: 0.03126181289553642\n",
      "step: 2473, loss: 0.00020425001275725663\n",
      "step: 2474, loss: 0.00033192691626027226\n",
      "step: 2475, loss: 0.0003413829253986478\n",
      "step: 2476, loss: 8.725350926397368e-05\n",
      "step: 2477, loss: 0.00013642007252201438\n",
      "step: 2478, loss: 0.00018452305812388659\n",
      "step: 2479, loss: 9.874247916741297e-05\n",
      "step: 2480, loss: 0.0007075482863001525\n",
      "step: 2481, loss: 0.0007582315593026578\n",
      "step: 2482, loss: 0.03922615945339203\n",
      "step: 2483, loss: 0.0005711893900297582\n",
      "step: 2484, loss: 7.082932279445231e-05\n",
      "step: 2485, loss: 0.0012270529987290502\n",
      "step: 2486, loss: 0.00016057529137469828\n",
      "step: 2487, loss: 0.0001409689721185714\n",
      "step: 2488, loss: 0.00027315071201883256\n",
      "step: 2489, loss: 0.0008835925254970789\n",
      "step: 2490, loss: 0.0004553296894300729\n",
      "step: 2491, loss: 0.00017984092119149864\n",
      "step: 2492, loss: 0.00015904629253782332\n",
      "step: 2493, loss: 0.00016139561193995178\n",
      "step: 2494, loss: 0.00019744731253013015\n",
      "step: 2495, loss: 0.0001867522660177201\n",
      "step: 2496, loss: 0.000305401801597327\n",
      "step: 2497, loss: 0.00045352132292464375\n",
      "step: 2498, loss: 0.06040280684828758\n",
      "step: 2499, loss: 0.0002948423207271844\n",
      "step: 2500, loss: 0.0002778465277515352\n",
      "step: 2501, loss: 0.00015864217130001634\n",
      "step: 2502, loss: 0.0002779951610136777\n",
      "step: 2503, loss: 0.00034150059218518436\n",
      "step: 2504, loss: 0.0009675367036834359\n",
      "step: 2505, loss: 8.559400885133073e-05\n",
      "step: 2506, loss: 0.0006089264643378556\n",
      "step: 2507, loss: 0.0003485354536678642\n",
      "step: 2508, loss: 0.0002111985522788018\n",
      "step: 2509, loss: 0.00017584387387614697\n",
      "step: 2510, loss: 6.785822188248858e-05\n",
      "step: 2511, loss: 0.00027426358428783715\n",
      "step: 2512, loss: 0.0001363793562632054\n",
      "step: 2513, loss: 0.00010628748714225367\n",
      "step: 2514, loss: 0.000251572928391397\n",
      "step: 2515, loss: 0.049079686403274536\n",
      "step: 2516, loss: 0.00024657335598021746\n",
      "step: 2517, loss: 0.00033507824991829693\n",
      "step: 2518, loss: 5.555909956456162e-05\n",
      "step: 2519, loss: 0.00024029803171288222\n",
      "step: 2520, loss: 0.000757937494199723\n",
      "step: 2521, loss: 0.00025539734633639455\n",
      "step: 2522, loss: 0.00032195114181376994\n",
      "step: 2523, loss: 0.0001348697696812451\n",
      "step: 2524, loss: 0.00018113192345481366\n",
      "step: 2525, loss: 6.306135037448257e-05\n",
      "step: 2526, loss: 0.00010145040869247168\n",
      "step: 2527, loss: 0.015803834423422813\n",
      "step: 2528, loss: 0.00026723381597548723\n",
      "step: 2529, loss: 7.40298128221184e-05\n",
      "step: 2530, loss: 0.000525664712768048\n",
      "step: 2531, loss: 0.000573646102566272\n",
      "step: 2532, loss: 0.00010870475671254098\n",
      "step: 2533, loss: 0.00026391263236291707\n",
      "step: 2534, loss: 0.00013397980364970863\n",
      "step: 2535, loss: 0.00016535610484424978\n",
      "step: 2536, loss: 0.00014577980618923903\n",
      "step: 2537, loss: 0.00011270499089732766\n",
      "step: 2538, loss: 0.00012262286327313632\n",
      "step: 2539, loss: 0.00020302545453887433\n",
      "step: 2540, loss: 0.000334690063027665\n",
      "step: 2541, loss: 0.0007854833384044468\n",
      "step: 2542, loss: 0.46946045756340027\n",
      "step: 2543, loss: 0.0008012109901756048\n",
      "step: 2544, loss: 0.00039650528924539685\n",
      "step: 2545, loss: 6.691770249744877e-05\n",
      "step: 2546, loss: 0.0021955061238259077\n",
      "step: 2547, loss: 0.00015136704314500093\n",
      "step: 2548, loss: 0.00010410029790364206\n",
      "step: 2549, loss: 0.030127422884106636\n",
      "step: 2550, loss: 0.00022743656882084906\n",
      "step: 2551, loss: 0.00016921812493819743\n",
      "step: 2552, loss: 0.00013772884267382324\n",
      "step: 2553, loss: 9.22935432754457e-05\n",
      "step: 2554, loss: 0.00015786514268256724\n",
      "step: 2555, loss: 0.04483538493514061\n",
      "step: 2556, loss: 0.0004833914281334728\n",
      "step: 2557, loss: 0.00027159418095834553\n",
      "step: 2558, loss: 7.761074812151492e-05\n",
      "step: 2559, loss: 0.00023047166178002954\n",
      "step: 2560, loss: 0.0004557879001367837\n",
      "step: 2561, loss: 0.00016322666488122195\n",
      "step: 2562, loss: 0.00021637465397361666\n",
      "step: 2563, loss: 0.008144652470946312\n",
      "step: 2564, loss: 0.001175390905700624\n",
      "step: 2565, loss: 0.0004898349288851023\n",
      "step: 2566, loss: 0.00011519741383381188\n",
      "step: 2567, loss: 0.0006404125015251338\n",
      "step: 2568, loss: 0.0007444425136782229\n",
      "step: 2569, loss: 0.0001463407970732078\n",
      "step: 2570, loss: 0.00017299944011028856\n",
      "step: 2571, loss: 0.0002221827453467995\n",
      "step: 2572, loss: 8.343812078237534e-05\n",
      "step: 2573, loss: 0.00022527553664986044\n",
      "step: 2574, loss: 0.0006852269289083779\n",
      "step: 2575, loss: 0.00028613919857889414\n",
      "step: 2576, loss: 0.04621655493974686\n",
      "step: 2577, loss: 0.000676523894071579\n",
      "step: 2578, loss: 0.00018738058861345053\n",
      "step: 2579, loss: 0.0006592866266146302\n",
      "step: 2580, loss: 0.00011778459884226322\n",
      "step: 2581, loss: 0.0005687580560334027\n",
      "step: 2582, loss: 0.0005452222540043294\n",
      "step: 2583, loss: 0.00033450775663368404\n",
      "step: 2584, loss: 0.0003056128625757992\n",
      "step: 2585, loss: 0.0001055958928191103\n",
      "step: 2586, loss: 0.00010162923717871308\n",
      "step: 2587, loss: 0.00032831067801453173\n",
      "step: 2588, loss: 0.0004294475947972387\n",
      "step: 2589, loss: 0.00030607031658291817\n",
      "step: 2590, loss: 0.0008206083439290524\n",
      "step: 2591, loss: 0.00014980105333961546\n",
      "step: 2592, loss: 0.0001207057575811632\n",
      "step: 2593, loss: 0.05833887681365013\n",
      "step: 2594, loss: 5.0015412853099406e-05\n",
      "step: 2595, loss: 0.0004302388406358659\n",
      "step: 2596, loss: 0.0001632588100619614\n",
      "step: 2597, loss: 0.0006589972763322294\n",
      "step: 2598, loss: 0.00016395014245063066\n",
      "step: 2599, loss: 0.012578598223626614\n",
      "step: 2600, loss: 0.00017322560597676784\n",
      "step: 2601, loss: 0.00027910788776353\n",
      "step: 2602, loss: 9.365290316054597e-05\n",
      "step: 2603, loss: 0.0005805597174912691\n",
      "step: 2604, loss: 0.00046133314026519656\n",
      "step: 2605, loss: 0.0014128524344414473\n",
      "step: 2606, loss: 0.0003164038062095642\n",
      "step: 2607, loss: 9.350905020255595e-05\n",
      "step: 2608, loss: 0.0006211005384102464\n",
      "step: 2609, loss: 0.00018709739379119128\n",
      "step: 2610, loss: 0.00013340111763682216\n",
      "step: 2611, loss: 0.0001416508894180879\n",
      "step: 2612, loss: 0.0001951945887412876\n",
      "step: 2613, loss: 0.00010700867278501391\n",
      "step: 2614, loss: 0.00010965691762976348\n",
      "step: 2615, loss: 0.00011121690477011725\n",
      "step: 2616, loss: 0.0001157100050477311\n",
      "step: 2617, loss: 5.877548028365709e-05\n",
      "step: 2618, loss: 0.00023526042059529573\n",
      "step: 2619, loss: 0.0005085950251668692\n",
      "step: 2620, loss: 6.930928793735802e-05\n",
      "step: 2621, loss: 0.00479107815772295\n",
      "step: 2622, loss: 0.00011411788000259548\n",
      "step: 2623, loss: 0.0001741188607411459\n",
      "step: 2624, loss: 0.00013179678353480995\n",
      "step: 2625, loss: 4.5896322262706235e-05\n",
      "step: 2626, loss: 0.00019820226589217782\n",
      "step: 2627, loss: 0.00043398301932029426\n",
      "step: 2628, loss: 0.00026054310728795826\n",
      "step: 2629, loss: 0.0003960122412536293\n",
      "step: 2630, loss: 0.03134766221046448\n",
      "step: 2631, loss: 0.00023476527712773532\n",
      "step: 2632, loss: 0.00014086546434555203\n",
      "step: 2633, loss: 0.0004155149217694998\n",
      "step: 2634, loss: 0.00015464237367268652\n",
      "step: 2635, loss: 0.0005194310215301812\n",
      "step: 2636, loss: 0.000119730771984905\n",
      "step: 2637, loss: 0.0003127332020085305\n",
      "step: 2638, loss: 0.0015780621906742454\n",
      "step: 2639, loss: 0.00013923190999776125\n",
      "step: 2640, loss: 0.00030438564135693014\n",
      "step: 2641, loss: 0.0003479158622212708\n",
      "step: 2642, loss: 0.00011258790618740022\n",
      "step: 2643, loss: 0.04873412102460861\n",
      "step: 2644, loss: 0.00031791083165444434\n",
      "step: 2645, loss: 0.0005537414690479636\n",
      "step: 2646, loss: 0.0005096973618492484\n",
      "step: 2647, loss: 6.455284892581403e-05\n",
      "step: 2648, loss: 0.00011871331662405282\n",
      "step: 2649, loss: 0.000218488959944807\n",
      "step: 2650, loss: 0.0005742626381106675\n",
      "step: 2651, loss: 7.028678373899311e-05\n",
      "step: 2652, loss: 0.0005265835206955671\n",
      "step: 2653, loss: 0.0003483884793240577\n",
      "step: 2654, loss: 0.0001839990000007674\n",
      "step: 2655, loss: 0.00828771572560072\n",
      "step: 2656, loss: 0.00013524986570701003\n",
      "step: 2657, loss: 0.0004505073884502053\n",
      "step: 2658, loss: 0.0013520946959033608\n",
      "step: 2659, loss: 0.00013824352936353534\n",
      "step: 2660, loss: 0.000923505867831409\n",
      "step: 2661, loss: 0.017523249611258507\n",
      "step: 2662, loss: 6.079704326111823e-05\n",
      "step: 2663, loss: 0.0007419087924063206\n",
      "step: 2664, loss: 0.00020337312889751047\n",
      "step: 2665, loss: 0.0006518685258924961\n",
      "step: 2666, loss: 8.495897054672241e-05\n",
      "step: 2667, loss: 0.00038400792982429266\n",
      "step: 2668, loss: 9.034547110786662e-05\n",
      "step: 2669, loss: 0.00022987132251728326\n",
      "step: 2670, loss: 7.786751666571945e-05\n",
      "step: 2671, loss: 0.00040490072569809854\n",
      "step: 2672, loss: 0.042048316448926926\n",
      "step: 2673, loss: 0.0002193661202909425\n",
      "step: 2674, loss: 0.023047734051942825\n",
      "step: 2675, loss: 9.6055657195393e-05\n",
      "step: 2676, loss: 5.65510054002516e-05\n",
      "step: 2677, loss: 0.00039997586281970143\n",
      "step: 2678, loss: 0.0003529780078679323\n",
      "step: 2679, loss: 0.00018701642693486065\n",
      "step: 2680, loss: 0.0001741986197885126\n",
      "step: 2681, loss: 0.0001834506547311321\n",
      "step: 2682, loss: 0.02303043194115162\n",
      "step: 2683, loss: 0.0005592537927441299\n",
      "step: 2684, loss: 0.022253012284636497\n",
      "step: 2685, loss: 0.001508532208390534\n",
      "step: 2686, loss: 0.00021656723401974887\n",
      "step: 2687, loss: 0.00018833059584721923\n",
      "step: 2688, loss: 0.0003539270255714655\n",
      "step: 2689, loss: 7.04507838236168e-05\n",
      "step: 2690, loss: 0.0663529634475708\n",
      "step: 2691, loss: 0.0004918119520880282\n",
      "step: 2692, loss: 0.00025741165154613554\n",
      "step: 2693, loss: 9.187410614686087e-05\n",
      "step: 2694, loss: 0.00034366160980425775\n",
      "step: 2695, loss: 0.0001195883669424802\n",
      "step: 2696, loss: 0.00013611999747809023\n",
      "step: 2697, loss: 0.06496985256671906\n",
      "step: 2698, loss: 0.0002614432305563241\n",
      "step: 2699, loss: 0.0003268041764385998\n",
      "step: 2700, loss: 0.00021662807557731867\n",
      "step: 2701, loss: 0.0001385340583510697\n",
      "step: 2702, loss: 0.00020705036877188832\n",
      "step: 2703, loss: 8.487779996357858e-05\n",
      "step: 2704, loss: 0.00016681570559740067\n",
      "step: 2705, loss: 0.00013446356751956046\n",
      "step: 2706, loss: 0.017760779708623886\n",
      "step: 2707, loss: 0.0003365914453752339\n",
      "step: 2708, loss: 0.00010878340981435031\n",
      "step: 2709, loss: 0.00018738707876764238\n",
      "step: 2710, loss: 0.015484984964132309\n",
      "step: 2711, loss: 0.000152528693433851\n",
      "step: 2712, loss: 8.618689753348008e-05\n",
      "step: 2713, loss: 0.0005634164554066956\n",
      "step: 2714, loss: 0.0001243850711034611\n",
      "step: 2715, loss: 0.00012812392378691584\n",
      "step: 2716, loss: 0.00024300023505929857\n",
      "step: 2717, loss: 0.00012975107529200613\n",
      "step: 2718, loss: 0.06341932713985443\n",
      "step: 2719, loss: 0.0003059199370909482\n",
      "step: 2720, loss: 0.00022501098283100873\n",
      "step: 2721, loss: 0.0005509374896064401\n",
      "step: 2722, loss: 0.0005483332788571715\n",
      "step: 2723, loss: 0.0003656197804957628\n",
      "step: 2724, loss: 0.0001124683694797568\n",
      "step: 2725, loss: 0.00014548042963724583\n",
      "step: 2726, loss: 0.0001136091523221694\n",
      "step: 2727, loss: 0.00020052187028340995\n",
      "step: 2728, loss: 0.00014769169501960278\n",
      "step: 2729, loss: 0.0003409243654459715\n",
      "step: 2730, loss: 0.0006800607079640031\n",
      "step: 2731, loss: 6.159977056086063e-05\n",
      "step: 2732, loss: 0.0002943006402347237\n",
      "step: 2733, loss: 0.00011288231326034293\n",
      "step: 2734, loss: 8.605131006333977e-05\n",
      "step: 2735, loss: 0.0001405345683451742\n",
      "step: 2736, loss: 0.000287991133518517\n",
      "step: 2737, loss: 0.0011442428221926093\n",
      "step: 2738, loss: 0.0001435970771126449\n",
      "step: 2739, loss: 0.00012475014955271035\n",
      "step: 2740, loss: 0.00909468438476324\n",
      "step: 2741, loss: 0.0002555851824581623\n",
      "step: 2742, loss: 7.624140562256798e-05\n",
      "step: 2743, loss: 6.91069508320652e-05\n",
      "step: 2744, loss: 0.00041479236097075045\n",
      "step: 2745, loss: 0.07425089180469513\n",
      "step: 2746, loss: 0.00020639544527512044\n",
      "step: 2747, loss: 0.018630556762218475\n",
      "step: 2748, loss: 0.0003152036224491894\n",
      "step: 2749, loss: 0.0015616960590705276\n",
      "step: 2750, loss: 0.00019207150035072118\n",
      "step: 2751, loss: 0.00022152285964693874\n",
      "step: 2752, loss: 0.0002862057590391487\n",
      "step: 2753, loss: 9.01005114428699e-05\n",
      "step: 2754, loss: 9.253194730263203e-05\n",
      "step: 2755, loss: 0.0001609760511200875\n",
      "step: 2756, loss: 0.00017369950364809483\n",
      "step: 2757, loss: 0.00011571896902751178\n",
      "step: 2758, loss: 0.0001842993515310809\n",
      "step: 2759, loss: 0.00024620891781523824\n",
      "step: 2760, loss: 7.441895286319777e-05\n",
      "step: 2761, loss: 9.557161683915183e-05\n",
      "step: 2762, loss: 0.000223086477490142\n",
      "step: 2763, loss: 0.00020355256856419146\n",
      "step: 2764, loss: 0.0006954369600862265\n",
      "step: 2765, loss: 0.00044176357914693654\n",
      "step: 2766, loss: 0.0007804891210980713\n",
      "step: 2767, loss: 0.0003670572186820209\n",
      "step: 2768, loss: 0.0014261798933148384\n",
      "step: 2769, loss: 0.0018949444638565183\n",
      "step: 2770, loss: 0.00019353341485839337\n",
      "step: 2771, loss: 0.00010665662557585165\n",
      "step: 2772, loss: 0.00507261510938406\n",
      "step: 2773, loss: 0.00023644980683457106\n",
      "step: 2774, loss: 0.0003800156991928816\n",
      "step: 2775, loss: 0.0002882054541260004\n",
      "step: 2776, loss: 5.716632222174667e-05\n",
      "step: 2777, loss: 0.00010556247434578836\n",
      "step: 2778, loss: 0.00020916314679197967\n",
      "step: 2779, loss: 0.06132792308926582\n",
      "step: 2780, loss: 0.00022509729024022818\n",
      "step: 2781, loss: 0.000643160252366215\n",
      "step: 2782, loss: 0.004087143111974001\n",
      "step: 2783, loss: 0.00605511711910367\n",
      "step: 2784, loss: 0.00018290079606231302\n",
      "step: 2785, loss: 0.0001418605970684439\n",
      "step: 2786, loss: 0.005846896208822727\n",
      "step: 2787, loss: 0.00014532064960803837\n",
      "step: 2788, loss: 0.000254698476055637\n",
      "step: 2789, loss: 0.00012683078239206225\n",
      "step: 2790, loss: 0.0001482159277657047\n",
      "step: 2791, loss: 0.002421114593744278\n",
      "step: 2792, loss: 0.06020420044660568\n",
      "step: 2793, loss: 0.00014489857130683959\n",
      "step: 2794, loss: 0.04520555958151817\n",
      "step: 2795, loss: 0.0002500388363841921\n",
      "step: 2796, loss: 0.00043848055065609515\n",
      "step: 2797, loss: 0.00035982951521873474\n",
      "step: 2798, loss: 0.04881764575839043\n",
      "step: 2799, loss: 7.795912824803963e-05\n",
      "step: 2800, loss: 0.0001734188263071701\n",
      "step: 2801, loss: 9.365309233544394e-05\n",
      "step: 2802, loss: 0.0004903385997749865\n",
      "step: 2803, loss: 0.00016547147242818028\n",
      "step: 2804, loss: 9.848929767031223e-05\n",
      "step: 2805, loss: 0.00018267793348059058\n",
      "step: 2806, loss: 0.0003224002430215478\n",
      "step: 2807, loss: 0.07804755866527557\n",
      "step: 2808, loss: 0.00010610874596750364\n",
      "step: 2809, loss: 0.0001646371529204771\n",
      "step: 2810, loss: 0.000226351257879287\n",
      "step: 2811, loss: 0.00010793692490551621\n",
      "step: 2812, loss: 0.013466380536556244\n",
      "step: 2813, loss: 0.0006431777728721499\n",
      "step: 2814, loss: 0.00010167711297981441\n",
      "step: 2815, loss: 0.00023425108520314097\n",
      "step: 2816, loss: 0.0002293188008479774\n",
      "step: 2817, loss: 0.00021584755450021476\n",
      "step: 2818, loss: 0.00010225635196547955\n",
      "step: 2819, loss: 0.005214412696659565\n",
      "step: 2820, loss: 0.00044603223796002567\n",
      "step: 2821, loss: 0.0001324710319750011\n",
      "step: 2822, loss: 9.848991612670943e-05\n",
      "step: 2823, loss: 0.0001344158372376114\n",
      "step: 2824, loss: 7.303756865439937e-05\n",
      "step: 2825, loss: 0.030412888154387474\n",
      "step: 2826, loss: 0.00023037067148834467\n",
      "step: 2827, loss: 4.678011828218587e-05\n",
      "step: 2828, loss: 0.03671935200691223\n",
      "step: 2829, loss: 0.00014904608542565256\n",
      "step: 2830, loss: 0.00012394401710480452\n",
      "step: 2831, loss: 0.00010868202662095428\n",
      "step: 2832, loss: 0.000482892181025818\n",
      "step: 2833, loss: 0.00012057081767125055\n",
      "step: 2834, loss: 0.0001986511197173968\n",
      "step: 2835, loss: 0.000555377802811563\n",
      "step: 2836, loss: 0.00012014670937787741\n",
      "step: 2837, loss: 0.00022075633751228452\n",
      "step: 2838, loss: 0.00015095366688910872\n",
      "step: 2839, loss: 0.051666684448719025\n",
      "step: 2840, loss: 0.00023535553191322833\n",
      "step: 2841, loss: 0.0003989305696450174\n",
      "step: 2842, loss: 6.0850172303617e-05\n",
      "step: 2843, loss: 0.0003002635494340211\n",
      "step: 2844, loss: 0.0007834581192582846\n",
      "step: 2845, loss: 0.0002466784499119967\n",
      "step: 2846, loss: 0.0001681249705143273\n",
      "step: 2847, loss: 0.0002443227276671678\n",
      "step: 2848, loss: 0.00011223368346691132\n",
      "step: 2849, loss: 0.0005547262262552977\n",
      "step: 2850, loss: 0.0004130568995606154\n",
      "step: 2851, loss: 0.00025701234699226916\n",
      "step: 2852, loss: 0.00013061947538517416\n",
      "step: 2853, loss: 8.980404527392238e-05\n",
      "step: 2854, loss: 9.721634705783799e-05\n",
      "step: 2855, loss: 0.00013468554243445396\n",
      "step: 2856, loss: 0.0012124122586101294\n",
      "step: 2857, loss: 0.00035810659755952656\n",
      "step: 2858, loss: 0.00029445893596857786\n",
      "step: 2859, loss: 0.00031743687577545643\n",
      "step: 2860, loss: 0.0003343694261275232\n",
      "step: 2861, loss: 0.00012659507046919316\n",
      "step: 2862, loss: 0.00024571450194343925\n",
      "step: 2863, loss: 9.285673149861395e-05\n",
      "step: 2864, loss: 0.000484683841932565\n",
      "step: 2865, loss: 0.0001133952391683124\n",
      "step: 2866, loss: 0.00012014401727356017\n",
      "step: 2867, loss: 0.00025061535416170955\n",
      "step: 2868, loss: 0.00022414315026253462\n",
      "step: 2869, loss: 2.9315280698938295e-05\n",
      "step: 2870, loss: 4.888584226137027e-05\n",
      "step: 2871, loss: 0.00020057246729265898\n",
      "step: 2872, loss: 9.882664016913623e-05\n",
      "step: 2873, loss: 0.0004409239045344293\n",
      "step: 2874, loss: 0.000525765004567802\n",
      "step: 2875, loss: 0.1521652787923813\n",
      "step: 2876, loss: 0.0004786864737980068\n",
      "step: 2877, loss: 7.815047138137743e-05\n",
      "step: 2878, loss: 0.00020198056881781667\n",
      "step: 2879, loss: 0.00010156675853068009\n",
      "step: 2880, loss: 3.553855640348047e-05\n",
      "step: 2881, loss: 0.00017268564261030406\n",
      "step: 2882, loss: 0.0001527856511529535\n",
      "step: 2883, loss: 0.0003541288897395134\n",
      "step: 2884, loss: 0.010646556504070759\n",
      "step: 2885, loss: 0.0007036950555630028\n",
      "step: 2886, loss: 0.00019145004625897855\n",
      "step: 2887, loss: 0.00022205158893484622\n",
      "step: 2888, loss: 0.00034014825359918177\n",
      "step: 2889, loss: 0.35229453444480896\n",
      "step: 2890, loss: 0.0024722202215343714\n",
      "step: 2891, loss: 6.823949661338702e-05\n",
      "step: 2892, loss: 7.87679964560084e-05\n",
      "step: 2893, loss: 0.00034913449781015515\n",
      "step: 2894, loss: 0.00034351018257439137\n",
      "step: 2895, loss: 0.0012141990009695292\n",
      "step: 2896, loss: 0.00016497982142027467\n",
      "step: 2897, loss: 0.00022583497047889978\n",
      "step: 2898, loss: 7.949499558890238e-05\n",
      "step: 2899, loss: 0.000153026296175085\n",
      "step: 2900, loss: 0.0001984050904866308\n",
      "step: 2901, loss: 0.0001450160489184782\n",
      "step: 2902, loss: 0.0002496183733455837\n",
      "step: 2903, loss: 0.0004106677952222526\n",
      "step: 2904, loss: 0.00011352314322721213\n",
      "step: 2905, loss: 0.0004280494758859277\n",
      "step: 2906, loss: 0.0005766741815023124\n",
      "step: 2907, loss: 0.0003590761625673622\n",
      "step: 2908, loss: 0.00068798812571913\n",
      "step: 2909, loss: 9.517520811641589e-05\n",
      "step: 2910, loss: 0.055839285254478455\n",
      "step: 2911, loss: 0.0004796805151272565\n",
      "step: 2912, loss: 0.0001607340236660093\n",
      "step: 2913, loss: 0.00024135468993335962\n",
      "step: 2914, loss: 0.00014998047845438123\n",
      "step: 2915, loss: 0.0007616955554112792\n",
      "step: 2916, loss: 0.0005139855202287436\n",
      "step: 2917, loss: 0.00027693648007698357\n",
      "step: 2918, loss: 0.0001455924502806738\n",
      "step: 2919, loss: 0.0005786286201328039\n",
      "step: 2920, loss: 0.00011361249926267192\n",
      "step: 2921, loss: 0.0001223505096277222\n",
      "step: 2922, loss: 9.648381092119962e-05\n",
      "step: 2923, loss: 0.0002474701323080808\n",
      "step: 2924, loss: 0.0008728682878427207\n",
      "step: 2925, loss: 0.0002163126482628286\n",
      "step: 2926, loss: 0.00013615633361041546\n",
      "step: 2927, loss: 0.024704832583665848\n",
      "step: 2928, loss: 0.0008550314814783633\n",
      "step: 2929, loss: 0.0003429189382586628\n",
      "step: 2930, loss: 0.02020174451172352\n",
      "step: 2931, loss: 0.0006637924234382808\n",
      "step: 2932, loss: 0.0020503804553300142\n",
      "step: 2933, loss: 0.00012664703535847366\n",
      "step: 2934, loss: 0.00020635142573155463\n",
      "step: 2935, loss: 7.7517528552562e-05\n",
      "step: 2936, loss: 0.00025891841505654156\n",
      "step: 2937, loss: 8.912698103813455e-05\n",
      "step: 2938, loss: 0.0008529041078872979\n",
      "step: 2939, loss: 0.0004471259890124202\n",
      "step: 2940, loss: 0.00012333874474279583\n",
      "step: 2941, loss: 0.00016753346426412463\n",
      "step: 2942, loss: 9.9186749139335e-05\n",
      "step: 2943, loss: 0.0009095736313611269\n",
      "step: 2944, loss: 0.008027959614992142\n",
      "step: 2945, loss: 0.0001378285960527137\n",
      "step: 2946, loss: 6.334041972877458e-05\n",
      "step: 2947, loss: 0.014846529811620712\n",
      "step: 2948, loss: 0.0008489275933243334\n",
      "step: 2949, loss: 9.30772876017727e-05\n",
      "step: 2950, loss: 0.00011631141387624666\n",
      "step: 2951, loss: 0.00029410031856969\n",
      "step: 2952, loss: 0.024913227185606956\n",
      "step: 2953, loss: 0.00013612391194328666\n",
      "step: 2954, loss: 0.00014700180327054113\n",
      "step: 2955, loss: 0.0007037224131636322\n",
      "step: 2956, loss: 0.000329757749568671\n",
      "step: 2957, loss: 0.0002139458229066804\n",
      "step: 2958, loss: 0.0003161093918606639\n",
      "step: 2959, loss: 0.015243534930050373\n",
      "step: 2960, loss: 0.00025689988979138434\n",
      "step: 2961, loss: 9.914377005770802e-05\n",
      "step: 2962, loss: 0.00017629146168474108\n",
      "step: 2963, loss: 0.00020415168546605855\n",
      "step: 2964, loss: 0.2567916810512543\n",
      "step: 2965, loss: 0.0001967732241610065\n",
      "step: 2966, loss: 0.0002685284707695246\n",
      "step: 2967, loss: 5.6500848586438224e-05\n",
      "step: 2968, loss: 4.979259756510146e-05\n",
      "step: 2969, loss: 6.675521581200883e-05\n",
      "step: 2970, loss: 0.00021385088621173054\n",
      "step: 2971, loss: 0.0004889731644652784\n",
      "step: 2972, loss: 0.0001481550425523892\n",
      "step: 2973, loss: 6.22915686108172e-05\n",
      "step: 2974, loss: 0.00012163464998593554\n",
      "step: 2975, loss: 0.00052432413212955\n",
      "step: 2976, loss: 0.00013381494500208646\n",
      "step: 2977, loss: 0.029018037021160126\n",
      "step: 2978, loss: 0.006685158703476191\n",
      "step: 2979, loss: 0.0001670929486863315\n",
      "step: 2980, loss: 8.137303666444495e-05\n",
      "step: 2981, loss: 0.018063172698020935\n",
      "step: 2982, loss: 0.0001254482049262151\n",
      "step: 2983, loss: 7.56976951379329e-05\n",
      "step: 2984, loss: 0.0005573267117142677\n",
      "step: 2985, loss: 0.00011042319238185883\n",
      "step: 2986, loss: 0.00026511208852753043\n",
      "step: 2987, loss: 0.0027786879800260067\n",
      "step: 2988, loss: 0.000445700396085158\n",
      "step: 2989, loss: 0.06903897225856781\n",
      "step: 2990, loss: 0.0004421664052642882\n",
      "step: 2991, loss: 8.404555410379544e-05\n",
      "step: 2992, loss: 0.00012919634173158556\n",
      "step: 2993, loss: 0.00020038483489770442\n",
      "step: 2994, loss: 0.0002348307316424325\n",
      "step: 2995, loss: 0.00020392294391058385\n",
      "step: 2996, loss: 0.0002717292227316648\n",
      "step: 2997, loss: 9.784129360923544e-05\n",
      "step: 2998, loss: 8.767577674007043e-05\n",
      "step: 2999, loss: 0.00013639553799293935\n",
      "step: 3000, loss: 0.030158547684550285\n",
      "step: 3001, loss: 0.00013587874127551913\n",
      "step: 3002, loss: 0.000191211118362844\n",
      "step: 3003, loss: 0.02212996408343315\n",
      "step: 3004, loss: 0.015782635658979416\n",
      "step: 3005, loss: 5.969993435428478e-05\n",
      "step: 3006, loss: 0.0005198391154408455\n",
      "step: 3007, loss: 0.00012361496919766068\n",
      "step: 3008, loss: 0.00014747008390258998\n",
      "step: 3009, loss: 0.00010641465632943437\n",
      "step: 3010, loss: 0.0002293793368153274\n",
      "step: 3011, loss: 3.8617454265477136e-05\n",
      "step: 3012, loss: 8.128929766826332e-05\n",
      "step: 3013, loss: 0.00016344858158845454\n",
      "step: 3014, loss: 0.00014660651504527777\n",
      "step: 3015, loss: 0.0003287244471721351\n",
      "step: 3016, loss: 0.00016104249516502023\n",
      "step: 3017, loss: 0.00011712012928910553\n",
      "step: 3018, loss: 0.000152355307363905\n",
      "step: 3019, loss: 0.00021359931270126253\n",
      "step: 3020, loss: 0.08079174906015396\n",
      "step: 3021, loss: 0.0001488205452915281\n",
      "step: 3022, loss: 0.0003294200578238815\n",
      "step: 3023, loss: 8.05094387033023e-05\n",
      "step: 3024, loss: 0.0001962538663065061\n",
      "step: 3025, loss: 9.387363388668746e-05\n",
      "step: 3026, loss: 0.00016080433852039278\n",
      "step: 3027, loss: 6.36408367427066e-05\n",
      "step: 3028, loss: 0.0004554583574645221\n",
      "step: 3029, loss: 7.638683018740267e-05\n",
      "step: 3030, loss: 0.00011070360778830945\n",
      "step: 3031, loss: 7.476573955500498e-05\n",
      "step: 3032, loss: 8.250319660874084e-05\n",
      "step: 3033, loss: 0.0005901546101085842\n",
      "step: 3034, loss: 0.0006353491335175931\n",
      "step: 3035, loss: 0.0007204846478998661\n",
      "step: 3036, loss: 0.00017293929704464972\n",
      "step: 3037, loss: 8.121127029880881e-05\n",
      "step: 3038, loss: 0.00013708355254493654\n",
      "step: 3039, loss: 0.0001935722102643922\n",
      "step: 3040, loss: 0.00012663494271691889\n",
      "step: 3041, loss: 0.00013067666441202164\n",
      "step: 3042, loss: 0.0001276620605494827\n",
      "step: 3043, loss: 0.00018485714099369943\n",
      "step: 3044, loss: 0.009543957188725471\n",
      "step: 3045, loss: 0.00019384529150556773\n",
      "step: 3046, loss: 0.00017663226753938943\n",
      "step: 3047, loss: 0.00015664671082049608\n",
      "step: 3048, loss: 0.00021770159946754575\n",
      "step: 3049, loss: 0.0002454548084642738\n",
      "step: 3050, loss: 0.0004690172499977052\n",
      "step: 3051, loss: 0.0005266307853162289\n",
      "step: 3052, loss: 0.00016376077837776393\n",
      "step: 3053, loss: 0.00020088946621399373\n",
      "step: 3054, loss: 0.00019915540178772062\n",
      "step: 3055, loss: 8.682585030328482e-05\n",
      "step: 3056, loss: 0.0004207171150483191\n",
      "step: 3057, loss: 0.0038714236579835415\n",
      "step: 3058, loss: 0.00021474836103152484\n",
      "step: 3059, loss: 7.807704241713509e-05\n",
      "step: 3060, loss: 0.001057168236002326\n",
      "step: 3061, loss: 0.00019982502271886915\n",
      "step: 3062, loss: 0.0001442040374968201\n",
      "step: 3063, loss: 0.00016573113680351526\n",
      "step: 3064, loss: 0.00014277415175456554\n",
      "step: 3065, loss: 0.00017027213471010327\n",
      "step: 3066, loss: 9.537152800476179e-05\n",
      "step: 3067, loss: 0.0003360161790624261\n",
      "step: 3068, loss: 5.9838526794919744e-05\n",
      "step: 3069, loss: 0.00017924592248164117\n",
      "step: 3070, loss: 0.00038955596392042935\n",
      "step: 3071, loss: 6.846812175353989e-05\n",
      "step: 3072, loss: 0.0001419898180756718\n",
      "step: 3073, loss: 0.00043542179628275335\n",
      "step: 3074, loss: 0.00015173628344200552\n",
      "step: 3075, loss: 4.420838376972824e-05\n",
      "step: 3076, loss: 0.0001112988029490225\n",
      "step: 3077, loss: 0.0023944550193846226\n",
      "step: 3078, loss: 0.0007965672994032502\n",
      "step: 3079, loss: 0.0002528279146645218\n",
      "step: 3080, loss: 0.11539439111948013\n",
      "step: 3081, loss: 0.0007632793276570737\n",
      "step: 3082, loss: 7.880961493356153e-05\n",
      "step: 3083, loss: 0.00019434720161370933\n",
      "step: 3084, loss: 0.00025354375247843564\n",
      "step: 3085, loss: 0.00042419458623044193\n",
      "step: 3086, loss: 0.0001374154380755499\n",
      "step: 3087, loss: 0.0001252125803148374\n",
      "step: 3088, loss: 0.0007446028175763786\n",
      "step: 3089, loss: 0.0004379973979666829\n",
      "step: 3090, loss: 7.957444177009165e-05\n",
      "step: 3091, loss: 0.0002909416798502207\n",
      "step: 3092, loss: 6.973637209739536e-05\n",
      "step: 3093, loss: 0.011258171871304512\n",
      "step: 3094, loss: 0.00042242149356752634\n",
      "step: 3095, loss: 0.000103482321719639\n",
      "step: 3096, loss: 0.0001451861608074978\n",
      "step: 3097, loss: 0.001182024134323001\n",
      "step: 3098, loss: 0.0001260674325749278\n",
      "step: 3099, loss: 8.106115274131298e-05\n",
      "step: 3100, loss: 9.995272557716817e-05\n",
      "step: 3101, loss: 0.00019068036635871977\n",
      "step: 3102, loss: 0.0008647702634334564\n",
      "step: 3103, loss: 0.0004295471007935703\n",
      "step: 3104, loss: 5.985783354844898e-05\n",
      "step: 3105, loss: 0.0005606933846138418\n",
      "step: 3106, loss: 0.00013962808588985354\n",
      "step: 3107, loss: 0.02370450273156166\n",
      "step: 3108, loss: 6.703397957608104e-05\n",
      "step: 3109, loss: 0.00035982855479232967\n",
      "step: 3110, loss: 0.0002358759375056252\n",
      "step: 3111, loss: 0.00023448480351362377\n",
      "step: 3112, loss: 0.0002505732118152082\n",
      "step: 3113, loss: 0.00013119980576448143\n",
      "step: 3114, loss: 0.00012404264998622239\n",
      "step: 3115, loss: 0.0007389003876596689\n",
      "step: 3116, loss: 0.0002105037128785625\n",
      "step: 3117, loss: 0.00019365981279406697\n",
      "step: 3118, loss: 0.0002583202440291643\n",
      "step: 3119, loss: 0.00010643230780260637\n",
      "step: 3120, loss: 0.0001698538544587791\n",
      "step: 3121, loss: 0.00014444423140957952\n",
      "step: 3122, loss: 0.00026347971288487315\n",
      "step: 3123, loss: 0.0002400855883024633\n",
      "step: 3124, loss: 0.00029836816247552633\n",
      "step: 3125, loss: 0.00016625173157081008\n",
      "step: 3126, loss: 0.00011018712393706664\n",
      "step: 3127, loss: 0.0001795799907995388\n",
      "step: 3128, loss: 9.992172999773175e-05\n",
      "step: 3129, loss: 0.0001289392967009917\n",
      "step: 3130, loss: 0.00011631337838480249\n",
      "step: 3131, loss: 0.00014873062900733203\n",
      "step: 3132, loss: 0.0003067170036956668\n",
      "step: 3133, loss: 9.459021384827793e-05\n",
      "step: 3134, loss: 0.00015166442608460784\n",
      "step: 3135, loss: 0.0001616960798855871\n",
      "step: 3136, loss: 0.00016198933008126915\n",
      "step: 3137, loss: 0.00013264481094665825\n",
      "step: 3138, loss: 0.031874507665634155\n",
      "step: 3139, loss: 0.00012604129733517766\n",
      "step: 3140, loss: 0.0010115917539224029\n",
      "step: 3141, loss: 0.00012661163054872304\n",
      "step: 3142, loss: 0.02298945188522339\n",
      "step: 3143, loss: 0.00011338013428030536\n",
      "step: 3144, loss: 0.000689254084136337\n",
      "step: 3145, loss: 9.47448643273674e-05\n",
      "step: 3146, loss: 6.717242649756372e-05\n",
      "step: 3147, loss: 0.00017772256978787482\n",
      "step: 3148, loss: 5.1593626267276704e-05\n",
      "step: 3149, loss: 0.000104842874861788\n",
      "step: 3150, loss: 0.0006722353864461184\n",
      "step: 3151, loss: 6.847429904155433e-05\n",
      "step: 3152, loss: 0.063558429479599\n",
      "step: 3153, loss: 0.0008074966608546674\n",
      "step: 3154, loss: 4.4818247260991484e-05\n",
      "step: 3155, loss: 0.0002812733582686633\n",
      "step: 3156, loss: 0.08458087593317032\n",
      "step: 3157, loss: 0.00018843782891053706\n",
      "step: 3158, loss: 0.020509373396635056\n",
      "step: 3159, loss: 5.6918823247542605e-05\n",
      "step: 3160, loss: 0.00026013856404460967\n",
      "step: 3161, loss: 0.0001737277052598074\n",
      "step: 3162, loss: 6.656462937826291e-05\n",
      "step: 3163, loss: 6.044780457159504e-05\n",
      "step: 3164, loss: 0.0002408303989795968\n",
      "step: 3165, loss: 0.00030402163974940777\n",
      "step: 3166, loss: 3.442502929829061e-05\n",
      "step: 3167, loss: 0.00018704475951381028\n",
      "step: 3168, loss: 0.0003826582687906921\n",
      "step: 3169, loss: 7.946845289552584e-05\n",
      "step: 3170, loss: 0.0002564263704698533\n",
      "step: 3171, loss: 5.196192432777025e-05\n",
      "step: 3172, loss: 6.679359648842365e-05\n",
      "step: 3173, loss: 0.0024137345608323812\n",
      "step: 3174, loss: 7.836932491045445e-05\n",
      "step: 3175, loss: 4.690220521297306e-05\n",
      "step: 3176, loss: 4.936804180033505e-05\n",
      "step: 3177, loss: 8.929239265853539e-05\n",
      "step: 3178, loss: 0.0001777328143361956\n",
      "step: 3179, loss: 0.00012673114542849362\n",
      "step: 3180, loss: 8.262376650236547e-05\n",
      "step: 3181, loss: 0.04352547228336334\n",
      "step: 3182, loss: 0.00011126251047244295\n",
      "step: 3183, loss: 7.969652506290004e-05\n",
      "step: 3184, loss: 0.00024110126832965761\n",
      "step: 3185, loss: 8.441870886599645e-05\n",
      "step: 3186, loss: 0.0003974847204517573\n",
      "step: 3187, loss: 7.852513954276219e-05\n",
      "step: 3188, loss: 0.00010524752724450082\n",
      "step: 3189, loss: 5.9007052186643705e-05\n",
      "step: 3190, loss: 9.088440128834918e-05\n",
      "step: 3191, loss: 0.00017586362082511187\n",
      "step: 3192, loss: 0.00014136047684587538\n",
      "step: 3193, loss: 8.539526606909931e-05\n",
      "step: 3194, loss: 6.075223063817248e-05\n",
      "step: 3195, loss: 0.00019183455151505768\n",
      "step: 3196, loss: 0.00019107013940811157\n",
      "step: 3197, loss: 0.0005461468826979399\n",
      "step: 3198, loss: 0.0002886803704313934\n",
      "step: 3199, loss: 0.0002568352210801095\n",
      "step: 3200, loss: 0.00032887569977901876\n",
      "step: 3201, loss: 0.0001802073820726946\n",
      "step: 3202, loss: 0.0001377285225316882\n",
      "step: 3203, loss: 0.00037283875280991197\n",
      "step: 3204, loss: 6.415021925931796e-05\n",
      "step: 3205, loss: 0.0008395969634875655\n",
      "step: 3206, loss: 0.00014034482592251152\n",
      "step: 3207, loss: 6.416373798856512e-05\n",
      "step: 3208, loss: 0.00020217604469507933\n",
      "step: 3209, loss: 8.961266576079652e-05\n",
      "step: 3210, loss: 6.856766412965953e-05\n",
      "step: 3211, loss: 0.002473280532285571\n",
      "step: 3212, loss: 7.079874922055751e-05\n",
      "step: 3213, loss: 6.418083648895845e-05\n",
      "step: 3214, loss: 0.00020598187984433025\n",
      "step: 3215, loss: 0.0001499400386819616\n",
      "step: 3216, loss: 0.0001555784692754969\n",
      "step: 3217, loss: 6.966645014472306e-05\n",
      "step: 3218, loss: 0.00011138543777633458\n",
      "step: 3219, loss: 3.7620062357746065e-05\n",
      "step: 3220, loss: 0.00013447701348923147\n",
      "step: 3221, loss: 0.0001362389448331669\n",
      "step: 3222, loss: 0.0001368321100017056\n",
      "step: 3223, loss: 0.00018688566342461854\n",
      "step: 3224, loss: 8.460813842248172e-05\n",
      "step: 3225, loss: 0.00024281564401462674\n",
      "step: 3226, loss: 0.017224561423063278\n",
      "step: 3227, loss: 0.0002360045473324135\n",
      "step: 3228, loss: 9.259312355425209e-05\n",
      "step: 3229, loss: 0.00021324034605640918\n",
      "step: 3230, loss: 7.476044265786186e-05\n",
      "step: 3231, loss: 5.431579120340757e-05\n",
      "step: 3232, loss: 0.0013185185380280018\n",
      "step: 3233, loss: 0.00010482992365723476\n",
      "step: 3234, loss: 8.826896373648196e-05\n",
      "step: 3235, loss: 6.012583617120981e-05\n",
      "step: 3236, loss: 0.00011945646110689268\n",
      "step: 3237, loss: 0.036681048572063446\n",
      "step: 3238, loss: 5.575838076765649e-05\n",
      "step: 3239, loss: 0.000216946194996126\n",
      "step: 3240, loss: 0.00042504406883381307\n",
      "step: 3241, loss: 0.00011010022717528045\n",
      "step: 3242, loss: 0.0002374500036239624\n",
      "step: 3243, loss: 0.00013254776422400028\n",
      "step: 3244, loss: 0.0001305850746575743\n",
      "step: 3245, loss: 0.010103092528879642\n",
      "step: 3246, loss: 0.00022390483354683965\n",
      "step: 3247, loss: 0.000529846060089767\n",
      "step: 3248, loss: 5.4795695177745074e-05\n",
      "step: 3249, loss: 0.02333703823387623\n",
      "step: 3250, loss: 9.069748193724081e-05\n",
      "step: 3251, loss: 4.5339082134887576e-05\n",
      "step: 3252, loss: 0.04659504443407059\n",
      "step: 3253, loss: 0.00022782433370593935\n",
      "step: 3254, loss: 0.00015514562255702913\n",
      "step: 3255, loss: 8.676813740748912e-05\n",
      "step: 3256, loss: 0.00020064740965608507\n",
      "step: 3257, loss: 0.00015916820848360658\n",
      "step: 3258, loss: 0.00018165126675739884\n",
      "step: 3259, loss: 0.00011159227869939059\n",
      "step: 3260, loss: 8.601107401773334e-05\n",
      "step: 3261, loss: 0.0001795787684386596\n",
      "step: 3262, loss: 0.00019693585636559874\n",
      "step: 3263, loss: 0.00012213707668706775\n",
      "step: 3264, loss: 0.00018326374993193895\n",
      "step: 3265, loss: 0.00014405499678105116\n",
      "step: 3266, loss: 0.00011492660269141197\n",
      "step: 3267, loss: 5.439401502371766e-05\n",
      "step: 3268, loss: 3.188529808539897e-05\n",
      "step: 3269, loss: 0.00048105831956490874\n",
      "step: 3270, loss: 0.0012367302551865578\n",
      "step: 3271, loss: 0.00018225004896521568\n",
      "step: 3272, loss: 0.0638829916715622\n",
      "step: 3273, loss: 0.00016720377607271075\n",
      "step: 3274, loss: 0.0002344519889447838\n",
      "step: 3275, loss: 8.804367098491639e-05\n",
      "step: 3276, loss: 0.029858384281396866\n",
      "step: 3277, loss: 0.006167967803776264\n",
      "step: 3278, loss: 0.0005364425596781075\n",
      "step: 3279, loss: 0.018817631527781487\n",
      "step: 3280, loss: 0.0008776708855293691\n",
      "step: 3281, loss: 9.669004793977365e-05\n",
      "step: 3282, loss: 0.00018875581736210734\n",
      "step: 3283, loss: 0.0003272800531703979\n",
      "step: 3284, loss: 0.00012420843995641917\n",
      "step: 3285, loss: 8.40746215544641e-05\n",
      "step: 3286, loss: 0.00039035669760778546\n",
      "step: 3287, loss: 0.0005446792347356677\n",
      "step: 3288, loss: 0.0001518679055152461\n",
      "step: 3289, loss: 0.0004682411381509155\n",
      "step: 3290, loss: 0.00020796713943127543\n",
      "step: 3291, loss: 9.071370732272044e-05\n",
      "step: 3292, loss: 0.00014354620361700654\n",
      "step: 3293, loss: 0.0001995674829231575\n",
      "step: 3294, loss: 0.00020984919683542103\n",
      "step: 3295, loss: 0.00016209491877816617\n",
      "step: 3296, loss: 0.06274277716875076\n",
      "step: 3297, loss: 0.00015212340804282576\n",
      "step: 3298, loss: 0.005020126700401306\n",
      "step: 3299, loss: 3.477075006230734e-05\n",
      "step: 3300, loss: 0.00015332912153098732\n",
      "step: 3301, loss: 0.0001678591943345964\n",
      "step: 3302, loss: 0.0001524511317256838\n",
      "step: 3303, loss: 0.02825954742729664\n",
      "step: 3304, loss: 0.0005145930917933583\n",
      "step: 3305, loss: 0.0001454625598853454\n",
      "step: 3306, loss: 0.00014240696327760816\n",
      "step: 3307, loss: 0.00012830839841626585\n",
      "step: 3308, loss: 6.595407467102632e-05\n",
      "step: 3309, loss: 4.441959026735276e-05\n",
      "step: 3310, loss: 0.0003733244084287435\n",
      "step: 3311, loss: 0.00012499514559749514\n",
      "step: 3312, loss: 4.657534009311348e-05\n",
      "step: 3313, loss: 0.0002534858649596572\n",
      "step: 3314, loss: 0.0003293595800641924\n",
      "step: 3315, loss: 0.0007139137596823275\n",
      "step: 3316, loss: 0.00010088738054037094\n",
      "step: 3317, loss: 0.0003195536555722356\n",
      "step: 3318, loss: 0.00020158996630925685\n",
      "step: 3319, loss: 5.9988022258039564e-05\n",
      "step: 3320, loss: 0.009504619054496288\n",
      "step: 3321, loss: 0.00010976976045640185\n",
      "step: 3322, loss: 0.00019941231585107744\n",
      "step: 3323, loss: 8.830748265609145e-05\n",
      "step: 3324, loss: 0.00011873540643136948\n",
      "step: 3325, loss: 0.07564510405063629\n",
      "step: 3326, loss: 0.0003141665074508637\n",
      "step: 3327, loss: 3.6663474020315334e-05\n",
      "step: 3328, loss: 4.041156353196129e-05\n",
      "step: 3329, loss: 6.837915861979127e-05\n",
      "step: 3330, loss: 0.021352795884013176\n",
      "step: 3331, loss: 9.855369717115536e-05\n",
      "step: 3332, loss: 9.999092435464263e-05\n",
      "step: 3333, loss: 7.624996214872226e-05\n",
      "step: 3334, loss: 0.0001500025682616979\n",
      "step: 3335, loss: 0.00024188784300349653\n",
      "step: 3336, loss: 0.03818412497639656\n",
      "step: 3337, loss: 0.0001588302111485973\n",
      "step: 3338, loss: 0.00010290942009305581\n",
      "step: 3339, loss: 0.00010340996232116595\n",
      "step: 3340, loss: 0.00019511501886881888\n",
      "step: 3341, loss: 5.449726086226292e-05\n",
      "step: 3342, loss: 0.00048153623356483877\n",
      "step: 3343, loss: 0.00015531359531451017\n",
      "step: 3344, loss: 7.374600681941956e-05\n",
      "step: 3345, loss: 0.00019542925292626023\n",
      "step: 3346, loss: 0.00027127645444124937\n",
      "step: 3347, loss: 3.434425525483675e-05\n",
      "step: 3348, loss: 0.0003477831487543881\n",
      "step: 3349, loss: 0.0008835154585540295\n",
      "step: 3350, loss: 0.0005746839451603591\n",
      "step: 3351, loss: 0.00016500279889442027\n",
      "step: 3352, loss: 8.956046804087237e-05\n",
      "step: 3353, loss: 9.410760685568675e-05\n",
      "step: 3354, loss: 9.458577551413327e-05\n",
      "step: 3355, loss: 6.572520942427218e-05\n",
      "step: 3356, loss: 0.00016772559320088476\n",
      "step: 3357, loss: 0.0014720677863806486\n",
      "step: 3358, loss: 0.00023075053468346596\n",
      "step: 3359, loss: 0.03587866947054863\n",
      "step: 3360, loss: 0.0001486726978328079\n",
      "step: 3361, loss: 0.00027822700212709606\n",
      "step: 3362, loss: 0.00014118874969426543\n",
      "step: 3363, loss: 0.00014130216732155532\n",
      "step: 3364, loss: 0.00020162088912911713\n",
      "step: 3365, loss: 0.0002814781619235873\n",
      "step: 3366, loss: 0.00017249229131266475\n",
      "step: 3367, loss: 7.345587073359638e-05\n",
      "step: 3368, loss: 0.009084626100957394\n",
      "step: 3369, loss: 4.721118966699578e-05\n",
      "step: 3370, loss: 0.00010724636376835406\n",
      "step: 3371, loss: 5.476048318087123e-05\n",
      "step: 3372, loss: 0.00028079855837859213\n",
      "step: 3373, loss: 0.00028297040262259543\n",
      "step: 3374, loss: 0.000815622741356492\n",
      "step: 3375, loss: 0.00034959850017912686\n",
      "step: 3376, loss: 0.00013812817633152008\n",
      "step: 3377, loss: 0.00012028393393848091\n",
      "step: 3378, loss: 0.0002713153953664005\n",
      "step: 3379, loss: 0.0005500143161043525\n",
      "step: 3380, loss: 7.218346581794322e-05\n",
      "step: 3381, loss: 0.00023116219381336123\n",
      "step: 3382, loss: 0.0001123174442909658\n",
      "step: 3383, loss: 0.0005386940320022404\n",
      "step: 3384, loss: 0.00021969032241031528\n",
      "step: 3385, loss: 0.00021738400391768664\n",
      "step: 3386, loss: 5.547325054067187e-05\n",
      "step: 3387, loss: 9.726340067572892e-05\n",
      "step: 3388, loss: 0.00026928784791380167\n",
      "step: 3389, loss: 0.0002454692148603499\n",
      "step: 3390, loss: 8.666688518133014e-05\n",
      "step: 3391, loss: 0.010791786015033722\n",
      "step: 3392, loss: 9.484011388849467e-05\n",
      "step: 3393, loss: 0.00015975534915924072\n",
      "step: 3394, loss: 6.048886280041188e-05\n",
      "step: 3395, loss: 0.00011167378397658467\n",
      "step: 3396, loss: 5.532412251341157e-05\n",
      "step: 3397, loss: 0.000104859696875792\n",
      "step: 3398, loss: 0.00028208227013237774\n",
      "step: 3399, loss: 0.02828103117644787\n",
      "step: 3400, loss: 7.353830733336508e-05\n",
      "step: 3401, loss: 0.009395021945238113\n",
      "step: 3402, loss: 0.00010483479854883626\n",
      "step: 3403, loss: 7.149413431761786e-05\n",
      "step: 3404, loss: 5.982127913739532e-05\n",
      "step: 3405, loss: 9.833934745984152e-05\n",
      "step: 3406, loss: 9.31073009269312e-05\n",
      "step: 3407, loss: 6.086601933930069e-05\n",
      "step: 3408, loss: 0.00012091960525140166\n",
      "step: 3409, loss: 4.156046634307131e-05\n",
      "step: 3410, loss: 0.00023474247427657247\n",
      "step: 3411, loss: 0.004620502237230539\n",
      "step: 3412, loss: 0.000195923144929111\n",
      "step: 3413, loss: 8.476130460621789e-05\n",
      "step: 3414, loss: 0.00017763067444320768\n",
      "step: 3415, loss: 0.00022483948851004243\n",
      "step: 3416, loss: 0.0007099553477019072\n",
      "step: 3417, loss: 4.91093160235323e-05\n",
      "step: 3418, loss: 0.0001576399226905778\n",
      "step: 3419, loss: 0.035597532987594604\n",
      "step: 3420, loss: 0.00042884499998763204\n",
      "step: 3421, loss: 8.903149864636362e-05\n",
      "step: 3422, loss: 5.727354437112808e-05\n",
      "step: 3423, loss: 0.00012621731730177999\n",
      "step: 3424, loss: 4.689443449024111e-05\n",
      "step: 3425, loss: 0.000352235249010846\n",
      "step: 3426, loss: 0.07722120732069016\n",
      "step: 3427, loss: 7.515463948948309e-05\n",
      "step: 3428, loss: 0.03271009027957916\n",
      "step: 3429, loss: 0.00010909738193731755\n",
      "step: 3430, loss: 0.00020975783991161734\n",
      "step: 3431, loss: 5.9816702560056e-05\n",
      "step: 3432, loss: 0.009976360015571117\n",
      "step: 3433, loss: 0.00010951479634968564\n",
      "step: 3434, loss: 8.54913450893946e-05\n",
      "step: 3435, loss: 0.0004082820669282228\n",
      "step: 3436, loss: 0.0001047166224452667\n",
      "step: 3437, loss: 4.436960807652213e-05\n",
      "step: 3438, loss: 0.0035217790864408016\n",
      "step: 3439, loss: 4.836043808609247e-05\n",
      "step: 3440, loss: 0.00012582722411025316\n",
      "step: 3441, loss: 0.00010606004798319191\n",
      "step: 3442, loss: 0.00010318998101865873\n",
      "step: 3443, loss: 5.361538933357224e-05\n",
      "step: 3444, loss: 0.0275952760130167\n",
      "step: 3445, loss: 6.321992259472609e-05\n",
      "step: 3446, loss: 0.00014514985377900302\n",
      "step: 3447, loss: 7.204952999018133e-05\n",
      "step: 3448, loss: 1.2588130630319938e-05\n",
      "step: 3449, loss: 0.00013050138659309596\n",
      "step: 3450, loss: 0.00016004314238671213\n",
      "step: 3451, loss: 0.00010301326983608305\n",
      "step: 3452, loss: 0.0001103329923353158\n",
      "step: 3453, loss: 9.043505269801244e-05\n",
      "step: 3454, loss: 7.114056643331423e-05\n",
      "step: 3455, loss: 9.503010369371623e-05\n",
      "step: 3456, loss: 3.69235931430012e-05\n",
      "step: 3457, loss: 4.891985736321658e-05\n",
      "step: 3458, loss: 7.1447626396548e-05\n",
      "step: 3459, loss: 6.662611122010276e-05\n",
      "step: 3460, loss: 0.00027016070089302957\n",
      "step: 3461, loss: 7.796313002472743e-05\n",
      "step: 3462, loss: 0.00013717512774746865\n",
      "step: 3463, loss: 4.311164957471192e-05\n",
      "step: 3464, loss: 9.641954966355115e-05\n",
      "step: 3465, loss: 0.0001307214261032641\n",
      "step: 3466, loss: 8.867088035913184e-05\n",
      "step: 3467, loss: 0.00010582298273220658\n",
      "step: 3468, loss: 0.0007882969221100211\n",
      "step: 3469, loss: 0.02404840663075447\n",
      "step: 3470, loss: 6.393918010871857e-05\n",
      "step: 3471, loss: 9.629723354009911e-05\n",
      "step: 3472, loss: 0.0002301259955856949\n",
      "step: 3473, loss: 0.00027589211822487414\n",
      "step: 3474, loss: 0.00014884844131302088\n",
      "step: 3475, loss: 8.724549843464047e-05\n",
      "step: 3476, loss: 0.0002702746423892677\n",
      "step: 3477, loss: 0.0004009941185358912\n",
      "step: 3478, loss: 0.0003262510581407696\n",
      "step: 3479, loss: 7.15483256499283e-05\n",
      "step: 3480, loss: 0.00023337315360549837\n",
      "step: 3481, loss: 8.764674566918984e-05\n",
      "step: 3482, loss: 5.566504842136055e-05\n",
      "step: 3483, loss: 0.0001408249227097258\n",
      "step: 3484, loss: 0.0002327170077478513\n",
      "step: 3485, loss: 8.128223271341994e-05\n",
      "step: 3486, loss: 0.00029072733013890684\n",
      "step: 3487, loss: 0.00022796980920247734\n",
      "step: 3488, loss: 0.00021063926396891475\n",
      "step: 3489, loss: 7.886267849244177e-05\n",
      "step: 3490, loss: 0.0002257735322928056\n",
      "step: 3491, loss: 0.00010011409176513553\n",
      "step: 3492, loss: 0.0001543578546261415\n",
      "step: 3493, loss: 0.00017861068772617728\n",
      "step: 3494, loss: 0.06847894936800003\n",
      "step: 3495, loss: 0.0001684049202594906\n",
      "step: 3496, loss: 6.0268597735557705e-05\n",
      "step: 3497, loss: 0.0006753825582563877\n",
      "step: 3498, loss: 0.0001251895009772852\n",
      "step: 3499, loss: 0.0008445070125162601\n",
      "step: 3500, loss: 0.0007018093601800501\n",
      "step: 3501, loss: 7.881011697463691e-05\n",
      "step: 3502, loss: 0.00011337704927427694\n",
      "step: 3503, loss: 8.294931467389688e-05\n",
      "step: 3504, loss: 8.531034109182656e-05\n",
      "step: 3505, loss: 0.0001422767381882295\n",
      "step: 3506, loss: 6.050171214155853e-05\n",
      "step: 3507, loss: 0.0002673788694664836\n",
      "step: 3508, loss: 6.879014108562842e-05\n",
      "step: 3509, loss: 0.00010724757885327563\n",
      "step: 3510, loss: 0.0003985357761848718\n",
      "step: 3511, loss: 8.173114474629983e-05\n",
      "step: 3512, loss: 0.00012366972805466503\n",
      "step: 3513, loss: 0.00020487209258135408\n",
      "step: 3514, loss: 0.0008784101810306311\n",
      "step: 3515, loss: 8.535025699529797e-05\n",
      "step: 3516, loss: 0.00036008114693686366\n",
      "step: 3517, loss: 8.086061279755086e-05\n",
      "step: 3518, loss: 0.00012802652781829238\n",
      "step: 3519, loss: 3.0021181373740546e-05\n",
      "step: 3520, loss: 0.0375245176255703\n",
      "step: 3521, loss: 0.00015812681522220373\n",
      "step: 3522, loss: 7.699262641835958e-05\n",
      "step: 3523, loss: 3.947116056224331e-05\n",
      "step: 3524, loss: 0.00020942265109624714\n",
      "step: 3525, loss: 5.6619570386828855e-05\n",
      "step: 3526, loss: 0.000140800402732566\n",
      "step: 3527, loss: 0.00011622192687354982\n",
      "step: 3528, loss: 0.027885565534234047\n",
      "step: 3529, loss: 0.04292939975857735\n",
      "step: 3530, loss: 6.211650179466233e-05\n",
      "step: 3531, loss: 4.859367982135154e-05\n",
      "step: 3532, loss: 0.05094445124268532\n",
      "step: 3533, loss: 5.470917676575482e-05\n",
      "step: 3534, loss: 4.11254950449802e-05\n",
      "step: 3535, loss: 0.0014850967563688755\n",
      "step: 3536, loss: 0.00010725183528847992\n",
      "step: 3537, loss: 2.798380774038378e-05\n",
      "step: 3538, loss: 8.651165990158916e-05\n",
      "step: 3539, loss: 0.00025218536029569805\n",
      "step: 3540, loss: 4.109185829292983e-05\n",
      "step: 3541, loss: 0.0001420860062353313\n",
      "step: 3542, loss: 0.00019820622401311994\n",
      "step: 3543, loss: 6.800943083362654e-05\n",
      "step: 3544, loss: 0.00018573258421383798\n",
      "step: 3545, loss: 0.00017090720939449966\n",
      "step: 3546, loss: 0.00016906649398151785\n",
      "step: 3547, loss: 6.989402754697949e-05\n",
      "step: 3548, loss: 3.524395287968218e-05\n",
      "step: 3549, loss: 0.00011163797171320766\n",
      "step: 3550, loss: 4.834662831854075e-05\n",
      "step: 3551, loss: 7.158854714361951e-05\n",
      "step: 3552, loss: 5.6738917919574305e-05\n",
      "step: 3553, loss: 4.353053373051807e-05\n",
      "step: 3554, loss: 6.580841727554798e-05\n",
      "step: 3555, loss: 0.00012195912131574005\n",
      "step: 3556, loss: 0.00011343742517055944\n",
      "step: 3557, loss: 0.00010087834380101413\n",
      "step: 3558, loss: 0.00014109555922914296\n",
      "step: 3559, loss: 0.0006584516377188265\n",
      "step: 3560, loss: 0.00011263302440056577\n",
      "step: 3561, loss: 5.891349064768292e-05\n",
      "step: 3562, loss: 0.0001527830318082124\n",
      "step: 3563, loss: 0.0001569893938722089\n",
      "step: 3564, loss: 0.00012245897960383445\n",
      "step: 3565, loss: 0.0001601945550646633\n",
      "step: 3566, loss: 7.173943595262244e-05\n",
      "step: 3567, loss: 9.23442275961861e-05\n",
      "step: 3568, loss: 6.298098742263392e-05\n",
      "step: 3569, loss: 9.342811972601339e-05\n",
      "step: 3570, loss: 8.866455755196512e-05\n",
      "step: 3571, loss: 0.0641356036067009\n",
      "step: 3572, loss: 8.363867527805269e-05\n",
      "step: 3573, loss: 9.403112926520407e-05\n",
      "step: 3574, loss: 0.00028245177236385643\n",
      "step: 3575, loss: 0.00016462743224110454\n",
      "step: 3576, loss: 5.1636943680932745e-05\n",
      "step: 3577, loss: 0.00020397249318193644\n",
      "step: 3578, loss: 4.2131785448873416e-05\n",
      "step: 3579, loss: 0.007103325799107552\n",
      "step: 3580, loss: 0.00022339390125125647\n",
      "step: 3581, loss: 0.0001712345110718161\n",
      "step: 3582, loss: 8.920591062633321e-05\n",
      "step: 3583, loss: 0.00045264256186783314\n",
      "step: 3584, loss: 8.80625011632219e-05\n",
      "step: 3585, loss: 0.00027026914176531136\n",
      "step: 3586, loss: 0.0002542810107115656\n",
      "step: 3587, loss: 4.519433787208982e-05\n",
      "step: 3588, loss: 0.00010420976468594745\n",
      "step: 3589, loss: 0.039249237626791\n",
      "step: 3590, loss: 9.202931687468663e-05\n",
      "step: 3591, loss: 0.0004545130068436265\n",
      "step: 3592, loss: 0.06662055850028992\n",
      "step: 3593, loss: 8.288023673230782e-05\n",
      "step: 3594, loss: 7.970109436428174e-05\n",
      "step: 3595, loss: 7.545288826804608e-05\n",
      "step: 3596, loss: 4.8670972319087014e-05\n",
      "step: 3597, loss: 3.4147560654673725e-05\n",
      "step: 3598, loss: 5.1651215471792966e-05\n",
      "step: 3599, loss: 0.0002757380425464362\n",
      "step: 3600, loss: 5.485376823344268e-05\n",
      "step: 3601, loss: 3.890781226800755e-05\n",
      "step: 3602, loss: 3.288342122687027e-05\n",
      "step: 3603, loss: 0.0002774152089841664\n",
      "step: 3604, loss: 0.0002155601978302002\n",
      "step: 3605, loss: 4.8074452934088185e-05\n",
      "step: 3606, loss: 0.053598374128341675\n",
      "step: 3607, loss: 0.02611422725021839\n",
      "step: 3608, loss: 0.00014214179827831686\n",
      "step: 3609, loss: 0.00017235276754945517\n",
      "step: 3610, loss: 7.220450061140582e-05\n",
      "step: 3611, loss: 7.496175385313109e-05\n",
      "step: 3612, loss: 7.529424328822643e-05\n",
      "step: 3613, loss: 8.841371163725853e-05\n",
      "step: 3614, loss: 6.977585144340992e-05\n",
      "step: 3615, loss: 0.0004447118262760341\n",
      "step: 3616, loss: 2.6796109523274936e-05\n",
      "step: 3617, loss: 0.05779959261417389\n",
      "step: 3618, loss: 0.00013233190111350268\n",
      "step: 3619, loss: 6.120938633102924e-05\n",
      "step: 3620, loss: 5.5642816732870415e-05\n",
      "step: 3621, loss: 0.0003387811884749681\n",
      "step: 3622, loss: 3.286086212028749e-05\n",
      "step: 3623, loss: 4.836145672015846e-05\n",
      "step: 3624, loss: 3.5872064472641796e-05\n",
      "step: 3625, loss: 3.128953539999202e-05\n",
      "step: 3626, loss: 9.543971827952191e-05\n",
      "step: 3627, loss: 4.204406286589801e-05\n",
      "step: 3628, loss: 4.6153232688084245e-05\n",
      "step: 3629, loss: 0.00015649817942176014\n",
      "step: 3630, loss: 3.3636231819400564e-05\n",
      "step: 3631, loss: 0.00011451973841758445\n",
      "step: 3632, loss: 0.0003403397276997566\n",
      "step: 3633, loss: 0.00014233350520953536\n",
      "step: 3634, loss: 9.72150664892979e-05\n",
      "step: 3635, loss: 5.041638360125944e-05\n",
      "step: 3636, loss: 0.00016696380043867975\n",
      "step: 3637, loss: 0.00018050457583740354\n",
      "step: 3638, loss: 0.04370640218257904\n",
      "step: 3639, loss: 6.184636004036292e-05\n",
      "step: 3640, loss: 0.000443693163106218\n",
      "step: 3641, loss: 0.00010332039528293535\n",
      "step: 3642, loss: 0.00024101878807414323\n",
      "step: 3643, loss: 0.00031790268258191645\n",
      "step: 3644, loss: 6.684396066702902e-05\n",
      "step: 3645, loss: 4.5019329263595864e-05\n",
      "step: 3646, loss: 0.00011373387678759173\n",
      "step: 3647, loss: 0.00015104546037036926\n",
      "step: 3648, loss: 3.587532410165295e-05\n",
      "step: 3649, loss: 5.590975342784077e-05\n",
      "step: 3650, loss: 5.490039984579198e-05\n",
      "step: 3651, loss: 2.364998908888083e-05\n",
      "step: 3652, loss: 5.302822319208644e-05\n",
      "step: 3653, loss: 7.524339889641851e-05\n",
      "step: 3654, loss: 0.023612625896930695\n",
      "step: 3655, loss: 2.405468694632873e-05\n",
      "step: 3656, loss: 0.00018540100427344441\n",
      "step: 3657, loss: 6.461940938606858e-05\n",
      "step: 3658, loss: 4.130239904043265e-05\n",
      "step: 3659, loss: 4.383252598927356e-05\n",
      "step: 3660, loss: 6.757457595085725e-05\n",
      "step: 3661, loss: 0.011956678703427315\n",
      "step: 3662, loss: 5.151436198502779e-05\n",
      "step: 3663, loss: 3.95207098335959e-05\n",
      "step: 3664, loss: 3.990645200246945e-05\n",
      "step: 3665, loss: 2.7361633328837343e-05\n",
      "step: 3666, loss: 0.00010899100743699819\n",
      "step: 3667, loss: 0.00017348836991004646\n",
      "step: 3668, loss: 9.986192162614316e-05\n",
      "step: 3669, loss: 0.0319511778652668\n",
      "step: 3670, loss: 3.768013993976638e-05\n",
      "step: 3671, loss: 0.00016009317187126726\n",
      "step: 3672, loss: 6.991941336309537e-05\n",
      "step: 3673, loss: 0.001045329961925745\n",
      "step: 3674, loss: 2.4896256945794448e-05\n",
      "step: 3675, loss: 2.7012210921384394e-05\n",
      "step: 3676, loss: 9.278194920625538e-05\n",
      "step: 3677, loss: 5.660370152327232e-05\n",
      "step: 3678, loss: 0.00046636140905320644\n",
      "step: 3679, loss: 6.439769640564919e-05\n",
      "step: 3680, loss: 6.992783164605498e-05\n",
      "step: 3681, loss: 3.32484814862255e-05\n",
      "step: 3682, loss: 6.598738400498405e-05\n",
      "step: 3683, loss: 0.00013238158135209233\n",
      "step: 3684, loss: 4.421799530973658e-05\n",
      "step: 3685, loss: 0.00017276292783208191\n",
      "step: 3686, loss: 9.859702549874783e-05\n",
      "step: 3687, loss: 0.00833642017096281\n",
      "step: 3688, loss: 0.0002621659659780562\n",
      "step: 3689, loss: 6.444883911171928e-05\n",
      "step: 3690, loss: 2.7466570827527903e-05\n",
      "step: 3691, loss: 6.541403126902878e-05\n",
      "step: 3692, loss: 7.396865112241358e-05\n",
      "step: 3693, loss: 0.00017256980936508626\n",
      "step: 3694, loss: 0.0001235184317920357\n",
      "step: 3695, loss: 8.622472523711622e-05\n",
      "step: 3696, loss: 4.654603617382236e-05\n",
      "step: 3697, loss: 5.713019345421344e-05\n",
      "step: 3698, loss: 0.00014505669241771102\n",
      "step: 3699, loss: 0.00011716223525581881\n",
      "step: 3700, loss: 6.39562786091119e-05\n",
      "step: 3701, loss: 0.00010474584996700287\n",
      "step: 3702, loss: 8.674644050188363e-05\n",
      "step: 3703, loss: 3.979518078267574e-05\n",
      "step: 3704, loss: 4.1301911551272497e-05\n",
      "step: 3705, loss: 3.114428182016127e-05\n",
      "step: 3706, loss: 6.823734292993322e-05\n",
      "step: 3707, loss: 0.021136268973350525\n",
      "step: 3708, loss: 9.68018066487275e-05\n",
      "step: 3709, loss: 0.00013103995297569782\n",
      "step: 3710, loss: 0.0007475011516362429\n",
      "step: 3711, loss: 0.00031279042013920844\n",
      "step: 3712, loss: 0.014308945275843143\n",
      "step: 3713, loss: 0.04064276069402695\n",
      "step: 3714, loss: 0.00013591593597084284\n",
      "step: 3715, loss: 9.620506170904264e-05\n",
      "step: 3716, loss: 0.0008500325493514538\n",
      "step: 3717, loss: 0.00017511766054667532\n",
      "step: 3718, loss: 3.6832545447396114e-05\n",
      "step: 3719, loss: 4.252705548424274e-05\n",
      "step: 3720, loss: 4.7688288759673014e-05\n",
      "step: 3721, loss: 0.025555890053510666\n",
      "step: 3722, loss: 4.994415212422609e-05\n",
      "step: 3723, loss: 5.448496085591614e-05\n",
      "step: 3724, loss: 0.0003816763637587428\n",
      "step: 3725, loss: 0.03805793076753616\n",
      "step: 3726, loss: 8.1087120634038e-05\n",
      "step: 3727, loss: 5.288665124680847e-05\n",
      "step: 3728, loss: 8.602604066254571e-05\n",
      "step: 3729, loss: 3.2903113606153056e-05\n",
      "step: 3730, loss: 5.328730185283348e-05\n",
      "step: 3731, loss: 6.268567085498944e-05\n",
      "step: 3732, loss: 5.1658185839187354e-05\n",
      "step: 3733, loss: 4.5287135435501114e-05\n",
      "step: 3734, loss: 0.00010683749133022502\n",
      "step: 3735, loss: 7.006854139035568e-05\n",
      "step: 3736, loss: 6.491239764727652e-05\n",
      "step: 3737, loss: 0.00016966716793831438\n",
      "step: 3738, loss: 0.00011182404705323279\n",
      "step: 3739, loss: 0.0006709543522447348\n",
      "step: 3740, loss: 4.527687997324392e-05\n",
      "step: 3741, loss: 6.818267138442025e-05\n",
      "step: 3742, loss: 0.00025408531655557454\n",
      "step: 3743, loss: 3.3839252864709124e-05\n",
      "step: 3744, loss: 0.022775644436478615\n",
      "step: 3745, loss: 4.800477472599596e-05\n",
      "step: 3746, loss: 5.7184075558325276e-05\n",
      "step: 3747, loss: 0.0001599404204171151\n",
      "step: 3748, loss: 0.00011921716941287741\n",
      "step: 3749, loss: 1.8912831365014426e-05\n",
      "step: 3750, loss: 9.256860357709229e-05\n",
      "step: 3751, loss: 0.0003661664668470621\n",
      "step: 3752, loss: 0.0008630616939626634\n",
      "step: 3753, loss: 5.755414895247668e-05\n",
      "step: 3754, loss: 0.004686425440013409\n",
      "step: 3755, loss: 3.217701305402443e-05\n",
      "step: 3756, loss: 9.2114663857501e-05\n",
      "step: 3757, loss: 0.00014655849372502416\n",
      "step: 3758, loss: 0.00022237314260564744\n",
      "step: 3759, loss: 4.2745439714053646e-05\n",
      "step: 3760, loss: 0.020191378891468048\n",
      "step: 3761, loss: 0.04918503388762474\n",
      "step: 3762, loss: 4.166111830272712e-05\n",
      "step: 3763, loss: 7.490806456189603e-05\n",
      "step: 3764, loss: 0.00011102024291176349\n",
      "step: 3765, loss: 4.0920913306763396e-05\n",
      "step: 3766, loss: 0.00014037001528777182\n",
      "step: 3767, loss: 6.344673602143303e-05\n",
      "step: 3768, loss: 0.00022850411187391728\n",
      "step: 3769, loss: 3.570473927538842e-05\n",
      "step: 3770, loss: 0.00010947928967652842\n",
      "step: 3771, loss: 5.0885104428743944e-05\n",
      "step: 3772, loss: 0.00010813078552018851\n",
      "step: 3773, loss: 8.226372301578522e-05\n",
      "step: 3774, loss: 0.00010303761519026011\n",
      "step: 3775, loss: 0.00012806477025151253\n",
      "step: 3776, loss: 0.0003624558448791504\n",
      "step: 3777, loss: 0.00010670458868844435\n",
      "step: 3778, loss: 8.495207293890417e-05\n",
      "step: 3779, loss: 1.190366947412258e-05\n",
      "step: 3780, loss: 0.0001960944791790098\n",
      "step: 3781, loss: 4.964371328242123e-05\n",
      "step: 3782, loss: 0.00016239163232967257\n",
      "step: 3783, loss: 0.0007602577097713947\n",
      "step: 3784, loss: 0.0006841099821031094\n",
      "step: 3785, loss: 2.7322976166033186e-05\n",
      "step: 3786, loss: 3.164196095895022e-05\n",
      "step: 3787, loss: 6.52177186566405e-05\n",
      "step: 3788, loss: 2.281302113260608e-05\n",
      "step: 3789, loss: 6.124343781266361e-05\n",
      "step: 3790, loss: 3.427896081120707e-05\n",
      "step: 3791, loss: 0.00021888973424211144\n",
      "step: 3792, loss: 0.0001797774457372725\n",
      "step: 3793, loss: 6.396502431016415e-05\n",
      "step: 3794, loss: 4.865273876930587e-05\n",
      "step: 3795, loss: 6.81523306411691e-05\n",
      "step: 3796, loss: 0.08184430003166199\n",
      "step: 3797, loss: 0.0002538800472393632\n",
      "step: 3798, loss: 0.0002568870841059834\n",
      "step: 3799, loss: 0.06243496388196945\n",
      "step: 3800, loss: 7.221097621368244e-05\n",
      "step: 3801, loss: 7.950252620503306e-05\n",
      "step: 3802, loss: 3.861786535708234e-05\n",
      "step: 3803, loss: 0.01854262873530388\n",
      "step: 3804, loss: 0.00023936046636663377\n",
      "step: 3805, loss: 8.706583321327344e-05\n",
      "step: 3806, loss: 0.0002809414581861347\n",
      "step: 3807, loss: 0.0001808190136216581\n",
      "step: 3808, loss: 7.735030521871522e-05\n",
      "step: 3809, loss: 0.00029126161825843155\n",
      "step: 3810, loss: 0.03752625733613968\n",
      "step: 3811, loss: 0.00011956689559156075\n",
      "step: 3812, loss: 6.547589146066457e-05\n",
      "step: 3813, loss: 0.00010760599980130792\n",
      "step: 3814, loss: 3.714860213221982e-05\n",
      "step: 3815, loss: 8.768720726948231e-05\n",
      "step: 3816, loss: 5.267275628284551e-05\n",
      "step: 3817, loss: 5.736180173698813e-05\n",
      "step: 3818, loss: 0.00012525930651463568\n",
      "step: 3819, loss: 0.0001365258649457246\n",
      "step: 3820, loss: 4.4976597564527765e-05\n",
      "step: 3821, loss: 0.0006934255943633616\n",
      "step: 3822, loss: 0.00012370738841127604\n",
      "step: 3823, loss: 0.00022802010062150657\n",
      "step: 3824, loss: 9.359119576402009e-05\n",
      "step: 3825, loss: 0.04310550168156624\n",
      "step: 3826, loss: 5.8296489442000166e-05\n",
      "step: 3827, loss: 0.0003155796730425209\n",
      "step: 3828, loss: 4.8857229558052495e-05\n",
      "step: 3829, loss: 0.0007537818164564669\n",
      "step: 3830, loss: 9.277873323298991e-05\n",
      "step: 3831, loss: 9.453172970097512e-05\n",
      "step: 3832, loss: 0.007374653592705727\n",
      "step: 3833, loss: 0.00014627778728026897\n",
      "step: 3834, loss: 2.739365299930796e-05\n",
      "step: 3835, loss: 8.752526628086343e-05\n",
      "step: 3836, loss: 9.500114538241178e-05\n",
      "step: 3837, loss: 3.085519347223453e-05\n",
      "step: 3838, loss: 3.860641299979761e-05\n",
      "step: 3839, loss: 0.044328976422548294\n",
      "step: 3840, loss: 9.83372883638367e-05\n",
      "step: 3841, loss: 0.0007022905629128218\n",
      "step: 3842, loss: 0.00017347687389701605\n",
      "step: 3843, loss: 5.7511275372235104e-05\n",
      "step: 3844, loss: 3.45195185218472e-05\n",
      "step: 3845, loss: 0.00011502236156957224\n",
      "step: 3846, loss: 0.0001161817999673076\n",
      "step: 3847, loss: 0.0006761609110981226\n",
      "step: 3848, loss: 0.0028892795089632273\n",
      "step: 3849, loss: 5.6460732594132423e-05\n",
      "step: 3850, loss: 3.6786750570172444e-05\n",
      "step: 3851, loss: 0.00014799473865423352\n",
      "step: 3852, loss: 7.892389839980751e-05\n",
      "step: 3853, loss: 0.00011886896390933543\n",
      "step: 3854, loss: 0.0492541640996933\n",
      "step: 3855, loss: 8.484668069286272e-05\n",
      "step: 3856, loss: 9.675737237557769e-05\n",
      "step: 3857, loss: 0.00011124218144686893\n",
      "step: 3858, loss: 3.343723437865265e-05\n",
      "step: 3859, loss: 0.00010162233957089484\n",
      "step: 3860, loss: 4.183695637038909e-05\n",
      "step: 3861, loss: 2.5517994799884036e-05\n",
      "step: 3862, loss: 2.6446501578902826e-05\n",
      "step: 3863, loss: 0.00026944244746118784\n",
      "step: 3864, loss: 9.464180038776249e-05\n",
      "step: 3865, loss: 5.231920295045711e-05\n",
      "step: 3866, loss: 4.39753093814943e-05\n",
      "step: 3867, loss: 4.563482434605248e-05\n",
      "step: 3868, loss: 4.511488077696413e-05\n",
      "step: 3869, loss: 5.58545216335915e-05\n",
      "step: 3870, loss: 0.0003244148683734238\n",
      "step: 3871, loss: 5.400540248956531e-05\n",
      "step: 3872, loss: 3.994241342297755e-05\n",
      "step: 3873, loss: 8.689779497217387e-05\n",
      "step: 3874, loss: 5.979258276056498e-05\n",
      "step: 3875, loss: 3.398347689653747e-05\n",
      "step: 3876, loss: 5.87034119234886e-05\n",
      "step: 3877, loss: 0.000140532007208094\n",
      "step: 3878, loss: 0.0001744054607115686\n",
      "step: 3879, loss: 5.89063492952846e-05\n",
      "step: 3880, loss: 2.5458781237830408e-05\n",
      "step: 3881, loss: 6.237460911506787e-05\n",
      "step: 3882, loss: 4.5340908400248736e-05\n",
      "step: 3883, loss: 0.03513079509139061\n",
      "step: 3884, loss: 0.00012170223635621369\n",
      "step: 3885, loss: 3.084174022660591e-05\n",
      "step: 3886, loss: 0.00017344378284178674\n",
      "step: 3887, loss: 0.00013323150051292032\n",
      "step: 3888, loss: 3.5267643397673965e-05\n",
      "step: 3889, loss: 6.91952373017557e-05\n",
      "step: 3890, loss: 0.0006281009991653264\n",
      "step: 3891, loss: 4.3884236220037565e-05\n",
      "step: 3892, loss: 4.162768891546875e-05\n",
      "step: 3893, loss: 3.269841545261443e-05\n",
      "step: 3894, loss: 6.696800846839324e-05\n",
      "step: 3895, loss: 3.828043190878816e-05\n",
      "step: 3896, loss: 3.458754872553982e-05\n",
      "step: 3897, loss: 9.4246344815474e-05\n",
      "step: 3898, loss: 0.0008514390792697668\n",
      "step: 3899, loss: 3.2885876862565055e-05\n",
      "step: 3900, loss: 6.943527841940522e-05\n",
      "step: 3901, loss: 1.9892386262654327e-05\n",
      "step: 3902, loss: 0.00013353084796108305\n",
      "step: 3903, loss: 4.6056964492890984e-05\n",
      "step: 3904, loss: 5.7213859690818936e-05\n",
      "step: 3905, loss: 6.0269325331319124e-05\n",
      "step: 3906, loss: 7.313651440199465e-05\n",
      "step: 3907, loss: 0.00010929284326266497\n",
      "step: 3908, loss: 0.00010752378148026764\n",
      "step: 3909, loss: 3.600347918109037e-05\n",
      "step: 3910, loss: 5.291066554491408e-05\n",
      "step: 3911, loss: 2.7405845685279928e-05\n",
      "step: 3912, loss: 0.01439516618847847\n",
      "step: 3913, loss: 5.905769648961723e-05\n",
      "step: 3914, loss: 2.657705408637412e-05\n",
      "step: 3915, loss: 0.00021852899226360023\n",
      "step: 3916, loss: 4.175396679784171e-05\n",
      "step: 3917, loss: 3.666974953375757e-05\n",
      "step: 3918, loss: 6.645920075243339e-05\n",
      "step: 3919, loss: 6.314345955615863e-05\n",
      "step: 3920, loss: 8.177820564014837e-05\n",
      "step: 3921, loss: 0.00011875665950356051\n",
      "step: 3922, loss: 6.404954183381051e-05\n",
      "step: 3923, loss: 7.471000571968034e-05\n",
      "step: 3924, loss: 3.775491859414615e-05\n",
      "step: 3925, loss: 4.070426439284347e-05\n",
      "step: 3926, loss: 0.02714432217180729\n",
      "step: 3927, loss: 1.3923297956353053e-05\n",
      "step: 3928, loss: 0.0011476891813799739\n",
      "step: 3929, loss: 0.00014424043183680624\n",
      "step: 3930, loss: 8.263012568932027e-05\n",
      "step: 3931, loss: 1.5484696632483974e-05\n",
      "step: 3932, loss: 0.0008143681334331632\n",
      "step: 3933, loss: 6.850332283647731e-05\n",
      "step: 3934, loss: 3.496882345643826e-05\n",
      "step: 3935, loss: 7.297794945770875e-05\n",
      "step: 3936, loss: 8.621822780696675e-05\n",
      "step: 3937, loss: 3.500743332551792e-05\n",
      "step: 3938, loss: 0.0001287740160478279\n",
      "step: 3939, loss: 0.00015621655620634556\n",
      "step: 3940, loss: 6.843886512797326e-05\n",
      "step: 3941, loss: 0.00025158352218568325\n",
      "step: 3942, loss: 0.00016244240396190435\n",
      "step: 3943, loss: 6.172689609229565e-05\n",
      "step: 3944, loss: 0.0003726012946572155\n",
      "step: 3945, loss: 5.928290920564905e-05\n",
      "step: 3946, loss: 4.5376560592558235e-05\n",
      "step: 3947, loss: 5.958553447271697e-05\n",
      "step: 3948, loss: 8.566847100155428e-05\n",
      "step: 3949, loss: 2.574329664639663e-05\n",
      "step: 3950, loss: 3.794724034378305e-05\n",
      "step: 3951, loss: 0.0002682693302631378\n",
      "step: 3952, loss: 0.0003464855544734746\n",
      "step: 3953, loss: 8.410598093178123e-05\n",
      "step: 3954, loss: 0.0002090446650981903\n",
      "step: 3955, loss: 8.604035974713042e-05\n",
      "step: 3956, loss: 5.848219007020816e-05\n",
      "step: 3957, loss: 2.0859602955169976e-05\n",
      "step: 3958, loss: 0.0006690166774205863\n",
      "step: 3959, loss: 5.9371202951297164e-05\n",
      "step: 3960, loss: 7.582882244605571e-05\n",
      "step: 3961, loss: 0.021471576765179634\n",
      "step: 3962, loss: 7.171728066168725e-05\n",
      "step: 3963, loss: 0.00017516748630441725\n",
      "step: 3964, loss: 4.2645904613891616e-05\n",
      "step: 3965, loss: 8.597806299803779e-05\n",
      "step: 3966, loss: 0.00048571982188150287\n",
      "step: 3967, loss: 0.00025083220680244267\n",
      "step: 3968, loss: 1.978269028768409e-05\n",
      "step: 3969, loss: 0.0001224396692123264\n",
      "step: 3970, loss: 4.110040026716888e-05\n",
      "step: 3971, loss: 9.993585990741849e-05\n",
      "step: 3972, loss: 0.024349704384803772\n",
      "step: 3973, loss: 3.854339593090117e-05\n",
      "step: 3974, loss: 0.00011433132749516517\n",
      "step: 3975, loss: 0.056843314319849014\n",
      "step: 3976, loss: 9.485589544055983e-05\n",
      "step: 3977, loss: 0.02411809004843235\n",
      "step: 3978, loss: 4.648781759897247e-05\n",
      "step: 3979, loss: 6.259165820665658e-05\n",
      "step: 3980, loss: 7.057183393044397e-05\n",
      "step: 3981, loss: 3.7367466575233266e-05\n",
      "step: 3982, loss: 3.9286675018956885e-05\n",
      "step: 3983, loss: 5.209718074183911e-05\n",
      "step: 3984, loss: 0.03332120552659035\n",
      "step: 3985, loss: 3.992329220636748e-05\n",
      "step: 3986, loss: 4.8107140173669904e-05\n",
      "step: 3987, loss: 5.7089746405836195e-05\n",
      "step: 3988, loss: 0.0001291266962653026\n",
      "step: 3989, loss: 6.588163523701951e-05\n",
      "step: 3990, loss: 3.118643508059904e-05\n",
      "step: 3991, loss: 1.749444709275849e-05\n",
      "step: 3992, loss: 4.5403168769553304e-05\n",
      "step: 3993, loss: 0.00019073158910032362\n",
      "step: 3994, loss: 2.444283018121496e-05\n",
      "step: 3995, loss: 0.048439763486385345\n",
      "step: 3996, loss: 7.054996240185574e-05\n",
      "step: 3997, loss: 0.0001073560124495998\n",
      "step: 3998, loss: 0.00014712843403685838\n",
      "step: 3999, loss: 4.912636723020114e-05\n",
      "step: 4000, loss: 0.00029043774702586234\n",
      "step: 4001, loss: 6.337135710055009e-05\n",
      "step: 4002, loss: 8.22886941023171e-05\n",
      "step: 4003, loss: 3.5190900234738365e-05\n",
      "step: 4004, loss: 9.089203376788646e-05\n",
      "step: 4005, loss: 4.481034920900129e-05\n",
      "step: 4006, loss: 5.183582106838003e-05\n",
      "step: 4007, loss: 3.86128704121802e-05\n",
      "step: 4008, loss: 0.00011582781735341996\n",
      "step: 4009, loss: 6.289509474299848e-05\n",
      "step: 4010, loss: 0.010765518061816692\n",
      "step: 4011, loss: 0.15826083719730377\n",
      "step: 4012, loss: 0.0006088903755880892\n",
      "step: 4013, loss: 0.020211059600114822\n",
      "step: 4014, loss: 0.00011198651191079989\n",
      "step: 4015, loss: 0.00010267855395795777\n",
      "step: 4016, loss: 4.295210965210572e-05\n",
      "step: 4017, loss: 0.00012066501221852377\n",
      "step: 4018, loss: 5.05408643221017e-05\n",
      "step: 4019, loss: 5.4609834478469566e-05\n",
      "step: 4020, loss: 0.000574207806494087\n",
      "step: 4021, loss: 2.3768323444528505e-05\n",
      "step: 4022, loss: 0.0002111734647769481\n",
      "step: 4023, loss: 0.05786500498652458\n",
      "step: 4024, loss: 7.807536167092621e-05\n",
      "step: 4025, loss: 0.09124474227428436\n",
      "step: 4026, loss: 0.00019857200095430017\n",
      "step: 4027, loss: 6.592823774553835e-05\n",
      "step: 4028, loss: 3.470562296570279e-05\n",
      "step: 4029, loss: 6.084438064135611e-05\n",
      "step: 4030, loss: 9.072659304365516e-05\n",
      "step: 4031, loss: 0.00028869332163594663\n",
      "step: 4032, loss: 0.0014723980566486716\n",
      "step: 4033, loss: 4.488153717829846e-05\n",
      "step: 4034, loss: 0.007820211350917816\n",
      "step: 4035, loss: 0.00022848941443953663\n",
      "step: 4036, loss: 9.052318637259305e-05\n",
      "step: 4037, loss: 0.00012996715668123215\n",
      "step: 4038, loss: 2.7297612177790143e-05\n",
      "step: 4039, loss: 7.082185038598254e-05\n",
      "step: 4040, loss: 7.962671224959195e-05\n",
      "step: 4041, loss: 0.00018399031250737607\n",
      "step: 4042, loss: 0.00011760182678699493\n",
      "step: 4043, loss: 5.6388442317256704e-05\n",
      "step: 4044, loss: 5.0216196541441604e-05\n",
      "step: 4045, loss: 7.56980080041103e-05\n",
      "step: 4046, loss: 4.246536627761088e-05\n",
      "step: 4047, loss: 1.5017407349660061e-05\n",
      "step: 4048, loss: 0.00012858817353844643\n",
      "step: 4049, loss: 4.370134411146864e-05\n",
      "step: 4050, loss: 0.0006798840477131307\n",
      "step: 4051, loss: 5.792806769022718e-05\n",
      "step: 4052, loss: 7.797856233082712e-05\n",
      "step: 4053, loss: 6.799044786021113e-05\n",
      "step: 4054, loss: 9.029907232616097e-05\n",
      "step: 4055, loss: 5.063562639406882e-05\n",
      "step: 4056, loss: 0.00019873205746989697\n",
      "step: 4057, loss: 5.7652763644000515e-05\n",
      "step: 4058, loss: 7.076029578456655e-05\n",
      "step: 4059, loss: 0.00026575312949717045\n",
      "step: 4060, loss: 0.00038049009162932634\n",
      "step: 4061, loss: 0.00010553866013651714\n",
      "step: 4062, loss: 7.468615513062105e-05\n",
      "step: 4063, loss: 0.00019339028222020715\n",
      "step: 4064, loss: 0.00023237861751113087\n",
      "step: 4065, loss: 0.04266927391290665\n",
      "step: 4066, loss: 9.566695371177047e-05\n",
      "step: 4067, loss: 8.683618216309696e-05\n",
      "step: 4068, loss: 3.0433784559136257e-05\n",
      "step: 4069, loss: 0.0006370374467223883\n",
      "step: 4070, loss: 8.804288518149406e-05\n",
      "step: 4071, loss: 2.781208422675263e-05\n",
      "step: 4072, loss: 0.00010732264490798116\n",
      "step: 4073, loss: 1.7701706383377314e-05\n",
      "step: 4074, loss: 7.08812294760719e-05\n",
      "step: 4075, loss: 3.2892556191654876e-05\n",
      "step: 4076, loss: 2.804029281833209e-05\n",
      "step: 4077, loss: 6.342728738673031e-05\n",
      "step: 4078, loss: 3.174245648551732e-05\n",
      "step: 4079, loss: 0.00013930046407040209\n",
      "step: 4080, loss: 3.433436722843908e-05\n",
      "step: 4081, loss: 3.411987199797295e-05\n",
      "step: 4082, loss: 3.875526454066858e-05\n",
      "step: 4083, loss: 7.011184061411768e-05\n",
      "step: 4084, loss: 4.065306711709127e-05\n",
      "step: 4085, loss: 5.245815191301517e-05\n",
      "step: 4086, loss: 4.195411383989267e-05\n",
      "step: 4087, loss: 3.331332845846191e-05\n",
      "step: 4088, loss: 2.3828119083191268e-05\n",
      "step: 4089, loss: 4.319597064750269e-05\n",
      "step: 4090, loss: 9.517335456621367e-06\n",
      "step: 4091, loss: 5.7302513596368954e-05\n",
      "step: 4092, loss: 7.625773287145421e-05\n",
      "step: 4093, loss: 7.890479901107028e-05\n",
      "step: 4094, loss: 1.9814035113085993e-05\n",
      "step: 4095, loss: 0.005082505289465189\n",
      "step: 4096, loss: 5.876272916793823e-05\n",
      "step: 4097, loss: 7.59082759032026e-05\n",
      "step: 4098, loss: 2.394571856711991e-05\n",
      "step: 4099, loss: 3.934973938157782e-05\n",
      "step: 4100, loss: 0.003240094752982259\n",
      "step: 4101, loss: 9.860348654910922e-05\n",
      "step: 4102, loss: 0.02883978560566902\n",
      "step: 4103, loss: 0.00010424469655845314\n",
      "step: 4104, loss: 4.9309353926219046e-05\n",
      "step: 4105, loss: 0.0033269033301621675\n",
      "step: 4106, loss: 0.03327535465359688\n",
      "step: 4107, loss: 8.551411156076938e-05\n",
      "step: 4108, loss: 0.00016031207633204758\n",
      "step: 4109, loss: 5.071552368463017e-05\n",
      "step: 4110, loss: 0.00017139007104560733\n",
      "step: 4111, loss: 0.0002741554635576904\n",
      "step: 4112, loss: 4.188433740637265e-05\n",
      "step: 4113, loss: 2.6395444365334697e-05\n",
      "step: 4114, loss: 8.93046089913696e-05\n",
      "step: 4115, loss: 4.026027090731077e-05\n",
      "step: 4116, loss: 9.051286906469613e-05\n",
      "step: 4117, loss: 0.00012155677541159093\n",
      "step: 4118, loss: 6.678741192445159e-05\n",
      "step: 4119, loss: 7.389389065792784e-05\n",
      "step: 4120, loss: 3.597180693759583e-05\n",
      "step: 4121, loss: 7.499493949580938e-05\n",
      "step: 4122, loss: 5.721510387957096e-05\n",
      "step: 4123, loss: 0.00010310739889973775\n",
      "step: 4124, loss: 0.00011666398495435715\n",
      "step: 4125, loss: 7.078007911331952e-05\n",
      "step: 4126, loss: 5.537613469641656e-05\n",
      "step: 4127, loss: 1.924670687003527e-05\n",
      "step: 4128, loss: 0.0015839629340916872\n",
      "step: 4129, loss: 4.582344263326377e-05\n",
      "step: 4130, loss: 6.832581129856408e-05\n",
      "step: 4131, loss: 6.081124229240231e-05\n",
      "step: 4132, loss: 5.497026359080337e-05\n",
      "step: 4133, loss: 4.610448013409041e-05\n",
      "step: 4134, loss: 6.30717768217437e-05\n",
      "step: 4135, loss: 0.00019371708913240582\n",
      "step: 4136, loss: 0.001672733691520989\n",
      "step: 4137, loss: 6.0219295846764e-05\n",
      "step: 4138, loss: 8.97269492270425e-05\n",
      "step: 4139, loss: 4.764877667184919e-05\n",
      "step: 4140, loss: 0.014771911315619946\n",
      "step: 4141, loss: 2.325581772311125e-05\n",
      "step: 4142, loss: 0.00048425549175590277\n",
      "step: 4143, loss: 3.355748413014226e-05\n",
      "step: 4144, loss: 9.557184966979548e-05\n",
      "step: 4145, loss: 3.218715937691741e-05\n",
      "step: 4146, loss: 5.647914804285392e-05\n",
      "step: 4147, loss: 0.00020635529654100537\n",
      "step: 4148, loss: 5.9769074141513556e-05\n",
      "step: 4149, loss: 0.00013012131967116147\n",
      "step: 4150, loss: 2.5660545361461118e-05\n",
      "step: 4151, loss: 1.4625725270889234e-05\n",
      "step: 4152, loss: 0.00014405629190150648\n",
      "step: 4153, loss: 5.196444544708356e-05\n",
      "step: 4154, loss: 4.453732617548667e-05\n",
      "step: 4155, loss: 0.04231300204992294\n",
      "step: 4156, loss: 4.104354593437165e-05\n",
      "step: 4157, loss: 0.051812540739774704\n",
      "step: 4158, loss: 4.3157073378097266e-05\n",
      "step: 4159, loss: 7.667335739824921e-05\n",
      "step: 4160, loss: 0.012711131945252419\n",
      "step: 4161, loss: 7.4596049671527e-05\n",
      "step: 4162, loss: 9.18878722586669e-05\n",
      "step: 4163, loss: 0.00014204865146894008\n",
      "step: 4164, loss: 8.044933201745152e-05\n",
      "step: 4165, loss: 0.00011128176265629008\n",
      "step: 4166, loss: 0.00017154208035208285\n",
      "step: 4167, loss: 8.658874867251143e-05\n",
      "step: 4168, loss: 2.521574242564384e-05\n",
      "step: 4169, loss: 3.737772203749046e-05\n",
      "step: 4170, loss: 0.00029626284958794713\n",
      "step: 4171, loss: 0.0001845989318098873\n",
      "step: 4172, loss: 0.00041369942482560873\n",
      "step: 4173, loss: 0.00011793602607212961\n",
      "step: 4174, loss: 0.00013421999756246805\n",
      "step: 4175, loss: 5.7916953664971516e-05\n",
      "step: 4176, loss: 2.5475257643847726e-05\n",
      "step: 4177, loss: 2.7595120627665892e-05\n",
      "step: 4178, loss: 5.5330809118459e-05\n",
      "step: 4179, loss: 0.00020659608708228916\n",
      "step: 4180, loss: 7.535069744335487e-05\n",
      "step: 4181, loss: 0.00010898499749600887\n",
      "step: 4182, loss: 0.00010594199557090178\n",
      "step: 4183, loss: 8.829434227664024e-05\n",
      "step: 4184, loss: 5.865910861757584e-05\n",
      "step: 4185, loss: 4.565766721498221e-05\n",
      "step: 4186, loss: 2.000709719141014e-05\n",
      "step: 4187, loss: 0.00010566952551016584\n",
      "step: 4188, loss: 0.042792607098817825\n",
      "step: 4189, loss: 0.00014561745047103614\n",
      "step: 4190, loss: 0.00015122961485758424\n",
      "step: 4191, loss: 7.449809345416725e-05\n",
      "step: 4192, loss: 5.5692689784336835e-05\n",
      "step: 4193, loss: 6.493324326584116e-05\n",
      "step: 4194, loss: 5.0744827603921294e-05\n",
      "step: 4195, loss: 7.143527909647673e-05\n",
      "step: 4196, loss: 6.428644701372832e-05\n",
      "step: 4197, loss: 3.066710996790789e-05\n",
      "step: 4198, loss: 4.203632124699652e-05\n",
      "step: 4199, loss: 6.565538205904886e-05\n",
      "step: 4200, loss: 8.603888272773474e-05\n",
      "step: 4201, loss: 0.031034447252750397\n",
      "step: 4202, loss: 0.00013204502465669066\n",
      "step: 4203, loss: 0.00026332621928304434\n",
      "step: 4204, loss: 9.387765021529049e-05\n",
      "step: 4205, loss: 0.008619911968708038\n",
      "step: 4206, loss: 7.08868246874772e-05\n",
      "step: 4207, loss: 6.970688991714269e-05\n",
      "step: 4208, loss: 8.509246981702745e-05\n",
      "step: 4209, loss: 2.7097064958070405e-05\n",
      "step: 4210, loss: 0.00011885584535775706\n",
      "step: 4211, loss: 0.00011274436837993562\n",
      "step: 4212, loss: 6.190303975017741e-05\n",
      "step: 4213, loss: 5.08150587847922e-05\n",
      "step: 4214, loss: 4.008056566817686e-05\n",
      "step: 4215, loss: 3.0517230698023923e-05\n",
      "step: 4216, loss: 7.411179831251502e-05\n",
      "step: 4217, loss: 8.228262595366687e-05\n",
      "step: 4218, loss: 8.092196367215365e-05\n",
      "step: 4219, loss: 6.765571743017063e-05\n",
      "step: 4220, loss: 0.00013138377107679844\n",
      "step: 4221, loss: 3.7098874599905685e-05\n",
      "step: 4222, loss: 9.569255053065717e-05\n",
      "step: 4223, loss: 0.0001360333408229053\n",
      "step: 4224, loss: 0.0001144608759204857\n",
      "step: 4225, loss: 0.0002543170703575015\n",
      "step: 4226, loss: 3.4433967812219635e-05\n",
      "step: 4227, loss: 0.0001255715760635212\n",
      "step: 4228, loss: 8.904760761652142e-05\n",
      "step: 4229, loss: 7.903506048023701e-05\n",
      "step: 4230, loss: 0.0017598826671019197\n",
      "step: 4231, loss: 8.278313180198893e-05\n",
      "step: 4232, loss: 3.385030504432507e-05\n",
      "step: 4233, loss: 4.61743948108051e-05\n",
      "step: 4234, loss: 4.833189814235084e-05\n",
      "step: 4235, loss: 4.4433167204260826e-05\n",
      "step: 4236, loss: 3.0324512408697046e-05\n",
      "step: 4237, loss: 6.398549885489047e-05\n",
      "step: 4238, loss: 4.254674058756791e-05\n",
      "step: 4239, loss: 3.207663758075796e-05\n",
      "step: 4240, loss: 9.461652371101081e-05\n",
      "step: 4241, loss: 9.073194087250158e-05\n",
      "step: 4242, loss: 3.755095531232655e-05\n",
      "step: 4243, loss: 0.00016484170919284225\n",
      "step: 4244, loss: 0.00033306621480733156\n",
      "step: 4245, loss: 0.004444637335836887\n",
      "step: 4246, loss: 0.07192759215831757\n",
      "step: 4247, loss: 0.0001223086437676102\n",
      "step: 4248, loss: 2.7492515073390678e-05\n",
      "step: 4249, loss: 4.198312672087923e-05\n",
      "step: 4250, loss: 0.00011847077257698402\n",
      "step: 4251, loss: 0.04509780928492546\n",
      "step: 4252, loss: 2.0592638975358568e-05\n",
      "step: 4253, loss: 3.12180163746234e-05\n",
      "step: 4254, loss: 0.00011546666064532474\n",
      "step: 4255, loss: 0.00015184145013336092\n",
      "step: 4256, loss: 6.987820961512625e-05\n",
      "step: 4257, loss: 4.282408917788416e-05\n",
      "step: 4258, loss: 0.00013229144678916782\n",
      "step: 4259, loss: 5.835307820234448e-05\n",
      "step: 4260, loss: 4.469386476557702e-05\n",
      "step: 4261, loss: 9.240949293598533e-05\n",
      "step: 4262, loss: 0.00026488598086871207\n",
      "step: 4263, loss: 3.899717194144614e-05\n",
      "step: 4264, loss: 0.00022999398061074317\n",
      "step: 4265, loss: 4.523238749243319e-05\n",
      "step: 4266, loss: 8.613183308625594e-05\n",
      "step: 4267, loss: 3.73988805222325e-05\n",
      "step: 4268, loss: 6.753019260941073e-05\n",
      "step: 4269, loss: 8.764347876422107e-05\n",
      "step: 4270, loss: 1.93706619029399e-05\n",
      "step: 4271, loss: 9.461311856284738e-05\n",
      "step: 4272, loss: 0.00010258401016471907\n",
      "step: 4273, loss: 0.0002231550170108676\n",
      "step: 4274, loss: 3.710941018653102e-05\n",
      "step: 4275, loss: 4.232793435221538e-05\n",
      "step: 4276, loss: 0.05427692458033562\n",
      "step: 4277, loss: 4.793247353518382e-05\n",
      "step: 4278, loss: 0.09247905761003494\n",
      "step: 4279, loss: 0.0008001370006240904\n",
      "step: 4280, loss: 0.00023763233912177384\n",
      "step: 4281, loss: 0.00016490202688146383\n",
      "step: 4282, loss: 4.226929013384506e-05\n",
      "step: 4283, loss: 3.3718963095452636e-05\n",
      "step: 4284, loss: 8.804889512248337e-05\n",
      "step: 4285, loss: 0.026092316955327988\n",
      "step: 4286, loss: 2.345332359254826e-05\n",
      "step: 4287, loss: 8.194315159926191e-05\n",
      "step: 4288, loss: 4.599005478667095e-05\n",
      "step: 4289, loss: 4.982474274584092e-05\n",
      "step: 4290, loss: 5.9920559579040855e-05\n",
      "step: 4291, loss: 2.0882205717498437e-05\n",
      "step: 4292, loss: 3.258604192524217e-05\n",
      "step: 4293, loss: 5.152762241777964e-05\n",
      "step: 4294, loss: 2.8460823159548454e-05\n",
      "step: 4295, loss: 5.7867568102665246e-05\n",
      "step: 4296, loss: 0.011733786202967167\n",
      "step: 4297, loss: 0.00015865135355852544\n",
      "step: 4298, loss: 0.0002874844358302653\n",
      "step: 4299, loss: 1.8810580513672903e-05\n",
      "step: 4300, loss: 0.0003079701855313033\n",
      "step: 4301, loss: 0.019408371299505234\n",
      "step: 4302, loss: 0.00021305827249307185\n",
      "step: 4303, loss: 4.969650399289094e-05\n",
      "step: 4304, loss: 8.41348446556367e-05\n",
      "step: 4305, loss: 7.41855037631467e-05\n",
      "step: 4306, loss: 3.1115752790356055e-05\n",
      "step: 4307, loss: 6.590018892893568e-05\n",
      "step: 4308, loss: 5.852780304849148e-05\n",
      "step: 4309, loss: 0.00012030098878312856\n",
      "step: 4310, loss: 0.0001538586220704019\n",
      "step: 4311, loss: 7.12134933564812e-05\n",
      "step: 4312, loss: 0.00011505448492243886\n",
      "step: 4313, loss: 7.18156443326734e-05\n",
      "step: 4314, loss: 0.00014699246094096452\n",
      "step: 4315, loss: 5.639773007715121e-05\n",
      "step: 4316, loss: 0.00011300369078526273\n",
      "step: 4317, loss: 9.637226321501657e-05\n",
      "step: 4318, loss: 2.817164931911975e-05\n",
      "step: 4319, loss: 2.109099477820564e-05\n",
      "step: 4320, loss: 3.707895666593686e-05\n",
      "step: 4321, loss: 0.0052560120820999146\n",
      "step: 4322, loss: 0.00021029116760473698\n",
      "step: 4323, loss: 3.171372736687772e-05\n",
      "step: 4324, loss: 7.540406659245491e-05\n",
      "step: 4325, loss: 3.7100500776432455e-05\n",
      "step: 4326, loss: 1.860768679762259e-05\n",
      "step: 4327, loss: 7.363266922766343e-05\n",
      "step: 4328, loss: 0.00011589036148507148\n",
      "step: 4329, loss: 5.545549720409326e-05\n",
      "step: 4330, loss: 5.205104025662877e-05\n",
      "step: 4331, loss: 0.005165961571037769\n",
      "step: 4332, loss: 3.372495120856911e-05\n",
      "step: 4333, loss: 2.846102688636165e-05\n",
      "step: 4334, loss: 2.9441451260936446e-05\n",
      "step: 4335, loss: 4.5913799112895504e-05\n",
      "step: 4336, loss: 5.129704368300736e-05\n",
      "step: 4337, loss: 4.7266370529541746e-05\n",
      "step: 4338, loss: 3.900758019881323e-05\n",
      "step: 4339, loss: 0.0002178425929741934\n",
      "step: 4340, loss: 8.182747114915401e-05\n",
      "step: 4341, loss: 0.00010127710993401706\n",
      "step: 4342, loss: 7.842710328986868e-05\n",
      "step: 4343, loss: 7.293824455700815e-05\n",
      "step: 4344, loss: 5.523719300981611e-05\n",
      "step: 4345, loss: 5.7001845561899245e-05\n",
      "step: 4346, loss: 0.000190539110917598\n",
      "step: 4347, loss: 4.9459675210528076e-05\n",
      "step: 4348, loss: 5.299499389366247e-05\n",
      "step: 4349, loss: 3.5353667044546455e-05\n",
      "step: 4350, loss: 1.8057029592455365e-05\n",
      "step: 4351, loss: 6.563379429280758e-05\n",
      "step: 4352, loss: 0.0002822834358084947\n",
      "step: 4353, loss: 3.996714440290816e-05\n",
      "step: 4354, loss: 5.20121720910538e-05\n",
      "step: 4355, loss: 0.0001609711180208251\n",
      "step: 4356, loss: 7.244902371894568e-05\n",
      "step: 4357, loss: 3.372881474206224e-05\n",
      "step: 4358, loss: 3.053390537388623e-05\n",
      "step: 4359, loss: 8.336595055880025e-05\n",
      "step: 4360, loss: 3.388656841707416e-05\n",
      "step: 4361, loss: 3.887584534822963e-05\n",
      "step: 4362, loss: 2.9363654903136194e-05\n",
      "step: 4363, loss: 8.740455814404413e-05\n",
      "step: 4364, loss: 2.2977314074523747e-05\n",
      "step: 4365, loss: 9.916782437358052e-05\n",
      "step: 4366, loss: 3.1547726393910125e-05\n",
      "step: 4367, loss: 3.8731835957150906e-05\n",
      "step: 4368, loss: 4.9613998271524906e-05\n",
      "step: 4369, loss: 3.857504270854406e-05\n",
      "step: 4370, loss: 5.809230788145214e-05\n",
      "step: 4371, loss: 1.9292057913844474e-05\n",
      "step: 4372, loss: 2.156394111807458e-05\n",
      "step: 4373, loss: 3.0738381610717624e-05\n",
      "step: 4374, loss: 0.00016594650514889508\n",
      "step: 4375, loss: 3.6206667573424056e-05\n",
      "step: 4376, loss: 9.080066229216754e-05\n",
      "step: 4377, loss: 0.06357504427433014\n",
      "step: 4378, loss: 3.808232577284798e-05\n",
      "step: 4379, loss: 6.902177119627595e-05\n",
      "step: 4380, loss: 5.0194466894026846e-05\n",
      "step: 4381, loss: 9.726499411044642e-05\n",
      "step: 4382, loss: 5.857153155375272e-05\n",
      "step: 4383, loss: 0.00014784552331548184\n",
      "step: 4384, loss: 6.673004827462137e-05\n",
      "step: 4385, loss: 0.04544275626540184\n",
      "step: 4386, loss: 1.9212220649933442e-05\n",
      "step: 4387, loss: 4.498319685808383e-05\n",
      "step: 4388, loss: 2.993319503730163e-05\n",
      "step: 4389, loss: 1.818071905290708e-05\n",
      "step: 4390, loss: 4.839633038500324e-05\n",
      "step: 4391, loss: 3.672617822303437e-05\n",
      "step: 4392, loss: 2.3248525394592434e-05\n",
      "step: 4393, loss: 0.00011423076648497954\n",
      "step: 4394, loss: 3.7585501559078693e-05\n",
      "step: 4395, loss: 6.067440699553117e-05\n",
      "step: 4396, loss: 4.6654156903969124e-05\n",
      "step: 4397, loss: 5.2994484576629475e-05\n",
      "step: 4398, loss: 8.699735917616636e-05\n",
      "step: 4399, loss: 0.00019820351735688746\n",
      "step: 4400, loss: 0.00011259959865128621\n",
      "step: 4401, loss: 2.52684058068553e-05\n",
      "step: 4402, loss: 8.744962542550638e-05\n",
      "step: 4403, loss: 5.969433914287947e-05\n",
      "step: 4404, loss: 7.422286580549553e-05\n",
      "step: 4405, loss: 4.3728585296776146e-05\n",
      "step: 4406, loss: 3.399608249310404e-05\n",
      "step: 4407, loss: 4.599605381372385e-05\n",
      "step: 4408, loss: 0.00011242340406170115\n",
      "step: 4409, loss: 9.087219223147258e-05\n",
      "step: 4410, loss: 5.58875581191387e-05\n",
      "step: 4411, loss: 0.00017652501992415637\n",
      "step: 4412, loss: 6.466294871643186e-05\n",
      "step: 4413, loss: 2.9269778679008596e-05\n",
      "step: 4414, loss: 4.5086628233548254e-05\n",
      "step: 4415, loss: 9.888775821309537e-05\n",
      "step: 4416, loss: 2.9128505047992803e-05\n",
      "step: 4417, loss: 0.02777833491563797\n",
      "step: 4418, loss: 8.311824785778299e-05\n",
      "step: 4419, loss: 9.103379852604121e-05\n",
      "step: 4420, loss: 4.569596785586327e-05\n",
      "step: 4421, loss: 5.455209975480102e-05\n",
      "step: 4422, loss: 0.03837992250919342\n",
      "step: 4423, loss: 3.4682419936871156e-05\n",
      "step: 4424, loss: 1.8951039237435907e-05\n",
      "step: 4425, loss: 4.906582034891471e-05\n",
      "step: 4426, loss: 6.861882866360247e-05\n",
      "step: 4427, loss: 0.035658594220876694\n",
      "step: 4428, loss: 0.00011833767348434776\n",
      "step: 4429, loss: 0.00018386391457170248\n",
      "step: 4430, loss: 3.1327617762144655e-05\n",
      "step: 4431, loss: 3.843116428470239e-05\n",
      "step: 4432, loss: 3.095229476457462e-05\n",
      "step: 4433, loss: 0.000152652122778818\n",
      "step: 4434, loss: 0.00019525755487848073\n",
      "step: 4435, loss: 5.012795372749679e-05\n",
      "step: 4436, loss: 2.8463407943490893e-05\n",
      "step: 4437, loss: 6.177614704938605e-05\n",
      "step: 4438, loss: 0.00021713896421715617\n",
      "step: 4439, loss: 3.7401303416118026e-05\n",
      "step: 4440, loss: 8.800675277598202e-05\n",
      "step: 4441, loss: 0.00012930417142342776\n",
      "step: 4442, loss: 4.717594129033387e-05\n",
      "step: 4443, loss: 2.893063719966449e-05\n",
      "step: 4444, loss: 0.00019296511891297996\n",
      "step: 4445, loss: 5.401128146331757e-05\n",
      "step: 4446, loss: 2.8009346351609565e-05\n",
      "step: 4447, loss: 0.021083062514662743\n",
      "step: 4448, loss: 0.09805260598659515\n",
      "step: 4449, loss: 1.4585339158657007e-05\n",
      "step: 4450, loss: 3.76087446056772e-05\n",
      "step: 4451, loss: 2.2159367290441878e-05\n",
      "step: 4452, loss: 0.00010599879897199571\n",
      "step: 4453, loss: 1.908869671751745e-05\n",
      "step: 4454, loss: 6.394350930349901e-05\n",
      "step: 4455, loss: 4.26531805715058e-05\n",
      "step: 4456, loss: 6.769939500372857e-05\n",
      "step: 4457, loss: 3.3554908441146836e-05\n",
      "step: 4458, loss: 1.3024421605223324e-05\n",
      "step: 4459, loss: 0.045843347907066345\n",
      "step: 4460, loss: 6.799009861424565e-05\n",
      "step: 4461, loss: 6.002007648930885e-05\n",
      "step: 4462, loss: 4.4558448280440643e-05\n",
      "step: 4463, loss: 0.0004803776100743562\n",
      "step: 4464, loss: 1.9551842342480086e-05\n",
      "step: 4465, loss: 3.282104080426507e-05\n",
      "step: 4466, loss: 4.4404223444871604e-05\n",
      "step: 4467, loss: 2.542191396059934e-05\n",
      "step: 4468, loss: 4.040906787849963e-05\n",
      "step: 4469, loss: 0.03486519679427147\n",
      "step: 4470, loss: 0.04452984780073166\n",
      "step: 4471, loss: 3.377159737283364e-05\n",
      "step: 4472, loss: 0.0003037569404114038\n",
      "step: 4473, loss: 7.560134690720588e-05\n",
      "step: 4474, loss: 2.463820783304982e-05\n",
      "step: 4475, loss: 2.2640504539594986e-05\n",
      "step: 4476, loss: 1.6519428754691035e-05\n",
      "step: 4477, loss: 0.00020973778737243265\n",
      "step: 4478, loss: 8.219809387810528e-05\n",
      "step: 4479, loss: 7.890570850577205e-05\n",
      "step: 4480, loss: 4.5140852307667956e-05\n",
      "step: 4481, loss: 2.825065894285217e-05\n",
      "step: 4482, loss: 3.0539838917320594e-05\n",
      "step: 4483, loss: 3.192698204657063e-05\n",
      "step: 4484, loss: 2.8891297915834002e-05\n",
      "step: 4485, loss: 6.732245674356818e-05\n",
      "step: 4486, loss: 0.011188519187271595\n",
      "step: 4487, loss: 6.87445281073451e-05\n",
      "step: 4488, loss: 0.06442505121231079\n",
      "step: 4489, loss: 0.00018392299534752965\n",
      "step: 4490, loss: 5.2657804189948365e-05\n",
      "step: 4491, loss: 4.857589374296367e-05\n",
      "step: 4492, loss: 0.00010001010377891362\n",
      "step: 4493, loss: 0.00010777110583148897\n",
      "step: 4494, loss: 4.9409427447244525e-05\n",
      "step: 4495, loss: 9.385240991832688e-05\n",
      "step: 4496, loss: 0.025866637006402016\n",
      "step: 4497, loss: 9.19788217288442e-05\n",
      "step: 4498, loss: 4.372472903924063e-05\n",
      "step: 4499, loss: 0.00012067921488778666\n",
      "step: 4500, loss: 6.270457379287109e-05\n",
      "step: 4501, loss: 5.8285921113565564e-05\n",
      "step: 4502, loss: 0.022065382450819016\n",
      "step: 4503, loss: 5.090153717901558e-05\n",
      "step: 4504, loss: 3.0290304493973963e-05\n",
      "step: 4505, loss: 6.0894060879945755e-05\n",
      "step: 4506, loss: 3.236678458051756e-05\n",
      "step: 4507, loss: 0.00013062429206911474\n",
      "step: 4508, loss: 0.05796876549720764\n",
      "step: 4509, loss: 2.872756522265263e-05\n",
      "step: 4510, loss: 0.0001622622658032924\n",
      "step: 4511, loss: 6.324554124148563e-05\n",
      "step: 4512, loss: 0.00014438468497246504\n",
      "step: 4513, loss: 0.00025586760602891445\n",
      "step: 4514, loss: 5.145856266608462e-05\n",
      "step: 4515, loss: 8.066375448834151e-05\n",
      "step: 4516, loss: 0.0001160374958999455\n",
      "step: 4517, loss: 5.7946013839682564e-05\n",
      "step: 4518, loss: 4.32381930295378e-05\n",
      "step: 4519, loss: 0.05209715664386749\n",
      "step: 4520, loss: 0.00013108218263369054\n",
      "step: 4521, loss: 3.455605110502802e-05\n",
      "step: 4522, loss: 1.680081186350435e-05\n",
      "step: 4523, loss: 6.935456622159109e-05\n",
      "step: 4524, loss: 8.546931348973885e-05\n",
      "step: 4525, loss: 0.009341100230813026\n",
      "step: 4526, loss: 7.856055890442804e-05\n",
      "step: 4527, loss: 2.9098309823893942e-05\n",
      "step: 4528, loss: 1.9704018995980732e-05\n",
      "step: 4529, loss: 3.6103996535530314e-05\n",
      "step: 4530, loss: 3.9645881770411506e-05\n",
      "step: 4531, loss: 2.256737570860423e-05\n",
      "step: 4532, loss: 3.7378387787612155e-05\n",
      "step: 4533, loss: 2.5008199372678064e-05\n",
      "step: 4534, loss: 2.8044943974236958e-05\n",
      "step: 4535, loss: 8.413913019467145e-05\n",
      "step: 4536, loss: 9.845940076047555e-05\n",
      "step: 4537, loss: 4.1661773138912395e-05\n",
      "step: 4538, loss: 4.2762967495946214e-05\n",
      "step: 4539, loss: 8.174862887244672e-05\n",
      "step: 4540, loss: 4.3983232899336144e-05\n",
      "step: 4541, loss: 6.887616473250091e-05\n",
      "step: 4542, loss: 3.2713429391151294e-05\n",
      "step: 4543, loss: 0.00013398626470007002\n",
      "step: 4544, loss: 7.466015813406557e-05\n",
      "step: 4545, loss: 9.462530215387233e-06\n",
      "step: 4546, loss: 7.754252146696672e-05\n",
      "step: 4547, loss: 2.3781851268722676e-05\n",
      "step: 4548, loss: 0.00012522778706625104\n",
      "step: 4549, loss: 0.02407481148838997\n",
      "step: 4550, loss: 4.7997735237004235e-05\n",
      "step: 4551, loss: 0.039037514477968216\n",
      "step: 4552, loss: 4.119049845030531e-05\n",
      "step: 4553, loss: 3.678706707432866e-05\n",
      "step: 4554, loss: 0.0314425453543663\n",
      "step: 4555, loss: 6.596889579668641e-05\n",
      "step: 4556, loss: 5.256428266875446e-05\n",
      "step: 4557, loss: 0.0001004058140097186\n",
      "step: 4558, loss: 4.373813499114476e-05\n",
      "step: 4559, loss: 2.5911389457178302e-05\n",
      "step: 4560, loss: 2.463635610183701e-05\n",
      "step: 4561, loss: 9.731519821798429e-05\n",
      "step: 4562, loss: 7.046769314911216e-05\n",
      "step: 4563, loss: 3.8987545849522576e-05\n",
      "step: 4564, loss: 2.7713807867257856e-05\n",
      "step: 4565, loss: 1.4805311366217211e-05\n",
      "step: 4566, loss: 2.2846126739750616e-05\n",
      "step: 4567, loss: 3.4184413379989564e-05\n",
      "step: 4568, loss: 0.0023137349635362625\n",
      "step: 4569, loss: 9.152422717306763e-05\n",
      "step: 4570, loss: 5.714384315069765e-05\n",
      "step: 4571, loss: 1.742491076583974e-05\n",
      "step: 4572, loss: 4.581187749863602e-05\n",
      "step: 4573, loss: 5.457181032397784e-05\n",
      "step: 4574, loss: 3.285062484792434e-05\n",
      "step: 4575, loss: 2.9930197342764586e-05\n",
      "step: 4576, loss: 0.0001379717723466456\n",
      "step: 4577, loss: 2.679774297575932e-05\n",
      "step: 4578, loss: 9.550780669087544e-05\n",
      "step: 4579, loss: 3.5327197110746056e-05\n",
      "step: 4580, loss: 5.701747795683332e-05\n",
      "step: 4581, loss: 2.589951327536255e-05\n",
      "step: 4582, loss: 9.63816128205508e-05\n",
      "step: 4583, loss: 2.2233754862099886e-05\n",
      "step: 4584, loss: 5.314518784871325e-05\n",
      "step: 4585, loss: 0.05943020433187485\n",
      "step: 4586, loss: 3.242796447011642e-05\n",
      "step: 4587, loss: 1.327443533227779e-05\n",
      "step: 4588, loss: 8.279982284875587e-05\n",
      "step: 4589, loss: 2.1242396542220376e-05\n",
      "step: 4590, loss: 2.0133546058787033e-05\n",
      "step: 4591, loss: 4.8879057430895045e-05\n",
      "step: 4592, loss: 5.814597170683555e-05\n",
      "step: 4593, loss: 5.486785448738374e-05\n",
      "step: 4594, loss: 0.03439457342028618\n",
      "step: 4595, loss: 6.720594683429226e-05\n",
      "step: 4596, loss: 8.760885975789279e-05\n",
      "step: 4597, loss: 4.899760097032413e-05\n",
      "step: 4598, loss: 3.809205736615695e-05\n",
      "step: 4599, loss: 6.645117537118495e-05\n",
      "step: 4600, loss: 2.8509959520306438e-05\n",
      "step: 4601, loss: 6.589524127775803e-05\n",
      "step: 4602, loss: 3.986691444879398e-05\n",
      "step: 4603, loss: 4.957043711328879e-05\n",
      "step: 4604, loss: 1.899827657325659e-05\n",
      "step: 4605, loss: 0.00014907485456205904\n",
      "step: 4606, loss: 0.015906669199466705\n",
      "step: 4607, loss: 0.0002062899002339691\n",
      "step: 4608, loss: 0.05218955874443054\n",
      "step: 4609, loss: 9.357725502923131e-05\n",
      "step: 4610, loss: 3.649364589364268e-05\n",
      "step: 4611, loss: 0.00015128804079722613\n",
      "step: 4612, loss: 4.292616722523235e-05\n",
      "step: 4613, loss: 3.4099713957402855e-05\n",
      "step: 4614, loss: 6.769409083062783e-05\n",
      "step: 4615, loss: 3.4656164643820375e-05\n",
      "step: 4616, loss: 3.681682937894948e-05\n",
      "step: 4617, loss: 3.959479363402352e-05\n",
      "step: 4618, loss: 3.745421054190956e-05\n",
      "step: 4619, loss: 2.8104574084863998e-05\n",
      "step: 4620, loss: 2.0265493731130846e-05\n",
      "step: 4621, loss: 5.748057446908206e-05\n",
      "step: 4622, loss: 8.208605322579388e-06\n",
      "step: 4623, loss: 4.200961484457366e-05\n",
      "step: 4624, loss: 1.782329854904674e-05\n",
      "step: 4625, loss: 0.0022745353635400534\n",
      "step: 4626, loss: 9.68553067650646e-05\n",
      "step: 4627, loss: 2.9249555154819973e-05\n",
      "step: 4628, loss: 3.895152985933237e-05\n",
      "step: 4629, loss: 0.020018422976136208\n",
      "step: 4630, loss: 7.689215999562293e-05\n",
      "step: 4631, loss: 3.345741788507439e-05\n",
      "step: 4632, loss: 4.035254823975265e-05\n",
      "step: 4633, loss: 4.7268698835978284e-05\n",
      "step: 4634, loss: 5.5692802561679855e-05\n",
      "step: 4635, loss: 1.995558886846993e-05\n",
      "step: 4636, loss: 8.724191138753667e-05\n",
      "step: 4637, loss: 2.4455790480715223e-05\n",
      "step: 4638, loss: 2.473642052791547e-05\n",
      "step: 4639, loss: 2.307734393980354e-05\n",
      "step: 4640, loss: 3.6063102015759796e-05\n",
      "step: 4641, loss: 1.987513132917229e-05\n",
      "step: 4642, loss: 1.5592009731335565e-05\n",
      "step: 4643, loss: 2.5949484552256763e-05\n",
      "step: 4644, loss: 0.0003114795545116067\n",
      "step: 4645, loss: 3.488649963401258e-05\n",
      "step: 4646, loss: 3.100203684880398e-05\n",
      "step: 4647, loss: 7.394680142169818e-05\n",
      "step: 4648, loss: 2.382016282354016e-05\n",
      "step: 4649, loss: 9.642281656851992e-05\n",
      "step: 4650, loss: 0.00014493532944470644\n",
      "step: 4651, loss: 1.4516645023832098e-05\n",
      "step: 4652, loss: 0.0002568035270087421\n",
      "step: 4653, loss: 3.475135599728674e-05\n",
      "step: 4654, loss: 0.00016459563630633056\n",
      "step: 4655, loss: 2.767549449345097e-05\n",
      "step: 4656, loss: 0.04156283289194107\n",
      "step: 4657, loss: 0.00010385892528574914\n",
      "step: 4658, loss: 5.1322203944437206e-05\n",
      "step: 4659, loss: 3.4418630093568936e-05\n",
      "step: 4660, loss: 2.262259295093827e-05\n",
      "step: 4661, loss: 3.962388655054383e-05\n",
      "step: 4662, loss: 3.963303242926486e-05\n",
      "step: 4663, loss: 2.2828306100564077e-05\n",
      "step: 4664, loss: 3.6640107282437384e-05\n",
      "step: 4665, loss: 0.0001524037797935307\n",
      "step: 4666, loss: 2.676164876902476e-05\n",
      "step: 4667, loss: 1.4561938769475091e-05\n",
      "step: 4668, loss: 4.9535832658875734e-05\n",
      "step: 4669, loss: 3.05294270219747e-05\n",
      "step: 4670, loss: 0.03219325840473175\n",
      "step: 4671, loss: 1.9972887457697652e-05\n",
      "step: 4672, loss: 9.092117397813126e-05\n",
      "step: 4673, loss: 3.6316283512860537e-05\n",
      "step: 4674, loss: 0.00014893882325850427\n",
      "step: 4675, loss: 1.8121287212125026e-05\n",
      "step: 4676, loss: 8.689129026606679e-05\n",
      "step: 4677, loss: 6.320676038740203e-05\n",
      "step: 4678, loss: 1.6566506019444205e-05\n",
      "step: 4679, loss: 3.462355743977241e-05\n",
      "step: 4680, loss: 3.3137897844426334e-05\n",
      "step: 4681, loss: 0.00022062154312152416\n",
      "step: 4682, loss: 9.465011316933669e-06\n",
      "step: 4683, loss: 1.7308399037574418e-05\n",
      "step: 4684, loss: 2.443370431137737e-05\n",
      "step: 4685, loss: 2.1964251573081128e-05\n",
      "step: 4686, loss: 5.5821594287408516e-05\n",
      "step: 4687, loss: 1.3682312783203088e-05\n",
      "step: 4688, loss: 2.1424762962851673e-05\n",
      "step: 4689, loss: 0.0013548685237765312\n",
      "step: 4690, loss: 0.00028212208417244256\n",
      "step: 4691, loss: 9.649407002143562e-05\n",
      "step: 4692, loss: 2.2333382730721496e-05\n",
      "step: 4693, loss: 0.00151394575368613\n",
      "step: 4694, loss: 5.790003706351854e-05\n",
      "step: 4695, loss: 1.4964965885155834e-05\n",
      "step: 4696, loss: 3.0219054679037072e-05\n",
      "step: 4697, loss: 6.394136289600283e-05\n",
      "step: 4698, loss: 7.305117469513789e-05\n",
      "step: 4699, loss: 2.7180281904293224e-05\n",
      "step: 4700, loss: 0.000239722547121346\n",
      "step: 4701, loss: 8.672681724419817e-05\n",
      "step: 4702, loss: 4.4785560021409765e-05\n",
      "step: 4703, loss: 0.00028540563653223217\n",
      "step: 4704, loss: 5.1959599659312516e-05\n",
      "step: 4705, loss: 2.427159051876515e-05\n",
      "step: 4706, loss: 2.659341407706961e-05\n",
      "step: 4707, loss: 0.00047037002514116466\n",
      "step: 4708, loss: 2.2969859855948016e-05\n",
      "step: 4709, loss: 1.87029527296545e-05\n",
      "step: 4710, loss: 1.699586027825717e-05\n",
      "step: 4711, loss: 0.00019453521235845983\n",
      "step: 4712, loss: 2.2325701138470322e-05\n",
      "step: 4713, loss: 0.00012343534035608172\n",
      "step: 4714, loss: 6.246238626772538e-05\n",
      "step: 4715, loss: 2.202247742388863e-05\n",
      "step: 4716, loss: 0.00018580496544018388\n",
      "step: 4717, loss: 5.8563469792716205e-05\n",
      "step: 4718, loss: 0.04730281978845596\n",
      "step: 4719, loss: 5.77776700083632e-05\n",
      "step: 4720, loss: 8.438571239821613e-05\n",
      "step: 4721, loss: 0.00021303277753759176\n",
      "step: 4722, loss: 7.810520037310198e-05\n",
      "step: 4723, loss: 2.2302680008579046e-05\n",
      "step: 4724, loss: 5.699986650142819e-05\n",
      "step: 4725, loss: 0.0001317140704486519\n",
      "step: 4726, loss: 2.3775348381605e-05\n",
      "step: 4727, loss: 4.247198376106098e-05\n",
      "step: 4728, loss: 0.00011444586561992764\n",
      "step: 4729, loss: 4.207880192552693e-05\n",
      "step: 4730, loss: 0.011656403541564941\n",
      "step: 4731, loss: 3.376428867341019e-05\n",
      "step: 4732, loss: 6.114147254265845e-05\n",
      "step: 4733, loss: 0.03321460634469986\n",
      "step: 4734, loss: 6.13143783994019e-05\n",
      "step: 4735, loss: 3.251299131079577e-05\n",
      "step: 4736, loss: 1.4101723536441568e-05\n",
      "step: 4737, loss: 0.00020756936282850802\n",
      "step: 4738, loss: 5.339995914255269e-05\n",
      "step: 4739, loss: 0.11053898930549622\n",
      "step: 4740, loss: 2.850537930498831e-05\n",
      "step: 4741, loss: 5.9503345255507156e-05\n",
      "step: 4742, loss: 0.00015676915063522756\n",
      "step: 4743, loss: 8.214381523430347e-05\n",
      "step: 4744, loss: 2.946845233964268e-05\n",
      "step: 4745, loss: 0.014304974116384983\n",
      "step: 4746, loss: 1.709995194687508e-05\n",
      "step: 4747, loss: 0.09910648316144943\n",
      "step: 4748, loss: 3.275760536780581e-05\n",
      "step: 4749, loss: 1.809029890864622e-05\n",
      "step: 4750, loss: 8.592285303166136e-05\n",
      "step: 4751, loss: 3.5889264836441725e-05\n",
      "step: 4752, loss: 2.6504993002163246e-05\n",
      "step: 4753, loss: 2.2363383322954178e-05\n",
      "step: 4754, loss: 3.1754301744513214e-05\n",
      "step: 4755, loss: 3.9234062569448724e-05\n",
      "step: 4756, loss: 5.487364614964463e-05\n",
      "step: 4757, loss: 0.00013315246906131506\n",
      "step: 4758, loss: 6.76334893796593e-05\n",
      "step: 4759, loss: 3.0947107006795704e-05\n",
      "step: 4760, loss: 2.9743448976660147e-05\n",
      "step: 4761, loss: 3.7053931009722874e-05\n",
      "step: 4762, loss: 0.0006367796449922025\n",
      "step: 4763, loss: 0.00018103652109857649\n",
      "step: 4764, loss: 1.642156348680146e-05\n",
      "step: 4765, loss: 0.00011169637582497671\n",
      "step: 4766, loss: 2.7684987799148075e-05\n",
      "step: 4767, loss: 0.00013582910469267517\n",
      "step: 4768, loss: 8.536073437426239e-05\n",
      "step: 4769, loss: 6.601800851058215e-05\n",
      "step: 4770, loss: 2.4453307560179383e-05\n",
      "step: 4771, loss: 3.516743527143262e-05\n",
      "step: 4772, loss: 0.027562879025936127\n",
      "step: 4773, loss: 0.004881586413830519\n",
      "step: 4774, loss: 0.00019972868904005736\n",
      "step: 4775, loss: 8.779084600973874e-05\n",
      "step: 4776, loss: 0.000101674952020403\n",
      "step: 4777, loss: 9.75053189904429e-05\n",
      "step: 4778, loss: 0.00013697001850232482\n",
      "step: 4779, loss: 0.00011740720947273076\n",
      "step: 4780, loss: 2.9413178708637133e-05\n",
      "step: 4781, loss: 5.1549555792007595e-05\n",
      "step: 4782, loss: 0.03847610577940941\n",
      "step: 4783, loss: 0.0369708314538002\n",
      "step: 4784, loss: 5.067669917480089e-05\n",
      "step: 4785, loss: 3.740659303730354e-05\n",
      "step: 4786, loss: 2.29147044592537e-05\n",
      "step: 4787, loss: 0.00011120345152448863\n",
      "step: 4788, loss: 1.7484522686572745e-05\n",
      "step: 4789, loss: 8.448366133961827e-05\n",
      "step: 4790, loss: 0.00011174706742167473\n",
      "step: 4791, loss: 4.1682789742480963e-05\n",
      "step: 4792, loss: 2.727319224504754e-05\n",
      "step: 4793, loss: 3.8419238990172744e-05\n",
      "step: 4794, loss: 0.0001605008146725595\n",
      "step: 4795, loss: 5.897163282497786e-05\n",
      "step: 4796, loss: 4.788682781509124e-05\n",
      "step: 4797, loss: 0.00016911209968384355\n",
      "step: 4798, loss: 9.32825350901112e-05\n",
      "step: 4799, loss: 6.500648305518553e-05\n",
      "step: 4800, loss: 2.9929769880254753e-05\n",
      "step: 4801, loss: 4.780595190823078e-05\n",
      "step: 4802, loss: 9.96657254290767e-05\n",
      "step: 4803, loss: 3.702311732922681e-05\n",
      "step: 4804, loss: 9.368086466565728e-05\n",
      "step: 4805, loss: 3.10857649310492e-05\n",
      "step: 4806, loss: 0.00010719143756432459\n",
      "step: 4807, loss: 3.823932274826802e-05\n",
      "step: 4808, loss: 2.1119903976796195e-05\n",
      "step: 4809, loss: 1.8900673239841126e-05\n",
      "step: 4810, loss: 0.00017305607616435736\n",
      "step: 4811, loss: 0.00017693113477434963\n",
      "step: 4812, loss: 6.34925308986567e-05\n",
      "step: 4813, loss: 9.507898357696831e-05\n",
      "step: 4814, loss: 2.6936304493574426e-05\n",
      "step: 4815, loss: 6.96087590768002e-05\n",
      "step: 4816, loss: 2.0128638425376266e-05\n",
      "step: 4817, loss: 3.273275797255337e-05\n",
      "step: 4818, loss: 0.00010640566324582323\n",
      "step: 4819, loss: 1.9066377717535943e-05\n",
      "step: 4820, loss: 6.913592369528487e-05\n",
      "step: 4821, loss: 0.00011754783190554008\n",
      "step: 4822, loss: 0.00013811564713250846\n",
      "step: 4823, loss: 6.50364818284288e-05\n",
      "step: 4824, loss: 0.01673576794564724\n",
      "step: 4825, loss: 6.868290802231058e-05\n",
      "step: 4826, loss: 9.384367876918986e-05\n",
      "step: 4827, loss: 2.9484543119906448e-05\n",
      "step: 4828, loss: 0.00034296829835511744\n",
      "step: 4829, loss: 0.04214060679078102\n",
      "step: 4830, loss: 3.719432788784616e-05\n",
      "step: 4831, loss: 4.431211345945485e-05\n",
      "step: 4832, loss: 5.859031807631254e-05\n",
      "step: 4833, loss: 2.828600372595247e-05\n",
      "step: 4834, loss: 7.816736615495756e-05\n",
      "step: 4835, loss: 3.831468347925693e-05\n",
      "step: 4836, loss: 1.4094456673774403e-05\n",
      "step: 4837, loss: 7.85874217399396e-05\n",
      "step: 4838, loss: 1.9079605408478528e-05\n",
      "step: 4839, loss: 0.05651109293103218\n",
      "step: 4840, loss: 4.2285748349968344e-05\n",
      "step: 4841, loss: 8.644908666610718e-05\n",
      "step: 4842, loss: 3.5547931474866346e-05\n",
      "step: 4843, loss: 4.3014784750994295e-05\n",
      "step: 4844, loss: 2.3851489459048025e-05\n",
      "step: 4845, loss: 5.758649786002934e-05\n",
      "step: 4846, loss: 0.00014572148211300373\n",
      "step: 4847, loss: 5.2401981520233676e-05\n",
      "step: 4848, loss: 0.003704281523823738\n",
      "step: 4849, loss: 0.024005413055419922\n",
      "step: 4850, loss: 9.28037625271827e-05\n",
      "step: 4851, loss: 0.01918872818350792\n",
      "step: 4852, loss: 0.0001189018294098787\n",
      "step: 4853, loss: 2.9162609280319884e-05\n",
      "step: 4854, loss: 0.08501715958118439\n",
      "step: 4855, loss: 7.206237205537036e-05\n",
      "step: 4856, loss: 2.6139538022107445e-05\n",
      "step: 4857, loss: 3.420937355258502e-05\n",
      "step: 4858, loss: 4.1743420297279954e-05\n",
      "step: 4859, loss: 3.77199066861067e-05\n",
      "step: 4860, loss: 0.0006363942520692945\n",
      "step: 4861, loss: 1.50433843373321e-05\n",
      "step: 4862, loss: 4.9661839511827566e-06\n",
      "step: 4863, loss: 2.454071000101976e-05\n",
      "step: 4864, loss: 0.00014883329276926816\n",
      "step: 4865, loss: 4.387023363960907e-05\n",
      "step: 4866, loss: 0.0001192381969303824\n",
      "step: 4867, loss: 3.964445204474032e-05\n",
      "step: 4868, loss: 2.8463398848543875e-05\n",
      "step: 4869, loss: 8.348398841917515e-05\n",
      "step: 4870, loss: 2.4614946596557274e-05\n",
      "step: 4871, loss: 2.746840618783608e-05\n",
      "step: 4872, loss: 2.7465857783681713e-05\n",
      "step: 4873, loss: 0.019536349922418594\n",
      "step: 4874, loss: 0.00010938397463178262\n",
      "step: 4875, loss: 0.0016640438698232174\n",
      "step: 4876, loss: 9.436181244382169e-06\n",
      "step: 4877, loss: 0.04777050018310547\n",
      "step: 4878, loss: 4.9057034630095586e-05\n",
      "step: 4879, loss: 6.413782830350101e-05\n",
      "step: 4880, loss: 3.0562066967831925e-05\n",
      "step: 4881, loss: 7.178782834671438e-05\n",
      "step: 4882, loss: 0.00015600233746226877\n",
      "step: 4883, loss: 0.025599634274840355\n",
      "step: 4884, loss: 3.924439079128206e-05\n",
      "step: 4885, loss: 0.00010407171794213355\n",
      "step: 4886, loss: 0.001917217276059091\n",
      "step: 4887, loss: 0.0002400740486336872\n",
      "step: 4888, loss: 0.008312271907925606\n",
      "step: 4889, loss: 3.7509467802010477e-05\n",
      "step: 4890, loss: 4.775186607730575e-05\n",
      "step: 4891, loss: 9.296793723478913e-05\n",
      "step: 4892, loss: 6.10210990998894e-05\n",
      "step: 4893, loss: 0.00020168785704299808\n",
      "step: 4894, loss: 3.511525574140251e-05\n",
      "step: 4895, loss: 9.04645785340108e-05\n",
      "step: 4896, loss: 2.4828719688230194e-05\n",
      "step: 4897, loss: 0.00020133066573180258\n",
      "step: 4898, loss: 7.137474312912673e-05\n",
      "step: 4899, loss: 7.768135401420295e-05\n",
      "step: 4900, loss: 4.280004577594809e-05\n",
      "step: 4901, loss: 0.0003744020941667259\n",
      "step: 4902, loss: 5.3420342737808824e-05\n",
      "step: 4903, loss: 8.080621046246961e-05\n",
      "step: 4904, loss: 6.245837721507996e-05\n",
      "step: 4905, loss: 0.0003062558826059103\n",
      "step: 4906, loss: 7.863601058488712e-05\n",
      "step: 4907, loss: 6.147701787995175e-05\n",
      "step: 4908, loss: 0.00015109551895875484\n",
      "step: 4909, loss: 3.9830054447520524e-05\n",
      "step: 4910, loss: 6.933468102943152e-05\n",
      "step: 4911, loss: 9.33858027565293e-05\n",
      "step: 4912, loss: 8.79088620422408e-05\n",
      "step: 4913, loss: 3.163234214298427e-05\n",
      "step: 4914, loss: 0.00030878218240104616\n",
      "step: 4915, loss: 5.7687677326612175e-05\n",
      "step: 4916, loss: 9.262351522920653e-05\n",
      "step: 4917, loss: 0.000166887286468409\n",
      "step: 4918, loss: 1.5851464922889136e-05\n",
      "step: 4919, loss: 0.00017447923892177641\n",
      "step: 4920, loss: 5.556960968533531e-05\n",
      "step: 4921, loss: 0.00038983923150226474\n",
      "step: 4922, loss: 5.502566637005657e-05\n",
      "step: 4923, loss: 3.773249773075804e-05\n",
      "step: 4924, loss: 0.00018363639537710696\n",
      "step: 4925, loss: 0.058280326426029205\n",
      "step: 4926, loss: 0.00030102531309239566\n",
      "step: 4927, loss: 8.448045991826802e-05\n",
      "step: 4928, loss: 5.575197792495601e-05\n",
      "step: 4929, loss: 8.992833318188787e-05\n",
      "step: 4930, loss: 1.9419871023274027e-05\n",
      "step: 4931, loss: 6.398015830200166e-05\n",
      "step: 4932, loss: 0.00018052315863315016\n",
      "step: 4933, loss: 6.875893632241059e-06\n",
      "step: 4934, loss: 0.09557585418224335\n",
      "step: 4935, loss: 1.75201003003167e-05\n",
      "step: 4936, loss: 0.00011007438297383487\n",
      "step: 4937, loss: 8.548895857529715e-05\n",
      "step: 4938, loss: 0.03491653874516487\n",
      "step: 4939, loss: 4.775940396939404e-05\n",
      "step: 4940, loss: 0.00011828881542896852\n",
      "step: 4941, loss: 4.3598141928669065e-05\n",
      "step: 4942, loss: 2.6519244784140028e-05\n",
      "step: 4943, loss: 0.00013004534412175417\n",
      "step: 4944, loss: 3.272761750849895e-05\n",
      "step: 4945, loss: 3.833439768641256e-05\n",
      "step: 4946, loss: 3.153462603222579e-05\n",
      "step: 4947, loss: 3.32135969074443e-05\n",
      "step: 4948, loss: 2.792249506455846e-05\n",
      "step: 4949, loss: 4.708488631877117e-05\n",
      "step: 4950, loss: 8.532220817869529e-05\n",
      "step: 4951, loss: 1.4590125829272438e-05\n",
      "step: 4952, loss: 0.00012557799345813692\n",
      "step: 4953, loss: 6.502179167000577e-05\n",
      "step: 4954, loss: 6.733495683874935e-05\n",
      "step: 4955, loss: 0.00010313290840713307\n",
      "step: 4956, loss: 0.00014176667900756001\n",
      "step: 4957, loss: 2.1589863536064513e-05\n",
      "step: 4958, loss: 6.871599907753989e-05\n",
      "step: 4959, loss: 3.07872069242876e-05\n",
      "step: 4960, loss: 4.8341957153752446e-05\n",
      "step: 4961, loss: 0.00011909743625437841\n",
      "step: 4962, loss: 7.194008503574878e-05\n",
      "step: 4963, loss: 0.033973466604948044\n",
      "step: 4964, loss: 4.16200200561434e-05\n",
      "step: 4965, loss: 0.00016786178457550704\n",
      "step: 4966, loss: 0.0065684812143445015\n",
      "step: 4967, loss: 5.931668056291528e-05\n",
      "step: 4968, loss: 0.00012759746459778398\n",
      "step: 4969, loss: 3.581617056624964e-05\n",
      "step: 4970, loss: 5.37650084879715e-05\n",
      "step: 4971, loss: 2.7173065973329358e-05\n",
      "step: 4972, loss: 6.0648148064501584e-05\n",
      "step: 4973, loss: 4.321656524552964e-05\n",
      "step: 4974, loss: 0.00013535920879803598\n",
      "step: 4975, loss: 2.7335199774825014e-05\n",
      "step: 4976, loss: 6.146945815999061e-05\n",
      "step: 4977, loss: 6.264397234190255e-05\n",
      "step: 4978, loss: 8.754273585509509e-05\n",
      "step: 4979, loss: 6.45198961137794e-05\n",
      "step: 4980, loss: 1.7683552869129926e-05\n",
      "step: 4981, loss: 5.116478860145435e-05\n",
      "step: 4982, loss: 5.5685584811726585e-05\n",
      "step: 4983, loss: 3.453205499681644e-05\n",
      "step: 4984, loss: 3.282113539171405e-05\n",
      "step: 4985, loss: 0.02346140332520008\n",
      "step: 4986, loss: 2.2170599550008774e-05\n",
      "step: 4987, loss: 8.846815035212785e-05\n",
      "step: 4988, loss: 7.565923442598432e-05\n",
      "step: 4989, loss: 3.444388130446896e-05\n",
      "step: 4990, loss: 1.9513188817654736e-05\n",
      "step: 4991, loss: 1.7844891772256233e-05\n",
      "step: 4992, loss: 6.136202864581719e-05\n",
      "step: 4993, loss: 3.229055437259376e-05\n",
      "step: 4994, loss: 0.0583551786839962\n",
      "step: 4995, loss: 0.018572231754660606\n",
      "step: 4996, loss: 0.00013243711146060377\n",
      "step: 4997, loss: 1.8874350644182414e-05\n",
      "step: 4998, loss: 4.666188397095539e-05\n",
      "step: 4999, loss: 3.3680760679999366e-05\n",
      "step: 5000, loss: 0.00010108740389114246\n",
      "step: 5001, loss: 3.118529275525361e-05\n",
      "step: 5002, loss: 6.585908704437315e-05\n",
      "step: 5003, loss: 4.6106022637104616e-05\n",
      "step: 5004, loss: 5.787215195596218e-05\n",
      "step: 5005, loss: 2.2083386284066364e-05\n",
      "step: 5006, loss: 5.077399328001775e-05\n",
      "step: 5007, loss: 6.955092248972505e-05\n",
      "step: 5008, loss: 5.1392304158071056e-05\n",
      "step: 5009, loss: 4.150030144955963e-05\n",
      "step: 5010, loss: 2.4896256945794448e-05\n",
      "step: 5011, loss: 4.665467713493854e-05\n",
      "step: 5012, loss: 1.2041977242915891e-05\n",
      "step: 5013, loss: 5.378380592446774e-05\n",
      "step: 5014, loss: 2.4178149033105e-05\n",
      "step: 5015, loss: 2.3681745005887933e-05\n",
      "step: 5016, loss: 1.0354297955927905e-05\n",
      "step: 5017, loss: 5.19794957654085e-05\n",
      "step: 5018, loss: 4.576663923216984e-05\n",
      "step: 5019, loss: 6.44614701741375e-05\n",
      "step: 5020, loss: 3.0397146474570036e-05\n",
      "step: 5021, loss: 0.0015195708256214857\n",
      "step: 5022, loss: 2.355389551667031e-05\n",
      "step: 5023, loss: 6.40303551335819e-05\n",
      "step: 5024, loss: 7.751418888801709e-05\n",
      "step: 5025, loss: 1.5458477719221264e-05\n",
      "step: 5026, loss: 2.0188053895253688e-05\n",
      "step: 5027, loss: 4.5204149500932544e-05\n",
      "step: 5028, loss: 2.4969504011096433e-05\n",
      "step: 5029, loss: 4.0573264413978904e-05\n",
      "step: 5030, loss: 0.010873224586248398\n",
      "step: 5031, loss: 3.72612958017271e-05\n",
      "step: 5032, loss: 6.696373748127371e-05\n",
      "step: 5033, loss: 2.6708537916420028e-05\n",
      "step: 5034, loss: 3.0214978323783726e-05\n",
      "step: 5035, loss: 5.0651360652409494e-05\n",
      "step: 5036, loss: 6.263842806220055e-05\n",
      "step: 5037, loss: 3.6503759474726394e-05\n",
      "step: 5038, loss: 0.01787685975432396\n",
      "step: 5039, loss: 9.443196177016944e-05\n",
      "step: 5040, loss: 0.06760264188051224\n",
      "step: 5041, loss: 0.00012733966286759824\n",
      "step: 5042, loss: 4.196935333311558e-05\n",
      "step: 5043, loss: 2.6466044801054522e-05\n",
      "step: 5044, loss: 4.8686455556889996e-05\n",
      "step: 5045, loss: 3.0664265068480745e-05\n",
      "step: 5046, loss: 2.6625946702552028e-05\n",
      "step: 5047, loss: 2.2094252926763147e-05\n",
      "step: 5048, loss: 5.696256994269788e-05\n",
      "step: 5049, loss: 9.386023157276213e-05\n",
      "step: 5050, loss: 3.179044142598286e-05\n",
      "step: 5051, loss: 2.9819482733728364e-05\n",
      "step: 5052, loss: 3.818632467300631e-05\n",
      "step: 5053, loss: 7.292997179320082e-05\n",
      "step: 5054, loss: 0.057098086923360825\n",
      "step: 5055, loss: 2.2866539438837208e-05\n",
      "step: 5056, loss: 9.034188406076282e-05\n",
      "step: 5057, loss: 3.132675192318857e-05\n",
      "step: 5058, loss: 1.5625282685505226e-05\n",
      "step: 5059, loss: 1.0113497410202399e-05\n",
      "step: 5060, loss: 9.965855861082673e-05\n",
      "step: 5061, loss: 6.751948239980265e-05\n",
      "step: 5062, loss: 4.805210846825503e-05\n",
      "step: 5063, loss: 3.450546500971541e-05\n",
      "step: 5064, loss: 3.9987768104765564e-05\n",
      "step: 5065, loss: 1.7329075490124524e-05\n",
      "step: 5066, loss: 0.0425829142332077\n",
      "step: 5067, loss: 3.927492070943117e-05\n",
      "step: 5068, loss: 3.489956725388765e-05\n",
      "step: 5069, loss: 2.543121991038788e-05\n",
      "step: 5070, loss: 3.311856198706664e-05\n",
      "step: 5071, loss: 3.632198058767244e-05\n",
      "step: 5072, loss: 1.9910976334358566e-05\n",
      "step: 5073, loss: 1.3918336662754882e-05\n",
      "step: 5074, loss: 9.036170376930386e-05\n",
      "step: 5075, loss: 3.6781406379304826e-05\n",
      "step: 5076, loss: 0.00015796978550497442\n",
      "step: 5077, loss: 2.5838258807198144e-05\n",
      "step: 5078, loss: 3.72579088434577e-05\n",
      "step: 5079, loss: 7.328286301344633e-05\n",
      "step: 5080, loss: 0.0002927678870037198\n",
      "step: 5081, loss: 0.00011872978939209133\n",
      "step: 5082, loss: 2.1765692508779466e-05\n",
      "step: 5083, loss: 1.7023585314746015e-05\n",
      "step: 5084, loss: 4.919005004921928e-05\n",
      "step: 5085, loss: 1.6925649106269702e-05\n",
      "step: 5086, loss: 2.295709418831393e-05\n",
      "step: 5087, loss: 4.3291485781082883e-05\n",
      "step: 5088, loss: 6.167318497318774e-05\n",
      "step: 5089, loss: 4.383650593808852e-05\n",
      "step: 5090, loss: 2.0054490960319526e-05\n",
      "step: 5091, loss: 4.556103249342414e-06\n",
      "step: 5092, loss: 2.937454337370582e-05\n",
      "step: 5093, loss: 3.7749843613710254e-05\n",
      "step: 5094, loss: 1.4206500054569915e-05\n",
      "step: 5095, loss: 1.630965380172711e-05\n",
      "step: 5096, loss: 2.666346699697897e-05\n",
      "step: 5097, loss: 5.4751475545344874e-05\n",
      "step: 5098, loss: 1.465961122448789e-05\n",
      "step: 5099, loss: 0.0017191299702972174\n",
      "step: 5100, loss: 3.1116065656533465e-05\n",
      "step: 5101, loss: 2.365908403589856e-05\n",
      "step: 5102, loss: 3.0620460165664554e-05\n",
      "step: 5103, loss: 6.120622856542468e-05\n",
      "step: 5104, loss: 4.114310286240652e-05\n",
      "step: 5105, loss: 1.950649129867088e-05\n",
      "step: 5106, loss: 4.850669938605279e-05\n",
      "step: 5107, loss: 2.6463101676199585e-05\n",
      "step: 5108, loss: 2.7268377380096354e-05\n",
      "step: 5109, loss: 1.398039239575155e-05\n",
      "step: 5110, loss: 1.8662454749573953e-05\n",
      "step: 5111, loss: 8.156220428645611e-05\n",
      "step: 5112, loss: 1.4030176316737197e-05\n",
      "step: 5113, loss: 3.3274533052463084e-05\n",
      "step: 5114, loss: 0.002263417234644294\n",
      "step: 5115, loss: 6.0159280110383406e-05\n",
      "step: 5116, loss: 2.913157186412718e-05\n",
      "step: 5117, loss: 0.026887355372309685\n",
      "step: 5118, loss: 0.00021234300220385194\n",
      "step: 5119, loss: 4.1751223761821166e-05\n",
      "step: 5120, loss: 3.448532515903935e-05\n",
      "step: 5121, loss: 0.00023320816399063915\n",
      "step: 5122, loss: 0.03969607874751091\n",
      "step: 5123, loss: 1.9079167032032274e-05\n",
      "step: 5124, loss: 1.3591671631729696e-05\n",
      "step: 5125, loss: 1.5093674846866634e-05\n",
      "step: 5126, loss: 0.0001290509826503694\n",
      "step: 5127, loss: 1.220903686771635e-05\n",
      "step: 5128, loss: 1.5932511814753525e-05\n",
      "step: 5129, loss: 0.0002347153058508411\n",
      "step: 5130, loss: 8.107365283649415e-05\n",
      "step: 5131, loss: 2.364732608839404e-05\n",
      "step: 5132, loss: 0.052374228835105896\n",
      "step: 5133, loss: 2.422024408588186e-05\n",
      "step: 5134, loss: 4.236080712871626e-05\n",
      "step: 5135, loss: 3.340955663588829e-05\n",
      "step: 5136, loss: 2.1746300262748264e-05\n",
      "step: 5137, loss: 2.5520286726532504e-05\n",
      "step: 5138, loss: 7.196603837655857e-05\n",
      "step: 5139, loss: 8.164157043211162e-05\n",
      "step: 5140, loss: 0.023033451288938522\n",
      "step: 5141, loss: 0.014812566339969635\n",
      "step: 5142, loss: 1.4924361494195182e-05\n",
      "step: 5143, loss: 2.7914838938158937e-05\n",
      "step: 5144, loss: 2.3571006749989465e-05\n",
      "step: 5145, loss: 0.0027434418443590403\n",
      "step: 5146, loss: 9.085844794753939e-06\n",
      "step: 5147, loss: 3.951904727728106e-05\n",
      "step: 5148, loss: 1.4788612133997958e-05\n",
      "step: 5149, loss: 3.563447535270825e-05\n",
      "step: 5150, loss: 3.6296522011980414e-05\n",
      "step: 5151, loss: 0.029063403606414795\n",
      "step: 5152, loss: 6.877696432638913e-05\n",
      "step: 5153, loss: 5.887660518055782e-05\n",
      "step: 5154, loss: 7.589440792798996e-05\n",
      "step: 5155, loss: 6.195026799105108e-05\n",
      "step: 5156, loss: 0.05634176358580589\n",
      "step: 5157, loss: 3.7425223126774654e-05\n",
      "step: 5158, loss: 3.356770321261138e-05\n",
      "step: 5159, loss: 2.069582296826411e-05\n",
      "step: 5160, loss: 7.298940909095109e-05\n",
      "step: 5161, loss: 1.7201105947606266e-05\n",
      "step: 5162, loss: 3.3880445698741823e-05\n",
      "step: 5163, loss: 4.4136279029771686e-05\n",
      "step: 5164, loss: 8.995947428047657e-05\n",
      "step: 5165, loss: 1.768918809830211e-05\n",
      "step: 5166, loss: 0.00011751089186873287\n",
      "step: 5167, loss: 1.8264170648762956e-05\n",
      "step: 5168, loss: 3.104723509750329e-05\n",
      "step: 5169, loss: 2.9426095352391712e-05\n",
      "step: 5170, loss: 0.00010281724826199934\n",
      "step: 5171, loss: 5.810744914924726e-05\n",
      "step: 5172, loss: 2.219922680524178e-05\n",
      "step: 5173, loss: 5.4158947023097426e-05\n",
      "step: 5174, loss: 5.380873699323274e-05\n",
      "step: 5175, loss: 4.225366865284741e-05\n",
      "step: 5176, loss: 6.0022572142770514e-05\n",
      "step: 5177, loss: 0.04838093742728233\n",
      "step: 5178, loss: 0.00010897039464907721\n",
      "step: 5179, loss: 8.574544335715473e-05\n",
      "step: 5180, loss: 4.024948430014774e-05\n",
      "step: 5181, loss: 4.259160778019577e-05\n",
      "step: 5182, loss: 0.06154400855302811\n",
      "step: 5183, loss: 3.639614078565501e-05\n",
      "step: 5184, loss: 3.73400398530066e-05\n",
      "step: 5185, loss: 4.124972838326357e-05\n",
      "step: 5186, loss: 1.2454634088499006e-05\n",
      "step: 5187, loss: 3.2929772714851424e-05\n",
      "step: 5188, loss: 6.673212737950962e-06\n",
      "step: 5189, loss: 8.155005343724042e-05\n",
      "step: 5190, loss: 3.484618355287239e-05\n",
      "step: 5191, loss: 2.8881737307528965e-05\n",
      "step: 5192, loss: 4.665110100177117e-05\n",
      "step: 5193, loss: 9.48881461226847e-06\n",
      "step: 5194, loss: 0.00015594552678521723\n",
      "step: 5195, loss: 4.896477912552655e-05\n",
      "step: 5196, loss: 1.3992362255521584e-05\n",
      "step: 5197, loss: 2.560945722507313e-05\n",
      "step: 5198, loss: 2.3122924176277593e-05\n",
      "step: 5199, loss: 2.105619751091581e-05\n",
      "step: 5200, loss: 2.188345570175443e-05\n",
      "step: 5201, loss: 2.174388646380976e-05\n",
      "step: 5202, loss: 0.048803385347127914\n",
      "step: 5203, loss: 0.016620228067040443\n",
      "step: 5204, loss: 2.365499676670879e-05\n",
      "step: 5205, loss: 3.9333572203759104e-05\n",
      "step: 5206, loss: 4.21876429754775e-05\n",
      "step: 5207, loss: 2.7157022486790083e-05\n",
      "step: 5208, loss: 4.182892735116184e-05\n",
      "step: 5209, loss: 4.393847120809369e-05\n",
      "step: 5210, loss: 2.0495737771852873e-05\n",
      "step: 5211, loss: 5.274016803014092e-05\n",
      "step: 5212, loss: 2.0266283172531985e-05\n",
      "step: 5213, loss: 8.399019134230912e-05\n",
      "step: 5214, loss: 9.815371413424145e-06\n",
      "step: 5215, loss: 4.728941712528467e-05\n",
      "step: 5216, loss: 3.8786678487667814e-05\n",
      "step: 5217, loss: 1.5860616258578375e-05\n",
      "step: 5218, loss: 5.8790472394321114e-05\n",
      "step: 5219, loss: 9.265157132176682e-05\n",
      "step: 5220, loss: 3.521159669617191e-05\n",
      "step: 5221, loss: 0.00039357648347504437\n",
      "step: 5222, loss: 0.00014156360703054816\n",
      "step: 5223, loss: 2.541550202295184e-05\n",
      "step: 5224, loss: 0.03328888118267059\n",
      "step: 5225, loss: 2.2604639525525272e-05\n",
      "step: 5226, loss: 0.011426934041082859\n",
      "step: 5227, loss: 5.5251373851206154e-05\n",
      "step: 5228, loss: 3.684616240207106e-05\n",
      "step: 5229, loss: 3.6913705116603523e-05\n",
      "step: 5230, loss: 3.287495201220736e-05\n",
      "step: 5231, loss: 0.003435423132032156\n",
      "step: 5232, loss: 1.527713357063476e-05\n",
      "step: 5233, loss: 7.365571218542755e-05\n",
      "step: 5234, loss: 4.238917972543277e-05\n",
      "step: 5235, loss: 1.9444179997663014e-05\n",
      "step: 5236, loss: 3.844694219878875e-05\n",
      "step: 5237, loss: 6.487179780378938e-05\n",
      "step: 5238, loss: 0.00023030622105579823\n",
      "step: 5239, loss: 4.98323242936749e-05\n",
      "step: 5240, loss: 4.748194623971358e-05\n",
      "step: 5241, loss: 1.7337104509351775e-05\n",
      "step: 5242, loss: 2.1815945729031228e-05\n",
      "step: 5243, loss: 7.016467134235427e-05\n",
      "step: 5244, loss: 5.4249852837529033e-05\n",
      "step: 5245, loss: 2.173505345126614e-05\n",
      "step: 5246, loss: 0.005443096626549959\n",
      "step: 5247, loss: 7.978697249200195e-05\n",
      "step: 5248, loss: 0.00012827679165638983\n",
      "step: 5249, loss: 0.034831903874874115\n",
      "step: 5250, loss: 5.427818905445747e-05\n",
      "step: 5251, loss: 0.013541154563426971\n",
      "step: 5252, loss: 8.753257134230807e-05\n",
      "step: 5253, loss: 3.2648931664880365e-05\n",
      "step: 5254, loss: 5.0695380195975304e-05\n",
      "step: 5255, loss: 3.235268741264008e-05\n",
      "step: 5256, loss: 6.259958900045604e-05\n",
      "step: 5257, loss: 2.882769149437081e-05\n",
      "step: 5258, loss: 2.7698504709405825e-05\n",
      "step: 5259, loss: 2.9399139748420566e-05\n",
      "step: 5260, loss: 0.0001459406194044277\n",
      "step: 5261, loss: 6.461074372055009e-05\n",
      "step: 5262, loss: 2.324591696378775e-05\n",
      "step: 5263, loss: 6.72630121698603e-05\n",
      "step: 5264, loss: 5.604338366538286e-05\n",
      "step: 5265, loss: 6.092965486459434e-05\n",
      "step: 5266, loss: 0.00014883666881360114\n",
      "step: 5267, loss: 2.4373955966439098e-05\n",
      "step: 5268, loss: 3.709354132297449e-05\n",
      "step: 5269, loss: 3.8120360841276124e-05\n",
      "step: 5270, loss: 0.00017064636631403118\n",
      "step: 5271, loss: 0.03852478787302971\n",
      "step: 5272, loss: 5.374288957682438e-05\n",
      "step: 5273, loss: 3.00870760838734e-05\n",
      "step: 5274, loss: 2.513207982701715e-05\n",
      "step: 5275, loss: 1.4554142580891494e-05\n",
      "step: 5276, loss: 9.209854397340678e-06\n",
      "step: 5277, loss: 0.005801172461360693\n",
      "step: 5278, loss: 4.511869337875396e-05\n",
      "step: 5279, loss: 1.7276079233852215e-05\n",
      "step: 5280, loss: 2.7920272259507328e-05\n",
      "step: 5281, loss: 2.0079743990208954e-05\n",
      "step: 5282, loss: 2.9537641239585355e-05\n",
      "step: 5283, loss: 2.7297586711938493e-05\n",
      "step: 5284, loss: 4.137725045438856e-05\n",
      "step: 5285, loss: 3.515985736157745e-05\n",
      "step: 5286, loss: 2.161996417271439e-05\n",
      "step: 5287, loss: 2.252701233373955e-05\n",
      "step: 5288, loss: 3.7972127756802365e-05\n",
      "step: 5289, loss: 2.0730343749164604e-05\n",
      "step: 5290, loss: 0.003843791550025344\n",
      "step: 5291, loss: 0.017270348966121674\n",
      "step: 5292, loss: 2.63633319264045e-05\n",
      "step: 5293, loss: 3.146120798191987e-05\n",
      "step: 5294, loss: 0.00016627198783680797\n",
      "step: 5295, loss: 2.7425829102867283e-05\n",
      "step: 5296, loss: 1.543384678370785e-05\n",
      "step: 5297, loss: 3.7162833905313164e-05\n",
      "step: 5298, loss: 2.6469271688256413e-05\n",
      "step: 5299, loss: 0.024141544476151466\n",
      "step: 5300, loss: 2.2190464733284898e-05\n",
      "step: 5301, loss: 0.00024773902259767056\n",
      "step: 5302, loss: 6.604138616239652e-05\n",
      "step: 5303, loss: 3.0429393518716097e-05\n",
      "step: 5304, loss: 7.087238191161305e-05\n",
      "step: 5305, loss: 3.819386620307341e-05\n",
      "step: 5306, loss: 5.1129991334164515e-05\n",
      "step: 5307, loss: 3.590018968679942e-05\n",
      "step: 5308, loss: 2.0223791580065154e-05\n",
      "step: 5309, loss: 2.9980403269291855e-05\n",
      "step: 5310, loss: 4.814192288904451e-05\n",
      "step: 5311, loss: 5.087994213681668e-05\n",
      "step: 5312, loss: 5.7094701332971454e-05\n",
      "step: 5313, loss: 0.00030024375882931054\n",
      "step: 5314, loss: 0.00023853399034123868\n",
      "step: 5315, loss: 1.915334360091947e-05\n",
      "step: 5316, loss: 0.09482245147228241\n",
      "step: 5317, loss: 6.754655623808503e-05\n",
      "step: 5318, loss: 1.780856473487802e-05\n",
      "step: 5319, loss: 6.0380916693247855e-05\n",
      "step: 5320, loss: 4.768899088958278e-05\n",
      "step: 5321, loss: 3.308297527837567e-05\n",
      "step: 5322, loss: 0.0001982544199563563\n",
      "step: 5323, loss: 0.06462325900793076\n",
      "step: 5324, loss: 3.4550612326711416e-05\n",
      "step: 5325, loss: 4.69040205643978e-05\n",
      "step: 5326, loss: 1.2073189282091334e-05\n",
      "step: 5327, loss: 3.95835486415308e-05\n",
      "step: 5328, loss: 1.5663223166484386e-05\n",
      "step: 5329, loss: 4.657848330680281e-05\n",
      "step: 5330, loss: 0.004612752702087164\n",
      "step: 5331, loss: 1.3467096323438454e-05\n",
      "step: 5332, loss: 3.01270902127726e-05\n",
      "step: 5333, loss: 6.378348189173266e-05\n",
      "step: 5334, loss: 2.3384587620967068e-05\n",
      "step: 5335, loss: 3.2450439903186634e-05\n",
      "step: 5336, loss: 2.577075792942196e-05\n",
      "step: 5337, loss: 2.4186163500417024e-05\n",
      "step: 5338, loss: 0.00017455937631893903\n",
      "step: 5339, loss: 0.00016185795539058745\n",
      "step: 5340, loss: 3.208037742297165e-05\n",
      "step: 5341, loss: 7.620798714924604e-05\n",
      "step: 5342, loss: 2.8162572561996058e-05\n",
      "step: 5343, loss: 3.215283140889369e-05\n",
      "step: 5344, loss: 0.04280500486493111\n",
      "step: 5345, loss: 5.254698407952674e-06\n",
      "step: 5346, loss: 3.7644280382664874e-05\n",
      "step: 5347, loss: 3.859370917780325e-05\n",
      "step: 5348, loss: 3.4573942684801295e-05\n",
      "step: 5349, loss: 1.1450880265329033e-05\n",
      "step: 5350, loss: 0.00019232377235312015\n",
      "step: 5351, loss: 2.293103898409754e-05\n",
      "step: 5352, loss: 2.2655387510894798e-05\n",
      "step: 5353, loss: 2.029312599916011e-05\n",
      "step: 5354, loss: 8.91881609277334e-06\n",
      "step: 5355, loss: 1.4769205336051527e-05\n",
      "step: 5356, loss: 7.517739140894264e-05\n",
      "step: 5357, loss: 1.0754631148301996e-05\n",
      "step: 5358, loss: 2.034735916822683e-05\n",
      "step: 5359, loss: 2.4776794816716574e-05\n",
      "step: 5360, loss: 1.1117193025711458e-05\n",
      "step: 5361, loss: 4.0273509512189776e-05\n",
      "step: 5362, loss: 0.0001334562839474529\n",
      "step: 5363, loss: 4.547723074210808e-05\n",
      "step: 5364, loss: 2.5843368348432705e-05\n",
      "step: 5365, loss: 1.0423054845887236e-05\n",
      "step: 5366, loss: 3.078803274547681e-05\n",
      "step: 5367, loss: 3.212139927200042e-05\n",
      "step: 5368, loss: 0.038817256689071655\n",
      "step: 5369, loss: 2.060311089735478e-05\n",
      "step: 5370, loss: 2.1705820472561754e-05\n",
      "step: 5371, loss: 1.4566695426765364e-05\n",
      "step: 5372, loss: 0.00026407575933262706\n",
      "step: 5373, loss: 0.00013056596799287945\n",
      "step: 5374, loss: 8.899878594093025e-06\n",
      "step: 5375, loss: 1.672878715908155e-05\n",
      "step: 5376, loss: 3.4295579098397866e-05\n",
      "step: 5377, loss: 2.226124342996627e-05\n",
      "step: 5378, loss: 5.5082789913285524e-05\n",
      "step: 5379, loss: 7.60225739213638e-05\n",
      "step: 5380, loss: 3.517288132570684e-05\n",
      "step: 5381, loss: 2.6369061743025668e-05\n",
      "step: 5382, loss: 8.420583071711008e-06\n",
      "step: 5383, loss: 5.8485100453253835e-05\n",
      "step: 5384, loss: 0.00010744223254732788\n",
      "step: 5385, loss: 1.6399157175328583e-05\n",
      "step: 5386, loss: 5.6814358686096966e-05\n",
      "step: 5387, loss: 3.628923877840862e-05\n",
      "step: 5388, loss: 1.6900754417292774e-05\n",
      "step: 5389, loss: 8.087637252174318e-05\n",
      "step: 5390, loss: 3.620487041189335e-05\n",
      "step: 5391, loss: 0.05344439670443535\n",
      "step: 5392, loss: 1.0173113878408913e-05\n",
      "step: 5393, loss: 1.8929609723272733e-05\n",
      "step: 5394, loss: 1.1233938494115137e-05\n",
      "step: 5395, loss: 2.7085072360932827e-05\n",
      "step: 5396, loss: 2.941787170129828e-05\n",
      "step: 5397, loss: 2.0390411009429954e-05\n",
      "step: 5398, loss: 3.3649113902356476e-05\n",
      "step: 5399, loss: 4.8340807552449405e-05\n",
      "step: 5400, loss: 2.3976144802873023e-05\n",
      "step: 5401, loss: 0.046002164483070374\n",
      "step: 5402, loss: 2.919176586146932e-05\n",
      "step: 5403, loss: 2.698157732083928e-05\n",
      "step: 5404, loss: 2.3871203666203655e-05\n",
      "step: 5405, loss: 1.7088723325286992e-05\n",
      "step: 5406, loss: 3.434929021750577e-05\n",
      "step: 5407, loss: 4.0296978113474324e-05\n",
      "step: 5408, loss: 1.2278094800421968e-05\n",
      "step: 5409, loss: 3.915952402167022e-05\n",
      "step: 5410, loss: 1.922373667184729e-05\n",
      "step: 5411, loss: 7.593412010464817e-05\n",
      "step: 5412, loss: 9.143182978732511e-06\n",
      "step: 5413, loss: 3.8583115383517e-05\n",
      "step: 5414, loss: 1.724388675938826e-05\n",
      "step: 5415, loss: 2.8498852771008387e-05\n",
      "step: 5416, loss: 5.138906999491155e-05\n",
      "step: 5417, loss: 7.826953515177593e-05\n",
      "step: 5418, loss: 9.579192555975169e-05\n",
      "step: 5419, loss: 4.117283606319688e-05\n",
      "step: 5420, loss: 2.9439115678542294e-05\n",
      "step: 5421, loss: 6.110451795393601e-05\n",
      "step: 5422, loss: 0.012828913517296314\n",
      "step: 5423, loss: 1.542953759781085e-05\n",
      "step: 5424, loss: 2.345095344935544e-05\n",
      "step: 5425, loss: 2.484855940565467e-05\n",
      "step: 5426, loss: 7.738496060483158e-05\n",
      "step: 5427, loss: 0.00010015887528425083\n",
      "step: 5428, loss: 4.894979429082014e-05\n",
      "step: 5429, loss: 1.9229470126447268e-05\n",
      "step: 5430, loss: 2.992877307406161e-05\n",
      "step: 5431, loss: 3.848850610665977e-05\n",
      "step: 5432, loss: 2.6115654691238888e-05\n",
      "step: 5433, loss: 2.3883972971816547e-05\n",
      "step: 5434, loss: 3.507807559799403e-05\n",
      "step: 5435, loss: 0.017596956342458725\n",
      "step: 5436, loss: 0.002385395811870694\n",
      "step: 5437, loss: 3.298309457022697e-05\n",
      "step: 5438, loss: 4.718359195976518e-05\n",
      "step: 5439, loss: 1.7022621250362135e-05\n",
      "step: 5440, loss: 1.6419180610682815e-05\n",
      "step: 5441, loss: 8.713853458175436e-05\n",
      "step: 5442, loss: 3.233134702895768e-05\n",
      "step: 5443, loss: 3.063784970436245e-05\n",
      "step: 5444, loss: 3.907854625140317e-05\n",
      "step: 5445, loss: 5.944653457845561e-05\n",
      "step: 5446, loss: 2.042360392806586e-05\n",
      "step: 5447, loss: 0.06013813987374306\n",
      "step: 5448, loss: 3.4236185456393287e-05\n",
      "step: 5449, loss: 1.744711698847823e-05\n",
      "step: 5450, loss: 1.6635867723380215e-05\n",
      "step: 5451, loss: 1.474773307563737e-05\n",
      "step: 5452, loss: 1.435929698345717e-05\n",
      "step: 5453, loss: 1.3667554412677418e-05\n",
      "step: 5454, loss: 1.3808080439048354e-05\n",
      "step: 5455, loss: 4.9628812121227384e-05\n",
      "step: 5456, loss: 1.6321364455507137e-05\n",
      "step: 5457, loss: 3.3824428101070225e-05\n",
      "step: 5458, loss: 1.2397151294862852e-05\n",
      "step: 5459, loss: 3.45749213011004e-05\n",
      "step: 5460, loss: 1.0136935088667087e-05\n",
      "step: 5461, loss: 1.3894352377974428e-05\n",
      "step: 5462, loss: 2.3828421035432257e-05\n",
      "step: 5463, loss: 1.1751340935006738e-05\n",
      "step: 5464, loss: 5.380736547522247e-05\n",
      "step: 5465, loss: 1.5169661310210358e-05\n",
      "step: 5466, loss: 1.0852360901481006e-05\n",
      "step: 5467, loss: 6.100998234614963e-06\n",
      "step: 5468, loss: 3.190358984284103e-05\n",
      "step: 5469, loss: 1.4724223547091242e-05\n",
      "step: 5470, loss: 3.579680560505949e-05\n",
      "step: 5471, loss: 0.027433175593614578\n",
      "step: 5472, loss: 0.0060155619867146015\n",
      "step: 5473, loss: 2.597701495687943e-05\n",
      "step: 5474, loss: 3.614510933402926e-05\n",
      "step: 5475, loss: 2.1108260625624098e-05\n",
      "step: 5476, loss: 9.498173312749714e-06\n",
      "step: 5477, loss: 3.8485846744151786e-05\n",
      "step: 5478, loss: 3.061900133616291e-05\n",
      "step: 5479, loss: 2.2764428649679758e-05\n",
      "step: 5480, loss: 2.1682255464838818e-05\n",
      "step: 5481, loss: 2.3682960090809502e-05\n",
      "step: 5482, loss: 0.00010361614840803668\n",
      "step: 5483, loss: 5.436854553408921e-05\n",
      "step: 5484, loss: 1.0897913853114005e-05\n",
      "step: 5485, loss: 2.3320066247833893e-05\n",
      "step: 5486, loss: 6.983814091654494e-05\n",
      "step: 5487, loss: 2.0027649952680804e-05\n",
      "step: 5488, loss: 3.0288778361864388e-05\n",
      "step: 5489, loss: 1.717189297778532e-05\n",
      "step: 5490, loss: 1.232100203196751e-05\n",
      "step: 5491, loss: 4.6320539695443586e-05\n",
      "step: 5492, loss: 3.2884287065826356e-05\n",
      "step: 5493, loss: 7.828205707482994e-05\n",
      "step: 5494, loss: 1.1913065463886596e-05\n",
      "step: 5495, loss: 1.204449017677689e-05\n",
      "step: 5496, loss: 3.198976264684461e-05\n",
      "step: 5497, loss: 2.101052086800337e-05\n",
      "step: 5498, loss: 0.010741145350039005\n",
      "step: 5499, loss: 6.207081605680287e-05\n",
      "step: 5500, loss: 0.00026170071214437485\n",
      "step: 5501, loss: 4.57333771919366e-05\n",
      "step: 5502, loss: 2.9773724236292765e-05\n",
      "step: 5503, loss: 3.7378889828687534e-05\n",
      "step: 5504, loss: 0.027464132755994797\n",
      "step: 5505, loss: 2.6625702957971953e-05\n",
      "step: 5506, loss: 1.3989823855808936e-05\n",
      "step: 5507, loss: 2.0297933588153683e-05\n",
      "step: 5508, loss: 0.06322678923606873\n",
      "step: 5509, loss: 2.2035133952158503e-05\n",
      "step: 5510, loss: 1.2869073543697596e-05\n",
      "step: 5511, loss: 2.7598998713074252e-05\n",
      "step: 5512, loss: 1.9181705283699557e-05\n",
      "step: 5513, loss: 3.146782910334878e-05\n",
      "step: 5514, loss: 3.702845788211562e-05\n",
      "step: 5515, loss: 2.730218147917185e-05\n",
      "step: 5516, loss: 1.2185146260890178e-05\n",
      "step: 5517, loss: 0.00012775167124345899\n",
      "step: 5518, loss: 4.2026334995171055e-05\n",
      "step: 5519, loss: 0.005805063992738724\n",
      "step: 5520, loss: 3.7996695027686656e-05\n",
      "step: 5521, loss: 0.0002379457582719624\n",
      "step: 5522, loss: 3.184379238518886e-05\n",
      "step: 5523, loss: 4.0991326386574656e-05\n",
      "step: 5524, loss: 0.002608480164781213\n",
      "step: 5525, loss: 2.0742754713864997e-05\n",
      "step: 5526, loss: 2.4947645215434022e-05\n",
      "step: 5527, loss: 2.5794819521252066e-05\n",
      "step: 5528, loss: 2.359871541557368e-05\n",
      "step: 5529, loss: 2.344206404814031e-05\n",
      "step: 5530, loss: 2.3086455257725902e-05\n",
      "step: 5531, loss: 3.6523535527521744e-05\n",
      "step: 5532, loss: 1.8004462617682293e-05\n",
      "step: 5533, loss: 4.160124808549881e-05\n",
      "step: 5534, loss: 2.7549403966986574e-05\n",
      "step: 5535, loss: 0.032159317284822464\n",
      "step: 5536, loss: 3.901588570442982e-05\n",
      "step: 5537, loss: 5.887791485292837e-05\n",
      "step: 5538, loss: 7.962495874380693e-05\n",
      "step: 5539, loss: 6.540987669723108e-05\n",
      "step: 5540, loss: 4.090274524060078e-05\n",
      "step: 5541, loss: 4.429439650266431e-05\n",
      "step: 5542, loss: 3.417563857510686e-05\n",
      "step: 5543, loss: 2.237868648080621e-05\n",
      "step: 5544, loss: 4.7429701226064935e-05\n",
      "step: 5545, loss: 0.007503415923565626\n",
      "step: 5546, loss: 0.07333741337060928\n",
      "step: 5547, loss: 2.066800880129449e-05\n",
      "step: 5548, loss: 4.074749449500814e-05\n",
      "step: 5549, loss: 4.535941479844041e-05\n",
      "step: 5550, loss: 2.307888280483894e-05\n",
      "step: 5551, loss: 3.526869113557041e-05\n",
      "step: 5552, loss: 3.574333095457405e-05\n",
      "step: 5553, loss: 0.02449977956712246\n",
      "step: 5554, loss: 3.809868576354347e-05\n",
      "step: 5555, loss: 2.3041089662001468e-05\n",
      "step: 5556, loss: 4.6193250454962254e-05\n",
      "step: 5557, loss: 3.683512477437034e-05\n",
      "step: 5558, loss: 3.945430580643006e-05\n",
      "step: 5559, loss: 3.4905420761788264e-05\n",
      "step: 5560, loss: 3.6455334338825196e-05\n",
      "step: 5561, loss: 2.2915241061127745e-05\n",
      "step: 5562, loss: 7.837005250621587e-05\n",
      "step: 5563, loss: 1.832615635066759e-05\n",
      "step: 5564, loss: 5.3419727919390425e-05\n",
      "step: 5565, loss: 0.03941792622208595\n",
      "step: 5566, loss: 3.46711058227811e-05\n",
      "step: 5567, loss: 2.123367812600918e-05\n",
      "step: 5568, loss: 4.846495721722022e-05\n",
      "step: 5569, loss: 3.416500112507492e-05\n",
      "step: 5570, loss: 1.7661915990174748e-05\n",
      "step: 5571, loss: 1.610381696082186e-05\n",
      "step: 5572, loss: 8.36166727822274e-05\n",
      "step: 5573, loss: 3.945804110117024e-06\n",
      "step: 5574, loss: 0.00010997729987138882\n",
      "step: 5575, loss: 2.7607316951616667e-05\n",
      "step: 5576, loss: 8.275272193714045e-06\n",
      "step: 5577, loss: 7.17112488928251e-05\n",
      "step: 5578, loss: 2.0946748918504454e-05\n",
      "step: 5579, loss: 2.0383826267789118e-05\n",
      "step: 5580, loss: 1.1105244084319565e-05\n",
      "step: 5581, loss: 0.023465383797883987\n",
      "step: 5582, loss: 3.802217179327272e-05\n",
      "step: 5583, loss: 1.9034541764995083e-05\n",
      "step: 5584, loss: 0.00016308123304042965\n",
      "step: 5585, loss: 3.629958155215718e-05\n",
      "step: 5586, loss: 3.327678859932348e-05\n",
      "step: 5587, loss: 7.712628757872153e-06\n",
      "step: 5588, loss: 3.151301643811166e-05\n",
      "step: 5589, loss: 1.9313405573484488e-05\n",
      "step: 5590, loss: 1.9712550056283362e-05\n",
      "step: 5591, loss: 1.9162558601237833e-05\n",
      "step: 5592, loss: 2.0154076992184855e-05\n",
      "step: 5593, loss: 4.0646427805768326e-05\n",
      "step: 5594, loss: 2.948647306766361e-05\n",
      "step: 5595, loss: 3.258390643168241e-05\n",
      "step: 5596, loss: 4.900316707789898e-05\n",
      "step: 5597, loss: 1.043770225805929e-05\n",
      "step: 5598, loss: 0.03653135523200035\n",
      "step: 5599, loss: 5.835233605466783e-05\n",
      "step: 5600, loss: 5.064558718004264e-05\n",
      "step: 5601, loss: 1.2237323971930891e-05\n",
      "step: 5602, loss: 0.00013119709910824895\n",
      "step: 5603, loss: 1.4702763110108208e-05\n",
      "step: 5604, loss: 4.573494152282365e-05\n",
      "step: 5605, loss: 3.7088004319230095e-05\n",
      "step: 5606, loss: 4.3233379983576015e-05\n",
      "step: 5607, loss: 5.272085400065407e-05\n",
      "step: 5608, loss: 8.891736797522753e-05\n",
      "step: 5609, loss: 3.0043125661904924e-05\n",
      "step: 5610, loss: 3.083320916630328e-05\n",
      "step: 5611, loss: 1.7258476873394102e-05\n",
      "step: 5612, loss: 2.0268300431780517e-05\n",
      "step: 5613, loss: 1.7267107978113927e-05\n",
      "step: 5614, loss: 6.187849066918716e-05\n",
      "step: 5615, loss: 1.914852146001067e-05\n",
      "step: 5616, loss: 2.229519850516226e-05\n",
      "step: 5617, loss: 9.284661791753024e-05\n",
      "step: 5618, loss: 1.2218435585964471e-05\n",
      "step: 5619, loss: 1.7060463505913503e-05\n",
      "step: 5620, loss: 1.6955353203229606e-05\n",
      "step: 5621, loss: 9.498357030679472e-06\n",
      "step: 5622, loss: 5.6833025155356154e-05\n",
      "step: 5623, loss: 4.007745155831799e-05\n",
      "step: 5624, loss: 2.6066194550367072e-05\n",
      "step: 5625, loss: 0.028204143047332764\n",
      "step: 5626, loss: 2.2043072021915577e-05\n",
      "step: 5627, loss: 1.2821556083508767e-05\n",
      "step: 5628, loss: 0.06979840993881226\n",
      "step: 5629, loss: 5.8551668189466e-05\n",
      "step: 5630, loss: 0.0028244787827134132\n",
      "step: 5631, loss: 1.416350914951181e-05\n",
      "step: 5632, loss: 2.65715552814072e-05\n",
      "step: 5633, loss: 2.3576976673211902e-05\n",
      "step: 5634, loss: 1.4085024304222316e-05\n",
      "step: 5635, loss: 1.6819727534311824e-05\n",
      "step: 5636, loss: 3.559355536708608e-05\n",
      "step: 5637, loss: 2.4573953851358965e-05\n",
      "step: 5638, loss: 1.5954385162331164e-05\n",
      "step: 5639, loss: 2.281137312820647e-05\n",
      "step: 5640, loss: 2.6540497856331058e-05\n",
      "step: 5641, loss: 0.030157269909977913\n",
      "step: 5642, loss: 8.566431642975658e-05\n",
      "step: 5643, loss: 4.3336767703294754e-05\n",
      "step: 5644, loss: 2.456655965943355e-05\n",
      "step: 5645, loss: 2.1193107386352494e-05\n",
      "step: 5646, loss: 3.203734377166256e-05\n",
      "step: 5647, loss: 3.584758815122768e-05\n",
      "step: 5648, loss: 3.691039819386788e-05\n",
      "step: 5649, loss: 1.0885925803449936e-05\n",
      "step: 5650, loss: 0.03676192834973335\n",
      "step: 5651, loss: 3.238775389036164e-05\n",
      "step: 5652, loss: 4.0490667743142694e-05\n",
      "step: 5653, loss: 1.561524004500825e-05\n",
      "step: 5654, loss: 2.6047620849567465e-05\n",
      "step: 5655, loss: 1.475745375500992e-05\n",
      "step: 5656, loss: 4.24544996349141e-05\n",
      "step: 5657, loss: 1.859826261352282e-05\n",
      "step: 5658, loss: 2.3980723199201748e-05\n",
      "step: 5659, loss: 1.9717435861821286e-05\n",
      "step: 5660, loss: 1.3069479791738559e-05\n",
      "step: 5661, loss: 3.1153067538980395e-05\n",
      "step: 5662, loss: 4.019886182504706e-05\n",
      "step: 5663, loss: 9.212014447257388e-06\n",
      "step: 5664, loss: 0.0028864010237157345\n",
      "step: 5665, loss: 2.4090464648907073e-05\n",
      "step: 5666, loss: 3.8485322875203565e-05\n",
      "step: 5667, loss: 2.157938615710009e-05\n",
      "step: 5668, loss: 1.9545645045582205e-05\n",
      "step: 5669, loss: 2.7751240850193426e-05\n",
      "step: 5670, loss: 3.2366828236263245e-05\n",
      "step: 5671, loss: 6.068439324735664e-05\n",
      "step: 5672, loss: 1.1789392374339513e-05\n",
      "step: 5673, loss: 0.02394144982099533\n",
      "step: 5674, loss: 1.9793333194684237e-05\n",
      "step: 5675, loss: 1.344162956229411e-05\n",
      "step: 5676, loss: 3.2727501093177125e-05\n",
      "step: 5677, loss: 1.197074288938893e-05\n",
      "step: 5678, loss: 2.9524522688006982e-05\n",
      "step: 5679, loss: 2.1040945284767076e-05\n",
      "step: 5680, loss: 2.098167169606313e-05\n",
      "step: 5681, loss: 2.828156721079722e-05\n",
      "step: 5682, loss: 3.9202350308187306e-05\n",
      "step: 5683, loss: 0.06904061138629913\n",
      "step: 5684, loss: 2.1196849047555588e-05\n",
      "step: 5685, loss: 1.18868019853835e-05\n",
      "step: 5686, loss: 1.365057505609002e-05\n",
      "step: 5687, loss: 1.057832378137391e-05\n",
      "step: 5688, loss: 1.6750040231272578e-05\n",
      "step: 5689, loss: 1.9753835658775643e-05\n",
      "step: 5690, loss: 8.448558946838602e-05\n",
      "step: 5691, loss: 1.2220631106174551e-05\n",
      "step: 5692, loss: 2.9804672522004694e-05\n",
      "step: 5693, loss: 3.0069852073211223e-05\n",
      "step: 5694, loss: 2.4070106519502588e-05\n",
      "step: 5695, loss: 1.8125483620679006e-05\n",
      "step: 5696, loss: 2.0003933968837373e-05\n",
      "step: 5697, loss: 2.2098380213719793e-05\n",
      "step: 5698, loss: 0.003587468760088086\n",
      "step: 5699, loss: 0.00011281506158411503\n",
      "step: 5700, loss: 2.2245023501454853e-05\n",
      "step: 5701, loss: 1.1822521628346294e-05\n",
      "step: 5702, loss: 0.02356918528676033\n",
      "step: 5703, loss: 0.026871807873249054\n",
      "step: 5704, loss: 3.310570173198357e-05\n",
      "step: 5705, loss: 3.1345338356914e-05\n",
      "step: 5706, loss: 0.040432438254356384\n",
      "step: 5707, loss: 1.2139523278165143e-05\n",
      "step: 5708, loss: 1.2221031283843331e-05\n",
      "step: 5709, loss: 4.505208562477492e-05\n",
      "step: 5710, loss: 5.735394370276481e-05\n",
      "step: 5711, loss: 2.377221426286269e-05\n",
      "step: 5712, loss: 3.596926762838848e-05\n",
      "step: 5713, loss: 6.584999027836602e-06\n",
      "step: 5714, loss: 2.953485818579793e-05\n",
      "step: 5715, loss: 1.0616234249027912e-05\n",
      "step: 5716, loss: 1.2061160305165686e-05\n",
      "step: 5717, loss: 2.0044764823978767e-05\n",
      "step: 5718, loss: 3.476890560705215e-05\n",
      "step: 5719, loss: 4.3804429878946394e-05\n",
      "step: 5720, loss: 1.961063389899209e-05\n",
      "step: 5721, loss: 1.177978720079409e-05\n",
      "step: 5722, loss: 1.6451333067379892e-05\n",
      "step: 5723, loss: 4.03697085857857e-05\n",
      "step: 5724, loss: 1.7346450476907194e-05\n",
      "step: 5725, loss: 3.778811151278205e-05\n",
      "step: 5726, loss: 1.4771974747418426e-05\n",
      "step: 5727, loss: 3.3741016522981226e-05\n",
      "step: 5728, loss: 1.0249125807604287e-05\n",
      "step: 5729, loss: 3.158393519697711e-05\n",
      "step: 5730, loss: 2.6986961529473774e-05\n",
      "step: 5731, loss: 2.7410975235397927e-05\n",
      "step: 5732, loss: 0.0277439933270216\n",
      "step: 5733, loss: 3.738241866813041e-05\n",
      "step: 5734, loss: 0.06104731187224388\n",
      "step: 5735, loss: 3.473332253633998e-05\n",
      "step: 5736, loss: 1.3784862858301494e-05\n",
      "step: 5737, loss: 2.7118001526105218e-05\n",
      "step: 5738, loss: 2.6327996238251217e-05\n",
      "step: 5739, loss: 8.062643610173836e-05\n",
      "step: 5740, loss: 1.5420100680785254e-05\n",
      "step: 5741, loss: 1.746113230183255e-05\n",
      "step: 5742, loss: 1.0332852070860099e-05\n",
      "step: 5743, loss: 1.279532807529904e-05\n",
      "step: 5744, loss: 2.1352139810915105e-05\n",
      "step: 5745, loss: 1.54814788402291e-05\n",
      "step: 5746, loss: 6.770823802071391e-06\n",
      "step: 5747, loss: 0.027788015082478523\n",
      "step: 5748, loss: 6.509418017230928e-05\n",
      "step: 5749, loss: 8.486206934321672e-05\n",
      "step: 5750, loss: 1.860855809354689e-05\n",
      "step: 5751, loss: 2.8062779165338725e-05\n",
      "step: 5752, loss: 2.840507477230858e-05\n",
      "step: 5753, loss: 1.9922328647226095e-05\n",
      "step: 5754, loss: 2.0760197003255598e-05\n",
      "step: 5755, loss: 8.437405995209701e-06\n",
      "step: 5756, loss: 1.2914589206047822e-05\n",
      "step: 5757, loss: 3.5850298445438966e-05\n",
      "step: 5758, loss: 6.491216481663287e-05\n",
      "step: 5759, loss: 2.9149292458896525e-05\n",
      "step: 5760, loss: 1.2480774785217363e-05\n",
      "step: 5761, loss: 7.13101417204598e-06\n",
      "step: 5762, loss: 2.0994082660763524e-05\n",
      "step: 5763, loss: 1.2571365004987456e-05\n",
      "step: 5764, loss: 1.5739929949631914e-05\n",
      "step: 5765, loss: 1.658131441217847e-05\n",
      "step: 5766, loss: 6.377467070706189e-05\n",
      "step: 5767, loss: 4.44047327619046e-05\n",
      "step: 5768, loss: 9.694648906588554e-05\n",
      "step: 5769, loss: 5.17604494234547e-05\n",
      "step: 5770, loss: 3.016938535438385e-05\n",
      "step: 5771, loss: 0.00010310812649549916\n",
      "step: 5772, loss: 5.713461359846406e-05\n",
      "step: 5773, loss: 1.1803598681581207e-05\n",
      "step: 5774, loss: 0.039288926869630814\n",
      "step: 5775, loss: 1.593691558809951e-05\n",
      "step: 5776, loss: 3.28632268065121e-05\n",
      "step: 5777, loss: 2.250423676741775e-05\n",
      "step: 5778, loss: 4.100004298379645e-05\n",
      "step: 5779, loss: 2.185329867643304e-05\n",
      "step: 5780, loss: 1.3119708455633372e-05\n",
      "step: 5781, loss: 1.920246722875163e-05\n",
      "step: 5782, loss: 1.0182441656070296e-05\n",
      "step: 5783, loss: 0.00011051062756450847\n",
      "step: 5784, loss: 0.01828397437930107\n",
      "step: 5785, loss: 3.475764242466539e-05\n",
      "step: 5786, loss: 3.657719207694754e-05\n",
      "step: 5787, loss: 9.636696631787345e-06\n",
      "step: 5788, loss: 1.6113845049403608e-05\n",
      "step: 5789, loss: 3.846408799290657e-05\n",
      "step: 5790, loss: 0.029104012995958328\n",
      "step: 5791, loss: 1.5367895684903488e-05\n",
      "step: 5792, loss: 9.717636203276925e-06\n",
      "step: 5793, loss: 2.2617583454120904e-05\n",
      "step: 5794, loss: 3.37219207722228e-05\n",
      "step: 5795, loss: 1.2170507943665143e-05\n",
      "step: 5796, loss: 1.1405654731788673e-05\n",
      "step: 5797, loss: 9.269373549614102e-06\n",
      "step: 5798, loss: 0.09065424650907516\n",
      "step: 5799, loss: 1.2631008758035023e-05\n",
      "step: 5800, loss: 2.2176618585945107e-05\n",
      "step: 5801, loss: 4.7018816985655576e-05\n",
      "step: 5802, loss: 2.3394701202050783e-05\n",
      "step: 5803, loss: 3.195126191712916e-05\n",
      "step: 5804, loss: 2.6510295356274582e-05\n",
      "step: 5805, loss: 0.09302785992622375\n",
      "step: 5806, loss: 3.536275471560657e-05\n",
      "step: 5807, loss: 2.354984826524742e-05\n",
      "step: 5808, loss: 0.009099234826862812\n",
      "step: 5809, loss: 5.0771432142937556e-05\n",
      "step: 5810, loss: 3.5768967791227624e-05\n",
      "step: 5811, loss: 1.7805812603910454e-05\n",
      "step: 5812, loss: 1.5615474694641307e-05\n",
      "step: 5813, loss: 1.8568336599855684e-05\n",
      "step: 5814, loss: 1.3624937309941743e-05\n",
      "step: 5815, loss: 9.181262612401042e-06\n",
      "step: 5816, loss: 1.4642695532529615e-05\n",
      "step: 5817, loss: 3.6035256925970316e-05\n",
      "step: 5818, loss: 3.3945754694286734e-05\n",
      "step: 5819, loss: 4.5070275518810377e-05\n",
      "step: 5820, loss: 3.0404491553781554e-05\n",
      "step: 5821, loss: 6.127860251581296e-05\n",
      "step: 5822, loss: 2.1876943719689734e-05\n",
      "step: 5823, loss: 4.354967677500099e-05\n",
      "step: 5824, loss: 0.03676459193229675\n",
      "step: 5825, loss: 0.00010259319242322817\n",
      "step: 5826, loss: 2.177572423534002e-05\n",
      "step: 5827, loss: 0.00010892141290241852\n",
      "step: 5828, loss: 0.00011438780347816646\n",
      "step: 5829, loss: 1.2926648196298629e-05\n",
      "step: 5830, loss: 4.99380003020633e-05\n",
      "step: 5831, loss: 2.6157147658523172e-05\n",
      "step: 5832, loss: 8.205114863812923e-05\n",
      "step: 5833, loss: 4.730451837531291e-05\n",
      "step: 5834, loss: 6.184503490658244e-06\n",
      "step: 5835, loss: 4.199308023089543e-05\n",
      "step: 5836, loss: 0.00013333780225366354\n",
      "step: 5837, loss: 0.00013400521129369736\n",
      "step: 5838, loss: 4.3396408727858216e-05\n",
      "step: 5839, loss: 2.705399674596265e-05\n",
      "step: 5840, loss: 2.0322264390415512e-05\n",
      "step: 5841, loss: 1.4986350834078621e-05\n",
      "step: 5842, loss: 0.001393011654727161\n",
      "step: 5843, loss: 2.9568978789029643e-05\n",
      "step: 5844, loss: 3.49963411281351e-05\n",
      "step: 5845, loss: 2.9334654755075462e-05\n",
      "step: 5846, loss: 6.744718575646402e-06\n",
      "step: 5847, loss: 4.3078758608317e-05\n",
      "step: 5848, loss: 3.32516101479996e-05\n",
      "step: 5849, loss: 6.178250623634085e-05\n",
      "step: 5850, loss: 5.953367508482188e-05\n",
      "step: 5851, loss: 3.202876905561425e-05\n",
      "step: 5852, loss: 8.997218537842855e-05\n",
      "step: 5853, loss: 4.014149817521684e-05\n",
      "step: 5854, loss: 3.028341780009214e-05\n",
      "step: 5855, loss: 0.00019461227930150926\n",
      "step: 5856, loss: 4.8091555072460324e-05\n",
      "step: 5857, loss: 2.2838481527287513e-05\n",
      "step: 5858, loss: 3.398830449441448e-05\n",
      "step: 5859, loss: 4.9848480557557195e-05\n",
      "step: 5860, loss: 3.0535011319443583e-05\n",
      "step: 5861, loss: 3.9428927266271785e-05\n",
      "step: 5862, loss: 4.466933023650199e-05\n",
      "step: 5863, loss: 2.03637591766892e-05\n",
      "step: 5864, loss: 0.00011950844782404602\n",
      "step: 5865, loss: 2.9371043638093397e-05\n",
      "step: 5866, loss: 0.04150177910923958\n",
      "step: 5867, loss: 7.189958705566823e-05\n",
      "step: 5868, loss: 1.4735496733919717e-05\n",
      "step: 5869, loss: 1.6135143596329726e-05\n",
      "step: 5870, loss: 6.0680682508973405e-05\n",
      "step: 5871, loss: 4.536517371889204e-05\n",
      "step: 5872, loss: 6.840690912213176e-05\n",
      "step: 5873, loss: 4.8961155698634684e-05\n",
      "step: 5874, loss: 2.9532153348554857e-05\n",
      "step: 5875, loss: 2.0041403331561014e-05\n",
      "step: 5876, loss: 0.026366647332906723\n",
      "step: 5877, loss: 3.913539330824278e-05\n",
      "step: 5878, loss: 0.0425386056303978\n",
      "step: 5879, loss: 2.4729077267693356e-05\n",
      "step: 5880, loss: 3.970739999203943e-05\n",
      "step: 5881, loss: 0.008006788790225983\n",
      "step: 5882, loss: 8.71017764438875e-05\n",
      "step: 5883, loss: 1.9026130757993087e-05\n",
      "step: 5884, loss: 3.1071034754859284e-05\n",
      "step: 5885, loss: 1.3148041944077704e-05\n",
      "step: 5886, loss: 0.0018395169172436\n",
      "step: 5887, loss: 5.632677493849769e-05\n",
      "step: 5888, loss: 0.03583739697933197\n",
      "step: 5889, loss: 7.791219104547054e-06\n",
      "step: 5890, loss: 3.150844349875115e-05\n",
      "step: 5891, loss: 2.5339149942738004e-05\n",
      "step: 5892, loss: 1.456308746128343e-05\n",
      "step: 5893, loss: 1.2286946002859622e-05\n",
      "step: 5894, loss: 1.9599223378463648e-05\n",
      "step: 5895, loss: 2.445696918584872e-05\n",
      "step: 5896, loss: 8.145250467350706e-05\n",
      "step: 5897, loss: 0.03828210011124611\n",
      "step: 5898, loss: 2.3860860892455094e-05\n",
      "step: 5899, loss: 2.5928400646080263e-05\n",
      "step: 5900, loss: 1.0132361239811871e-05\n",
      "step: 5901, loss: 9.19256272027269e-05\n",
      "step: 5902, loss: 2.0247061911504716e-05\n",
      "step: 5903, loss: 1.7729984392644837e-05\n",
      "step: 5904, loss: 1.562288889545016e-05\n",
      "step: 5905, loss: 6.5623338741716e-05\n",
      "step: 5906, loss: 2.5308565454906784e-05\n",
      "step: 5907, loss: 0.042136043310165405\n",
      "step: 5908, loss: 4.5274187868926674e-05\n",
      "step: 5909, loss: 4.445287777343765e-05\n",
      "step: 5910, loss: 5.888821760891005e-06\n",
      "step: 5911, loss: 0.0036566127091646194\n",
      "step: 5912, loss: 1.811874062696006e-05\n",
      "step: 5913, loss: 2.277030398545321e-05\n",
      "step: 5914, loss: 4.3484411435201764e-05\n",
      "step: 5915, loss: 1.2621126188605558e-05\n",
      "step: 5916, loss: 1.2573823369166348e-05\n",
      "step: 5917, loss: 9.502971806796268e-06\n",
      "step: 5918, loss: 3.132323399768211e-05\n",
      "step: 5919, loss: 2.2381378585123457e-05\n",
      "step: 5920, loss: 5.197424616198987e-05\n",
      "step: 5921, loss: 7.910311069281306e-06\n",
      "step: 5922, loss: 1.7693982954369858e-05\n",
      "step: 5923, loss: 2.2748743504052982e-05\n",
      "step: 5924, loss: 1.547019746794831e-05\n",
      "step: 5925, loss: 6.444196333177388e-05\n",
      "step: 5926, loss: 2.7118208890897222e-05\n",
      "step: 5927, loss: 2.7682772270054556e-05\n",
      "step: 5928, loss: 3.6738376365974545e-05\n",
      "step: 5929, loss: 7.192267366917804e-05\n",
      "step: 5930, loss: 8.029729542613495e-06\n",
      "step: 5931, loss: 3.644540629466064e-05\n",
      "step: 5932, loss: 2.1184618162806146e-05\n",
      "step: 5933, loss: 7.0785013122076634e-06\n",
      "step: 5934, loss: 2.3881064407760277e-05\n",
      "step: 5935, loss: 0.0268259160220623\n",
      "step: 5936, loss: 1.0368455150455702e-05\n",
      "step: 5937, loss: 1.2621304449567106e-05\n",
      "step: 5938, loss: 1.2366378541628364e-05\n",
      "step: 5939, loss: 0.00016551479347981513\n",
      "step: 5940, loss: 1.363212322758045e-05\n",
      "step: 5941, loss: 9.541284271108452e-06\n",
      "step: 5942, loss: 4.19763928221073e-05\n",
      "step: 5943, loss: 0.0001295146648772061\n",
      "step: 5944, loss: 7.065143290674314e-05\n",
      "step: 5945, loss: 1.8242848454974592e-05\n",
      "step: 5946, loss: 4.057090700371191e-05\n",
      "step: 5947, loss: 0.002178400522097945\n",
      "step: 5948, loss: 1.5773070117575116e-05\n",
      "step: 5949, loss: 1.7057904187822714e-05\n",
      "step: 5950, loss: 9.49576588027412e-06\n",
      "step: 5951, loss: 1.363206683890894e-05\n",
      "step: 5952, loss: 0.00012000446440652013\n",
      "step: 5953, loss: 3.2200034183915704e-05\n",
      "step: 5954, loss: 4.663893923861906e-05\n",
      "step: 5955, loss: 3.135581937385723e-05\n",
      "step: 5956, loss: 1.725802394503262e-05\n",
      "step: 5957, loss: 1.9276529201306403e-05\n",
      "step: 5958, loss: 3.9517541154054925e-05\n",
      "step: 5959, loss: 1.880523814179469e-05\n",
      "step: 5960, loss: 1.438067283743294e-05\n",
      "step: 5961, loss: 3.2122054108185694e-05\n",
      "step: 5962, loss: 7.410968100884929e-05\n",
      "step: 5963, loss: 2.5387831556145102e-05\n",
      "step: 5964, loss: 1.0911953722825274e-05\n",
      "step: 5965, loss: 5.6337426030950155e-06\n",
      "step: 5966, loss: 0.00030450578196905553\n",
      "step: 5967, loss: 1.031118881655857e-05\n",
      "step: 5968, loss: 2.9433103918563575e-05\n",
      "step: 5969, loss: 5.143129237694666e-05\n",
      "step: 5970, loss: 2.1613815988530405e-05\n",
      "step: 5971, loss: 1.080455240298761e-05\n",
      "step: 5972, loss: 0.03278057649731636\n",
      "step: 5973, loss: 2.8621647288673557e-05\n",
      "step: 5974, loss: 2.1369787646108307e-05\n",
      "step: 5975, loss: 8.611377779743634e-06\n",
      "step: 5976, loss: 1.4831000044068787e-05\n",
      "step: 5977, loss: 1.7744665456120856e-05\n",
      "step: 5978, loss: 4.398393502924591e-05\n",
      "step: 5979, loss: 2.9014661777182482e-05\n",
      "step: 5980, loss: 1.5539470041403547e-05\n",
      "step: 5981, loss: 1.5636778698535636e-05\n",
      "step: 5982, loss: 1.3446342563838698e-05\n",
      "step: 5983, loss: 3.786139495787211e-05\n",
      "step: 5984, loss: 5.695756044588052e-06\n",
      "step: 5985, loss: 2.42663318203995e-05\n",
      "step: 5986, loss: 2.6768631869344972e-05\n",
      "step: 5987, loss: 1.3117150047037285e-05\n",
      "step: 5988, loss: 6.5776621340774e-05\n",
      "step: 5989, loss: 0.0005782761727459729\n",
      "step: 5990, loss: 1.3017132005188614e-05\n",
      "step: 5991, loss: 3.211723014828749e-05\n",
      "step: 5992, loss: 2.8717226086882874e-05\n",
      "step: 5993, loss: 2.2772866941522807e-05\n",
      "step: 5994, loss: 2.5319570340798236e-05\n",
      "step: 5995, loss: 8.091641575447284e-06\n",
      "step: 5996, loss: 1.091917965823086e-05\n",
      "step: 5997, loss: 5.424709524959326e-05\n",
      "step: 5998, loss: 1.7052871044143103e-05\n",
      "step: 5999, loss: 6.405227031791583e-05\n",
      "step: 6000, loss: 2.11889037018409e-05\n",
      "step: 6001, loss: 1.9987071937066503e-05\n",
      "step: 6002, loss: 2.1325613488443196e-05\n",
      "step: 6003, loss: 2.1039199054939672e-05\n",
      "step: 6004, loss: 4.079296559211798e-05\n",
      "step: 6005, loss: 2.3432876332663e-05\n",
      "step: 6006, loss: 1.502401937614195e-05\n",
      "step: 6007, loss: 3.931580067728646e-05\n",
      "step: 6008, loss: 1.5937643183860928e-05\n",
      "step: 6009, loss: 5.1670918765012175e-05\n",
      "step: 6010, loss: 1.0442361599416472e-05\n",
      "step: 6011, loss: 2.6732584956334904e-05\n",
      "step: 6012, loss: 3.95305069105234e-05\n",
      "step: 6013, loss: 1.576838258188218e-05\n",
      "step: 6014, loss: 2.497487912478391e-05\n",
      "step: 6015, loss: 3.281267709098756e-05\n",
      "step: 6016, loss: 1.0482868674444035e-05\n",
      "step: 6017, loss: 5.43743371963501e-05\n",
      "step: 6018, loss: 2.6730365789262578e-05\n",
      "step: 6019, loss: 2.9271599487401545e-05\n",
      "step: 6020, loss: 2.2957323380978778e-05\n",
      "step: 6021, loss: 3.256062700529583e-05\n",
      "step: 6022, loss: 3.587807805160992e-05\n",
      "step: 6023, loss: 2.429525920888409e-05\n",
      "step: 6024, loss: 2.0636118279071525e-05\n",
      "step: 6025, loss: 1.4922183254384436e-05\n",
      "step: 6026, loss: 1.3687194041267503e-05\n",
      "step: 6027, loss: 1.3782138921669684e-05\n",
      "step: 6028, loss: 1.0845251381397247e-05\n",
      "step: 6029, loss: 2.7527365091373213e-05\n",
      "step: 6030, loss: 2.578399289632216e-05\n",
      "step: 6031, loss: 3.335445580887608e-05\n",
      "step: 6032, loss: 2.1022067812737077e-05\n",
      "step: 6033, loss: 3.250902227591723e-05\n",
      "step: 6034, loss: 5.791199509985745e-05\n",
      "step: 6035, loss: 2.3772776330588385e-05\n",
      "step: 6036, loss: 8.225306373788044e-06\n",
      "step: 6037, loss: 8.859284207574092e-06\n",
      "step: 6038, loss: 0.040672216564416885\n",
      "step: 6039, loss: 3.098951492574997e-05\n",
      "step: 6040, loss: 0.037310365587472916\n",
      "step: 6041, loss: 0.0001623006974114105\n",
      "step: 6042, loss: 1.7114718502853066e-05\n",
      "step: 6043, loss: 1.6633954146527685e-05\n",
      "step: 6044, loss: 5.540752226806944e-06\n",
      "step: 6045, loss: 8.758893272897694e-06\n",
      "step: 6046, loss: 0.027819233015179634\n",
      "step: 6047, loss: 1.64595967362402e-05\n",
      "step: 6048, loss: 2.4570017558289692e-05\n",
      "step: 6049, loss: 2.3093953132047318e-05\n",
      "step: 6050, loss: 6.770967956981622e-06\n",
      "step: 6051, loss: 1.457089001632994e-05\n",
      "step: 6052, loss: 4.142443140153773e-05\n",
      "step: 6053, loss: 1.912363040901255e-05\n",
      "step: 6054, loss: 2.4662669602548704e-05\n",
      "step: 6055, loss: 3.1684587156632915e-05\n",
      "step: 6056, loss: 2.883700108213816e-05\n",
      "step: 6057, loss: 9.243217391485814e-06\n",
      "step: 6058, loss: 2.1775034838356078e-05\n",
      "step: 6059, loss: 4.8983845772454515e-05\n",
      "step: 6060, loss: 1.9622490071924403e-05\n",
      "step: 6061, loss: 6.642630614805967e-05\n",
      "step: 6062, loss: 0.0631527230143547\n",
      "step: 6063, loss: 1.6253326975856908e-05\n",
      "step: 6064, loss: 1.4611789993068669e-05\n",
      "step: 6065, loss: 5.373938620323315e-05\n",
      "step: 6066, loss: 0.00047607847955077887\n",
      "step: 6067, loss: 1.6461357517982833e-05\n",
      "step: 6068, loss: 0.025157716125249863\n",
      "step: 6069, loss: 0.00021971866954118013\n",
      "step: 6070, loss: 2.111039611918386e-05\n",
      "step: 6071, loss: 8.108208021440078e-06\n",
      "step: 6072, loss: 7.006084342719987e-05\n",
      "step: 6073, loss: 0.05760566145181656\n",
      "step: 6074, loss: 0.0752287209033966\n",
      "step: 6075, loss: 4.338876169640571e-05\n",
      "step: 6076, loss: 4.1981129470514134e-05\n",
      "step: 6077, loss: 5.649208105751313e-05\n",
      "step: 6078, loss: 1.5538611478405073e-05\n",
      "step: 6079, loss: 0.14803063869476318\n",
      "step: 6080, loss: 0.0003193842712789774\n",
      "step: 6081, loss: 9.187238902086392e-05\n",
      "step: 6082, loss: 1.0878047760343179e-05\n",
      "step: 6083, loss: 3.776278754230589e-05\n",
      "step: 6084, loss: 2.0950223188265227e-05\n",
      "step: 6085, loss: 9.983065683627501e-05\n",
      "step: 6086, loss: 6.571015546796843e-05\n",
      "step: 6087, loss: 0.001160566695034504\n",
      "step: 6088, loss: 4.104948675376363e-05\n",
      "step: 6089, loss: 1.617348061699886e-05\n",
      "step: 6090, loss: 2.127970765286591e-05\n",
      "step: 6091, loss: 7.069433922879398e-05\n",
      "step: 6092, loss: 2.0911194951622747e-05\n",
      "step: 6093, loss: 2.166078229492996e-05\n",
      "step: 6094, loss: 1.6198981029447168e-05\n",
      "step: 6095, loss: 3.075207132496871e-05\n",
      "step: 6096, loss: 1.5679977877880447e-05\n",
      "step: 6097, loss: 0.00012986941146664321\n",
      "step: 6098, loss: 2.1499256035895087e-05\n",
      "step: 6099, loss: 0.03488672897219658\n",
      "step: 6100, loss: 1.793716728570871e-05\n",
      "step: 6101, loss: 1.0053843652713113e-05\n",
      "step: 6102, loss: 1.6865153156686574e-05\n",
      "step: 6103, loss: 1.2795469046977814e-05\n",
      "step: 6104, loss: 1.0051468052552082e-05\n",
      "step: 6105, loss: 2.2349981009028852e-05\n",
      "step: 6106, loss: 6.607796967728063e-05\n",
      "step: 6107, loss: 3.348652535350993e-05\n",
      "step: 6108, loss: 1.4925753021088894e-05\n",
      "step: 6109, loss: 2.897327794926241e-05\n",
      "step: 6110, loss: 1.4194446521287318e-05\n",
      "step: 6111, loss: 7.765117516100872e-06\n",
      "step: 6112, loss: 4.340942541602999e-05\n",
      "step: 6113, loss: 0.006033903919160366\n",
      "step: 6114, loss: 9.200711792800575e-05\n",
      "step: 6115, loss: 4.564747723634355e-05\n",
      "step: 6116, loss: 2.8067192033631727e-05\n",
      "step: 6117, loss: 7.12060063960962e-05\n",
      "step: 6118, loss: 2.1906287656747736e-05\n",
      "step: 6119, loss: 0.0026238677091896534\n",
      "step: 6120, loss: 1.0160663805436343e-05\n",
      "step: 6121, loss: 1.5782170521561056e-05\n",
      "step: 6122, loss: 1.100488498195773e-05\n",
      "step: 6123, loss: 4.4133477786090225e-05\n",
      "step: 6124, loss: 2.1981884856359102e-05\n",
      "step: 6125, loss: 1.708421768853441e-05\n",
      "step: 6126, loss: 1.1279276804998517e-05\n",
      "step: 6127, loss: 2.1570340322796255e-05\n",
      "step: 6128, loss: 0.0063025071285665035\n",
      "step: 6129, loss: 0.0007932219887152314\n",
      "step: 6130, loss: 0.00064456945983693\n",
      "step: 6131, loss: 9.30367605178617e-05\n",
      "step: 6132, loss: 0.06268896907567978\n",
      "step: 6133, loss: 1.2389988114591688e-05\n",
      "step: 6134, loss: 7.79849142418243e-06\n",
      "step: 6135, loss: 0.00013042260252404958\n",
      "step: 6136, loss: 2.2262025595409796e-05\n",
      "step: 6137, loss: 7.332403038162738e-05\n",
      "step: 6138, loss: 4.8754704039311036e-05\n",
      "step: 6139, loss: 0.008908474817872047\n",
      "step: 6140, loss: 2.3563281501992606e-05\n",
      "step: 6141, loss: 1.7735168512444943e-05\n",
      "step: 6142, loss: 5.607534694718197e-05\n",
      "step: 6143, loss: 1.1651166460069362e-05\n",
      "step: 6144, loss: 1.4938671483832877e-05\n",
      "step: 6145, loss: 0.08097860217094421\n",
      "step: 6146, loss: 5.013852387492079e-06\n",
      "step: 6147, loss: 1.985901326406747e-05\n",
      "step: 6148, loss: 1.9427872757660225e-05\n",
      "step: 6149, loss: 2.571567711129319e-05\n",
      "step: 6150, loss: 1.4602801456931047e-05\n",
      "step: 6151, loss: 2.6893145331996493e-05\n",
      "step: 6152, loss: 2.3484486519009806e-05\n",
      "step: 6153, loss: 3.659915819298476e-05\n",
      "step: 6154, loss: 3.4505104849813506e-05\n",
      "step: 6155, loss: 1.1832258678623475e-05\n",
      "step: 6156, loss: 1.3420276445685886e-05\n",
      "step: 6157, loss: 5.7794117310550064e-05\n",
      "step: 6158, loss: 5.4916468798182905e-05\n",
      "step: 6159, loss: 2.2557569536729716e-05\n",
      "step: 6160, loss: 0.04494388401508331\n",
      "step: 6161, loss: 7.0332130235328805e-06\n",
      "step: 6162, loss: 7.407996599795297e-05\n",
      "step: 6163, loss: 1.6368876458727755e-05\n",
      "step: 6164, loss: 1.6416010112152435e-05\n",
      "step: 6165, loss: 2.2028572857379913e-05\n",
      "step: 6166, loss: 1.0664103683666326e-05\n",
      "step: 6167, loss: 3.578559335437603e-05\n",
      "step: 6168, loss: 2.9243637982290238e-05\n",
      "step: 6169, loss: 2.5774967070901766e-05\n",
      "step: 6170, loss: 5.166515620658174e-05\n",
      "step: 6171, loss: 3.347429083078168e-05\n",
      "step: 6172, loss: 4.460679701878689e-05\n",
      "step: 6173, loss: 0.00026509782765060663\n",
      "step: 6174, loss: 3.095142892561853e-05\n",
      "step: 6175, loss: 9.29054876905866e-05\n",
      "step: 6176, loss: 1.464985416532727e-05\n",
      "step: 6177, loss: 2.0121082343393937e-05\n",
      "step: 6178, loss: 5.5555326980538666e-05\n",
      "step: 6179, loss: 0.00014727486995980144\n",
      "step: 6180, loss: 0.0004917890182696283\n",
      "step: 6181, loss: 0.0001542426907690242\n",
      "step: 6182, loss: 0.00021821442351210862\n",
      "step: 6183, loss: 4.8433750635012984e-05\n",
      "step: 6184, loss: 5.044367571827024e-05\n",
      "step: 6185, loss: 2.8465910872910172e-05\n",
      "step: 6186, loss: 1.755752418830525e-05\n",
      "step: 6187, loss: 2.9697897844016552e-05\n",
      "step: 6188, loss: 1.312646327278344e-05\n",
      "step: 6189, loss: 8.110475755529478e-05\n",
      "step: 6190, loss: 6.027128165442264e-06\n",
      "step: 6191, loss: 3.35100558004342e-05\n",
      "step: 6192, loss: 3.7800440622959286e-05\n",
      "step: 6193, loss: 0.00010628060408635065\n",
      "step: 6194, loss: 0.06133940815925598\n",
      "step: 6195, loss: 7.981523958733305e-05\n",
      "step: 6196, loss: 1.7620825019548647e-05\n",
      "step: 6197, loss: 5.8067911595571786e-05\n",
      "step: 6198, loss: 3.719474989338778e-05\n",
      "step: 6199, loss: 0.029440829530358315\n",
      "step: 6200, loss: 3.193533848389052e-05\n",
      "step: 6201, loss: 1.234239880432142e-05\n",
      "step: 6202, loss: 1.93940541066695e-05\n",
      "step: 6203, loss: 3.275125345680863e-05\n",
      "step: 6204, loss: 2.710035550990142e-05\n",
      "step: 6205, loss: 0.007048079278320074\n",
      "step: 6206, loss: 2.265248986077495e-05\n",
      "step: 6207, loss: 1.1581963917706162e-05\n",
      "step: 6208, loss: 3.5229040804551914e-05\n",
      "step: 6209, loss: 2.2340496798278764e-05\n",
      "step: 6210, loss: 1.0809429113578517e-05\n",
      "step: 6211, loss: 4.060081846546382e-05\n",
      "step: 6212, loss: 3.961839320254512e-05\n",
      "step: 6213, loss: 1.6030437109293416e-05\n",
      "step: 6214, loss: 2.7165471692569554e-05\n",
      "step: 6215, loss: 2.838780710590072e-05\n",
      "step: 6216, loss: 4.473027365747839e-05\n",
      "step: 6217, loss: 1.6337686247425154e-05\n",
      "step: 6218, loss: 0.0030967011116445065\n",
      "step: 6219, loss: 0.00011364324745954946\n",
      "step: 6220, loss: 2.599798608571291e-05\n",
      "step: 6221, loss: 4.815185093320906e-05\n",
      "step: 6222, loss: 0.05751056969165802\n",
      "step: 6223, loss: 1.7458625734434463e-05\n",
      "step: 6224, loss: 1.7212236343766563e-05\n",
      "step: 6225, loss: 7.033159636193886e-05\n",
      "step: 6226, loss: 0.00014047730655875057\n",
      "step: 6227, loss: 3.4370121284155175e-05\n",
      "step: 6228, loss: 7.040957279969007e-05\n",
      "step: 6229, loss: 8.775856258580461e-06\n",
      "step: 6230, loss: 2.8521224521682598e-05\n",
      "step: 6231, loss: 4.699175406130962e-05\n",
      "step: 6232, loss: 7.100444781826809e-05\n",
      "step: 6233, loss: 0.00012543058255687356\n",
      "step: 6234, loss: 0.00015685312973801047\n",
      "step: 6235, loss: 6.213036976987496e-05\n",
      "step: 6236, loss: 2.7128438887302764e-05\n",
      "step: 6237, loss: 1.7963438949664123e-05\n",
      "step: 6238, loss: 1.6202144252019934e-05\n",
      "step: 6239, loss: 1.3756054613622837e-05\n",
      "step: 6240, loss: 1.0315625331713818e-05\n",
      "step: 6241, loss: 1.805652937036939e-05\n",
      "step: 6242, loss: 3.421783912926912e-05\n",
      "step: 6243, loss: 2.7014426450477913e-05\n",
      "step: 6244, loss: 6.244425458135083e-05\n",
      "step: 6245, loss: 1.922967203427106e-05\n",
      "step: 6246, loss: 4.275636456441134e-05\n",
      "step: 6247, loss: 8.299211913254112e-06\n",
      "step: 6248, loss: 0.05913781002163887\n",
      "step: 6249, loss: 1.2762090591422748e-05\n",
      "step: 6250, loss: 1.6488829714944586e-05\n",
      "step: 6251, loss: 6.609162664972246e-05\n",
      "step: 6252, loss: 1.6440228137071244e-05\n",
      "step: 6253, loss: 8.27294570626691e-06\n",
      "step: 6254, loss: 1.5880012142588384e-05\n",
      "step: 6255, loss: 2.1447944163810462e-05\n",
      "step: 6256, loss: 0.00011620137956924736\n",
      "step: 6257, loss: 1.5858477127039805e-05\n",
      "step: 6258, loss: 0.030652087181806564\n",
      "step: 6259, loss: 8.137018085108139e-06\n",
      "step: 6260, loss: 2.841173227352556e-05\n",
      "step: 6261, loss: 3.132352503598668e-05\n",
      "step: 6262, loss: 1.9141269149258733e-05\n",
      "step: 6263, loss: 1.408288244419964e-05\n",
      "step: 6264, loss: 8.025003808143083e-06\n",
      "step: 6265, loss: 1.0716566976043396e-05\n",
      "step: 6266, loss: 2.413829497527331e-05\n",
      "step: 6267, loss: 0.06442306190729141\n",
      "step: 6268, loss: 0.0004983360413461924\n",
      "step: 6269, loss: 4.650811024475843e-05\n",
      "step: 6270, loss: 2.47688567469595e-05\n",
      "step: 6271, loss: 2.3220434741233476e-05\n",
      "step: 6272, loss: 1.101213365473086e-05\n",
      "step: 6273, loss: 0.00013927387772127986\n",
      "step: 6274, loss: 2.355431752221193e-05\n",
      "step: 6275, loss: 4.1797859012149274e-05\n",
      "step: 6276, loss: 1.8534952687332407e-05\n",
      "step: 6277, loss: 0.025511786341667175\n",
      "step: 6278, loss: 2.618074904603418e-05\n",
      "step: 6279, loss: 0.00028149865102022886\n",
      "step: 6280, loss: 8.306214294862002e-05\n",
      "step: 6281, loss: 6.221853982424363e-05\n",
      "step: 6282, loss: 5.979394700261764e-06\n",
      "step: 6283, loss: 1.3877643141313456e-05\n",
      "step: 6284, loss: 8.459179662168026e-05\n",
      "step: 6285, loss: 0.0004649972543120384\n",
      "step: 6286, loss: 3.891292362823151e-05\n",
      "step: 6287, loss: 2.247981319669634e-05\n",
      "step: 6288, loss: 1.3293491065269336e-05\n",
      "step: 6289, loss: 3.604106677812524e-05\n",
      "step: 6290, loss: 9.796241101867054e-06\n",
      "step: 6291, loss: 8.789025014266372e-05\n",
      "step: 6292, loss: 1.7805592506192625e-05\n",
      "step: 6293, loss: 1.4351895515574142e-05\n",
      "step: 6294, loss: 6.062761531211436e-05\n",
      "step: 6295, loss: 2.9948432711535133e-05\n",
      "step: 6296, loss: 5.3937299526296556e-05\n",
      "step: 6297, loss: 0.0351252518594265\n",
      "step: 6298, loss: 3.118776294286363e-05\n",
      "step: 6299, loss: 9.815336852625478e-06\n",
      "step: 6300, loss: 3.903535980498418e-05\n",
      "step: 6301, loss: 4.965591142536141e-05\n",
      "step: 6302, loss: 4.291294317226857e-05\n",
      "step: 6303, loss: 3.18062593578361e-05\n",
      "step: 6304, loss: 0.0018039620481431484\n",
      "step: 6305, loss: 9.38722223509103e-05\n",
      "step: 6306, loss: 2.0363129806355573e-05\n",
      "step: 6307, loss: 3.10452924168203e-05\n",
      "step: 6308, loss: 1.7184098396683112e-05\n",
      "step: 6309, loss: 7.657072274014354e-05\n",
      "step: 6310, loss: 0.00010407192894490436\n",
      "step: 6311, loss: 0.061575789004564285\n",
      "step: 6312, loss: 2.0413659512996674e-05\n",
      "step: 6313, loss: 1.955403422471136e-05\n",
      "step: 6314, loss: 5.7222492614528164e-05\n",
      "step: 6315, loss: 1.3396143003774341e-05\n",
      "step: 6316, loss: 1.765141678333748e-05\n",
      "step: 6317, loss: 1.5043296116346028e-05\n",
      "step: 6318, loss: 6.368121830746531e-05\n",
      "step: 6319, loss: 2.5141927835647948e-05\n",
      "step: 6320, loss: 4.913395605399273e-05\n",
      "step: 6321, loss: 2.4980830858112313e-05\n",
      "step: 6322, loss: 7.216754056571517e-06\n",
      "step: 6323, loss: 4.203191201668233e-05\n",
      "step: 6324, loss: 2.500333903299179e-05\n",
      "step: 6325, loss: 2.704460894165095e-05\n",
      "step: 6326, loss: 2.2923135475139134e-05\n",
      "step: 6327, loss: 0.000136093920446001\n",
      "step: 6328, loss: 2.6719790184870362e-05\n",
      "step: 6329, loss: 1.3491478966898285e-05\n",
      "step: 6330, loss: 6.079049489926547e-05\n",
      "step: 6331, loss: 2.625340857775882e-05\n",
      "step: 6332, loss: 2.2531479771714658e-05\n",
      "step: 6333, loss: 1.3904008483223151e-05\n",
      "step: 6334, loss: 1.340501239610603e-05\n",
      "step: 6335, loss: 6.216068140929565e-05\n",
      "step: 6336, loss: 2.0695069906651042e-05\n",
      "step: 6337, loss: 1.8710754375206307e-05\n",
      "step: 6338, loss: 0.00011575228563742712\n",
      "step: 6339, loss: 1.8858941984944977e-05\n",
      "step: 6340, loss: 0.0015700168441981077\n",
      "step: 6341, loss: 1.5219774468278047e-05\n",
      "step: 6342, loss: 0.007244592998176813\n",
      "step: 6343, loss: 1.1352837645972613e-05\n",
      "step: 6344, loss: 1.2373016943456605e-05\n",
      "step: 6345, loss: 4.146615901845507e-05\n",
      "step: 6346, loss: 1.1090699445048813e-05\n",
      "step: 6347, loss: 2.2401642127078958e-05\n",
      "step: 6348, loss: 6.990334895817796e-06\n",
      "step: 6349, loss: 1.0232590284431353e-05\n",
      "step: 6350, loss: 4.225996963214129e-05\n",
      "step: 6351, loss: 7.611657201778144e-05\n",
      "step: 6352, loss: 0.03482815623283386\n",
      "step: 6353, loss: 3.4986704122275114e-05\n",
      "step: 6354, loss: 1.7650814697844908e-05\n",
      "step: 6355, loss: 9.209699783241376e-06\n",
      "step: 6356, loss: 2.0346929886727594e-05\n",
      "step: 6357, loss: 1.1550712770258542e-05\n",
      "step: 6358, loss: 3.763714630622417e-05\n",
      "step: 6359, loss: 7.017656025709584e-05\n",
      "step: 6360, loss: 2.5141560399788432e-05\n",
      "step: 6361, loss: 3.411248326301575e-05\n",
      "step: 6362, loss: 2.9617693144246005e-05\n",
      "step: 6363, loss: 4.9331985792377964e-05\n",
      "step: 6364, loss: 4.032511424156837e-05\n",
      "step: 6365, loss: 1.8092407117364928e-05\n",
      "step: 6366, loss: 7.2263255788129754e-06\n",
      "step: 6367, loss: 5.349074854166247e-05\n",
      "step: 6368, loss: 1.8471613657311536e-05\n",
      "step: 6369, loss: 1.8750073650153354e-05\n",
      "step: 6370, loss: 0.00015414657536894083\n",
      "step: 6371, loss: 2.511790080461651e-05\n",
      "step: 6372, loss: 1.4139319318928756e-05\n",
      "step: 6373, loss: 0.00016504152154084295\n",
      "step: 6374, loss: 4.61224808532279e-05\n",
      "step: 6375, loss: 5.674290605384158e-06\n",
      "step: 6376, loss: 3.86512947443407e-05\n",
      "step: 6377, loss: 3.742122862604447e-05\n",
      "step: 6378, loss: 1.2017872904834803e-05\n",
      "step: 6379, loss: 0.09162221103906631\n",
      "step: 6380, loss: 1.7594344171811827e-05\n",
      "step: 6381, loss: 2.505032352928538e-05\n",
      "step: 6382, loss: 0.031849730759859085\n",
      "step: 6383, loss: 1.279754451388726e-05\n",
      "step: 6384, loss: 1.3447337551042438e-05\n",
      "step: 6385, loss: 1.5856679965509102e-05\n",
      "step: 6386, loss: 1.0080132597067859e-05\n",
      "step: 6387, loss: 9.083466466108803e-06\n",
      "step: 6388, loss: 0.015069067478179932\n",
      "step: 6389, loss: 5.5175343732116744e-05\n",
      "step: 6390, loss: 0.02792349085211754\n",
      "step: 6391, loss: 3.2610267226118594e-05\n",
      "step: 6392, loss: 1.8454884411767125e-05\n",
      "step: 6393, loss: 0.00014083899441175163\n",
      "step: 6394, loss: 2.026592665060889e-05\n",
      "step: 6395, loss: 1.6516703908564523e-05\n",
      "step: 6396, loss: 1.768499168974813e-05\n",
      "step: 6397, loss: 3.6511977668851614e-05\n",
      "step: 6398, loss: 3.544742867234163e-05\n",
      "step: 6399, loss: 6.80367520544678e-05\n",
      "step: 6400, loss: 3.126457522739656e-05\n",
      "step: 6401, loss: 3.832598667941056e-05\n",
      "step: 6402, loss: 3.834968083538115e-05\n",
      "step: 6403, loss: 9.591203706804663e-06\n",
      "step: 6404, loss: 1.0825201570696663e-05\n",
      "step: 6405, loss: 3.65375672117807e-05\n",
      "step: 6406, loss: 2.4124363335431553e-05\n",
      "step: 6407, loss: 1.9968822016380727e-05\n",
      "step: 6408, loss: 0.06897304207086563\n",
      "step: 6409, loss: 1.8178172467742115e-05\n",
      "step: 6410, loss: 5.1966380851808935e-05\n",
      "step: 6411, loss: 0.018105272203683853\n",
      "step: 6412, loss: 1.829361281124875e-05\n",
      "step: 6413, loss: 1.3944336387794465e-05\n",
      "step: 6414, loss: 9.274500189349055e-05\n",
      "step: 6415, loss: 1.3076672075840179e-05\n",
      "step: 6416, loss: 0.00016720830171834677\n",
      "step: 6417, loss: 4.397525481181219e-05\n",
      "step: 6418, loss: 3.1748182664159685e-05\n",
      "step: 6419, loss: 3.122118141618557e-05\n",
      "step: 6420, loss: 5.1235943828942254e-05\n",
      "step: 6421, loss: 2.48952583206119e-05\n",
      "step: 6422, loss: 3.520501559250988e-05\n",
      "step: 6423, loss: 0.00012482807505875826\n",
      "step: 6424, loss: 2.4364919227082282e-05\n",
      "step: 6425, loss: 0.044211018830537796\n",
      "step: 6426, loss: 2.1269159333314747e-05\n",
      "step: 6427, loss: 2.3260170564753935e-05\n",
      "step: 6428, loss: 3.892562017426826e-05\n",
      "step: 6429, loss: 8.213287401304115e-06\n",
      "step: 6430, loss: 5.3251831559464335e-05\n",
      "step: 6431, loss: 0.0012172049609944224\n",
      "step: 6432, loss: 7.439098408212885e-05\n",
      "step: 6433, loss: 0.000519758032169193\n",
      "step: 6434, loss: 0.0005947896861471236\n",
      "step: 6435, loss: 8.527123281965032e-05\n",
      "step: 6436, loss: 9.431521903024986e-06\n",
      "step: 6437, loss: 1.152192817244213e-05\n",
      "step: 6438, loss: 5.083020823803963e-06\n",
      "step: 6439, loss: 3.182292130077258e-05\n",
      "step: 6440, loss: 1.8540518794907257e-05\n",
      "step: 6441, loss: 0.005193315912038088\n",
      "step: 6442, loss: 1.805288775358349e-05\n",
      "step: 6443, loss: 5.0315717089688405e-05\n",
      "step: 6444, loss: 5.9864923969144e-06\n",
      "step: 6445, loss: 1.0811941137944814e-05\n",
      "step: 6446, loss: 6.036629201844335e-06\n",
      "step: 6447, loss: 2.2420374079956673e-05\n",
      "step: 6448, loss: 0.00010922763613052666\n",
      "step: 6449, loss: 3.4573244192870334e-05\n",
      "step: 6450, loss: 8.501747288391925e-06\n",
      "step: 6451, loss: 1.8608820028021e-05\n",
      "step: 6452, loss: 3.3125110348919407e-05\n",
      "step: 6453, loss: 2.2485293811769225e-05\n",
      "step: 6454, loss: 1.1140918104501907e-05\n",
      "step: 6455, loss: 2.0969391698599793e-05\n",
      "step: 6456, loss: 2.6114319553016685e-05\n",
      "step: 6457, loss: 1.733601493469905e-05\n",
      "step: 6458, loss: 0.0002164899924537167\n",
      "step: 6459, loss: 1.4253550943976734e-05\n",
      "step: 6460, loss: 2.0165809473837726e-05\n",
      "step: 6461, loss: 4.593822086462751e-05\n",
      "step: 6462, loss: 6.434516308218008e-06\n",
      "step: 6463, loss: 5.9661822888301685e-05\n",
      "step: 6464, loss: 6.07258734817151e-05\n",
      "step: 6465, loss: 6.83618345647119e-05\n",
      "step: 6466, loss: 3.2725260098231956e-05\n",
      "step: 6467, loss: 9.853010851657018e-06\n",
      "step: 6468, loss: 0.025397779420018196\n",
      "step: 6469, loss: 8.24422841105843e-06\n",
      "step: 6470, loss: 5.922244326939108e-06\n",
      "step: 6471, loss: 0.000156296300701797\n",
      "step: 6472, loss: 4.414083741721697e-05\n",
      "step: 6473, loss: 1.3943537851446308e-05\n",
      "step: 6474, loss: 7.908155566838104e-06\n",
      "step: 6475, loss: 1.82569601747673e-05\n",
      "step: 6476, loss: 2.4605251383036375e-05\n",
      "step: 6477, loss: 0.1518022119998932\n",
      "step: 6478, loss: 1.686704308667686e-05\n",
      "step: 6479, loss: 2.352516821702011e-05\n",
      "step: 6480, loss: 1.0683266737032682e-05\n",
      "step: 6481, loss: 1.246850206371164e-05\n",
      "step: 6482, loss: 0.00013579940423369408\n",
      "step: 6483, loss: 0.00043252299656160176\n",
      "step: 6484, loss: 0.000499306945130229\n",
      "step: 6485, loss: 0.00016420820611529052\n",
      "step: 6486, loss: 6.784519064240158e-05\n",
      "step: 6487, loss: 0.023651123046875\n",
      "step: 6488, loss: 6.702332029817626e-05\n",
      "step: 6489, loss: 2.2645181161351502e-05\n",
      "step: 6490, loss: 0.0002335838071303442\n",
      "step: 6491, loss: 0.0007068043341860175\n",
      "step: 6492, loss: 0.0001530452282167971\n",
      "step: 6493, loss: 0.01834607869386673\n",
      "step: 6494, loss: 4.3118045141454786e-05\n",
      "step: 6495, loss: 0.00034651762689463794\n",
      "step: 6496, loss: 7.015094888629392e-05\n",
      "step: 6497, loss: 0.004160355776548386\n",
      "step: 6498, loss: 0.0343383252620697\n",
      "step: 6499, loss: 3.0835512006888166e-05\n",
      "step: 6500, loss: 0.0006054629920981824\n",
      "step: 6501, loss: 0.0001887653343146667\n",
      "step: 6502, loss: 5.899693860555999e-05\n",
      "step: 6503, loss: 9.074748231796548e-05\n",
      "step: 6504, loss: 0.09102998673915863\n",
      "step: 6505, loss: 0.00013610109454020858\n",
      "step: 6506, loss: 0.00379262026399374\n",
      "step: 6507, loss: 3.4750846680253744e-05\n",
      "step: 6508, loss: 0.00014370489225257188\n",
      "step: 6509, loss: 6.897105049574748e-05\n",
      "step: 6510, loss: 0.0001493447198299691\n",
      "step: 6511, loss: 0.09148683398962021\n",
      "step: 6512, loss: 1.5622672435711138e-05\n",
      "step: 6513, loss: 0.00014491517504211515\n",
      "step: 6514, loss: 0.01224395539611578\n",
      "step: 6515, loss: 3.541644764482044e-05\n",
      "step: 6516, loss: 0.00013360308366827667\n",
      "step: 6517, loss: 0.3658454418182373\n",
      "step: 6518, loss: 0.0010563527466729283\n",
      "step: 6519, loss: 0.0008322447538375854\n",
      "step: 6520, loss: 0.0001043745141942054\n",
      "step: 6521, loss: 8.680939936311916e-05\n",
      "step: 6522, loss: 0.00048028017044998705\n",
      "step: 6523, loss: 0.000524337578099221\n",
      "step: 6524, loss: 0.0001761440362315625\n",
      "step: 6525, loss: 9.181941277347505e-05\n",
      "step: 6526, loss: 0.00010663547436706722\n",
      "step: 6527, loss: 0.08328042924404144\n",
      "step: 6528, loss: 0.0005336120375432074\n",
      "step: 6529, loss: 5.229178714216687e-05\n",
      "step: 6530, loss: 0.00658123753964901\n",
      "step: 6531, loss: 0.0023418134078383446\n",
      "step: 6532, loss: 0.10741466283798218\n",
      "step: 6533, loss: 5.2329480240587145e-05\n",
      "step: 6534, loss: 7.275893585756421e-05\n",
      "step: 6535, loss: 3.770773037103936e-05\n",
      "step: 6536, loss: 0.000200866925297305\n",
      "step: 6537, loss: 0.00047512835590168834\n",
      "step: 6538, loss: 0.0002308577240910381\n",
      "step: 6539, loss: 0.00016887698438949883\n",
      "step: 6540, loss: 0.00043262369581498206\n",
      "step: 6541, loss: 0.00014206429477781057\n",
      "step: 6542, loss: 0.0005938303656876087\n",
      "step: 6543, loss: 0.00015143718337640166\n",
      "step: 6544, loss: 6.420552381314337e-05\n",
      "step: 6545, loss: 0.027856528759002686\n",
      "step: 6546, loss: 0.0013707781909033656\n",
      "step: 6547, loss: 9.708506695460528e-05\n",
      "step: 6548, loss: 6.0732971178367734e-05\n",
      "step: 6549, loss: 0.0005510656046681106\n",
      "step: 6550, loss: 0.00034197044442407787\n",
      "step: 6551, loss: 0.00016687929746694863\n",
      "step: 6552, loss: 9.79350515990518e-05\n",
      "step: 6553, loss: 0.001478685298934579\n",
      "step: 6554, loss: 0.00038498794310726225\n",
      "step: 6555, loss: 0.002076941542327404\n",
      "step: 6556, loss: 0.0001040482529788278\n",
      "step: 6557, loss: 0.00048175998381339014\n",
      "step: 6558, loss: 6.962219049455598e-05\n",
      "step: 6559, loss: 0.00034681378747336566\n",
      "step: 6560, loss: 9.03448453755118e-05\n",
      "step: 6561, loss: 0.0022632170002907515\n",
      "step: 6562, loss: 0.000267333845840767\n",
      "step: 6563, loss: 1.592326952959411e-05\n",
      "step: 6564, loss: 5.5976168368943036e-05\n",
      "step: 6565, loss: 0.00010943964298348874\n",
      "step: 6566, loss: 0.06322107464075089\n",
      "step: 6567, loss: 0.0002079657424474135\n",
      "step: 6568, loss: 0.04384937137365341\n",
      "step: 6569, loss: 1.970388984773308e-05\n",
      "step: 6570, loss: 5.3170449973549694e-05\n",
      "step: 6571, loss: 0.024618882685899734\n",
      "step: 6572, loss: 6.661501538474113e-05\n",
      "step: 6573, loss: 0.004516249522566795\n",
      "step: 6574, loss: 5.844110273756087e-05\n",
      "step: 6575, loss: 9.904994658427313e-05\n",
      "step: 6576, loss: 0.004219750873744488\n",
      "step: 6577, loss: 4.178121162112802e-05\n",
      "step: 6578, loss: 0.0001279534917557612\n",
      "step: 6579, loss: 0.00028285308508202434\n",
      "step: 6580, loss: 2.165395562769845e-05\n",
      "step: 6581, loss: 0.0001833724236348644\n",
      "step: 6582, loss: 7.364227349171415e-05\n",
      "step: 6583, loss: 5.9339854487916455e-05\n",
      "step: 6584, loss: 0.02646181359887123\n",
      "step: 6585, loss: 0.002781302435323596\n",
      "step: 6586, loss: 2.041863808699418e-05\n",
      "step: 6587, loss: 6.149667024146765e-05\n",
      "step: 6588, loss: 1.3174086234357674e-05\n",
      "step: 6589, loss: 2.3525213691755198e-05\n",
      "step: 6590, loss: 8.715685544302687e-05\n",
      "step: 6591, loss: 0.0005835274932906032\n",
      "step: 6592, loss: 7.029939297353849e-05\n",
      "step: 6593, loss: 5.8078971051145345e-05\n",
      "step: 6594, loss: 6.497919093817472e-05\n",
      "step: 6595, loss: 7.774874393362552e-05\n",
      "step: 6596, loss: 1.778731166268699e-05\n",
      "step: 6597, loss: 3.06195106531959e-05\n",
      "step: 6598, loss: 0.00032935512717813253\n",
      "step: 6599, loss: 5.796928962809034e-05\n",
      "step: 6600, loss: 0.0002433890476822853\n",
      "step: 6601, loss: 0.003507923800498247\n",
      "step: 6602, loss: 0.0005939634284004569\n",
      "step: 6603, loss: 7.195748185040429e-05\n",
      "step: 6604, loss: 0.0003077393339481205\n",
      "step: 6605, loss: 0.0003005764738190919\n",
      "step: 6606, loss: 0.00012642104411497712\n",
      "step: 6607, loss: 0.0003901003219652921\n",
      "step: 6608, loss: 0.0038824041839689016\n",
      "step: 6609, loss: 0.00011673912376863882\n",
      "step: 6610, loss: 0.00022682778944727033\n",
      "step: 6611, loss: 0.00010842773917829618\n",
      "step: 6612, loss: 0.00010654550715116784\n",
      "step: 6613, loss: 0.0001785665808711201\n",
      "step: 6614, loss: 0.0918101817369461\n",
      "step: 6615, loss: 0.0003968848322983831\n",
      "step: 6616, loss: 8.623945177532732e-05\n",
      "step: 6617, loss: 0.00024471766664646566\n",
      "step: 6618, loss: 0.0001756511628627777\n",
      "step: 6619, loss: 0.000299322244245559\n",
      "step: 6620, loss: 0.0006296352948993444\n",
      "step: 6621, loss: 0.03373728692531586\n",
      "step: 6622, loss: 1.5023997548269108e-05\n",
      "step: 6623, loss: 4.7399142204085365e-05\n",
      "step: 6624, loss: 0.00032415310852229595\n",
      "step: 6625, loss: 0.00019214084022678435\n",
      "step: 6626, loss: 0.003433655248954892\n",
      "step: 6627, loss: 0.0009676861809566617\n",
      "step: 6628, loss: 6.894180114613846e-05\n",
      "step: 6629, loss: 3.892758104484528e-05\n",
      "step: 6630, loss: 0.0002540202403906733\n",
      "step: 6631, loss: 2.8142296287114732e-05\n",
      "step: 6632, loss: 0.00015850314230192453\n",
      "step: 6633, loss: 0.0057474104687571526\n",
      "step: 6634, loss: 0.006357942707836628\n",
      "step: 6635, loss: 0.0021570674143731594\n",
      "step: 6636, loss: 0.0003272015892434865\n",
      "step: 6637, loss: 0.0004957866040058434\n",
      "step: 6638, loss: 0.0009538091835565865\n",
      "step: 6639, loss: 0.027726881206035614\n",
      "step: 6640, loss: 0.00033679621992632747\n",
      "step: 6641, loss: 0.0006564363138750196\n",
      "step: 6642, loss: 0.00016261231212411076\n",
      "step: 6643, loss: 0.08377419412136078\n",
      "step: 6644, loss: 5.3227679018164054e-05\n",
      "step: 6645, loss: 4.334527693572454e-05\n",
      "step: 6646, loss: 1.5150321814871859e-05\n",
      "step: 6647, loss: 0.0009703273535706103\n",
      "step: 6648, loss: 5.482075721374713e-05\n",
      "step: 6649, loss: 0.0001300891162827611\n",
      "step: 6650, loss: 6.578027387149632e-05\n",
      "step: 6651, loss: 0.00017820700304582715\n",
      "step: 6652, loss: 0.0003626527322921902\n",
      "step: 6653, loss: 0.0013273321092128754\n",
      "step: 6654, loss: 0.007297062780708075\n",
      "step: 6655, loss: 0.003518804209306836\n",
      "step: 6656, loss: 0.0005696978187188506\n",
      "step: 6657, loss: 0.001404656795784831\n",
      "step: 6658, loss: 0.00032124912831932306\n",
      "step: 6659, loss: 0.00028150438447482884\n",
      "step: 6660, loss: 0.0051308004185557365\n",
      "step: 6661, loss: 0.027678746730089188\n",
      "step: 6662, loss: 0.0017340383492410183\n",
      "step: 6663, loss: 0.00022032544075045735\n",
      "step: 6664, loss: 0.0015271727461367846\n",
      "step: 6665, loss: 0.0011836315970867872\n",
      "step: 6666, loss: 0.00011438014189479873\n",
      "step: 6667, loss: 0.0002032062184298411\n",
      "step: 6668, loss: 0.010509039275348186\n",
      "step: 6669, loss: 0.042820800095796585\n",
      "step: 6670, loss: 0.004248934797942638\n",
      "step: 6671, loss: 0.00014557578833773732\n",
      "step: 6672, loss: 0.02007611282169819\n",
      "step: 6673, loss: 0.0013246372109279037\n",
      "step: 6674, loss: 0.008888662792742252\n",
      "step: 6675, loss: 0.00012188356049591675\n",
      "step: 6676, loss: 0.05328353866934776\n",
      "step: 6677, loss: 5.4801785154268146e-05\n",
      "step: 6678, loss: 0.008524020202457905\n",
      "step: 6679, loss: 0.0002130901557393372\n",
      "step: 6680, loss: 0.0007366047939285636\n",
      "step: 6681, loss: 0.000495183456223458\n",
      "step: 6682, loss: 0.00035052583552896976\n",
      "step: 6683, loss: 0.08430685847997665\n",
      "step: 6684, loss: 0.04959067329764366\n",
      "step: 6685, loss: 0.0017301462357863784\n",
      "step: 6686, loss: 0.005188726354390383\n",
      "step: 6687, loss: 0.011016695760190487\n",
      "step: 6688, loss: 0.0035979310050606728\n",
      "step: 6689, loss: 0.001237008604221046\n",
      "step: 6690, loss: 0.010874335654079914\n",
      "step: 6691, loss: 0.0013178050285205245\n",
      "step: 6692, loss: 0.0011822422966361046\n",
      "step: 6693, loss: 0.019592195749282837\n",
      "step: 6694, loss: 0.0018180347979068756\n",
      "step: 6695, loss: 0.0005035836948081851\n",
      "step: 6696, loss: 4.291832010494545e-05\n",
      "step: 6697, loss: 0.12297523766756058\n",
      "step: 6698, loss: 0.0006176674505695701\n",
      "step: 6699, loss: 0.0006297099753282964\n",
      "step: 6700, loss: 0.0030106636695563793\n",
      "step: 6701, loss: 0.20309250056743622\n",
      "step: 6702, loss: 0.004951565060764551\n",
      "step: 6703, loss: 0.025343850255012512\n",
      "step: 6704, loss: 0.000466999044874683\n",
      "step: 6705, loss: 0.003397723427042365\n",
      "step: 6706, loss: 0.045463915914297104\n",
      "step: 6707, loss: 0.0010802991455420852\n",
      "step: 6708, loss: 0.0783032551407814\n",
      "step: 6709, loss: 0.00018934026593342423\n",
      "step: 6710, loss: 0.0006710129091516137\n",
      "step: 6711, loss: 0.006434334442019463\n",
      "step: 6712, loss: 0.05145280435681343\n",
      "step: 6713, loss: 0.019312750548124313\n",
      "step: 6714, loss: 3.0365790735231712e-05\n",
      "step: 6715, loss: 0.005905468016862869\n",
      "step: 6716, loss: 0.006378618068993092\n",
      "step: 6717, loss: 0.0009029613574966788\n",
      "step: 6718, loss: 0.0007097546476870775\n",
      "step: 6719, loss: 0.0006217049667611718\n",
      "step: 6720, loss: 0.0578167587518692\n",
      "step: 6721, loss: 0.0003662029921542853\n",
      "step: 6722, loss: 0.03627193346619606\n",
      "step: 6723, loss: 0.03834940865635872\n",
      "step: 6724, loss: 0.1467466950416565\n",
      "step: 6725, loss: 0.016045626252889633\n",
      "step: 6726, loss: 0.006361081730574369\n",
      "step: 6727, loss: 0.0012730119051411748\n",
      "step: 6728, loss: 0.023468049243092537\n",
      "step: 6729, loss: 0.0003480322193354368\n",
      "step: 6730, loss: 0.013090793043375015\n",
      "step: 6731, loss: 0.03538288176059723\n",
      "step: 6732, loss: 0.06989264488220215\n",
      "step: 6733, loss: 0.00036807521246373653\n",
      "step: 6734, loss: 0.061612602323293686\n",
      "step: 6735, loss: 0.09300947189331055\n",
      "step: 6736, loss: 0.03328923135995865\n",
      "step: 6737, loss: 0.0003470134688541293\n",
      "step: 6738, loss: 0.00012093251280020922\n",
      "step: 6739, loss: 0.23665988445281982\n",
      "step: 6740, loss: 0.0017273416742682457\n",
      "step: 6741, loss: 0.16975267231464386\n",
      "step: 6742, loss: 1.6573751054238528e-05\n",
      "step: 6743, loss: 0.0002002528344746679\n",
      "step: 6744, loss: 0.13201558589935303\n",
      "step: 6745, loss: 0.02135073021054268\n",
      "step: 6746, loss: 0.07157698273658752\n",
      "step: 6747, loss: 0.009556642733514309\n",
      "step: 6748, loss: 0.16594119369983673\n",
      "step: 6749, loss: 0.004590838681906462\n",
      "step: 6750, loss: 0.0030247699469327927\n",
      "step: 6751, loss: 0.03762345016002655\n",
      "step: 6752, loss: 0.002361504826694727\n",
      "step: 6753, loss: 0.005218259058892727\n",
      "step: 6754, loss: 0.004113564267754555\n",
      "step: 6755, loss: 0.001082886941730976\n",
      "step: 6756, loss: 0.02263888344168663\n",
      "step: 6757, loss: 0.06876270473003387\n",
      "step: 6758, loss: 0.0004299066204112023\n",
      "step: 6759, loss: 0.12375562638044357\n",
      "step: 6760, loss: 0.0005182257737033069\n",
      "step: 6761, loss: 0.00020271319954190403\n",
      "step: 6762, loss: 0.03618955984711647\n",
      "step: 6763, loss: 0.06458857655525208\n",
      "step: 6764, loss: 0.11857662349939346\n",
      "step: 6765, loss: 0.07012338936328888\n",
      "step: 6766, loss: 0.34065762162208557\n",
      "step: 6767, loss: 0.0003445792244747281\n",
      "step: 6768, loss: 0.14623020589351654\n",
      "step: 6769, loss: 0.3957524001598358\n",
      "step: 6770, loss: 0.027494659647345543\n",
      "step: 6771, loss: 0.036899641156196594\n",
      "step: 6772, loss: 0.15319226682186127\n",
      "step: 6773, loss: 0.12200361490249634\n",
      "step: 6774, loss: 0.0006761268014088273\n",
      "step: 6775, loss: 0.0006405762396752834\n",
      "step: 6776, loss: 0.20977136492729187\n",
      "step: 6777, loss: 0.1423828899860382\n",
      "step: 6778, loss: 0.19051356613636017\n",
      "step: 6779, loss: 0.12174461036920547\n",
      "step: 6780, loss: 0.004204432480037212\n",
      "step: 6781, loss: 0.09981394559144974\n",
      "step: 6782, loss: 0.005444468464702368\n",
      "step: 6783, loss: 0.0002838013169821352\n",
      "step: 6784, loss: 0.21025587618350983\n",
      "step: 6785, loss: 0.05012708157300949\n",
      "step: 6786, loss: 0.022418312728405\n",
      "step: 6787, loss: 0.007647492457181215\n",
      "step: 6788, loss: 0.08371778577566147\n",
      "step: 6789, loss: 0.012096571736037731\n",
      "step: 6790, loss: 0.00508074602112174\n",
      "step: 6791, loss: 0.014360539615154266\n",
      "step: 6792, loss: 0.006736702285706997\n",
      "step: 6793, loss: 0.641623854637146\n",
      "step: 6794, loss: 0.0934101939201355\n",
      "step: 6795, loss: 0.09144533425569534\n",
      "step: 6796, loss: 0.2853279411792755\n",
      "step: 6797, loss: 0.0250431839376688\n",
      "step: 6798, loss: 0.03742801770567894\n",
      "step: 6799, loss: 0.7075144052505493\n",
      "step: 6800, loss: 0.04081776738166809\n",
      "step: 6801, loss: 0.00041664790478534997\n",
      "step: 6802, loss: 0.029329977929592133\n",
      "step: 6803, loss: 0.8336401581764221\n",
      "step: 6804, loss: 0.2361697256565094\n",
      "step: 6805, loss: 0.10085970908403397\n",
      "step: 6806, loss: 0.0820673406124115\n",
      "step: 6807, loss: 0.47200167179107666\n",
      "step: 6808, loss: 0.48998042941093445\n",
      "step: 6809, loss: 0.3736569881439209\n",
      "step: 6810, loss: 0.0830901637673378\n",
      "step: 6811, loss: 0.36495697498321533\n",
      "step: 6812, loss: 0.07928942143917084\n",
      "step: 6813, loss: 0.018316421657800674\n",
      "step: 6814, loss: 0.24378034472465515\n",
      "step: 6815, loss: 0.2673574388027191\n",
      "step: 6816, loss: 0.10690533369779587\n",
      "step: 6817, loss: 0.017550304532051086\n",
      "step: 6818, loss: 0.018666880205273628\n",
      "step: 6819, loss: 0.04930238798260689\n",
      "step: 6820, loss: 0.2979811131954193\n",
      "step: 6821, loss: 0.15300250053405762\n",
      "step: 6822, loss: 0.0036700379569083452\n",
      "step: 6823, loss: 0.07090011984109879\n",
      "step: 6824, loss: 0.42098677158355713\n",
      "step: 6825, loss: 0.06827467679977417\n",
      "step: 6826, loss: 0.021878516301512718\n",
      "step: 6827, loss: 0.12091526389122009\n",
      "step: 6828, loss: 0.06398892402648926\n",
      "step: 6829, loss: 0.1093624085187912\n",
      "step: 6830, loss: 0.3811783492565155\n",
      "step: 6831, loss: 0.017818210646510124\n",
      "step: 6832, loss: 0.2815956175327301\n",
      "step: 6833, loss: 0.02165340445935726\n",
      "step: 6834, loss: 0.2931213676929474\n",
      "step: 6835, loss: 0.4015471935272217\n",
      "step: 6836, loss: 0.24535101652145386\n",
      "step: 6837, loss: 0.20325277745723724\n",
      "step: 6838, loss: 0.09190312027931213\n",
      "step: 6839, loss: 0.026137104257941246\n",
      "step: 6840, loss: 0.5476331114768982\n",
      "step: 6841, loss: 0.08712174743413925\n",
      "step: 6842, loss: 0.10609766840934753\n",
      "step: 6843, loss: 0.12732312083244324\n",
      "step: 6844, loss: 0.1275152564048767\n",
      "step: 6845, loss: 0.3498603105545044\n",
      "step: 6846, loss: 0.0034204688854515553\n",
      "step: 6847, loss: 0.08721143752336502\n",
      "step: 6848, loss: 0.030953004956245422\n",
      "step: 6849, loss: 0.7039517164230347\n",
      "step: 6850, loss: 0.7199298739433289\n",
      "step: 6851, loss: 0.2531425952911377\n",
      "step: 6852, loss: 0.3629421591758728\n",
      "step: 6853, loss: 0.2042662650346756\n",
      "step: 6854, loss: 0.7760739922523499\n",
      "step: 6855, loss: 0.01880422793328762\n",
      "step: 6856, loss: 0.3939391076564789\n",
      "step: 6857, loss: 0.17317038774490356\n",
      "step: 6858, loss: 0.7016021013259888\n",
      "step: 6859, loss: 0.006711976137012243\n",
      "step: 6860, loss: 0.3363279402256012\n",
      "step: 6861, loss: 0.4359659254550934\n",
      "step: 6862, loss: 0.18043792247772217\n",
      "step: 6863, loss: 0.19613227248191833\n",
      "step: 6864, loss: 0.3182174563407898\n",
      "step: 6865, loss: 0.015289719216525555\n",
      "step: 6866, loss: 0.14633353054523468\n",
      "step: 6867, loss: 0.1839861422777176\n",
      "step: 6868, loss: 0.007074189838021994\n",
      "step: 6869, loss: 0.1390349268913269\n",
      "step: 6870, loss: 0.6849085092544556\n",
      "step: 6871, loss: 0.8226565718650818\n",
      "step: 6872, loss: 0.35642459988594055\n",
      "step: 6873, loss: 0.14084796607494354\n",
      "step: 6874, loss: 0.46911486983299255\n",
      "step: 6875, loss: 0.10383555293083191\n",
      "step: 6876, loss: 0.09824374318122864\n",
      "step: 6877, loss: 0.5077389478683472\n",
      "step: 6878, loss: 0.1572064906358719\n",
      "step: 6879, loss: 0.008526375517249107\n",
      "step: 6880, loss: 0.26146200299263\n",
      "step: 6881, loss: 0.025655530393123627\n",
      "step: 6882, loss: 0.473785400390625\n",
      "step: 6883, loss: 0.5614117383956909\n",
      "step: 6884, loss: 0.17226654291152954\n",
      "step: 6885, loss: 0.05323638394474983\n",
      "step: 6886, loss: 0.0022798471618443727\n",
      "step: 6887, loss: 0.0186256542801857\n",
      "step: 6888, loss: 0.4437977075576782\n",
      "step: 6889, loss: 0.15182790160179138\n",
      "step: 6890, loss: 0.6159271001815796\n",
      "step: 6891, loss: 0.10746815800666809\n",
      "step: 6892, loss: 0.2581756114959717\n",
      "step: 6893, loss: 0.10921920090913773\n",
      "step: 6894, loss: 0.2590254247188568\n",
      "step: 6895, loss: 0.03110709972679615\n",
      "step: 6896, loss: 0.0866512805223465\n",
      "step: 6897, loss: 0.09613575041294098\n",
      "step: 6898, loss: 0.03148621693253517\n",
      "step: 6899, loss: 0.24444064497947693\n",
      "step: 6900, loss: 0.5694452524185181\n",
      "step: 6901, loss: 0.13925449550151825\n",
      "step: 6902, loss: 0.19516082108020782\n",
      "step: 6903, loss: 0.510565459728241\n",
      "step: 6904, loss: 0.34441903233528137\n",
      "step: 6905, loss: 0.07551240175962448\n",
      "step: 6906, loss: 0.012938715517520905\n",
      "step: 6907, loss: 0.030790455639362335\n",
      "step: 6908, loss: 0.04948383942246437\n",
      "step: 6909, loss: 0.04593057185411453\n",
      "step: 6910, loss: 0.003890322521328926\n",
      "step: 6911, loss: 0.13546624779701233\n",
      "step: 6912, loss: 0.11399459093809128\n",
      "step: 6913, loss: 0.11676840484142303\n",
      "step: 6914, loss: 0.06007858365774155\n",
      "step: 6915, loss: 0.3101176917552948\n",
      "step: 6916, loss: 0.15994812548160553\n",
      "step: 6917, loss: 0.31405526399612427\n",
      "step: 6918, loss: 0.22233456373214722\n",
      "step: 6919, loss: 0.5285256505012512\n",
      "step: 6920, loss: 0.014330939389765263\n",
      "step: 6921, loss: 0.009936843067407608\n",
      "step: 6922, loss: 0.007984434254467487\n",
      "step: 6923, loss: 0.48744574189186096\n",
      "step: 6924, loss: 0.17987821996212006\n",
      "step: 6925, loss: 0.26916998624801636\n",
      "step: 6926, loss: 0.023295944556593895\n",
      "step: 6927, loss: 0.0027669351547956467\n",
      "step: 6928, loss: 0.22607816755771637\n",
      "step: 6929, loss: 0.9402097463607788\n",
      "step: 6930, loss: 0.04284707456827164\n",
      "step: 6931, loss: 0.15375909209251404\n",
      "step: 6932, loss: 0.3553124666213989\n",
      "step: 6933, loss: 0.2043779343366623\n",
      "step: 6934, loss: 0.16641046106815338\n",
      "step: 6935, loss: 0.3424869775772095\n",
      "step: 6936, loss: 0.2471252828836441\n",
      "step: 6937, loss: 0.18297822773456573\n",
      "step: 6938, loss: 0.04900357127189636\n",
      "step: 6939, loss: 0.8512248396873474\n",
      "step: 6940, loss: 0.14847023785114288\n",
      "step: 6941, loss: 0.4747949242591858\n",
      "step: 6942, loss: 0.020885087549686432\n",
      "step: 6943, loss: 0.5556203722953796\n",
      "step: 6944, loss: 0.664833664894104\n",
      "step: 6945, loss: 0.011769776232540607\n",
      "step: 6946, loss: 0.3570930063724518\n",
      "step: 6947, loss: 0.18183209002017975\n",
      "step: 6948, loss: 0.12513495981693268\n",
      "step: 6949, loss: 0.36290469765663147\n",
      "step: 6950, loss: 0.02976381592452526\n",
      "step: 6951, loss: 0.037416573613882065\n",
      "step: 6952, loss: 0.014827527105808258\n",
      "step: 6953, loss: 0.9143041968345642\n",
      "step: 6954, loss: 0.05302533507347107\n",
      "step: 6955, loss: 0.10861669480800629\n",
      "step: 6956, loss: 0.07497716695070267\n",
      "step: 6957, loss: 0.19766512513160706\n",
      "step: 6958, loss: 0.020981788635253906\n",
      "step: 6959, loss: 0.01714520901441574\n",
      "step: 6960, loss: 0.5631909370422363\n",
      "step: 6961, loss: 0.46166831254959106\n",
      "step: 6962, loss: 0.0926658883690834\n",
      "step: 6963, loss: 0.19046232104301453\n",
      "step: 6964, loss: 0.36049994826316833\n",
      "step: 6965, loss: 0.13016845285892487\n",
      "step: 6966, loss: 0.40880271792411804\n",
      "step: 6967, loss: 0.10985932499170303\n",
      "step: 6968, loss: 0.02590254321694374\n",
      "step: 6969, loss: 0.779604434967041\n",
      "step: 6970, loss: 0.08975979685783386\n",
      "step: 6971, loss: 0.3711932301521301\n",
      "step: 6972, loss: 0.40542998909950256\n",
      "step: 6973, loss: 0.2456563413143158\n",
      "step: 6974, loss: 0.0227801613509655\n",
      "step: 6975, loss: 0.3500848412513733\n",
      "step: 6976, loss: 0.30642029643058777\n",
      "step: 6977, loss: 0.11115071922540665\n",
      "step: 6978, loss: 0.39114758372306824\n",
      "step: 6979, loss: 0.42239028215408325\n",
      "step: 6980, loss: 0.2551620900630951\n",
      "step: 6981, loss: 0.3656978905200958\n",
      "step: 6982, loss: 0.08360917121171951\n",
      "step: 6983, loss: 0.18339717388153076\n",
      "step: 6984, loss: 0.04533122107386589\n",
      "step: 6985, loss: 0.09457370638847351\n",
      "step: 6986, loss: 0.6662794351577759\n",
      "step: 6987, loss: 0.07722313702106476\n",
      "step: 6988, loss: 0.126723513007164\n",
      "step: 6989, loss: 0.13241249322891235\n",
      "step: 6990, loss: 0.4501554071903229\n",
      "step: 6991, loss: 0.013686592690646648\n",
      "step: 6992, loss: 0.19996699690818787\n",
      "step: 6993, loss: 0.1528479903936386\n",
      "step: 6994, loss: 0.37633904814720154\n",
      "step: 6995, loss: 0.120000459253788\n",
      "step: 6996, loss: 0.15358200669288635\n",
      "step: 6997, loss: 0.2114170640707016\n",
      "step: 6998, loss: 0.2451767474412918\n",
      "step: 6999, loss: 0.3310105502605438\n",
      "step: 7000, loss: 0.05273523926734924\n",
      "step: 7001, loss: 0.07381889969110489\n",
      "step: 7002, loss: 0.2598063051700592\n",
      "step: 7003, loss: 0.41511523723602295\n",
      "step: 7004, loss: 0.368478387594223\n",
      "step: 7005, loss: 0.21218335628509521\n",
      "step: 7006, loss: 0.3497452139854431\n",
      "step: 7007, loss: 0.0016068133991211653\n",
      "step: 7008, loss: 0.11579939723014832\n",
      "step: 7009, loss: 0.00616428954526782\n",
      "step: 7010, loss: 0.05209944769740105\n",
      "step: 7011, loss: 0.005857323296368122\n",
      "step: 7012, loss: 0.00038092900649644434\n",
      "step: 7013, loss: 0.042105782777071\n",
      "step: 7014, loss: 0.4126005172729492\n",
      "step: 7015, loss: 0.00025299680419266224\n",
      "step: 7016, loss: 0.049067411571741104\n",
      "step: 7017, loss: 0.008762567304074764\n",
      "step: 7018, loss: 0.010338339023292065\n",
      "step: 7019, loss: 0.08906152844429016\n",
      "step: 7020, loss: 0.0018433370860293508\n",
      "step: 7021, loss: 0.20001497864723206\n",
      "step: 7022, loss: 0.12703126668930054\n",
      "step: 7023, loss: 0.17819038033485413\n",
      "step: 7024, loss: 0.009368675760924816\n",
      "step: 7025, loss: 0.00913759134709835\n",
      "step: 7026, loss: 0.007681501097977161\n",
      "step: 7027, loss: 0.0916016548871994\n",
      "step: 7028, loss: 0.0049417586997151375\n",
      "step: 7029, loss: 0.09303048253059387\n",
      "step: 7030, loss: 0.74493008852005\n",
      "step: 7031, loss: 0.028180856257677078\n",
      "step: 7032, loss: 0.08075924962759018\n",
      "step: 7033, loss: 0.038896236568689346\n",
      "step: 7034, loss: 0.18193456530570984\n",
      "step: 7035, loss: 0.1123020276427269\n",
      "step: 7036, loss: 0.00047418518806807697\n",
      "step: 7037, loss: 0.0035593032371252775\n",
      "step: 7038, loss: 0.0768122598528862\n",
      "step: 7039, loss: 0.09766460210084915\n",
      "step: 7040, loss: 0.0018934502732008696\n",
      "step: 7041, loss: 0.39440152049064636\n",
      "step: 7042, loss: 0.0007574041374027729\n",
      "step: 7043, loss: 0.5968266129493713\n",
      "step: 7044, loss: 0.2911234200000763\n",
      "step: 7045, loss: 0.00017810305871535093\n",
      "step: 7046, loss: 0.13947147130966187\n",
      "step: 7047, loss: 0.19541765749454498\n",
      "step: 7048, loss: 0.0688788890838623\n",
      "step: 7049, loss: 0.16643652319908142\n",
      "step: 7050, loss: 0.34792283177375793\n",
      "step: 7051, loss: 0.034926965832710266\n",
      "step: 7052, loss: 0.11594260483980179\n",
      "step: 7053, loss: 0.021827204152941704\n",
      "step: 7054, loss: 0.02472168393433094\n",
      "step: 7055, loss: 0.24009160697460175\n",
      "step: 7056, loss: 0.06193524971604347\n",
      "step: 7057, loss: 0.22244396805763245\n",
      "step: 7058, loss: 0.03635678440332413\n",
      "step: 7059, loss: 0.005229841452091932\n",
      "step: 7060, loss: 0.0014287875965237617\n",
      "step: 7061, loss: 0.010545855388045311\n",
      "step: 7062, loss: 0.021794797852635384\n",
      "step: 7063, loss: 0.10047869384288788\n",
      "step: 7064, loss: 0.00021772690524812788\n",
      "step: 7065, loss: 0.10222166031599045\n",
      "step: 7066, loss: 0.036779098212718964\n",
      "step: 7067, loss: 0.023432571440935135\n",
      "step: 7068, loss: 0.13235914707183838\n",
      "step: 7069, loss: 0.012064634822309017\n",
      "step: 7070, loss: 0.24299564957618713\n",
      "step: 7071, loss: 0.08331049978733063\n",
      "step: 7072, loss: 0.18990999460220337\n",
      "step: 7073, loss: 0.2956129312515259\n",
      "step: 7074, loss: 0.0006905682384967804\n",
      "step: 7075, loss: 0.1184569001197815\n",
      "step: 7076, loss: 0.14324359595775604\n",
      "step: 7077, loss: 0.3068634271621704\n",
      "step: 7078, loss: 0.13227421045303345\n",
      "step: 7079, loss: 0.06040862202644348\n",
      "step: 7080, loss: 0.0010482099605724216\n",
      "step: 7081, loss: 0.07883217930793762\n",
      "step: 7082, loss: 0.11369197070598602\n",
      "step: 7083, loss: 0.24978898465633392\n",
      "step: 7084, loss: 0.12092027068138123\n",
      "step: 7085, loss: 0.20714397728443146\n",
      "step: 7086, loss: 0.03704928606748581\n",
      "step: 7087, loss: 0.09363596886396408\n",
      "step: 7088, loss: 0.19930601119995117\n",
      "step: 7089, loss: 0.0001257332187378779\n",
      "step: 7090, loss: 0.012261402793228626\n",
      "step: 7091, loss: 0.0030057344119995832\n",
      "step: 7092, loss: 0.03227555379271507\n",
      "step: 7093, loss: 0.031272538006305695\n",
      "step: 7094, loss: 0.11180778592824936\n",
      "step: 7095, loss: 0.004810755606740713\n",
      "step: 7096, loss: 0.1233426183462143\n",
      "step: 7097, loss: 0.00015203043585643172\n",
      "step: 7098, loss: 0.17985717952251434\n",
      "step: 7099, loss: 0.050858110189437866\n",
      "step: 7100, loss: 0.17402102053165436\n",
      "step: 7101, loss: 0.2662428915500641\n",
      "step: 7102, loss: 0.2476351112127304\n",
      "step: 7103, loss: 0.008763765916228294\n",
      "step: 7104, loss: 0.0820695236325264\n",
      "step: 7105, loss: 0.1937434822320938\n",
      "step: 7106, loss: 0.000549767748452723\n",
      "step: 7107, loss: 0.38832199573516846\n",
      "step: 7108, loss: 0.025724822655320168\n",
      "step: 7109, loss: 0.1633773297071457\n",
      "step: 7110, loss: 0.009982372634112835\n",
      "step: 7111, loss: 0.0006024331669323146\n",
      "step: 7112, loss: 0.01157769188284874\n",
      "step: 7113, loss: 0.021275563165545464\n",
      "step: 7114, loss: 0.13495689630508423\n",
      "step: 7115, loss: 0.020505724474787712\n",
      "step: 7116, loss: 0.10539037734270096\n",
      "step: 7117, loss: 0.015170076861977577\n",
      "step: 7118, loss: 0.2371869683265686\n",
      "step: 7119, loss: 0.10710311681032181\n",
      "step: 7120, loss: 0.05468588322401047\n",
      "step: 7121, loss: 0.2298148274421692\n",
      "step: 7122, loss: 0.002136412775143981\n",
      "step: 7123, loss: 0.20676513016223907\n",
      "step: 7124, loss: 0.0034988599363714457\n",
      "step: 7125, loss: 0.013993081636726856\n",
      "step: 7126, loss: 0.10842424631118774\n",
      "step: 7127, loss: 0.31794798374176025\n",
      "step: 7128, loss: 0.016605360433459282\n",
      "step: 7129, loss: 0.004966897889971733\n",
      "step: 7130, loss: 0.011606017127633095\n",
      "step: 7131, loss: 0.10173940658569336\n",
      "step: 7132, loss: 0.05884483456611633\n",
      "step: 7133, loss: 0.02712199278175831\n",
      "step: 7134, loss: 0.11313952505588531\n",
      "step: 7135, loss: 0.09606035053730011\n",
      "step: 7136, loss: 0.06798920780420303\n",
      "step: 7137, loss: 0.35242322087287903\n",
      "step: 7138, loss: 0.15022343397140503\n",
      "step: 7139, loss: 0.004179672803729773\n",
      "step: 7140, loss: 0.3223527669906616\n",
      "step: 7141, loss: 0.017621373757719994\n",
      "step: 7142, loss: 0.1443309634923935\n",
      "step: 7143, loss: 0.004253573250025511\n",
      "step: 7144, loss: 0.0006291171885095537\n",
      "step: 7145, loss: 0.09356175363063812\n",
      "step: 7146, loss: 0.059704288840293884\n",
      "step: 7147, loss: 0.007236067205667496\n",
      "step: 7148, loss: 0.5124220848083496\n",
      "step: 7149, loss: 0.1631069928407669\n",
      "step: 7150, loss: 0.0611431747674942\n",
      "step: 7151, loss: 0.16702263057231903\n",
      "step: 7152, loss: 0.041672591120004654\n",
      "step: 7153, loss: 0.03525405004620552\n",
      "step: 7154, loss: 0.002634570933878422\n",
      "step: 7155, loss: 0.1369028240442276\n",
      "step: 7156, loss: 0.18576684594154358\n",
      "step: 7157, loss: 0.0850728303194046\n",
      "step: 7158, loss: 0.06847349554300308\n",
      "step: 7159, loss: 0.01050142664462328\n",
      "step: 7160, loss: 0.002614822005853057\n",
      "step: 7161, loss: 0.10551119595766068\n",
      "step: 7162, loss: 0.0005804419633932412\n",
      "step: 7163, loss: 0.12432599067687988\n",
      "step: 7164, loss: 0.04329342767596245\n",
      "step: 7165, loss: 0.06003914400935173\n",
      "step: 7166, loss: 0.0548967644572258\n",
      "step: 7167, loss: 0.0015362175181508064\n",
      "step: 7168, loss: 0.013845887035131454\n",
      "step: 7169, loss: 0.18229909241199493\n",
      "step: 7170, loss: 0.011149387806653976\n",
      "step: 7171, loss: 0.0021636823657900095\n",
      "step: 7172, loss: 0.045443277806043625\n",
      "step: 7173, loss: 0.001210829708725214\n",
      "step: 7174, loss: 0.012444134801626205\n",
      "step: 7175, loss: 0.013875982724130154\n",
      "step: 7176, loss: 0.014059820212423801\n",
      "step: 7177, loss: 0.0003262479731347412\n",
      "step: 7178, loss: 0.00043660655501298606\n",
      "step: 7179, loss: 0.014352367259562016\n",
      "step: 7180, loss: 0.165702223777771\n",
      "step: 7181, loss: 0.042754124850034714\n",
      "step: 7182, loss: 0.06912811845541\n",
      "step: 7183, loss: 0.00419357605278492\n",
      "step: 7184, loss: 0.07330098748207092\n",
      "step: 7185, loss: 0.023148151114583015\n",
      "step: 7186, loss: 0.1466582715511322\n",
      "step: 7187, loss: 0.0006382910651154816\n",
      "step: 7188, loss: 0.02896145172417164\n",
      "step: 7189, loss: 0.014768281951546669\n",
      "step: 7190, loss: 3.420278153498657e-05\n",
      "step: 7191, loss: 0.24799932539463043\n",
      "step: 7192, loss: 0.09910143911838531\n",
      "step: 7193, loss: 0.2438083291053772\n",
      "step: 7194, loss: 0.001935368636623025\n",
      "step: 7195, loss: 0.025205127894878387\n",
      "step: 7196, loss: 0.007549691945314407\n",
      "step: 7197, loss: 0.3462894856929779\n",
      "step: 7198, loss: 0.00782755482941866\n",
      "step: 7199, loss: 0.15015582740306854\n",
      "step: 7200, loss: 0.21583978831768036\n",
      "step: 7201, loss: 0.0015492598759010434\n",
      "step: 7202, loss: 0.18199336528778076\n",
      "step: 7203, loss: 0.06715933233499527\n",
      "step: 7204, loss: 0.003614450106397271\n",
      "step: 7205, loss: 0.06929748505353928\n",
      "step: 7206, loss: 0.04119876027107239\n",
      "step: 7207, loss: 0.06037946045398712\n",
      "step: 7208, loss: 0.0004127416468691081\n",
      "step: 7209, loss: 0.06589295715093613\n",
      "step: 7210, loss: 0.2530204653739929\n",
      "step: 7211, loss: 0.039715878665447235\n",
      "step: 7212, loss: 0.29107344150543213\n",
      "step: 7213, loss: 0.15972015261650085\n",
      "step: 7214, loss: 3.9966776967048645e-05\n",
      "step: 7215, loss: 0.12679043412208557\n",
      "step: 7216, loss: 0.05662708729505539\n",
      "step: 7217, loss: 0.19295623898506165\n",
      "step: 7218, loss: 0.2332330197095871\n",
      "step: 7219, loss: 0.02364327758550644\n",
      "step: 7220, loss: 0.00016828552179504186\n",
      "step: 7221, loss: 0.00018135225400328636\n",
      "step: 7222, loss: 0.2018970251083374\n",
      "step: 7223, loss: 0.19858771562576294\n",
      "step: 7224, loss: 0.0634739100933075\n",
      "step: 7225, loss: 0.009032498113811016\n",
      "step: 7226, loss: 0.00012483647151384503\n",
      "step: 7227, loss: 0.0008751729037612677\n",
      "step: 7228, loss: 0.2869114577770233\n",
      "step: 7229, loss: 0.03326381742954254\n",
      "step: 7230, loss: 0.38226786255836487\n",
      "step: 7231, loss: 0.0330950990319252\n",
      "step: 7232, loss: 0.0008011824684217572\n",
      "step: 7233, loss: 0.00016932042490225285\n",
      "step: 7234, loss: 0.020107774063944817\n",
      "step: 7235, loss: 0.03532704338431358\n",
      "step: 7236, loss: 0.0002902206324506551\n",
      "step: 7237, loss: 0.0189885962754488\n",
      "step: 7238, loss: 0.00011863214604090899\n",
      "step: 7239, loss: 0.11754260957241058\n",
      "step: 7240, loss: 0.29146865010261536\n",
      "step: 7241, loss: 0.00019962289661634713\n",
      "step: 7242, loss: 7.600866229040548e-05\n",
      "step: 7243, loss: 0.00032171382918022573\n",
      "step: 7244, loss: 0.00021917661069892347\n",
      "step: 7245, loss: 0.0003674898180179298\n",
      "step: 7246, loss: 0.028787262737751007\n",
      "step: 7247, loss: 0.00013040576595813036\n",
      "step: 7248, loss: 0.0053644622676074505\n",
      "step: 7249, loss: 0.00040013567195273936\n",
      "step: 7250, loss: 0.0003302593540865928\n",
      "step: 7251, loss: 0.2742348909378052\n",
      "step: 7252, loss: 0.01473553292453289\n",
      "step: 7253, loss: 0.0219343900680542\n",
      "step: 7254, loss: 0.011686270125210285\n",
      "step: 7255, loss: 0.0002029976312769577\n",
      "step: 7256, loss: 0.0006202518125064671\n",
      "step: 7257, loss: 0.004793755244463682\n",
      "step: 7258, loss: 0.0034391446970403194\n",
      "step: 7259, loss: 0.0003986447991337627\n",
      "step: 7260, loss: 0.004319873638451099\n",
      "step: 7261, loss: 0.003446736838668585\n",
      "step: 7262, loss: 0.00722699286416173\n",
      "step: 7263, loss: 0.0006587929092347622\n",
      "step: 7264, loss: 2.8353115339996293e-05\n",
      "step: 7265, loss: 0.013956662267446518\n",
      "step: 7266, loss: 0.0009798280661925673\n",
      "step: 7267, loss: 0.00010215698421234265\n",
      "step: 7268, loss: 1.6722602595109493e-05\n",
      "step: 7269, loss: 2.080810008919798e-05\n",
      "step: 7270, loss: 0.00038718045107088983\n",
      "step: 7271, loss: 2.3117732780519873e-05\n",
      "step: 7272, loss: 1.6104046153486706e-05\n",
      "step: 7273, loss: 0.012829146347939968\n",
      "step: 7274, loss: 2.615278572193347e-05\n",
      "step: 7275, loss: 0.030762789770960808\n",
      "step: 7276, loss: 0.11124660819768906\n",
      "step: 7277, loss: 0.00013451080303639174\n",
      "step: 7278, loss: 0.00011051195906475186\n",
      "step: 7279, loss: 0.0008987241890281439\n",
      "step: 7280, loss: 9.158545435639098e-05\n",
      "step: 7281, loss: 0.0018871650099754333\n",
      "step: 7282, loss: 2.1697345800930634e-05\n",
      "step: 7283, loss: 0.07023520022630692\n",
      "step: 7284, loss: 0.07716263085603714\n",
      "step: 7285, loss: 0.01012918446213007\n",
      "step: 7286, loss: 0.0002620148297864944\n",
      "step: 7287, loss: 0.0020216561388224363\n",
      "step: 7288, loss: 0.00011622937745414674\n",
      "step: 7289, loss: 0.0042586601339280605\n",
      "step: 7290, loss: 0.00026403096853755414\n",
      "step: 7291, loss: 0.0005295939045026898\n",
      "step: 7292, loss: 0.009780623018741608\n",
      "step: 7293, loss: 0.002578931162133813\n",
      "step: 7294, loss: 5.078306912764674e-07\n",
      "step: 7295, loss: 0.0016170531744137406\n",
      "step: 7296, loss: 0.00020831992151215672\n",
      "step: 7297, loss: 0.0044947355054318905\n",
      "step: 7298, loss: 7.654647924937308e-05\n",
      "step: 7299, loss: 2.7546027922653593e-05\n",
      "step: 7300, loss: 0.00032362088677473366\n",
      "step: 7301, loss: 0.0018063944298774004\n",
      "step: 7302, loss: 0.00038667424814775586\n",
      "step: 7303, loss: 3.554527575033717e-05\n",
      "step: 7304, loss: 0.007042286917567253\n",
      "step: 7305, loss: 0.05330805480480194\n",
      "step: 7306, loss: 4.326530688558705e-05\n",
      "step: 7307, loss: 0.04873334988951683\n",
      "step: 7308, loss: 0.009857458993792534\n",
      "step: 7309, loss: 0.0834151953458786\n",
      "step: 7310, loss: 0.00598412100225687\n",
      "step: 7311, loss: 0.08146102726459503\n",
      "step: 7312, loss: 0.2186884731054306\n",
      "step: 7313, loss: 0.03390449658036232\n",
      "step: 7314, loss: 0.0003357197856530547\n",
      "step: 7315, loss: 0.04003579914569855\n",
      "step: 7316, loss: 0.0004771761887241155\n",
      "step: 7317, loss: 0.0009635062306188047\n",
      "step: 7318, loss: 3.1801369914319366e-05\n",
      "step: 7319, loss: 0.06717099994421005\n",
      "step: 7320, loss: 0.007350826635956764\n",
      "step: 7321, loss: 0.0003728951851371676\n",
      "step: 7322, loss: 0.0019431401742622256\n",
      "step: 7323, loss: 0.0006108557572588325\n",
      "step: 7324, loss: 0.2135086953639984\n",
      "step: 7325, loss: 0.00533722760155797\n",
      "step: 7326, loss: 0.08147294074296951\n",
      "step: 7327, loss: 5.404478724813089e-05\n",
      "step: 7328, loss: 0.01251973956823349\n",
      "step: 7329, loss: 0.0007482288056053221\n",
      "step: 7330, loss: 2.2844560589874163e-05\n",
      "step: 7331, loss: 0.087770976126194\n",
      "step: 7332, loss: 0.002799482084810734\n",
      "step: 7333, loss: 0.06692798435688019\n",
      "step: 7334, loss: 6.737801595591009e-05\n",
      "step: 7335, loss: 0.007016103249043226\n",
      "step: 7336, loss: 0.003208944108337164\n",
      "step: 7337, loss: 0.0003237126802559942\n",
      "step: 7338, loss: 0.001845398684963584\n",
      "step: 7339, loss: 0.0018330817110836506\n",
      "step: 7340, loss: 0.00010685945017030463\n",
      "step: 7341, loss: 0.0003411649086046964\n",
      "step: 7342, loss: 0.002634863369166851\n",
      "step: 7343, loss: 0.00039731565630063415\n",
      "step: 7344, loss: 0.06622844934463501\n",
      "step: 7345, loss: 0.000668355030938983\n",
      "step: 7346, loss: 0.1504259556531906\n",
      "step: 7347, loss: 0.00034988942206837237\n",
      "step: 7348, loss: 1.0491769899090286e-05\n",
      "step: 7349, loss: 0.034976549446582794\n",
      "step: 7350, loss: 0.003874533111229539\n",
      "step: 7351, loss: 0.00017236880376003683\n",
      "step: 7352, loss: 0.000676961790304631\n",
      "step: 7353, loss: 0.0011159302666783333\n",
      "step: 7354, loss: 0.015548900701105595\n",
      "step: 7355, loss: 1.817815973481629e-05\n",
      "step: 7356, loss: 8.3447179349605e-05\n",
      "step: 7357, loss: 0.005915053654462099\n",
      "step: 7358, loss: 0.004993790294975042\n",
      "step: 7359, loss: 0.0005892924964427948\n",
      "step: 7360, loss: 0.05800999701023102\n",
      "step: 7361, loss: 0.10351018607616425\n",
      "step: 7362, loss: 0.050811294466257095\n",
      "step: 7363, loss: 0.0016970074502751231\n",
      "step: 7364, loss: 0.000180133618414402\n",
      "step: 7365, loss: 0.23567327857017517\n",
      "step: 7366, loss: 0.0016576647758483887\n",
      "step: 7367, loss: 0.10278994590044022\n",
      "step: 7368, loss: 0.019951267167925835\n",
      "step: 7369, loss: 0.0014714631251990795\n",
      "step: 7370, loss: 0.0010189184686169028\n",
      "step: 7371, loss: 0.0007562344544567168\n",
      "step: 7372, loss: 0.002384485211223364\n",
      "step: 7373, loss: 0.0327773243188858\n",
      "step: 7374, loss: 0.0459853895008564\n",
      "step: 7375, loss: 0.00011922861449420452\n",
      "step: 7376, loss: 0.032953545451164246\n",
      "step: 7377, loss: 4.4388300011632964e-05\n",
      "step: 7378, loss: 0.01994970440864563\n",
      "step: 7379, loss: 0.0004951542941853404\n",
      "step: 7380, loss: 0.004848124925047159\n",
      "step: 7381, loss: 1.8242333680973388e-05\n",
      "step: 7382, loss: 0.025253448635339737\n",
      "step: 7383, loss: 0.005801102612167597\n",
      "step: 7384, loss: 0.19506549835205078\n",
      "step: 7385, loss: 0.09081166982650757\n",
      "step: 7386, loss: 5.776697435067035e-05\n",
      "step: 7387, loss: 0.038582414388656616\n",
      "step: 7388, loss: 8.154287934303284e-05\n",
      "step: 7389, loss: 0.0007680898997932673\n",
      "step: 7390, loss: 6.937542821106035e-06\n",
      "step: 7391, loss: 0.021017417311668396\n",
      "step: 7392, loss: 0.002571643330156803\n",
      "step: 7393, loss: 0.0022086992394179106\n",
      "step: 7394, loss: 0.0636504665017128\n",
      "step: 7395, loss: 0.16777606308460236\n",
      "step: 7396, loss: 0.11586696654558182\n",
      "step: 7397, loss: 0.0006780150579288602\n",
      "step: 7398, loss: 0.00709360558539629\n",
      "step: 7399, loss: 0.0002259884204249829\n",
      "step: 7400, loss: 0.000841028755530715\n",
      "step: 7401, loss: 4.236522727296688e-05\n",
      "step: 7402, loss: 0.1820363998413086\n",
      "step: 7403, loss: 0.04128699377179146\n",
      "step: 7404, loss: 0.00019784599135164171\n",
      "step: 7405, loss: 0.00021597092563752085\n",
      "step: 7406, loss: 0.05524246394634247\n",
      "step: 7407, loss: 0.006400883197784424\n",
      "step: 7408, loss: 0.0007216563681140542\n",
      "step: 7409, loss: 0.1179875060915947\n",
      "step: 7410, loss: 0.10029251128435135\n",
      "step: 7411, loss: 0.052621062844991684\n",
      "step: 7412, loss: 0.0029852676670998335\n",
      "step: 7413, loss: 0.08244368433952332\n",
      "step: 7414, loss: 0.031756799668073654\n",
      "step: 7415, loss: 0.00023571499332319945\n",
      "step: 7416, loss: 0.06196906417608261\n",
      "step: 7417, loss: 6.977379962336272e-05\n",
      "step: 7418, loss: 1.2026385775243398e-05\n",
      "step: 7419, loss: 0.0007092245505191386\n",
      "step: 7420, loss: 0.38558194041252136\n",
      "step: 7421, loss: 0.2804752290248871\n",
      "step: 7422, loss: 0.0004456994647625834\n",
      "step: 7423, loss: 0.03534984961152077\n",
      "step: 7424, loss: 9.267533459933475e-05\n",
      "step: 7425, loss: 0.00013217577361501753\n",
      "step: 7426, loss: 0.00013511789438780397\n",
      "step: 7427, loss: 0.00036015850491821766\n",
      "step: 7428, loss: 0.06817591935396194\n",
      "step: 7429, loss: 0.006471316795796156\n",
      "step: 7430, loss: 0.0012340524699538946\n",
      "step: 7431, loss: 0.11665177345275879\n",
      "step: 7432, loss: 0.001125822775065899\n",
      "step: 7433, loss: 3.670813384815119e-05\n",
      "step: 7434, loss: 0.001942267525009811\n",
      "step: 7435, loss: 0.020856823772192\n",
      "step: 7436, loss: 0.0029575005173683167\n",
      "step: 7437, loss: 0.0010763132013380527\n",
      "step: 7438, loss: 0.01375396829098463\n",
      "step: 7439, loss: 0.00038417239557020366\n",
      "step: 7440, loss: 0.008426779881119728\n",
      "step: 7441, loss: 0.04091447964310646\n",
      "step: 7442, loss: 0.00456980150192976\n",
      "step: 7443, loss: 0.0003227916604373604\n",
      "step: 7444, loss: 0.0003132901038043201\n",
      "step: 7445, loss: 0.0010371250100433826\n",
      "step: 7446, loss: 2.0313374989200383e-05\n",
      "step: 7447, loss: 0.2280571162700653\n",
      "step: 7448, loss: 0.001904171658679843\n",
      "step: 7449, loss: 0.1869584321975708\n",
      "step: 7450, loss: 0.010634747333824635\n",
      "step: 7451, loss: 0.018593650311231613\n",
      "step: 7452, loss: 0.04007795453071594\n",
      "step: 7453, loss: 0.2847972512245178\n",
      "step: 7454, loss: 0.0022632565815001726\n",
      "step: 7455, loss: 0.0003993807767983526\n",
      "step: 7456, loss: 0.008117975667119026\n",
      "step: 7457, loss: 0.019200943410396576\n",
      "step: 7458, loss: 3.3832773624453694e-05\n",
      "step: 7459, loss: 3.148712494294159e-05\n",
      "step: 7460, loss: 0.00421806238591671\n",
      "step: 7461, loss: 0.0001313683169428259\n",
      "step: 7462, loss: 9.26546344999224e-05\n",
      "step: 7463, loss: 0.0044932495802640915\n",
      "step: 7464, loss: 0.0001171608455479145\n",
      "step: 7465, loss: 0.01145414263010025\n",
      "step: 7466, loss: 5.797127596451901e-05\n",
      "step: 7467, loss: 4.945259934174828e-05\n",
      "step: 7468, loss: 5.520529157365672e-05\n",
      "step: 7469, loss: 0.00019553264428395778\n",
      "step: 7470, loss: 0.0044981250539422035\n",
      "step: 7471, loss: 0.00032715831184759736\n",
      "step: 7472, loss: 0.00011525845911819488\n",
      "step: 7473, loss: 5.26766489201691e-05\n",
      "step: 7474, loss: 0.038280926644802094\n",
      "step: 7475, loss: 0.00028300174744799733\n",
      "step: 7476, loss: 0.0001293467648793012\n",
      "step: 7477, loss: 8.4966559370514e-06\n",
      "step: 7478, loss: 1.6088852134998888e-05\n",
      "step: 7479, loss: 1.4562909200321883e-05\n",
      "step: 7480, loss: 0.0021122917532920837\n",
      "step: 7481, loss: 7.643656135769561e-05\n",
      "step: 7482, loss: 0.00030988213256932795\n",
      "step: 7483, loss: 3.230925358366221e-05\n",
      "step: 7484, loss: 0.008471319451928139\n",
      "step: 7485, loss: 0.18219414353370667\n",
      "step: 7486, loss: 1.4655535778729245e-05\n",
      "step: 7487, loss: 2.268938987981528e-05\n",
      "step: 7488, loss: 0.0011130034690722823\n",
      "step: 7489, loss: 1.926213917613495e-05\n",
      "step: 7490, loss: 0.00020391908765304834\n",
      "step: 7491, loss: 9.867214430414606e-06\n",
      "step: 7492, loss: 6.697932258248329e-05\n",
      "step: 7493, loss: 0.007069859653711319\n",
      "step: 7494, loss: 5.683976269210689e-05\n",
      "step: 7495, loss: 0.00010279934213031083\n",
      "step: 7496, loss: 0.1482933759689331\n",
      "step: 7497, loss: 0.0004536692285910249\n",
      "step: 7498, loss: 0.0220493134111166\n",
      "step: 7499, loss: 0.0007590831955894828\n",
      "step: 7500, loss: 7.40454561309889e-05\n",
      "step: 7501, loss: 0.0002845548733603209\n",
      "step: 7502, loss: 1.957182576006744e-05\n",
      "step: 7503, loss: 1.4021858987689484e-05\n",
      "step: 7504, loss: 0.0003740793908946216\n",
      "step: 7505, loss: 3.3589185477467254e-05\n",
      "step: 7506, loss: 3.983189890277572e-05\n",
      "step: 7507, loss: 3.4356060496065766e-05\n",
      "step: 7508, loss: 7.836429176677484e-06\n",
      "step: 7509, loss: 1.7352791473967955e-05\n",
      "step: 7510, loss: 4.039805935462937e-05\n",
      "step: 7511, loss: 0.0001515592448413372\n",
      "step: 7512, loss: 1.2125155990361236e-05\n",
      "step: 7513, loss: 0.00010593844490358606\n",
      "step: 7514, loss: 0.0008583306334912777\n",
      "step: 7515, loss: 3.089659730903804e-05\n",
      "step: 7516, loss: 0.00012034213432343677\n",
      "step: 7517, loss: 0.0002965480962302536\n",
      "step: 7518, loss: 0.00018318476213607937\n",
      "step: 7519, loss: 0.0001402908528689295\n",
      "step: 7520, loss: 0.00023597065592184663\n",
      "step: 7521, loss: 2.2106692995293997e-05\n",
      "step: 7522, loss: 0.00012438211706466973\n",
      "step: 7523, loss: 0.0027429910842329264\n",
      "step: 7524, loss: 0.0003045960620511323\n",
      "step: 7525, loss: 1.0424726497149095e-05\n",
      "step: 7526, loss: 6.873246547911549e-06\n",
      "step: 7527, loss: 2.5271845515817404e-05\n",
      "step: 7528, loss: 3.2162236038857372e-06\n",
      "step: 7529, loss: 0.0028193420730531216\n",
      "step: 7530, loss: 3.4399028663756326e-05\n",
      "step: 7531, loss: 0.005230238661170006\n",
      "step: 7532, loss: 0.18591511249542236\n",
      "step: 7533, loss: 0.010887956246733665\n",
      "step: 7534, loss: 0.0003363905707374215\n",
      "step: 7535, loss: 0.0018255669856444001\n",
      "step: 7536, loss: 0.005068974569439888\n",
      "step: 7537, loss: 0.0001402547350153327\n",
      "step: 7538, loss: 8.763678488321602e-05\n",
      "step: 7539, loss: 1.5965282727847807e-05\n",
      "step: 7540, loss: 7.798781007295474e-05\n",
      "step: 7541, loss: 4.268303018761799e-05\n",
      "step: 7542, loss: 0.06602127104997635\n",
      "step: 7543, loss: 0.0001594044588273391\n",
      "step: 7544, loss: 6.390481576090679e-05\n",
      "step: 7545, loss: 5.51186349184718e-06\n",
      "step: 7546, loss: 2.551971556385979e-05\n",
      "step: 7547, loss: 0.05706680938601494\n",
      "step: 7548, loss: 2.272676647407934e-05\n",
      "step: 7549, loss: 0.0002337906917091459\n",
      "step: 7550, loss: 0.00032209581695497036\n",
      "step: 7551, loss: 0.0003961113980039954\n",
      "step: 7552, loss: 0.00013835768913850188\n",
      "step: 7553, loss: 1.0229517101834062e-05\n",
      "step: 7554, loss: 0.0028700672555714846\n",
      "step: 7555, loss: 8.610208169557154e-05\n",
      "step: 7556, loss: 0.00017472390027251095\n",
      "step: 7557, loss: 0.00011574891686905175\n",
      "step: 7558, loss: 0.00011942968558287248\n",
      "step: 7559, loss: 0.007539995945990086\n",
      "step: 7560, loss: 0.06148755922913551\n",
      "step: 7561, loss: 2.364827560086269e-05\n",
      "step: 7562, loss: 0.00021700200159102678\n",
      "step: 7563, loss: 9.97953611658886e-06\n",
      "step: 7564, loss: 0.00011098699906142429\n",
      "step: 7565, loss: 4.533579704002477e-05\n",
      "step: 7566, loss: 1.5490271835005842e-05\n",
      "step: 7567, loss: 8.593542588641867e-05\n",
      "step: 7568, loss: 9.537473670206964e-05\n",
      "step: 7569, loss: 0.25661832094192505\n",
      "step: 7570, loss: 0.00012839293049182743\n",
      "step: 7571, loss: 3.3370590244885534e-05\n",
      "step: 7572, loss: 0.0010183012345805764\n",
      "step: 7573, loss: 0.017856430262327194\n",
      "step: 7574, loss: 1.0378125807619654e-05\n",
      "step: 7575, loss: 0.002001167507842183\n",
      "step: 7576, loss: 2.337666228413582e-05\n",
      "step: 7577, loss: 2.113349000865128e-05\n",
      "step: 7578, loss: 3.5745113564189523e-05\n",
      "step: 7579, loss: 3.284530612290837e-05\n",
      "step: 7580, loss: 0.00012160031474195421\n",
      "step: 7581, loss: 0.00017431130982004106\n",
      "step: 7582, loss: 7.147494761738926e-05\n",
      "step: 7583, loss: 1.272247391170822e-05\n",
      "step: 7584, loss: 7.266381726367399e-05\n",
      "step: 7585, loss: 0.003566218540072441\n",
      "step: 7586, loss: 2.7836713343276642e-05\n",
      "step: 7587, loss: 0.003870295826345682\n",
      "step: 7588, loss: 0.04730316251516342\n",
      "step: 7589, loss: 3.8834925362607464e-05\n",
      "step: 7590, loss: 0.06321989744901657\n",
      "step: 7591, loss: 9.93223293335177e-05\n",
      "step: 7592, loss: 2.8267046218388714e-05\n",
      "step: 7593, loss: 3.061231154788402e-06\n",
      "step: 7594, loss: 0.0008570769568905234\n",
      "step: 7595, loss: 0.035761021077632904\n",
      "step: 7596, loss: 2.784657226584386e-06\n",
      "step: 7597, loss: 1.7785227100830525e-05\n",
      "step: 7598, loss: 8.262057963293046e-05\n",
      "step: 7599, loss: 7.783772161928937e-05\n",
      "step: 7600, loss: 3.8380083424272016e-05\n",
      "step: 7601, loss: 0.000533906277269125\n",
      "step: 7602, loss: 1.8415383237879723e-05\n",
      "step: 7603, loss: 1.3234393918537535e-05\n",
      "step: 7604, loss: 0.006492843851447105\n",
      "step: 7605, loss: 0.08316435664892197\n",
      "step: 7606, loss: 0.00010995995398843661\n",
      "step: 7607, loss: 0.0409882627427578\n",
      "step: 7608, loss: 1.8649190678843297e-05\n",
      "step: 7609, loss: 0.00028484745416790247\n",
      "step: 7610, loss: 0.010546531528234482\n",
      "step: 7611, loss: 0.044677410274744034\n",
      "step: 7612, loss: 0.00011092243948951364\n",
      "step: 7613, loss: 2.0498848243732937e-05\n",
      "step: 7614, loss: 3.001119512191508e-05\n",
      "step: 7615, loss: 6.518002010125201e-06\n",
      "step: 7616, loss: 4.96316934004426e-05\n",
      "step: 7617, loss: 6.846963515272364e-06\n",
      "step: 7618, loss: 0.001688320655375719\n",
      "step: 7619, loss: 0.08002506196498871\n",
      "step: 7620, loss: 4.5586210035253316e-05\n",
      "step: 7621, loss: 0.000997997703962028\n",
      "step: 7622, loss: 0.0013110804138705134\n",
      "step: 7623, loss: 0.00024344729899894446\n",
      "step: 7624, loss: 0.00017260255117435008\n",
      "step: 7625, loss: 6.1006019677734e-05\n",
      "step: 7626, loss: 0.000622943916823715\n",
      "step: 7627, loss: 0.0002044058928731829\n",
      "step: 7628, loss: 0.00011920600809389725\n",
      "step: 7629, loss: 0.00017792604921851307\n",
      "step: 7630, loss: 0.0004004926886409521\n",
      "step: 7631, loss: 9.552719347993843e-06\n",
      "step: 7632, loss: 8.461809920845553e-05\n",
      "step: 7633, loss: 0.0026286644861102104\n",
      "step: 7634, loss: 0.06783360242843628\n",
      "step: 7635, loss: 6.326645234366879e-05\n",
      "step: 7636, loss: 3.0321849408210255e-05\n",
      "step: 7637, loss: 6.215561006683856e-05\n",
      "step: 7638, loss: 0.00011900685058208182\n",
      "step: 7639, loss: 2.701908306335099e-05\n",
      "step: 7640, loss: 0.045690279453992844\n",
      "step: 7641, loss: 4.834385617868975e-05\n",
      "step: 7642, loss: 0.00011728476965799928\n",
      "step: 7643, loss: 0.0001263133017346263\n",
      "step: 7644, loss: 1.5736934074084274e-05\n",
      "step: 7645, loss: 0.043393220752477646\n",
      "step: 7646, loss: 4.897913822787814e-05\n",
      "step: 7647, loss: 0.035003114491701126\n",
      "step: 7648, loss: 0.00015735760098323226\n",
      "step: 7649, loss: 7.86273358244216e-06\n",
      "step: 7650, loss: 0.002623911015689373\n",
      "step: 7651, loss: 7.414767537738953e-07\n",
      "step: 7652, loss: 6.206796388141811e-05\n",
      "step: 7653, loss: 0.00012370242620818317\n",
      "step: 7654, loss: 0.0004985544946976006\n",
      "step: 7655, loss: 0.0002926632296293974\n",
      "step: 7656, loss: 0.0027702811639755964\n",
      "step: 7657, loss: 0.007181130815297365\n",
      "step: 7658, loss: 0.00036082734004594386\n",
      "step: 7659, loss: 8.730434274184518e-06\n",
      "step: 7660, loss: 2.7415522708906792e-05\n",
      "step: 7661, loss: 0.0009413945954293013\n",
      "step: 7662, loss: 3.478769212961197e-05\n",
      "step: 7663, loss: 6.703961844323203e-05\n",
      "step: 7664, loss: 0.03379599004983902\n",
      "step: 7665, loss: 7.19039817340672e-05\n",
      "step: 7666, loss: 3.0460125344689004e-05\n",
      "step: 7667, loss: 0.01232630480080843\n",
      "step: 7668, loss: 0.0015944276237860322\n",
      "step: 7669, loss: 0.00011809273564722389\n",
      "step: 7670, loss: 0.00041878505726344883\n",
      "step: 7671, loss: 4.591091055772267e-05\n",
      "step: 7672, loss: 5.168833740754053e-05\n",
      "step: 7673, loss: 0.0005083710420876741\n",
      "step: 7674, loss: 3.841408033622429e-05\n",
      "step: 7675, loss: 9.712978499010205e-05\n",
      "step: 7676, loss: 6.842590209998889e-07\n",
      "step: 7677, loss: 7.698543049627915e-05\n",
      "step: 7678, loss: 0.0006120088510215282\n",
      "step: 7679, loss: 0.00023717540898360312\n",
      "step: 7680, loss: 2.4950961233116686e-05\n",
      "step: 7681, loss: 0.007654803339391947\n",
      "step: 7682, loss: 5.744566806242801e-05\n",
      "step: 7683, loss: 0.0001942025264725089\n",
      "step: 7684, loss: 0.0010293644154444337\n",
      "step: 7685, loss: 0.07277362793684006\n",
      "step: 7686, loss: 6.540605681948364e-05\n",
      "step: 7687, loss: 0.000254535028943792\n",
      "step: 7688, loss: 0.00012501700257416815\n",
      "step: 7689, loss: 1.2440575119399e-05\n",
      "step: 7690, loss: 5.245603097137064e-05\n",
      "step: 7691, loss: 1.0267544894304592e-05\n",
      "step: 7692, loss: 2.2390349840861745e-05\n",
      "step: 7693, loss: 2.2054542569094338e-05\n",
      "step: 7694, loss: 0.005056323017925024\n",
      "step: 7695, loss: 0.00013408529048319906\n",
      "step: 7696, loss: 5.83248765906319e-05\n",
      "step: 7697, loss: 9.704309377411846e-06\n",
      "step: 7698, loss: 5.945636803517118e-05\n",
      "step: 7699, loss: 9.615455928724259e-05\n",
      "step: 7700, loss: 0.0019225551513954997\n",
      "step: 7701, loss: 8.790718129603192e-05\n",
      "step: 7702, loss: 3.961756738135591e-05\n",
      "step: 7703, loss: 2.781517105177045e-05\n",
      "step: 7704, loss: 0.011440536938607693\n",
      "step: 7705, loss: 1.7578549886820838e-05\n",
      "step: 7706, loss: 0.00030147499637678266\n",
      "step: 7707, loss: 1.1173297934874427e-05\n",
      "step: 7708, loss: 0.00029509846353903413\n",
      "step: 7709, loss: 0.0056823911145329475\n",
      "step: 7710, loss: 0.00016813035472296178\n",
      "step: 7711, loss: 0.00029377988539636135\n",
      "step: 7712, loss: 7.802809705026448e-05\n",
      "step: 7713, loss: 3.063638587263995e-06\n",
      "step: 7714, loss: 0.0001536992785986513\n",
      "step: 7715, loss: 0.00021799348178319633\n",
      "step: 7716, loss: 4.134058144700248e-06\n",
      "step: 7717, loss: 3.4950589906657115e-05\n",
      "step: 7718, loss: 0.0017825716640800238\n",
      "step: 7719, loss: 0.0012226402759552002\n",
      "step: 7720, loss: 5.163591049495153e-05\n",
      "step: 7721, loss: 7.664938311791047e-05\n",
      "step: 7722, loss: 7.771570381009951e-05\n",
      "step: 7723, loss: 0.0003693503967951983\n",
      "step: 7724, loss: 6.346337613649666e-05\n",
      "step: 7725, loss: 9.310580207966268e-05\n",
      "step: 7726, loss: 8.830380465951748e-06\n",
      "step: 7727, loss: 0.012459316290915012\n",
      "step: 7728, loss: 2.7286185286357068e-05\n",
      "step: 7729, loss: 3.223333578716847e-06\n",
      "step: 7730, loss: 0.00018675511819310486\n",
      "step: 7731, loss: 1.0369481969974004e-05\n",
      "step: 7732, loss: 0.0002498184912838042\n",
      "step: 7733, loss: 2.629711616464192e-06\n",
      "step: 7734, loss: 9.384848090121523e-05\n",
      "step: 7735, loss: 0.0013469826662912965\n",
      "step: 7736, loss: 0.00032012315932661295\n",
      "step: 7737, loss: 1.349315880361246e-05\n",
      "step: 7738, loss: 6.39828504063189e-05\n",
      "step: 7739, loss: 5.6085060350596905e-05\n",
      "step: 7740, loss: 3.905211997334845e-06\n",
      "step: 7741, loss: 2.3103784769773483e-05\n",
      "step: 7742, loss: 1.1006250133505091e-05\n",
      "step: 7743, loss: 0.0002387151907896623\n",
      "step: 7744, loss: 0.0008636136190034449\n",
      "step: 7745, loss: 6.218799535417929e-05\n",
      "step: 7746, loss: 0.00021718922653235495\n",
      "step: 7747, loss: 3.520927930367179e-05\n",
      "step: 7748, loss: 8.361689106095582e-05\n",
      "step: 7749, loss: 4.518970308708958e-05\n",
      "step: 7750, loss: 7.526233821408823e-05\n",
      "step: 7751, loss: 0.010179447941482067\n",
      "step: 7752, loss: 5.645431429002201e-06\n",
      "step: 7753, loss: 6.71654925099574e-05\n",
      "step: 7754, loss: 1.0212481356575154e-05\n",
      "step: 7755, loss: 7.388431549770758e-05\n",
      "step: 7756, loss: 3.111301111857756e-06\n",
      "step: 7757, loss: 0.00017221733287442476\n",
      "step: 7758, loss: 1.3771433259535115e-05\n",
      "step: 7759, loss: 9.278790457756259e-06\n",
      "step: 7760, loss: 0.00038253157981671393\n",
      "step: 7761, loss: 2.9551885745604523e-05\n",
      "step: 7762, loss: 2.9586994969577063e-06\n",
      "step: 7763, loss: 0.0003552464477252215\n",
      "step: 7764, loss: 1.8301840100320987e-05\n",
      "step: 7765, loss: 0.00013820879394188523\n",
      "step: 7766, loss: 2.6934481866192073e-05\n",
      "step: 7767, loss: 2.837125748555991e-06\n",
      "step: 7768, loss: 0.00014030585589352995\n",
      "step: 7769, loss: 0.010582812130451202\n",
      "step: 7770, loss: 7.703922892687842e-05\n",
      "step: 7771, loss: 7.539112993981689e-05\n",
      "step: 7772, loss: 6.019222837494453e-06\n",
      "step: 7773, loss: 6.780306193832075e-06\n",
      "step: 7774, loss: 0.0427507646381855\n",
      "step: 7775, loss: 6.780045168852666e-06\n",
      "step: 7776, loss: 8.670449460623786e-05\n",
      "step: 7777, loss: 0.00012502499157562852\n",
      "step: 7778, loss: 6.07230003879522e-06\n",
      "step: 7779, loss: 7.118145731510594e-05\n",
      "step: 7780, loss: 2.8104603188694455e-05\n",
      "step: 7781, loss: 0.09231414645910263\n",
      "step: 7782, loss: 1.9754161257878877e-05\n",
      "step: 7783, loss: 0.17892485857009888\n",
      "step: 7784, loss: 5.9573383623501286e-05\n",
      "step: 7785, loss: 2.4546394342905842e-05\n",
      "step: 7786, loss: 0.0001140910608228296\n",
      "step: 7787, loss: 1.0420049875392579e-05\n",
      "step: 7788, loss: 2.3602997316629626e-06\n",
      "step: 7789, loss: 9.865011816145852e-05\n",
      "step: 7790, loss: 3.758567254408263e-05\n",
      "step: 7791, loss: 0.00026488478761166334\n",
      "step: 7792, loss: 3.138362808385864e-05\n",
      "step: 7793, loss: 3.880324584315531e-05\n",
      "step: 7794, loss: 1.6944128219620325e-05\n",
      "step: 7795, loss: 7.738512067589909e-06\n",
      "step: 7796, loss: 4.495348912314512e-05\n",
      "step: 7797, loss: 1.631409759283997e-05\n",
      "step: 7798, loss: 0.0001895921304821968\n",
      "step: 7799, loss: 0.002830192679539323\n",
      "step: 7800, loss: 0.07088957726955414\n",
      "step: 7801, loss: 0.00011687498772516847\n",
      "step: 7802, loss: 2.1820962501806207e-05\n",
      "step: 7803, loss: 0.006668791640549898\n",
      "step: 7804, loss: 0.0005261882906779647\n",
      "step: 7805, loss: 5.0878898036899045e-05\n",
      "step: 7806, loss: 0.03407399356365204\n",
      "step: 7807, loss: 4.590068419929594e-05\n",
      "step: 7808, loss: 5.0127258873544633e-05\n",
      "step: 7809, loss: 1.2546809557534289e-05\n",
      "step: 7810, loss: 6.203495286172256e-05\n",
      "step: 7811, loss: 5.269037615107663e-07\n",
      "step: 7812, loss: 2.5305123926955275e-05\n",
      "step: 7813, loss: 5.888915097784775e-07\n",
      "step: 7814, loss: 3.24369320878759e-05\n",
      "step: 7815, loss: 1.3374923355513602e-06\n",
      "step: 7816, loss: 7.977116183610633e-06\n",
      "step: 7817, loss: 0.04912564903497696\n",
      "step: 7818, loss: 1.196286848426098e-05\n",
      "step: 7819, loss: 4.09052190661896e-05\n",
      "step: 7820, loss: 0.0012193325674161315\n",
      "step: 7821, loss: 0.0002946722088381648\n",
      "step: 7822, loss: 1.7785803265724098e-06\n",
      "step: 7823, loss: 7.864225153753068e-06\n",
      "step: 7824, loss: 3.956212094635703e-05\n",
      "step: 7825, loss: 2.0607774786185473e-05\n",
      "step: 7826, loss: 8.501265256199986e-06\n",
      "step: 7827, loss: 0.00018810421170201153\n",
      "step: 7828, loss: 0.000112660534796305\n",
      "step: 7829, loss: 1.7982067220145836e-05\n",
      "step: 7830, loss: 1.1119751434307545e-05\n",
      "step: 7831, loss: 0.00011166896001668647\n",
      "step: 7832, loss: 1.4928112250345293e-05\n",
      "step: 7833, loss: 2.6750190045277122e-06\n",
      "step: 7834, loss: 4.8238220188068226e-05\n",
      "step: 7835, loss: 2.2993401216808707e-05\n",
      "step: 7836, loss: 1.2012794286420103e-05\n",
      "step: 7837, loss: 5.374800821300596e-05\n",
      "step: 7838, loss: 0.0003566521918401122\n",
      "step: 7839, loss: 5.5166638048831373e-05\n",
      "step: 7840, loss: 1.5768391676829197e-05\n",
      "step: 7841, loss: 3.876510618283646e-06\n",
      "step: 7842, loss: 4.6513905544998124e-06\n",
      "step: 7843, loss: 1.2244719982845709e-05\n",
      "step: 7844, loss: 5.0282105803489685e-05\n",
      "step: 7845, loss: 7.319550786633044e-05\n",
      "step: 7846, loss: 1.867443643277511e-05\n",
      "step: 7847, loss: 3.2250733056571335e-05\n",
      "step: 7848, loss: 0.13722026348114014\n",
      "step: 7849, loss: 3.0434224754571915e-05\n",
      "step: 7850, loss: 0.00022112633450888097\n",
      "step: 7851, loss: 0.00024318257055711\n",
      "step: 7852, loss: 2.260099608974997e-05\n",
      "step: 7853, loss: 1.4680725143989548e-05\n",
      "step: 7854, loss: 0.001002705655992031\n",
      "step: 7855, loss: 5.475968282553367e-05\n",
      "step: 7856, loss: 9.280023732571863e-06\n",
      "step: 7857, loss: 0.07735677063465118\n",
      "step: 7858, loss: 8.010437340999488e-06\n",
      "step: 7859, loss: 2.034865610767156e-05\n",
      "step: 7860, loss: 4.435266964719631e-05\n",
      "step: 7861, loss: 7.322989404201508e-05\n",
      "step: 7862, loss: 5.495885488926433e-05\n",
      "step: 7863, loss: 0.0003301040851511061\n",
      "step: 7864, loss: 9.6311332526966e-06\n",
      "step: 7865, loss: 1.2769733984896448e-05\n",
      "step: 7866, loss: 0.0009860749123618007\n",
      "step: 7867, loss: 0.0018924063770100474\n",
      "step: 7868, loss: 6.515736458823085e-06\n",
      "step: 7869, loss: 0.000445335463155061\n",
      "step: 7870, loss: 8.63729746924946e-06\n",
      "step: 7871, loss: 0.00022868128144182265\n",
      "step: 7872, loss: 1.33212770379032e-05\n",
      "step: 7873, loss: 9.120441973209381e-05\n",
      "step: 7874, loss: 1.3266634596220683e-05\n",
      "step: 7875, loss: 0.00016122097440529615\n",
      "step: 7876, loss: 0.00010807282524183393\n",
      "step: 7877, loss: 0.0017586818430572748\n",
      "step: 7878, loss: 0.05333966761827469\n",
      "step: 7879, loss: 0.0008740281919017434\n",
      "step: 7880, loss: 0.038092099130153656\n",
      "step: 7881, loss: 9.988940291805193e-06\n",
      "step: 7882, loss: 9.025782492244616e-05\n",
      "step: 7883, loss: 1.1364862984919455e-05\n",
      "step: 7884, loss: 2.8047204978065565e-05\n",
      "step: 7885, loss: 9.146430238615721e-05\n",
      "step: 7886, loss: 5.012870315113105e-05\n",
      "step: 7887, loss: 2.4243143343483098e-05\n",
      "step: 7888, loss: 1.1366745638952125e-05\n",
      "step: 7889, loss: 3.0492956284433603e-06\n",
      "step: 7890, loss: 0.0021179919131100178\n",
      "step: 7891, loss: 4.0566595998825505e-05\n",
      "step: 7892, loss: 0.03947623819112778\n",
      "step: 7893, loss: 0.02421662025153637\n",
      "step: 7894, loss: 0.0015494606923311949\n",
      "step: 7895, loss: 2.1880036001675762e-05\n",
      "step: 7896, loss: 2.6609674023347907e-05\n",
      "step: 7897, loss: 2.4445153030683286e-05\n",
      "step: 7898, loss: 5.6145049711631145e-06\n",
      "step: 7899, loss: 1.0724692401709035e-05\n",
      "step: 7900, loss: 0.001150099327787757\n",
      "step: 7901, loss: 0.02625764161348343\n",
      "step: 7902, loss: 5.392946331994608e-05\n",
      "step: 7903, loss: 0.0015341570833697915\n",
      "step: 7904, loss: 0.06019619479775429\n",
      "step: 7905, loss: 0.00012363736459519714\n",
      "step: 7906, loss: 6.2033027461438905e-06\n",
      "step: 7907, loss: 0.0002756355388555676\n",
      "step: 7908, loss: 3.7716818042099476e-06\n",
      "step: 7909, loss: 1.0583692528598476e-05\n",
      "step: 7910, loss: 5.764455272583291e-05\n",
      "step: 7911, loss: 4.645656008506194e-05\n",
      "step: 7912, loss: 3.493528856779449e-05\n",
      "step: 7913, loss: 6.7630053308676e-06\n",
      "step: 7914, loss: 1.6957177649601363e-05\n",
      "step: 7915, loss: 5.655032509821467e-06\n",
      "step: 7916, loss: 2.688691165531054e-05\n",
      "step: 7917, loss: 2.7965984372713137e-06\n",
      "step: 7918, loss: 0.017402561381459236\n",
      "step: 7919, loss: 1.1229425354031264e-06\n",
      "step: 7920, loss: 1.184930056297162e-06\n",
      "step: 7921, loss: 6.083978405513335e-06\n",
      "step: 7922, loss: 2.190727900597267e-05\n",
      "step: 7923, loss: 0.1131666749715805\n",
      "step: 7924, loss: 5.3593826123687904e-06\n",
      "step: 7925, loss: 5.336529284249991e-05\n",
      "step: 7926, loss: 9.828954716795124e-06\n",
      "step: 7927, loss: 7.666203600820154e-06\n",
      "step: 7928, loss: 5.039903498982312e-06\n",
      "step: 7929, loss: 5.06376682096743e-06\n",
      "step: 7930, loss: 0.00013190189201850444\n",
      "step: 7931, loss: 0.0001790120149962604\n",
      "step: 7932, loss: 0.00010328707139706239\n",
      "step: 7933, loss: 0.00010835303692147136\n",
      "step: 7934, loss: 1.7380531289745704e-06\n",
      "step: 7935, loss: 4.4006759708281606e-05\n",
      "step: 7936, loss: 2.6964744392898865e-06\n",
      "step: 7937, loss: 2.2763728338759392e-05\n",
      "step: 7938, loss: 1.4638795846622088e-06\n",
      "step: 7939, loss: 0.00027905902243219316\n",
      "step: 7940, loss: 7.654375076526776e-05\n",
      "step: 7941, loss: 4.810484460904263e-05\n",
      "step: 7942, loss: 0.004419633653014898\n",
      "step: 7943, loss: 4.3854677642229944e-05\n",
      "step: 7944, loss: 3.7687284930143505e-05\n",
      "step: 7945, loss: 0.046437788754701614\n",
      "step: 7946, loss: 0.00013328030763659626\n",
      "step: 7947, loss: 6.825538548582699e-06\n",
      "step: 7948, loss: 5.111174687044695e-06\n",
      "step: 7949, loss: 9.95931350189494e-06\n",
      "step: 7950, loss: 0.0005462788394652307\n",
      "step: 7951, loss: 5.243565465207212e-05\n",
      "step: 7952, loss: 3.0969273211667314e-05\n",
      "step: 7953, loss: 2.629705022627604e-06\n",
      "step: 7954, loss: 0.008057817816734314\n",
      "step: 7955, loss: 6.0043297708034515e-05\n",
      "step: 7956, loss: 1.105901537812315e-05\n",
      "step: 7957, loss: 0.00012663842062465847\n",
      "step: 7958, loss: 3.987126547144726e-05\n",
      "step: 7959, loss: 4.331933268986177e-06\n",
      "step: 7960, loss: 7.325381739065051e-05\n",
      "step: 7961, loss: 4.270901990821585e-05\n",
      "step: 7962, loss: 4.07693769375328e-05\n",
      "step: 7963, loss: 0.0001727530761854723\n",
      "step: 7964, loss: 1.5044008705444867e-06\n",
      "step: 7965, loss: 3.2471261874889024e-06\n",
      "step: 7966, loss: 5.602812507277122e-07\n",
      "step: 7967, loss: 0.00047523327521048486\n",
      "step: 7968, loss: 4.817025183001533e-05\n",
      "step: 7969, loss: 0.03366940841078758\n",
      "step: 7970, loss: 0.00010331859812140465\n",
      "step: 7971, loss: 1.2899104149255436e-05\n",
      "step: 7972, loss: 4.958531462762039e-06\n",
      "step: 7973, loss: 4.815589636564255e-05\n",
      "step: 7974, loss: 9.941065400198568e-06\n",
      "step: 7975, loss: 8.200979209505022e-05\n",
      "step: 7976, loss: 0.0009939396986737847\n",
      "step: 7977, loss: 8.403228093811776e-06\n",
      "step: 7978, loss: 6.549763202201575e-05\n",
      "step: 7979, loss: 6.253470019146334e-06\n",
      "step: 7980, loss: 1.117358715418959e-05\n",
      "step: 7981, loss: 4.136174993618624e-06\n",
      "step: 7982, loss: 0.00014590853243134916\n",
      "step: 7983, loss: 0.0001333720429101959\n",
      "step: 7984, loss: 0.00011764785449486226\n",
      "step: 7985, loss: 4.28360617661383e-05\n",
      "step: 7986, loss: 5.992587102809921e-05\n",
      "step: 7987, loss: 4.047214679303579e-05\n",
      "step: 7988, loss: 4.334313871368067e-06\n",
      "step: 7989, loss: 1.809250534279272e-05\n",
      "step: 7990, loss: 0.00033734433236531913\n",
      "step: 7991, loss: 5.285246970743174e-06\n",
      "step: 7992, loss: 3.799432306550443e-05\n",
      "step: 7993, loss: 5.528623660211451e-06\n",
      "step: 7994, loss: 2.815983498294372e-05\n",
      "step: 7995, loss: 6.171888526296243e-05\n",
      "step: 7996, loss: 4.113881368539296e-05\n",
      "step: 7997, loss: 0.00010785514314193279\n",
      "step: 7998, loss: 1.6841377146192826e-05\n",
      "step: 7999, loss: 3.748865856323391e-05\n",
      "step: 8000, loss: 0.00023072640760801733\n",
      "step: 8001, loss: 2.456227048241999e-05\n",
      "step: 8002, loss: 2.5367491616634652e-05\n",
      "step: 8003, loss: 8.904050446290057e-06\n",
      "step: 8004, loss: 1.5476585758733563e-05\n",
      "step: 8005, loss: 8.221633834182285e-06\n",
      "step: 8006, loss: 3.6524900224321755e-06\n",
      "step: 8007, loss: 8.768182306084782e-05\n",
      "step: 8008, loss: 1.4993500371929258e-05\n",
      "step: 8009, loss: 5.299686563375872e-06\n",
      "step: 8010, loss: 1.0812866094056517e-05\n",
      "step: 8011, loss: 0.10039187222719193\n",
      "step: 8012, loss: 5.378221430873964e-06\n",
      "step: 8013, loss: 1.6883514035725966e-05\n",
      "step: 8014, loss: 1.676538522588089e-05\n",
      "step: 8015, loss: 9.118083835346624e-05\n",
      "step: 8016, loss: 3.833677055808948e-06\n",
      "step: 8017, loss: 0.00277542881667614\n",
      "step: 8018, loss: 0.09573391079902649\n",
      "step: 8019, loss: 0.00853851716965437\n",
      "step: 8020, loss: 1.927345510921441e-05\n",
      "step: 8021, loss: 4.746664217236685e-06\n",
      "step: 8022, loss: 2.393654540355783e-06\n",
      "step: 8023, loss: 2.9072149118292145e-05\n",
      "step: 8024, loss: 0.03549163043498993\n",
      "step: 8025, loss: 4.130463639739901e-05\n",
      "step: 8026, loss: 1.3357213902054355e-05\n",
      "step: 8027, loss: 0.0002487564051989466\n",
      "step: 8028, loss: 1.9343640815350227e-05\n",
      "step: 8029, loss: 9.751274774316698e-07\n",
      "step: 8030, loss: 0.029036853462457657\n",
      "step: 8031, loss: 1.027681901177857e-05\n",
      "step: 8032, loss: 0.0015201905043795705\n",
      "step: 8033, loss: 3.1708948426967254e-06\n",
      "step: 8034, loss: 0.039123427122831345\n",
      "step: 8035, loss: 5.690776106348494e-06\n",
      "step: 8036, loss: 9.683008102001622e-06\n",
      "step: 8037, loss: 7.001691119512543e-05\n",
      "step: 8038, loss: 3.6476797049544984e-06\n",
      "step: 8039, loss: 0.00011656196147669107\n",
      "step: 8040, loss: 0.001821234356611967\n",
      "step: 8041, loss: 2.5582021407899447e-06\n",
      "step: 8042, loss: 0.0023461205419152975\n",
      "step: 8043, loss: 3.715104321599938e-05\n",
      "step: 8044, loss: 1.7356694570480613e-06\n",
      "step: 8045, loss: 4.300730324757751e-06\n",
      "step: 8046, loss: 0.001287428312934935\n",
      "step: 8047, loss: 0.00015919558063615113\n",
      "step: 8048, loss: 1.0079504136228934e-05\n",
      "step: 8049, loss: 0.00014440999075304717\n",
      "step: 8050, loss: 0.00010969871073029935\n",
      "step: 8051, loss: 1.7412994566257112e-05\n",
      "step: 8052, loss: 6.03633952778182e-06\n",
      "step: 8053, loss: 1.869263724074699e-05\n",
      "step: 8054, loss: 9.727438055051607e-07\n",
      "step: 8055, loss: 3.622352596721612e-05\n",
      "step: 8056, loss: 1.0389161616330966e-05\n",
      "step: 8057, loss: 1.5306088698707754e-06\n",
      "step: 8058, loss: 2.0881045202258974e-05\n",
      "step: 8059, loss: 6.68422580929473e-05\n",
      "step: 8060, loss: 1.2683479326369707e-05\n",
      "step: 8061, loss: 7.45114593883045e-05\n",
      "step: 8062, loss: 3.1982228392735124e-05\n",
      "step: 8063, loss: 3.8312850847432856e-06\n",
      "step: 8064, loss: 6.298721018538345e-06\n",
      "step: 8065, loss: 8.314132719533518e-05\n",
      "step: 8066, loss: 0.00043238536454737186\n",
      "step: 8067, loss: 2.0154619051027112e-05\n",
      "step: 8068, loss: 1.52600205183262e-05\n",
      "step: 8069, loss: 7.359569281106815e-06\n",
      "step: 8070, loss: 4.476760659599677e-05\n",
      "step: 8071, loss: 2.0627176127163693e-05\n",
      "step: 8072, loss: 2.6888134016189724e-05\n",
      "step: 8073, loss: 1.2826004422095139e-05\n",
      "step: 8074, loss: 0.00014239262964110821\n",
      "step: 8075, loss: 5.545357907976722e-06\n",
      "step: 8076, loss: 0.0004359543090686202\n",
      "step: 8077, loss: 1.253861228178721e-05\n",
      "step: 8078, loss: 4.191217612969922e-06\n",
      "step: 8079, loss: 0.04562649130821228\n",
      "step: 8080, loss: 0.043737128376960754\n",
      "step: 8081, loss: 0.00014763981744181365\n",
      "step: 8082, loss: 1.3065209714113735e-06\n",
      "step: 8083, loss: 8.274922947748564e-06\n",
      "step: 8084, loss: 7.0922992563282605e-06\n",
      "step: 8085, loss: 0.0026113458443433046\n",
      "step: 8086, loss: 0.0001993108307942748\n",
      "step: 8087, loss: 6.863579983473755e-06\n",
      "step: 8088, loss: 5.888909981877077e-07\n",
      "step: 8089, loss: 0.09628068655729294\n",
      "step: 8090, loss: 1.8655307940207422e-05\n",
      "step: 8091, loss: 0.1072811633348465\n",
      "step: 8092, loss: 1.5544735560979461e-06\n",
      "step: 8093, loss: 1.0490373369975714e-06\n",
      "step: 8094, loss: 0.03727317228913307\n",
      "step: 8095, loss: 1.1277123803665745e-06\n",
      "step: 8096, loss: 7.971146987983957e-05\n",
      "step: 8097, loss: 2.0352732462924905e-05\n",
      "step: 8098, loss: 1.0624756214383524e-05\n",
      "step: 8099, loss: 8.058043022174388e-05\n",
      "step: 8100, loss: 6.649110218859278e-06\n",
      "step: 8101, loss: 0.00013064162340015173\n",
      "step: 8102, loss: 5.661958766722819e-06\n",
      "step: 8103, loss: 0.0006445951876230538\n",
      "step: 8104, loss: 9.857801160251256e-06\n",
      "step: 8105, loss: 7.547544555563945e-06\n",
      "step: 8106, loss: 7.554858257208252e-06\n",
      "step: 8107, loss: 6.570988625753671e-05\n",
      "step: 8108, loss: 5.8991154219256714e-05\n",
      "step: 8109, loss: 5.06833248437033e-06\n",
      "step: 8110, loss: 0.00015610596165060997\n",
      "step: 8111, loss: 0.0002029489551205188\n",
      "step: 8112, loss: 9.699154907139018e-05\n",
      "step: 8113, loss: 6.219701026566327e-05\n",
      "step: 8114, loss: 1.564804006193299e-05\n",
      "step: 8115, loss: 3.027197817573324e-05\n",
      "step: 8116, loss: 1.053920277627185e-05\n",
      "step: 8117, loss: 5.221188985160552e-06\n",
      "step: 8118, loss: 1.3173194929549936e-05\n",
      "step: 8119, loss: 2.8270738766877912e-05\n",
      "step: 8120, loss: 2.7557793146115728e-05\n",
      "step: 8121, loss: 2.3704784325673245e-05\n",
      "step: 8122, loss: 6.637061687797541e-06\n",
      "step: 8123, loss: 3.993413429270731e-06\n",
      "step: 8124, loss: 0.0008418748038820922\n",
      "step: 8125, loss: 6.47266351734288e-05\n",
      "step: 8126, loss: 9.536623792882892e-07\n",
      "step: 8127, loss: 5.054457119513245e-07\n",
      "step: 8128, loss: 0.00012531359971035272\n",
      "step: 8129, loss: 1.5878769772825763e-05\n",
      "step: 8130, loss: 1.327314748778008e-05\n",
      "step: 8131, loss: 3.278148142271675e-06\n",
      "step: 8132, loss: 4.620438630809076e-06\n",
      "step: 8133, loss: 5.304698788677342e-05\n",
      "step: 8134, loss: 3.752600832740427e-06\n",
      "step: 8135, loss: 0.06595326960086823\n",
      "step: 8136, loss: 6.20239952695556e-05\n",
      "step: 8137, loss: 4.298505245969864e-06\n",
      "step: 8138, loss: 5.097143002785742e-06\n",
      "step: 8139, loss: 3.42193670803681e-05\n",
      "step: 8140, loss: 1.5472409359063022e-05\n",
      "step: 8141, loss: 2.067060904664686e-06\n",
      "step: 8142, loss: 9.59951739787357e-06\n",
      "step: 8143, loss: 1.1324806337142945e-06\n",
      "step: 8144, loss: 1.1823305612779222e-05\n",
      "step: 8145, loss: 1.0042729627457447e-05\n",
      "step: 8146, loss: 4.6084369387244806e-06\n",
      "step: 8147, loss: 1.4463698789768387e-05\n",
      "step: 8148, loss: 1.6140696743605076e-06\n",
      "step: 8149, loss: 8.92220123205334e-05\n",
      "step: 8150, loss: 0.0002764257660601288\n",
      "step: 8151, loss: 1.0031468264060095e-05\n",
      "step: 8152, loss: 5.0327444114373066e-06\n",
      "step: 8153, loss: 5.311760560289258e-06\n",
      "step: 8154, loss: 2.662273072928656e-05\n",
      "step: 8155, loss: 1.558581061544828e-05\n",
      "step: 8156, loss: 8.410850568907335e-06\n",
      "step: 8157, loss: 6.301164376054658e-06\n",
      "step: 8158, loss: 2.3388552108372096e-06\n",
      "step: 8159, loss: 1.1781337889260612e-05\n",
      "step: 8160, loss: 2.1457651655509835e-07\n",
      "step: 8161, loss: 0.00022703621652908623\n",
      "step: 8162, loss: 1.0702973668230698e-05\n",
      "step: 8163, loss: 1.3636744370160159e-05\n",
      "step: 8164, loss: 4.6225918595155235e-06\n",
      "step: 8165, loss: 3.4497556953283492e-06\n",
      "step: 8166, loss: 0.01753954030573368\n",
      "step: 8167, loss: 3.3471901588200126e-06\n",
      "step: 8168, loss: 1.3525930626201443e-05\n",
      "step: 8169, loss: 2.137802221113816e-05\n",
      "step: 8170, loss: 1.0863670468097553e-05\n",
      "step: 8171, loss: 8.30856242828304e-06\n",
      "step: 8172, loss: 7.264345185831189e-06\n",
      "step: 8173, loss: 0.00032015814213082194\n",
      "step: 8174, loss: 8.940606335272605e-07\n",
      "step: 8175, loss: 2.245046925963834e-05\n",
      "step: 8176, loss: 0.026002071797847748\n",
      "step: 8177, loss: 5.3174349886830896e-05\n",
      "step: 8178, loss: 0.0026828425470739603\n",
      "step: 8179, loss: 4.825426913157571e-06\n",
      "step: 8180, loss: 0.16874317824840546\n",
      "step: 8181, loss: 0.0004982716636732221\n",
      "step: 8182, loss: 9.076184142031707e-06\n",
      "step: 8183, loss: 3.859887783619342e-06\n",
      "step: 8184, loss: 6.7756627686321735e-06\n",
      "step: 8185, loss: 1.1992366353297257e-06\n",
      "step: 8186, loss: 4.4535472625284456e-06\n",
      "step: 8187, loss: 3.490379867798765e-06\n",
      "step: 8188, loss: 9.254574251826853e-06\n",
      "step: 8189, loss: 0.009228567592799664\n",
      "step: 8190, loss: 3.089850451942766e-06\n",
      "step: 8191, loss: 1.2877970220870338e-05\n",
      "step: 8192, loss: 2.9754157822026173e-06\n",
      "step: 8193, loss: 2.238678234789404e-06\n",
      "step: 8194, loss: 1.6275023881462403e-05\n",
      "step: 8195, loss: 1.3971910448162816e-05\n",
      "step: 8196, loss: 2.27948712563375e-05\n",
      "step: 8197, loss: 3.219700010959059e-05\n",
      "step: 8198, loss: 2.779913529593614e-06\n",
      "step: 8199, loss: 2.2936197638045996e-05\n",
      "step: 8200, loss: 1.0866040611290373e-05\n",
      "step: 8201, loss: 1.777356374077499e-05\n",
      "step: 8202, loss: 3.1388986826641485e-05\n",
      "step: 8203, loss: 3.426012881391216e-06\n",
      "step: 8204, loss: 9.322063192485075e-07\n",
      "step: 8205, loss: 2.572590710769873e-05\n",
      "step: 8206, loss: 5.483609015755064e-07\n",
      "step: 8207, loss: 0.0002623599430080503\n",
      "step: 8208, loss: 0.00010343910980736837\n",
      "step: 8209, loss: 0.04878251627087593\n",
      "step: 8210, loss: 1.7380559711455135e-06\n",
      "step: 8211, loss: 2.326108551642392e-05\n",
      "step: 8212, loss: 9.29307771002641e-06\n",
      "step: 8213, loss: 1.7475867934990674e-06\n",
      "step: 8214, loss: 3.42591238222667e-06\n",
      "step: 8215, loss: 3.058380025322549e-05\n",
      "step: 8216, loss: 1.223081085299782e-06\n",
      "step: 8217, loss: 2.8758167900377885e-05\n",
      "step: 8218, loss: 1.4653447578893974e-05\n",
      "step: 8219, loss: 1.3901505553803872e-05\n",
      "step: 8220, loss: 0.00013532809680327773\n",
      "step: 8221, loss: 0.0003962855553254485\n",
      "step: 8222, loss: 5.5454693210776895e-06\n",
      "step: 8223, loss: 2.3189391868072562e-05\n",
      "step: 8224, loss: 9.68629046838032e-06\n",
      "step: 8225, loss: 7.802490472386125e-06\n",
      "step: 8226, loss: 4.407225060276687e-05\n",
      "step: 8227, loss: 1.3507041330740321e-05\n",
      "step: 8228, loss: 3.640523345893598e-06\n",
      "step: 8229, loss: 1.0774704605864827e-05\n",
      "step: 8230, loss: 1.0625620234350208e-05\n",
      "step: 8231, loss: 0.013155226595699787\n",
      "step: 8232, loss: 2.9300938422238687e-06\n",
      "step: 8233, loss: 4.694252311310265e-06\n",
      "step: 8234, loss: 4.4195698137627915e-05\n",
      "step: 8235, loss: 9.816621059144381e-06\n",
      "step: 8236, loss: 1.9673712813528255e-05\n",
      "step: 8237, loss: 1.5349300156231038e-05\n",
      "step: 8238, loss: 0.04894229769706726\n",
      "step: 8239, loss: 1.3208224345362396e-06\n",
      "step: 8240, loss: 0.00025811445084400475\n",
      "step: 8241, loss: 3.064042539335787e-05\n",
      "step: 8242, loss: 0.00012706761481240392\n",
      "step: 8243, loss: 4.521965092862956e-05\n",
      "step: 8244, loss: 7.731439109193161e-05\n",
      "step: 8245, loss: 2.120606768585276e-05\n",
      "step: 8246, loss: 7.693249244766776e-06\n",
      "step: 8247, loss: 1.3562625099439174e-05\n",
      "step: 8248, loss: 1.7604890672373585e-05\n",
      "step: 8249, loss: 8.245548087870702e-05\n",
      "step: 8250, loss: 1.5455379980267026e-05\n",
      "step: 8251, loss: 2.3985337975318544e-05\n",
      "step: 8252, loss: 4.744567922898568e-05\n",
      "step: 8253, loss: 6.658733582298737e-06\n",
      "step: 8254, loss: 4.386893976970896e-07\n",
      "step: 8255, loss: 0.0005043800338171422\n",
      "step: 8256, loss: 0.011152364313602448\n",
      "step: 8257, loss: 1.0659024155756924e-05\n",
      "step: 8258, loss: 4.9580448830965906e-05\n",
      "step: 8259, loss: 4.2977371776942164e-05\n",
      "step: 8260, loss: 5.650503567267151e-07\n",
      "step: 8261, loss: 6.49667117613717e-06\n",
      "step: 8262, loss: 4.055966201121919e-05\n",
      "step: 8263, loss: 5.4293293942464516e-05\n",
      "step: 8264, loss: 2.7536723337107105e-06\n",
      "step: 8265, loss: 0.0005481951520778239\n",
      "step: 8266, loss: 2.818027269313461e-06\n",
      "step: 8267, loss: 8.026724572118837e-06\n",
      "step: 8268, loss: 2.922975681940443e-06\n",
      "step: 8269, loss: 1.2139059435867239e-05\n",
      "step: 8270, loss: 5.7883680710801855e-05\n",
      "step: 8271, loss: 1.4891474165779073e-05\n",
      "step: 8272, loss: 8.324369446199853e-06\n",
      "step: 8273, loss: 2.479597424098756e-05\n",
      "step: 8274, loss: 1.1528089999046642e-05\n",
      "step: 8275, loss: 2.5891829409374623e-06\n",
      "step: 8276, loss: 1.5813799109309912e-05\n",
      "step: 8277, loss: 2.313494223926682e-05\n",
      "step: 8278, loss: 4.396213171276031e-06\n",
      "step: 8279, loss: 1.166403671959415e-05\n",
      "step: 8280, loss: 0.17839942872524261\n",
      "step: 8281, loss: 1.4614977317251032e-06\n",
      "step: 8282, loss: 1.768643778632395e-05\n",
      "step: 8283, loss: 1.6166170098586008e-05\n",
      "step: 8284, loss: 4.2277541069779545e-05\n",
      "step: 8285, loss: 4.329581315687392e-06\n",
      "step: 8286, loss: 7.843884759495268e-07\n",
      "step: 8287, loss: 2.1624282453558408e-06\n",
      "step: 8288, loss: 8.013632032088935e-05\n",
      "step: 8289, loss: 2.522439217500505e-06\n",
      "step: 8290, loss: 1.5270225048880093e-05\n",
      "step: 8291, loss: 2.4961677809187677e-06\n",
      "step: 8292, loss: 3.8486625271616504e-05\n",
      "step: 8293, loss: 2.851871795428451e-05\n",
      "step: 8294, loss: 2.397185926383827e-05\n",
      "step: 8295, loss: 1.073238945537014e-05\n",
      "step: 8296, loss: 7.390922860395222e-07\n",
      "step: 8297, loss: 0.0010370609816163778\n",
      "step: 8298, loss: 2.9015011477895314e-06\n",
      "step: 8299, loss: 5.559529654419748e-06\n",
      "step: 8300, loss: 2.439275340293534e-05\n",
      "step: 8301, loss: 1.2584450814756565e-05\n",
      "step: 8302, loss: 5.128240445628762e-06\n",
      "step: 8303, loss: 1.5401685686811106e-06\n",
      "step: 8304, loss: 2.0955814761691727e-05\n",
      "step: 8305, loss: 8.18682019598782e-06\n",
      "step: 8306, loss: 5.962620434729615e-06\n",
      "step: 8307, loss: 6.29591249889927e-06\n",
      "step: 8308, loss: 1.844009238993749e-05\n",
      "step: 8309, loss: 2.252314516226761e-05\n",
      "step: 8310, loss: 2.3878876163507812e-05\n",
      "step: 8311, loss: 7.850459041947033e-06\n",
      "step: 8312, loss: 4.7277235353249125e-06\n",
      "step: 8313, loss: 1.2500613593147136e-05\n",
      "step: 8314, loss: 3.692995051096659e-06\n",
      "step: 8315, loss: 0.014018693007528782\n",
      "step: 8316, loss: 8.940602356233285e-07\n",
      "step: 8317, loss: 1.3598382793134078e-05\n",
      "step: 8318, loss: 0.0001049124839482829\n",
      "step: 8319, loss: 3.499613012536429e-05\n",
      "step: 8320, loss: 0.02447264827787876\n",
      "step: 8321, loss: 7.0066375883470755e-06\n",
      "step: 8322, loss: 1.8202930732513778e-05\n",
      "step: 8323, loss: 1.2299248737690505e-05\n",
      "step: 8324, loss: 1.0114716133102775e-05\n",
      "step: 8325, loss: 8.058082130446564e-06\n",
      "step: 8326, loss: 4.1506496017973404e-06\n",
      "step: 8327, loss: 1.5217399777611718e-05\n",
      "step: 8328, loss: 5.178203537070658e-06\n",
      "step: 8329, loss: 1.2041049558320083e-05\n",
      "step: 8330, loss: 7.096355693647638e-05\n",
      "step: 8331, loss: 9.499650332145393e-05\n",
      "step: 8332, loss: 2.4765831767581403e-05\n",
      "step: 8333, loss: 5.041812619310804e-05\n",
      "step: 8334, loss: 4.6868253775755875e-06\n",
      "step: 8335, loss: 1.2087724599041394e-06\n",
      "step: 8336, loss: 3.6001142689201515e-07\n",
      "step: 8337, loss: 3.6501132854027674e-06\n",
      "step: 8338, loss: 4.315301794122206e-06\n",
      "step: 8339, loss: 2.4322082026628777e-05\n",
      "step: 8340, loss: 1.228493874805281e-05\n",
      "step: 8341, loss: 3.89729902963154e-05\n",
      "step: 8342, loss: 9.608178288544877e-07\n",
      "step: 8343, loss: 6.227275298442692e-05\n",
      "step: 8344, loss: 9.96092421701178e-05\n",
      "step: 8345, loss: 0.06848429143428802\n",
      "step: 8346, loss: 0.10019432008266449\n",
      "step: 8347, loss: 7.907796316430904e-06\n",
      "step: 8348, loss: 6.937459147593472e-06\n",
      "step: 8349, loss: 1.7031299648806453e-05\n",
      "step: 8350, loss: 5.309318567014998e-06\n",
      "step: 8351, loss: 5.723054346162826e-05\n",
      "step: 8352, loss: 1.639836045796983e-05\n",
      "step: 8353, loss: 1.3525586837204173e-05\n",
      "step: 8354, loss: 1.2623757356777787e-05\n",
      "step: 8355, loss: 0.062186017632484436\n",
      "step: 8356, loss: 5.733708803745685e-06\n",
      "step: 8357, loss: 9.606019011698663e-05\n",
      "step: 8358, loss: 6.103493888076628e-07\n",
      "step: 8359, loss: 1.6159734514076263e-05\n",
      "step: 8360, loss: 4.620078925654525e-06\n",
      "step: 8361, loss: 6.541806942550465e-05\n",
      "step: 8362, loss: 2.885166850319365e-06\n",
      "step: 8363, loss: 0.000989787862636149\n",
      "step: 8364, loss: 1.7113876310759224e-05\n",
      "step: 8365, loss: 2.440021671645809e-05\n",
      "step: 8366, loss: 4.9231271077587735e-06\n",
      "step: 8367, loss: 1.6608002624707296e-05\n",
      "step: 8368, loss: 8.803523087408394e-06\n",
      "step: 8369, loss: 6.7398195824353024e-06\n",
      "step: 8370, loss: 9.306811989517882e-06\n",
      "step: 8371, loss: 9.575801414030138e-06\n",
      "step: 8372, loss: 6.2390581661020406e-06\n",
      "step: 8373, loss: 3.87894942832645e-06\n",
      "step: 8374, loss: 0.00010056264727609232\n",
      "step: 8375, loss: 7.3690666795300785e-06\n",
      "step: 8376, loss: 8.711251211934723e-06\n",
      "step: 8377, loss: 3.297979128547013e-05\n",
      "step: 8378, loss: 1.1315458323224448e-05\n",
      "step: 8379, loss: 0.01895279437303543\n",
      "step: 8380, loss: 1.5792946214787662e-05\n",
      "step: 8381, loss: 1.7857381635622005e-06\n",
      "step: 8382, loss: 7.486319191229995e-07\n",
      "step: 8383, loss: 0.005361615214496851\n",
      "step: 8384, loss: 4.358195383247221e-06\n",
      "step: 8385, loss: 1.8018323316937312e-05\n",
      "step: 8386, loss: 1.9746921680052765e-05\n",
      "step: 8387, loss: 1.658408291405067e-05\n",
      "step: 8388, loss: 5.721583875128999e-06\n",
      "step: 8389, loss: 4.589321179082617e-06\n",
      "step: 8390, loss: 3.1232161745720077e-06\n",
      "step: 8391, loss: 1.3239041436463594e-05\n",
      "step: 8392, loss: 3.148866380797699e-05\n",
      "step: 8393, loss: 4.677471224567853e-06\n",
      "step: 8394, loss: 2.3736873117741197e-05\n",
      "step: 8395, loss: 1.2770664397976361e-05\n",
      "step: 8396, loss: 0.001643966301344335\n",
      "step: 8397, loss: 1.599126153450925e-05\n",
      "step: 8398, loss: 2.922964540630346e-06\n",
      "step: 8399, loss: 0.0004225158190820366\n",
      "step: 8400, loss: 1.5082036043168046e-05\n",
      "step: 8401, loss: 1.6730165953049436e-05\n",
      "step: 8402, loss: 0.026129217818379402\n",
      "step: 8403, loss: 5.001786576031009e-06\n",
      "step: 8404, loss: 4.029263322991028e-07\n",
      "step: 8405, loss: 1.3394324014370795e-05\n",
      "step: 8406, loss: 1.5353970184150967e-06\n",
      "step: 8407, loss: 4.6858491259627044e-05\n",
      "step: 8408, loss: 1.6093164276753669e-06\n",
      "step: 8409, loss: 7.113926130841719e-06\n",
      "step: 8410, loss: 0.0005175013793632388\n",
      "step: 8411, loss: 0.00011116795212728903\n",
      "step: 8412, loss: 1.2510604392446112e-05\n",
      "step: 8413, loss: 1.0045780982181896e-05\n",
      "step: 8414, loss: 3.16736004606355e-05\n",
      "step: 8415, loss: 7.390961513920047e-07\n",
      "step: 8416, loss: 2.36029336520005e-06\n",
      "step: 8417, loss: 5.163853074918734e-06\n",
      "step: 8418, loss: 7.247078883665381e-06\n",
      "step: 8419, loss: 9.314067938248627e-06\n",
      "step: 8420, loss: 0.00010033442958956584\n",
      "step: 8421, loss: 3.2471457416249905e-06\n",
      "step: 8422, loss: 1.6474488120366004e-06\n",
      "step: 8423, loss: 3.435537564655533e-06\n",
      "step: 8424, loss: 2.454985587974079e-05\n",
      "step: 8425, loss: 5.88362763664918e-06\n",
      "step: 8426, loss: 9.727384622237878e-07\n",
      "step: 8427, loss: 2.2322112272377126e-05\n",
      "step: 8428, loss: 2.6838790290639736e-05\n",
      "step: 8429, loss: 0.00016720900021027774\n",
      "step: 8430, loss: 3.8499882066389546e-05\n",
      "step: 8431, loss: 2.0718141513498267e-06\n",
      "step: 8432, loss: 2.2363303742167773e-06\n",
      "step: 8433, loss: 5.030485226598103e-06\n",
      "step: 8434, loss: 3.4045485790556995e-06\n",
      "step: 8435, loss: 2.1095835109008476e-05\n",
      "step: 8436, loss: 0.0002892804623115808\n",
      "step: 8437, loss: 7.285680567292729e-06\n",
      "step: 8438, loss: 7.58167857384251e-07\n",
      "step: 8439, loss: 4.757260830956511e-05\n",
      "step: 8440, loss: 0.01734793372452259\n",
      "step: 8441, loss: 2.310223408130696e-06\n",
      "step: 8442, loss: 0.00011814944446086884\n",
      "step: 8443, loss: 1.6903700270631816e-06\n",
      "step: 8444, loss: 2.868072442652192e-06\n",
      "step: 8445, loss: 5.0753133109537885e-05\n",
      "step: 8446, loss: 5.719283308280865e-06\n",
      "step: 8447, loss: 2.838486761902459e-05\n",
      "step: 8448, loss: 4.796793291461654e-06\n",
      "step: 8449, loss: 3.2823169021867216e-05\n",
      "step: 8450, loss: 2.1679328710888512e-05\n",
      "step: 8451, loss: 0.0003497482102829963\n",
      "step: 8452, loss: 2.6106394216185436e-05\n",
      "step: 8453, loss: 3.5879693314200267e-06\n",
      "step: 8454, loss: 2.7441506063041743e-06\n",
      "step: 8455, loss: 0.002245370065793395\n",
      "step: 8456, loss: 1.3446667708194582e-06\n",
      "step: 8457, loss: 6.484969503617322e-07\n",
      "step: 8458, loss: 0.00015436316607519984\n",
      "step: 8459, loss: 1.7822889276430942e-05\n",
      "step: 8460, loss: 5.933955435466487e-06\n",
      "step: 8461, loss: 4.570486635202542e-05\n",
      "step: 8462, loss: 3.008475323440507e-05\n",
      "step: 8463, loss: 5.82909660806763e-06\n",
      "step: 8464, loss: 1.5425553101522382e-06\n",
      "step: 8465, loss: 1.3808064068143722e-05\n",
      "step: 8466, loss: 1.1431476195866708e-05\n",
      "step: 8467, loss: 5.561924808716867e-06\n",
      "step: 8468, loss: 1.60605923156254e-05\n",
      "step: 8469, loss: 1.8238719121654867e-06\n",
      "step: 8470, loss: 4.565421932056779e-06\n",
      "step: 8471, loss: 4.482007170736324e-06\n",
      "step: 8472, loss: 2.2000484023010358e-05\n",
      "step: 8473, loss: 1.8488555724616162e-05\n",
      "step: 8474, loss: 3.4474453514121706e-06\n",
      "step: 8475, loss: 1.9547773263184354e-05\n",
      "step: 8476, loss: 1.6900528862606734e-05\n",
      "step: 8477, loss: 2.331680661882274e-06\n",
      "step: 8478, loss: 7.971493687364273e-06\n",
      "step: 8479, loss: 4.5115048123989254e-05\n",
      "step: 8480, loss: 2.34124081543996e-06\n",
      "step: 8481, loss: 9.330618013336789e-06\n",
      "step: 8482, loss: 7.351382373599336e-05\n",
      "step: 8483, loss: 7.402253686450422e-05\n",
      "step: 8484, loss: 5.473719738802174e-06\n",
      "step: 8485, loss: 5.98157294007251e-06\n",
      "step: 8486, loss: 7.153490878408775e-05\n",
      "step: 8487, loss: 3.5380157896724995e-06\n",
      "step: 8488, loss: 0.0688105970621109\n",
      "step: 8489, loss: 7.948155143822078e-06\n",
      "step: 8490, loss: 9.3017632025294e-05\n",
      "step: 8491, loss: 1.892430373118259e-05\n",
      "step: 8492, loss: 0.00021766706777270883\n",
      "step: 8493, loss: 8.25977258500643e-06\n",
      "step: 8494, loss: 5.204121862334432e-06\n",
      "step: 8495, loss: 0.00022982610971666873\n",
      "step: 8496, loss: 5.624374534818344e-05\n",
      "step: 8497, loss: 7.224051046250679e-07\n",
      "step: 8498, loss: 6.839899469923694e-06\n",
      "step: 8499, loss: 4.634617198462365e-06\n",
      "step: 8500, loss: 8.963293112174142e-06\n",
      "step: 8501, loss: 0.0003924598859157413\n",
      "step: 8502, loss: 1.804236126190517e-05\n",
      "step: 8503, loss: 7.03503383192583e-06\n",
      "step: 8504, loss: 2.51287951869017e-06\n",
      "step: 8505, loss: 3.213743639207678e-06\n",
      "step: 8506, loss: 1.1968530770900543e-06\n",
      "step: 8507, loss: 0.05667157098650932\n",
      "step: 8508, loss: 1.766263085301034e-05\n",
      "step: 8509, loss: 0.0015384334838017821\n",
      "step: 8510, loss: 1.3468512406689115e-05\n",
      "step: 8511, loss: 0.0003784118453040719\n",
      "step: 8512, loss: 2.8470256438595243e-05\n",
      "step: 8513, loss: 2.909540489781648e-05\n",
      "step: 8514, loss: 5.869620963494526e-06\n",
      "step: 8515, loss: 0.00010234329965896904\n",
      "step: 8516, loss: 0.12463249266147614\n",
      "step: 8517, loss: 0.00017317391757387668\n",
      "step: 8518, loss: 2.814003346429672e-05\n",
      "step: 8519, loss: 3.7379402783699334e-05\n",
      "step: 8520, loss: 6.992325779719977e-06\n",
      "step: 8521, loss: 1.5110543245100416e-05\n",
      "step: 8522, loss: 0.00010272159124724567\n",
      "step: 8523, loss: 1.641344351810403e-05\n",
      "step: 8524, loss: 2.348386260564439e-06\n",
      "step: 8525, loss: 5.945167868048884e-05\n",
      "step: 8526, loss: 2.746438985923305e-05\n",
      "step: 8527, loss: 2.810800197039498e-06\n",
      "step: 8528, loss: 0.0030796611681580544\n",
      "step: 8529, loss: 1.220195281348424e-05\n",
      "step: 8530, loss: 0.00022331986110657454\n",
      "step: 8531, loss: 4.6585760173911694e-06\n",
      "step: 8532, loss: 1.39524599944707e-05\n",
      "step: 8533, loss: 0.00012692937161773443\n",
      "step: 8534, loss: 2.841882178472588e-06\n",
      "step: 8535, loss: 0.03210877999663353\n",
      "step: 8536, loss: 1.383061407977948e-05\n",
      "step: 8537, loss: 0.0009044724865816534\n",
      "step: 8538, loss: 1.1748995348170865e-05\n",
      "step: 8539, loss: 5.026906364946626e-05\n",
      "step: 8540, loss: 3.083148112636991e-05\n",
      "step: 8541, loss: 0.152662456035614\n",
      "step: 8542, loss: 1.3195452083891723e-05\n",
      "step: 8543, loss: 2.624941316753393e-06\n",
      "step: 8544, loss: 1.4107034985499922e-05\n",
      "step: 8545, loss: 1.0012432539951988e-05\n",
      "step: 8546, loss: 3.3796008210629225e-05\n",
      "step: 8547, loss: 2.233947952845483e-06\n",
      "step: 8548, loss: 0.050195977091789246\n",
      "step: 8549, loss: 0.0002826034906320274\n",
      "step: 8550, loss: 4.308104962547077e-06\n",
      "step: 8551, loss: 2.105205385305453e-05\n",
      "step: 8552, loss: 1.7313168427790515e-05\n",
      "step: 8553, loss: 1.2980259270989336e-05\n",
      "step: 8554, loss: 0.031444426625967026\n",
      "step: 8555, loss: 7.564611678390065e-06\n",
      "step: 8556, loss: 1.4280805771704763e-05\n",
      "step: 8557, loss: 4.668134351959452e-06\n",
      "step: 8558, loss: 1.393818274664227e-05\n",
      "step: 8559, loss: 8.479958523821551e-06\n",
      "step: 8560, loss: 6.0961083363508806e-06\n",
      "step: 8561, loss: 0.0381002277135849\n",
      "step: 8562, loss: 9.560501439409563e-07\n",
      "step: 8563, loss: 1.0089208444696851e-05\n",
      "step: 8564, loss: 1.0743835446191952e-05\n",
      "step: 8565, loss: 3.859806383843534e-06\n",
      "step: 8566, loss: 5.483417680807179e-06\n",
      "step: 8567, loss: 5.705090188712347e-06\n",
      "step: 8568, loss: 9.288750879932195e-05\n",
      "step: 8569, loss: 0.09406284242868423\n",
      "step: 8570, loss: 4.669656482292339e-05\n",
      "step: 8571, loss: 9.642160875955597e-06\n",
      "step: 8572, loss: 9.297041287936736e-06\n",
      "step: 8573, loss: 6.061706517357379e-05\n",
      "step: 8574, loss: 5.2161399253236596e-06\n",
      "step: 8575, loss: 8.15365001471946e-06\n",
      "step: 8576, loss: 2.5072915377677418e-05\n",
      "step: 8577, loss: 3.7573493045783835e-06\n",
      "step: 8578, loss: 1.4098546671448275e-05\n",
      "step: 8579, loss: 2.7754242182709277e-05\n",
      "step: 8580, loss: 0.00010155457857763395\n",
      "step: 8581, loss: 1.2935401173308492e-05\n",
      "step: 8582, loss: 2.1261545043671504e-05\n",
      "step: 8583, loss: 4.6560444388887845e-06\n",
      "step: 8584, loss: 1.912096877276781e-06\n",
      "step: 8585, loss: 6.727888830937445e-06\n",
      "step: 8586, loss: 0.00015192812134046108\n",
      "step: 8587, loss: 2.172388121834956e-05\n",
      "step: 8588, loss: 6.847994427516824e-06\n",
      "step: 8589, loss: 4.2199980043733376e-07\n",
      "step: 8590, loss: 1.7627842680667527e-05\n",
      "step: 8591, loss: 3.7763052205264103e-06\n",
      "step: 8592, loss: 1.232234626513673e-05\n",
      "step: 8593, loss: 0.00023335036530625075\n",
      "step: 8594, loss: 2.8847550765931373e-06\n",
      "step: 8595, loss: 5.3534862672677264e-05\n",
      "step: 8596, loss: 2.8712160201394e-05\n",
      "step: 8597, loss: 5.7230634411098436e-05\n",
      "step: 8598, loss: 2.143351366612478e-06\n",
      "step: 8599, loss: 6.901818778715096e-06\n",
      "step: 8600, loss: 5.853016773471609e-06\n",
      "step: 8601, loss: 2.2577678464585915e-06\n",
      "step: 8602, loss: 6.1462433222914115e-06\n",
      "step: 8603, loss: 7.563166582258418e-05\n",
      "step: 8604, loss: 4.677082324633375e-05\n",
      "step: 8605, loss: 5.5938504374353215e-05\n",
      "step: 8606, loss: 1.0228101245957077e-06\n",
      "step: 8607, loss: 2.887792834371794e-05\n",
      "step: 8608, loss: 0.00011187100608367473\n",
      "step: 8609, loss: 0.07059338688850403\n",
      "step: 8610, loss: 5.099352165416349e-06\n",
      "step: 8611, loss: 1.5261050066328607e-05\n",
      "step: 8612, loss: 1.1015916243195534e-05\n",
      "step: 8613, loss: 1.7014650438795798e-05\n",
      "step: 8614, loss: 6.532254246849334e-06\n",
      "step: 8615, loss: 9.576430784363765e-06\n",
      "step: 8616, loss: 1.7614185708225705e-05\n",
      "step: 8617, loss: 3.287706704213633e-06\n",
      "step: 8618, loss: 2.5657762307673693e-05\n",
      "step: 8619, loss: 0.00011122988507850096\n",
      "step: 8620, loss: 1.4891432329022791e-05\n",
      "step: 8621, loss: 7.788342372805346e-06\n",
      "step: 8622, loss: 6.239037702471251e-06\n",
      "step: 8623, loss: 6.179177489684662e-06\n",
      "step: 8624, loss: 1.4471740996668814e-06\n",
      "step: 8625, loss: 6.51651862426661e-05\n",
      "step: 8626, loss: 0.00014093643403612077\n",
      "step: 8627, loss: 1.008501953947416e-06\n",
      "step: 8628, loss: 0.00018034427193924785\n",
      "step: 8629, loss: 1.0204180398432072e-06\n",
      "step: 8630, loss: 4.737149993161438e-06\n",
      "step: 8631, loss: 1.0565456250333227e-05\n",
      "step: 8632, loss: 5.1921913836849853e-05\n",
      "step: 8633, loss: 3.70041343558114e-05\n",
      "step: 8634, loss: 1.7237488236787613e-06\n",
      "step: 8635, loss: 1.937290289788507e-05\n",
      "step: 8636, loss: 1.0684390872484073e-05\n",
      "step: 8637, loss: 2.725079639276373e-06\n",
      "step: 8638, loss: 6.296160427154973e-05\n",
      "step: 8639, loss: 9.059863259608392e-07\n",
      "step: 8640, loss: 1.630768224458734e-06\n",
      "step: 8641, loss: 2.2604188416153193e-05\n",
      "step: 8642, loss: 8.429896297457162e-06\n",
      "step: 8643, loss: 2.1695836949220393e-06\n",
      "step: 8644, loss: 4.377155619295081e-06\n",
      "step: 8645, loss: 8.439969860773999e-07\n",
      "step: 8646, loss: 0.009934360161423683\n",
      "step: 8647, loss: 5.731118108087685e-06\n",
      "step: 8648, loss: 4.136480129091069e-06\n",
      "step: 8649, loss: 0.13955157995224\n",
      "step: 8650, loss: 3.0349987355293706e-05\n",
      "step: 8651, loss: 5.53470708837267e-05\n",
      "step: 8652, loss: 8.785232239461038e-06\n",
      "step: 8653, loss: 4.832370905205607e-06\n",
      "step: 8654, loss: 0.027927732095122337\n",
      "step: 8655, loss: 8.225405281336862e-07\n",
      "step: 8656, loss: 0.00013393406698014587\n",
      "step: 8657, loss: 1.2957970284332987e-05\n",
      "step: 8658, loss: 1.5296262063202448e-05\n",
      "step: 8659, loss: 2.8204124191688607e-06\n",
      "step: 8660, loss: 4.9255259000347e-06\n",
      "step: 8661, loss: 1.27281573440996e-05\n",
      "step: 8662, loss: 7.867804185934801e-08\n",
      "step: 8663, loss: 4.114918738196138e-06\n",
      "step: 8664, loss: 7.729241588094737e-06\n",
      "step: 8665, loss: 7.071388245094568e-05\n",
      "step: 8666, loss: 3.424675378482789e-05\n",
      "step: 8667, loss: 2.7562438845052384e-05\n",
      "step: 8668, loss: 6.547504017362371e-05\n",
      "step: 8669, loss: 4.7607113629055675e-06\n",
      "step: 8670, loss: 3.0025325031601824e-05\n",
      "step: 8671, loss: 4.024210284114815e-06\n",
      "step: 8672, loss: 6.14848931945744e-06\n",
      "step: 8673, loss: 0.00010805220517795533\n",
      "step: 8674, loss: 8.658381375425961e-06\n",
      "step: 8675, loss: 1.2439509191608522e-05\n",
      "step: 8676, loss: 3.197148544131778e-06\n",
      "step: 8677, loss: 2.3459990643459605e-06\n",
      "step: 8678, loss: 1.3547825801651925e-05\n",
      "step: 8679, loss: 1.466299727326259e-05\n",
      "step: 8680, loss: 7.352828833973035e-05\n",
      "step: 8681, loss: 8.496434020344168e-06\n",
      "step: 8682, loss: 1.158801114797825e-05\n",
      "step: 8683, loss: 2.25378626055317e-05\n",
      "step: 8684, loss: 1.740414177220373e-06\n",
      "step: 8685, loss: 8.708434506843332e-06\n",
      "step: 8686, loss: 1.3446688171825372e-06\n",
      "step: 8687, loss: 1.6497753676958382e-05\n",
      "step: 8688, loss: 7.771806849632412e-06\n",
      "step: 8689, loss: 7.575476047350094e-05\n",
      "step: 8690, loss: 0.08391131460666656\n",
      "step: 8691, loss: 1.5091709428816102e-06\n",
      "step: 8692, loss: 2.779875103442464e-06\n",
      "step: 8693, loss: 0.004586620256304741\n",
      "step: 8694, loss: 6.341875268844888e-05\n",
      "step: 8695, loss: 1.80718984665873e-06\n",
      "step: 8696, loss: 0.0030701435171067715\n",
      "step: 8697, loss: 2.27681448450312e-06\n",
      "step: 8698, loss: 6.968400157347787e-06\n",
      "step: 8699, loss: 8.630706247458875e-07\n",
      "step: 8700, loss: 3.3854767025331967e-06\n",
      "step: 8701, loss: 1.0466508229001192e-06\n",
      "step: 8702, loss: 2.2148751668282785e-06\n",
      "step: 8703, loss: 1.382186746923253e-05\n",
      "step: 8704, loss: 1.4233372667149524e-06\n",
      "step: 8705, loss: 4.30081718150177e-06\n",
      "step: 8706, loss: 3.134274447802454e-05\n",
      "step: 8707, loss: 3.108931650785962e-06\n",
      "step: 8708, loss: 1.8053317035082728e-05\n",
      "step: 8709, loss: 0.03629819303750992\n",
      "step: 8710, loss: 4.756322141474811e-06\n",
      "step: 8711, loss: 0.11899760365486145\n",
      "step: 8712, loss: 1.7404179288860178e-06\n",
      "step: 8713, loss: 2.467575313858106e-06\n",
      "step: 8714, loss: 3.29237832374929e-06\n",
      "step: 8715, loss: 1.2469218972910312e-06\n",
      "step: 8716, loss: 0.054408684372901917\n",
      "step: 8717, loss: 0.013506079092621803\n",
      "step: 8718, loss: 1.100870667869458e-05\n",
      "step: 8719, loss: 0.0037916689179837704\n",
      "step: 8720, loss: 3.996629675384611e-05\n",
      "step: 8721, loss: 1.0704907253966667e-06\n",
      "step: 8722, loss: 2.592318196548149e-05\n",
      "step: 8723, loss: 8.28660631668754e-06\n",
      "step: 8724, loss: 9.250595098819758e-07\n",
      "step: 8725, loss: 0.00010717455006670207\n",
      "step: 8726, loss: 9.366751328343526e-06\n",
      "step: 8727, loss: 3.0683711429446703e-06\n",
      "step: 8728, loss: 1.2425452950992621e-05\n",
      "step: 8729, loss: 0.05031311511993408\n",
      "step: 8730, loss: 4.351039933681022e-06\n",
      "step: 8731, loss: 2.8251954518054845e-06\n",
      "step: 8732, loss: 4.551025995169766e-06\n",
      "step: 8733, loss: 9.524370398139581e-06\n",
      "step: 8734, loss: 7.593373993586283e-06\n",
      "step: 8735, loss: 1.4325991287478246e-05\n",
      "step: 8736, loss: 1.1175005965924356e-05\n",
      "step: 8737, loss: 1.2468691238609608e-05\n",
      "step: 8738, loss: 7.244816515594721e-06\n",
      "step: 8739, loss: 8.877028449205682e-06\n",
      "step: 8740, loss: 3.142186642435263e-06\n",
      "step: 8741, loss: 5.4011616157367826e-05\n",
      "step: 8742, loss: 0.0004005137598142028\n",
      "step: 8743, loss: 2.0884990590275265e-06\n",
      "step: 8744, loss: 3.2733482839830685e-06\n",
      "step: 8745, loss: 1.8821154299075715e-05\n",
      "step: 8746, loss: 2.9346059818635695e-05\n",
      "step: 8747, loss: 5.7245182688347995e-05\n",
      "step: 8748, loss: 0.022641601040959358\n",
      "step: 8749, loss: 6.12730559623742e-07\n",
      "step: 8750, loss: 1.2913302271044813e-05\n",
      "step: 8751, loss: 5.448684169095941e-05\n",
      "step: 8752, loss: 1.2015163520118222e-05\n",
      "step: 8753, loss: 1.1348633961461019e-06\n",
      "step: 8754, loss: 1.8953032849822193e-05\n",
      "step: 8755, loss: 5.106771823193412e-06\n",
      "step: 8756, loss: 2.019364046645933e-06\n",
      "step: 8757, loss: 0.08931535482406616\n",
      "step: 8758, loss: 1.1241372703807428e-05\n",
      "step: 8759, loss: 1.1960572010139003e-05\n",
      "step: 8760, loss: 5.4357237786462065e-06\n",
      "step: 8761, loss: 0.0020808724220842123\n",
      "step: 8762, loss: 1.8071779095407692e-06\n",
      "step: 8763, loss: 5.864962986379396e-06\n",
      "step: 8764, loss: 8.712774615560193e-06\n",
      "step: 8765, loss: 3.896622365573421e-05\n",
      "step: 8766, loss: 3.197109435859602e-06\n",
      "step: 8767, loss: 3.1303927698900225e-06\n",
      "step: 8768, loss: 5.313742076396011e-05\n",
      "step: 8769, loss: 0.04883316159248352\n",
      "step: 8770, loss: 3.323557029943913e-05\n",
      "step: 8771, loss: 9.306996616942342e-06\n",
      "step: 8772, loss: 3.148972609778866e-05\n",
      "step: 8773, loss: 1.8308024664293043e-05\n",
      "step: 8774, loss: 4.498690941545647e-06\n",
      "step: 8775, loss: 3.1008588848635554e-05\n",
      "step: 8776, loss: 2.5391095732629765e-06\n",
      "step: 8777, loss: 6.61760859657079e-05\n",
      "step: 8778, loss: 1.5091137356648687e-05\n",
      "step: 8779, loss: 9.739505912875757e-06\n",
      "step: 8780, loss: 7.026011735433713e-05\n",
      "step: 8781, loss: 1.0728736015153117e-06\n",
      "step: 8782, loss: 2.891869144150405e-06\n",
      "step: 8783, loss: 1.9932082068407908e-05\n",
      "step: 8784, loss: 2.6916779916064115e-06\n",
      "step: 8785, loss: 5.006777996641176e-07\n",
      "step: 8786, loss: 1.6951333918768796e-06\n",
      "step: 8787, loss: 4.696535597759066e-06\n",
      "step: 8788, loss: 0.00037889351369813085\n",
      "step: 8789, loss: 3.37590608978644e-06\n",
      "step: 8790, loss: 0.05219992622733116\n",
      "step: 8791, loss: 3.1566107736580307e-06\n",
      "step: 8792, loss: 2.048067472060211e-05\n",
      "step: 8793, loss: 1.18015941552585e-06\n",
      "step: 8794, loss: 0.0005254906718619168\n",
      "step: 8795, loss: 2.5261508199037053e-05\n",
      "step: 8796, loss: 3.6619815091398777e-06\n",
      "step: 8797, loss: 3.197114665454137e-06\n",
      "step: 8798, loss: 7.200170557553065e-07\n",
      "step: 8799, loss: 2.078983925457578e-06\n",
      "step: 8800, loss: 2.3953491108841263e-05\n",
      "step: 8801, loss: 0.09225177764892578\n",
      "step: 8802, loss: 0.0006429578643292189\n",
      "step: 8803, loss: 4.663294930651318e-06\n",
      "step: 8804, loss: 2.394372313574422e-05\n",
      "step: 8805, loss: 5.674024578183889e-06\n",
      "step: 8806, loss: 1.1361005817889236e-05\n",
      "step: 8807, loss: 1.3637310985359363e-05\n",
      "step: 8808, loss: 2.117131771228742e-06\n",
      "step: 8809, loss: 6.439417120418511e-06\n",
      "step: 8810, loss: 4.412939688336337e-06\n",
      "step: 8811, loss: 1.585278550919611e-05\n",
      "step: 8812, loss: 8.559165962651605e-07\n",
      "step: 8813, loss: 1.5367400919785723e-05\n",
      "step: 8814, loss: 4.43295812146971e-06\n",
      "step: 8815, loss: 0.013936363160610199\n",
      "step: 8816, loss: 2.12779596040491e-05\n",
      "step: 8817, loss: 2.6249504117004108e-06\n",
      "step: 8818, loss: 9.013764611154329e-06\n",
      "step: 8819, loss: 7.48621516777348e-07\n",
      "step: 8820, loss: 1.7060829122783616e-05\n",
      "step: 8821, loss: 2.7727866836357862e-05\n",
      "step: 8822, loss: 1.4900921314620064e-06\n",
      "step: 8823, loss: 9.727988071972504e-06\n",
      "step: 8824, loss: 5.030855390941724e-05\n",
      "step: 8825, loss: 1.4161932995193638e-06\n",
      "step: 8826, loss: 6.897013372508809e-05\n",
      "step: 8827, loss: 5.45976945431903e-07\n",
      "step: 8828, loss: 2.686885454750154e-05\n",
      "step: 8829, loss: 2.9301338599907467e-06\n",
      "step: 8830, loss: 7.137870397855295e-06\n",
      "step: 8831, loss: 2.5271403501392342e-05\n",
      "step: 8832, loss: 0.00010783192556118593\n",
      "step: 8833, loss: 7.36274232622236e-05\n",
      "step: 8834, loss: 2.6213843739242293e-05\n",
      "step: 8835, loss: 8.829649232211523e-06\n",
      "step: 8836, loss: 0.00026769228861667216\n",
      "step: 8837, loss: 2.105602470692247e-05\n",
      "step: 8838, loss: 3.0755275020055706e-06\n",
      "step: 8839, loss: 1.716598603707098e-06\n",
      "step: 8840, loss: 3.6209432437317446e-05\n",
      "step: 8841, loss: 4.705840547103435e-05\n",
      "step: 8842, loss: 2.101608333759941e-05\n",
      "step: 8843, loss: 0.07197917252779007\n",
      "step: 8844, loss: 9.89264517556876e-05\n",
      "step: 8845, loss: 0.05635491758584976\n",
      "step: 8846, loss: 6.11749965173658e-06\n",
      "step: 8847, loss: 0.052984900772571564\n",
      "step: 8848, loss: 1.6808354530439829e-06\n",
      "step: 8849, loss: 3.337022280902602e-05\n",
      "step: 8850, loss: 1.5185643860604614e-05\n",
      "step: 8851, loss: 3.7597671962430468e-06\n",
      "step: 8852, loss: 2.8466770345403347e-06\n",
      "step: 8853, loss: 1.9025537767447531e-06\n",
      "step: 8854, loss: 0.00011767417890951037\n",
      "step: 8855, loss: 0.08687936514616013\n",
      "step: 8856, loss: 7.841113074391615e-06\n",
      "step: 8857, loss: 7.765476038912311e-05\n",
      "step: 8858, loss: 0.00012380641419440508\n",
      "step: 8859, loss: 1.2951986718690023e-05\n",
      "step: 8860, loss: 1.480378250562353e-05\n",
      "step: 8861, loss: 1.862487079051789e-05\n",
      "step: 8862, loss: 2.227041477453895e-05\n",
      "step: 8863, loss: 4.303303285269067e-06\n",
      "step: 8864, loss: 8.326942770509049e-05\n",
      "step: 8865, loss: 1.1979697774222586e-05\n",
      "step: 8866, loss: 1.7595127701497404e-06\n",
      "step: 8867, loss: 3.945691787521355e-06\n",
      "step: 8868, loss: 1.17539764232788e-06\n",
      "step: 8869, loss: 2.7485608370625414e-05\n",
      "step: 8870, loss: 0.00017340274644084275\n",
      "step: 8871, loss: 0.002066882560029626\n",
      "step: 8872, loss: 5.240165592113044e-06\n",
      "step: 8873, loss: 1.8596444988361327e-06\n",
      "step: 8874, loss: 9.111473445955198e-06\n",
      "step: 8875, loss: 2.751301508396864e-06\n",
      "step: 8876, loss: 1.24576063171844e-05\n",
      "step: 8877, loss: 9.769590178621002e-06\n",
      "step: 8878, loss: 2.040837898675818e-06\n",
      "step: 8879, loss: 3.3855332048915443e-07\n",
      "step: 8880, loss: 2.932428060375969e-06\n",
      "step: 8881, loss: 3.5762647598858166e-07\n",
      "step: 8882, loss: 3.4214182960567996e-05\n",
      "step: 8883, loss: 1.1063521014875732e-05\n",
      "step: 8884, loss: 3.469642251729965e-05\n",
      "step: 8885, loss: 2.0751049305545166e-05\n",
      "step: 8886, loss: 4.341271778685041e-05\n",
      "step: 8887, loss: 3.335342398713692e-06\n",
      "step: 8888, loss: 0.0007638552924618125\n",
      "step: 8889, loss: 2.5500008632661775e-05\n",
      "step: 8890, loss: 1.2108802366128657e-05\n",
      "step: 8891, loss: 2.534328359615756e-06\n",
      "step: 8892, loss: 1.943521601788234e-05\n",
      "step: 8893, loss: 2.9046868803561665e-05\n",
      "step: 8894, loss: 3.2257669317914406e-06\n",
      "step: 8895, loss: 1.292983415623894e-05\n",
      "step: 8896, loss: 4.956432348990347e-06\n",
      "step: 8897, loss: 0.0001449303381377831\n",
      "step: 8898, loss: 0.0013588507426902652\n",
      "step: 8899, loss: 2.7268424673820846e-05\n",
      "step: 8900, loss: 7.559740333817899e-05\n",
      "step: 8901, loss: 4.675033324019751e-06\n",
      "step: 8902, loss: 3.987559102824889e-05\n",
      "step: 8903, loss: 5.364399839891121e-07\n",
      "step: 8904, loss: 6.772467168048024e-05\n",
      "step: 8905, loss: 1.00500919870683e-05\n",
      "step: 8906, loss: 1.6361058442271315e-05\n",
      "step: 8907, loss: 4.3556050513871014e-05\n",
      "step: 8908, loss: 2.3936809157021344e-06\n",
      "step: 8909, loss: 8.044534479267895e-05\n",
      "step: 8910, loss: 1.6961637811618857e-05\n",
      "step: 8911, loss: 1.9104762031929567e-05\n",
      "step: 8912, loss: 4.961367267242167e-06\n",
      "step: 8913, loss: 1.3950289030617569e-05\n",
      "step: 8914, loss: 7.259395715664141e-06\n",
      "step: 8915, loss: 5.9289304772391915e-06\n",
      "step: 8916, loss: 4.2246674638590775e-06\n",
      "step: 8917, loss: 6.992198905209079e-06\n",
      "step: 8918, loss: 1.3311571819940582e-05\n",
      "step: 8919, loss: 1.0291858416167088e-05\n",
      "step: 8920, loss: 2.5300865672761574e-05\n",
      "step: 8921, loss: 6.072224550734973e-06\n",
      "step: 8922, loss: 0.0002101026475429535\n",
      "step: 8923, loss: 2.37460312746407e-06\n",
      "step: 8924, loss: 6.031943939888151e-07\n",
      "step: 8925, loss: 0.1578899323940277\n",
      "step: 8926, loss: 3.087462346229586e-06\n",
      "step: 8927, loss: 1.199237772198103e-06\n",
      "step: 8928, loss: 0.09307507425546646\n",
      "step: 8929, loss: 5.411940492194844e-06\n",
      "step: 8930, loss: 0.0006497523281723261\n",
      "step: 8931, loss: 8.96442941211717e-07\n",
      "step: 8932, loss: 1.51869699038798e-06\n",
      "step: 8933, loss: 2.473958556947764e-05\n",
      "step: 8934, loss: 6.401119662768906e-06\n",
      "step: 8935, loss: 6.68192751618335e-06\n",
      "step: 8936, loss: 1.4790020941291004e-05\n",
      "step: 8937, loss: 1.1571497452678159e-05\n",
      "step: 8938, loss: 3.7847199564566836e-05\n",
      "step: 8939, loss: 1.3934090929978993e-05\n",
      "step: 8940, loss: 1.1693482520058751e-05\n",
      "step: 8941, loss: 5.22090567756095e-06\n",
      "step: 8942, loss: 9.70352175500011e-07\n",
      "step: 8943, loss: 0.011577421799302101\n",
      "step: 8944, loss: 1.7880136510939337e-05\n",
      "step: 8945, loss: 5.242420229478739e-06\n",
      "step: 8946, loss: 1.00611862308142e-06\n",
      "step: 8947, loss: 1.215922566188965e-06\n",
      "step: 8948, loss: 2.503306632206659e-06\n",
      "step: 8949, loss: 5.299910480971448e-05\n",
      "step: 8950, loss: 2.305460839124862e-06\n",
      "step: 8951, loss: 9.33538467506878e-06\n",
      "step: 8952, loss: 5.602817623184819e-07\n",
      "step: 8953, loss: 1.537566276965663e-05\n",
      "step: 8954, loss: 1.0753334208857268e-05\n",
      "step: 8955, loss: 1.5401726614072686e-06\n",
      "step: 8956, loss: 7.811805699020624e-05\n",
      "step: 8957, loss: 0.0005435203202068806\n",
      "step: 8958, loss: 1.2492914720496628e-06\n",
      "step: 8959, loss: 4.646466550184414e-06\n",
      "step: 8960, loss: 8.845261731948995e-07\n",
      "step: 8961, loss: 9.139992471318692e-06\n",
      "step: 8962, loss: 4.129002263653092e-05\n",
      "step: 8963, loss: 9.54094412008999e-06\n",
      "step: 8964, loss: 7.364529665210284e-06\n",
      "step: 8965, loss: 7.123268915165681e-06\n",
      "step: 8966, loss: 0.07384798675775528\n",
      "step: 8967, loss: 1.175382976725814e-06\n",
      "step: 8968, loss: 2.869930540327914e-05\n",
      "step: 8969, loss: 2.4151445359166246e-06\n",
      "step: 8970, loss: 2.536722377044498e-06\n",
      "step: 8971, loss: 4.958772478858009e-06\n",
      "step: 8972, loss: 8.451100256934296e-06\n",
      "step: 8973, loss: 7.723758244537748e-06\n",
      "step: 8974, loss: 2.9198843549238518e-05\n",
      "step: 8975, loss: 4.515483851719182e-06\n",
      "step: 8976, loss: 3.3799671655287966e-05\n",
      "step: 8977, loss: 9.144927389570512e-06\n",
      "step: 8978, loss: 5.607409548247233e-06\n",
      "step: 8979, loss: 2.0207568013574928e-05\n",
      "step: 8980, loss: 6.96180052273121e-07\n",
      "step: 8981, loss: 1.218303395944531e-06\n",
      "step: 8982, loss: 4.887567115474667e-07\n",
      "step: 8983, loss: 0.015301516279578209\n",
      "step: 8984, loss: 0.004947354551404715\n",
      "step: 8985, loss: 3.742433909792453e-05\n",
      "step: 8986, loss: 1.5306260365832713e-06\n",
      "step: 8987, loss: 9.674103239376564e-06\n",
      "step: 8988, loss: 5.719304681406356e-06\n",
      "step: 8989, loss: 6.000640951242531e-06\n",
      "step: 8990, loss: 1.052781954058446e-05\n",
      "step: 8991, loss: 8.416141099587549e-07\n",
      "step: 8992, loss: 1.2444826097635087e-05\n",
      "step: 8993, loss: 6.8232138801249675e-06\n",
      "step: 8994, loss: 8.14345185062848e-06\n",
      "step: 8995, loss: 0.06218212470412254\n",
      "step: 8996, loss: 5.624443292617798e-05\n",
      "step: 8997, loss: 5.5549903663631994e-06\n",
      "step: 8998, loss: 1.624585820536595e-05\n",
      "step: 8999, loss: 1.0416729310236406e-05\n",
      "step: 9000, loss: 1.047016939992318e-05\n",
      "step: 9001, loss: 1.8154478311771527e-05\n",
      "step: 9002, loss: 0.0726708173751831\n",
      "step: 9003, loss: 1.52587531943027e-07\n",
      "step: 9004, loss: 9.79364358499879e-06\n",
      "step: 9005, loss: 1.0084137102239765e-05\n",
      "step: 9006, loss: 9.79762444330845e-06\n",
      "step: 9007, loss: 0.050205208361148834\n",
      "step: 9008, loss: 7.940514478832483e-06\n",
      "step: 9009, loss: 0.0231810063123703\n",
      "step: 9010, loss: 3.7596803394990275e-06\n",
      "step: 9011, loss: 1.2683751720032888e-06\n",
      "step: 9012, loss: 1.1001478014804889e-05\n",
      "step: 9013, loss: 3.385481477380381e-06\n",
      "step: 9014, loss: 3.988599928561598e-06\n",
      "step: 9015, loss: 9.550567483529449e-06\n",
      "step: 9016, loss: 2.176708903789404e-06\n",
      "step: 9017, loss: 2.329628296138253e-05\n",
      "step: 9018, loss: 2.7822468382510124e-06\n",
      "step: 9019, loss: 0.0001771832030499354\n",
      "step: 9020, loss: 0.04249824956059456\n",
      "step: 9021, loss: 0.03580392524600029\n",
      "step: 9022, loss: 2.047092857537791e-05\n",
      "step: 9023, loss: 6.431979727494763e-06\n",
      "step: 9024, loss: 2.7394688004278578e-05\n",
      "step: 9025, loss: 2.500923301340663e-06\n",
      "step: 9026, loss: 0.0001667291362537071\n",
      "step: 9027, loss: 4.6366461901925504e-05\n",
      "step: 9028, loss: 1.1042765436286572e-05\n",
      "step: 9029, loss: 0.00215811375528574\n",
      "step: 9030, loss: 1.0977960300806444e-05\n",
      "step: 9031, loss: 1.2816777598345652e-05\n",
      "step: 9032, loss: 2.615404355310602e-06\n",
      "step: 9033, loss: 3.1479514291277155e-05\n",
      "step: 9034, loss: 1.4686411304865032e-06\n",
      "step: 9035, loss: 3.955264219257515e-06\n",
      "step: 9036, loss: 3.2066309358924627e-06\n",
      "step: 9037, loss: 1.7785754380383878e-06\n",
      "step: 9038, loss: 9.221123946190346e-06\n",
      "step: 9039, loss: 1.3874527212465182e-05\n",
      "step: 9040, loss: 2.5741530407685786e-05\n",
      "step: 9041, loss: 5.202107331570005e-06\n",
      "step: 9042, loss: 6.320345619315049e-06\n",
      "step: 9043, loss: 0.006420115940272808\n",
      "step: 9044, loss: 6.527463028760394e-06\n",
      "step: 9045, loss: 0.014930511824786663\n",
      "step: 9046, loss: 1.974118458747398e-05\n",
      "step: 9047, loss: 6.428010237868875e-05\n",
      "step: 9048, loss: 2.741758407864836e-06\n",
      "step: 9049, loss: 3.869426109304186e-06\n",
      "step: 9050, loss: 4.4201592572790105e-06\n",
      "step: 9051, loss: 3.430773404033971e-06\n",
      "step: 9052, loss: 1.4278440175985452e-05\n",
      "step: 9053, loss: 0.01264834776520729\n",
      "step: 9054, loss: 7.757614184811246e-06\n",
      "step: 9055, loss: 6.565527201018995e-06\n",
      "step: 9056, loss: 3.7020115996710956e-05\n",
      "step: 9057, loss: 6.961745384614915e-07\n",
      "step: 9058, loss: 2.9799139156239107e-05\n",
      "step: 9059, loss: 5.3555351769318804e-05\n",
      "step: 9060, loss: 6.765966645616572e-06\n",
      "step: 9061, loss: 0.00017666422354523093\n",
      "step: 9062, loss: 1.4253068002290092e-05\n",
      "step: 9063, loss: 1.2168457942607347e-05\n",
      "step: 9064, loss: 1.5067839740368072e-06\n",
      "step: 9065, loss: 8.043562957027461e-06\n",
      "step: 9066, loss: 1.5966881619533524e-05\n",
      "step: 9067, loss: 3.805291999015026e-05\n",
      "step: 9068, loss: 3.664431233119103e-06\n",
      "step: 9069, loss: 7.264387022587471e-06\n",
      "step: 9070, loss: 8.267878911283333e-06\n",
      "step: 9071, loss: 4.503554464463377e-06\n",
      "step: 9072, loss: 0.3235463798046112\n",
      "step: 9073, loss: 0.00014274618297349662\n",
      "step: 9074, loss: 6.374912481987849e-05\n",
      "step: 9075, loss: 3.571389015633031e-06\n",
      "step: 9076, loss: 0.06644489616155624\n",
      "step: 9077, loss: 0.06403624266386032\n",
      "step: 9078, loss: 6.021145236445591e-05\n",
      "step: 9079, loss: 4.904969682684168e-05\n",
      "step: 9080, loss: 9.32880266191205e-06\n",
      "step: 9081, loss: 8.3646533312276e-05\n",
      "step: 9082, loss: 5.304591013555182e-06\n",
      "step: 9083, loss: 1.4415817531698849e-05\n",
      "step: 9084, loss: 2.3770178813720122e-05\n",
      "step: 9085, loss: 8.932355740398634e-06\n",
      "step: 9086, loss: 8.589490789745469e-06\n",
      "step: 9087, loss: 3.216046934539918e-06\n",
      "step: 9088, loss: 3.847970674542012e-06\n",
      "step: 9089, loss: 1.7714211253405665e-06\n",
      "step: 9090, loss: 1.1563251973711886e-06\n",
      "step: 9091, loss: 1.416114901076071e-05\n",
      "step: 9092, loss: 3.41161012329394e-06\n",
      "step: 9093, loss: 1.4798270967730787e-05\n",
      "step: 9094, loss: 1.4126512724033091e-05\n",
      "step: 9095, loss: 1.0943306278932141e-06\n",
      "step: 9096, loss: 0.004792860243469477\n",
      "step: 9097, loss: 4.164172787568532e-05\n",
      "step: 9098, loss: 6.1294886108953506e-06\n",
      "step: 9099, loss: 2.0873007088084705e-05\n",
      "step: 9100, loss: 4.328064460423775e-05\n",
      "step: 9101, loss: 3.850384473480517e-06\n",
      "step: 9102, loss: 3.80336532543879e-05\n",
      "step: 9103, loss: 5.121835056343116e-05\n",
      "step: 9104, loss: 1.4638759466834017e-06\n",
      "step: 9105, loss: 7.46850037103286e-06\n",
      "step: 9106, loss: 1.216001601278549e-05\n",
      "step: 9107, loss: 4.217405603412772e-06\n",
      "step: 9108, loss: 7.00910231898888e-06\n",
      "step: 9109, loss: 0.00020976545056328177\n",
      "step: 9110, loss: 2.7213478460907936e-05\n",
      "step: 9111, loss: 6.372182269842597e-06\n",
      "step: 9112, loss: 4.274623279343359e-06\n",
      "step: 9113, loss: 6.2127714954840485e-06\n",
      "step: 9114, loss: 1.957121457962785e-05\n",
      "step: 9115, loss: 2.422297939119744e-06\n",
      "step: 9116, loss: 4.87958277517464e-05\n",
      "step: 9117, loss: 4.107792847207747e-06\n",
      "step: 9118, loss: 8.921055268729106e-06\n",
      "step: 9119, loss: 5.2760366088477895e-05\n",
      "step: 9120, loss: 7.188013569248142e-06\n",
      "step: 9121, loss: 1.655606865824666e-05\n",
      "step: 9122, loss: 6.298302196228178e-06\n",
      "step: 9123, loss: 1.738058585942781e-06\n",
      "step: 9124, loss: 0.0002130889770342037\n",
      "step: 9125, loss: 1.3952381777926348e-05\n",
      "step: 9126, loss: 7.885569175414275e-06\n",
      "step: 9127, loss: 2.482510171830654e-05\n",
      "step: 9128, loss: 5.1305305532878265e-06\n",
      "step: 9129, loss: 3.7407019135571318e-06\n",
      "step: 9130, loss: 0.002510971389710903\n",
      "step: 9131, loss: 1.3613882401841693e-05\n",
      "step: 9132, loss: 7.182294211816043e-05\n",
      "step: 9133, loss: 7.223683496704325e-06\n",
      "step: 9134, loss: 6.08065674896352e-05\n",
      "step: 9135, loss: 7.299845037778141e-06\n",
      "step: 9136, loss: 0.00011881902173627168\n",
      "step: 9137, loss: 5.0360325985820964e-05\n",
      "step: 9138, loss: 0.00023725171922706068\n",
      "step: 9139, loss: 3.933702828362584e-06\n",
      "step: 9140, loss: 2.307858494532411e-06\n",
      "step: 9141, loss: 7.952728992677294e-06\n",
      "step: 9142, loss: 8.484075806336477e-06\n",
      "step: 9143, loss: 0.00010662341810530052\n",
      "step: 9144, loss: 2.3197453629109077e-06\n",
      "step: 9145, loss: 0.0003916943387594074\n",
      "step: 9146, loss: 1.3332512025954202e-05\n",
      "step: 9147, loss: 1.0132316674571484e-05\n",
      "step: 9148, loss: 0.00011144462041556835\n",
      "step: 9149, loss: 9.64761602517683e-06\n",
      "step: 9150, loss: 7.952551095513627e-05\n",
      "step: 9151, loss: 2.8724683943437412e-05\n",
      "step: 9152, loss: 1.1539341358002275e-06\n",
      "step: 9153, loss: 1.4308913705463056e-05\n",
      "step: 9154, loss: 0.0002548517659306526\n",
      "step: 9155, loss: 1.5616290056641446e-06\n",
      "step: 9156, loss: 0.14359745383262634\n",
      "step: 9157, loss: 1.2902352864330169e-05\n",
      "step: 9158, loss: 6.415170355467126e-05\n",
      "step: 9159, loss: 2.772717834886862e-06\n",
      "step: 9160, loss: 1.869046172942035e-05\n",
      "step: 9161, loss: 1.843431164161302e-05\n",
      "step: 9162, loss: 7.003892733337125e-06\n",
      "step: 9163, loss: 1.4614882957175723e-06\n",
      "step: 9164, loss: 1.7403548554284498e-05\n",
      "step: 9165, loss: 3.457396451267414e-05\n",
      "step: 9166, loss: 1.653748950047884e-05\n",
      "step: 9167, loss: 1.313673578806629e-06\n",
      "step: 9168, loss: 4.08393498219084e-06\n",
      "step: 9169, loss: 0.03185383602976799\n",
      "step: 9170, loss: 1.8664608433027752e-05\n",
      "step: 9171, loss: 7.411583737848559e-06\n",
      "step: 9172, loss: 0.0001384435745421797\n",
      "step: 9173, loss: 8.482183147862088e-06\n",
      "step: 9174, loss: 5.364263870433206e-06\n",
      "step: 9175, loss: 1.904921191453468e-06\n",
      "step: 9176, loss: 4.5336630137171596e-05\n",
      "step: 9177, loss: 3.003612619068008e-05\n",
      "step: 9178, loss: 1.7189723848787253e-06\n",
      "step: 9179, loss: 3.9169790397863835e-06\n",
      "step: 9180, loss: 1.983592710530502e-06\n",
      "step: 9181, loss: 1.1404089491406921e-05\n",
      "step: 9182, loss: 0.00010843082418432459\n",
      "step: 9183, loss: 1.857259803728084e-06\n",
      "step: 9184, loss: 2.5653309876361163e-06\n",
      "step: 9185, loss: 1.154680285253562e-05\n",
      "step: 9186, loss: 2.078979150610394e-06\n",
      "step: 9187, loss: 4.176848960923962e-06\n",
      "step: 9188, loss: 1.8273578461958095e-05\n",
      "step: 9189, loss: 3.8055488403188065e-05\n",
      "step: 9190, loss: 3.321091071484261e-06\n",
      "step: 9191, loss: 7.252257091749925e-06\n",
      "step: 9192, loss: 7.741171430097893e-06\n",
      "step: 9193, loss: 2.8228050723555498e-06\n",
      "step: 9194, loss: 1.1300933238089783e-06\n",
      "step: 9195, loss: 5.907303602725733e-06\n",
      "step: 9196, loss: 1.349631929770112e-05\n",
      "step: 9197, loss: 1.4019267837284133e-05\n",
      "step: 9198, loss: 5.333216449798783e-06\n",
      "step: 9199, loss: 2.1380081307142973e-05\n",
      "step: 9200, loss: 1.9064142179558985e-05\n",
      "step: 9201, loss: 1.7142027672889526e-06\n",
      "step: 9202, loss: 1.304127636103658e-06\n",
      "step: 9203, loss: 9.155174325314874e-07\n",
      "step: 9204, loss: 5.185311692912364e-06\n",
      "step: 9205, loss: 3.013557034137193e-06\n",
      "step: 9206, loss: 4.017229002784006e-06\n",
      "step: 9207, loss: 3.807476332440274e-06\n",
      "step: 9208, loss: 5.412090331446961e-07\n",
      "step: 9209, loss: 1.93232608580729e-05\n",
      "step: 9210, loss: 9.263810170523357e-06\n",
      "step: 9211, loss: 4.0201070078182966e-05\n",
      "step: 9212, loss: 3.0969299587013666e-06\n",
      "step: 9213, loss: 8.561154572817031e-06\n",
      "step: 9214, loss: 3.1255831345333718e-06\n",
      "step: 9215, loss: 9.132690138358157e-06\n",
      "step: 9216, loss: 7.883910257078242e-06\n",
      "step: 9217, loss: 4.741734755953075e-06\n",
      "step: 9218, loss: 2.8161721274955198e-05\n",
      "step: 9219, loss: 1.3089083950035274e-06\n",
      "step: 9220, loss: 1.214646272273967e-05\n",
      "step: 9221, loss: 1.3119223694957327e-05\n",
      "step: 9222, loss: 0.04427330940961838\n",
      "step: 9223, loss: 7.462053872586694e-06\n",
      "step: 9224, loss: 0.00039502038271166384\n",
      "step: 9225, loss: 0.07396302372217178\n",
      "step: 9226, loss: 3.2495859159098472e-06\n",
      "step: 9227, loss: 2.236295404145494e-05\n",
      "step: 9228, loss: 0.010762647725641727\n",
      "step: 9229, loss: 3.3802502002799883e-05\n",
      "step: 9230, loss: 3.6953977087250678e-06\n",
      "step: 9231, loss: 7.218264727271162e-06\n",
      "step: 9232, loss: 0.0001414348662365228\n",
      "step: 9233, loss: 0.10153869539499283\n",
      "step: 9234, loss: 4.105379503016593e-06\n",
      "step: 9235, loss: 2.5224321689165663e-06\n",
      "step: 9236, loss: 4.458377952687442e-05\n",
      "step: 9237, loss: 3.576197286747629e-06\n",
      "step: 9238, loss: 2.4771024982328527e-06\n",
      "step: 9239, loss: 8.25800725579029e-06\n",
      "step: 9240, loss: 4.224640179018024e-06\n",
      "step: 9241, loss: 1.2312940270930994e-05\n",
      "step: 9242, loss: 0.00012573089043144137\n",
      "step: 9243, loss: 5.125362440594472e-05\n",
      "step: 9244, loss: 8.559197794966167e-07\n",
      "step: 9245, loss: 9.918134082909091e-07\n",
      "step: 9246, loss: 8.308344149554614e-06\n",
      "step: 9247, loss: 2.4032144665397936e-06\n",
      "step: 9248, loss: 3.6854449717793614e-05\n",
      "step: 9249, loss: 1.43286990805791e-06\n",
      "step: 9250, loss: 4.0172250010073185e-05\n",
      "step: 9251, loss: 3.6523131257126806e-06\n",
      "step: 9252, loss: 1.4614915926358663e-06\n",
      "step: 9253, loss: 1.4161927310851752e-06\n",
      "step: 9254, loss: 1.7452097154091462e-06\n",
      "step: 9255, loss: 0.02298540435731411\n",
      "step: 9256, loss: 1.9902210624422878e-05\n",
      "step: 9257, loss: 5.412081236499944e-07\n",
      "step: 9258, loss: 0.0001510439906269312\n",
      "step: 9259, loss: 2.1433411347970832e-06\n",
      "step: 9260, loss: 2.1552755242737476e-06\n",
      "step: 9261, loss: 6.173396832309663e-05\n",
      "step: 9262, loss: 0.00016219068493228406\n",
      "step: 9263, loss: 3.8098119148344267e-06\n",
      "step: 9264, loss: 7.242729225254152e-06\n",
      "step: 9265, loss: 8.630678394183633e-07\n",
      "step: 9266, loss: 2.834864062606357e-06\n",
      "step: 9267, loss: 3.684375769807957e-05\n",
      "step: 9268, loss: 7.370033563347533e-05\n",
      "step: 9269, loss: 2.376963948336197e-06\n",
      "step: 9270, loss: 5.7395278417970985e-05\n",
      "step: 9271, loss: 2.920544147855253e-06\n",
      "step: 9272, loss: 3.200180799467489e-05\n",
      "step: 9273, loss: 7.510144541811314e-07\n",
      "step: 9274, loss: 3.176003156113438e-05\n",
      "step: 9275, loss: 0.0027510151267051697\n",
      "step: 9276, loss: 4.251416612532921e-05\n",
      "step: 9277, loss: 8.05265735834837e-05\n",
      "step: 9278, loss: 0.0010859541362151504\n",
      "step: 9279, loss: 8.558768968214281e-06\n",
      "step: 9280, loss: 8.130025435093557e-07\n",
      "step: 9281, loss: 4.023606379632838e-05\n",
      "step: 9282, loss: 1.764742046361789e-05\n",
      "step: 9283, loss: 7.97087122919038e-05\n",
      "step: 9284, loss: 2.5137867851299234e-05\n",
      "step: 9285, loss: 4.308128154661972e-06\n",
      "step: 9286, loss: 1.177771537186345e-06\n",
      "step: 9287, loss: 4.555899067781866e-06\n",
      "step: 9288, loss: 1.807191893021809e-06\n",
      "step: 9289, loss: 8.308100404974539e-06\n",
      "step: 9290, loss: 4.5270669943420216e-05\n",
      "step: 9291, loss: 0.05140094831585884\n",
      "step: 9292, loss: 1.4524165635521058e-05\n",
      "step: 9293, loss: 0.10922355949878693\n",
      "step: 9294, loss: 7.964657015691046e-06\n",
      "step: 9295, loss: 8.581903966842219e-05\n",
      "step: 9296, loss: 7.676633686060086e-06\n",
      "step: 9297, loss: 2.080245212709997e-05\n",
      "step: 9298, loss: 5.0924786592077e-06\n",
      "step: 9299, loss: 2.0146941096754745e-05\n",
      "step: 9300, loss: 6.689322162856115e-06\n",
      "step: 9301, loss: 2.2768522285332438e-06\n",
      "step: 9302, loss: 1.8691557670535985e-06\n",
      "step: 9303, loss: 0.000149616040289402\n",
      "step: 9304, loss: 1.04562777778483e-05\n",
      "step: 9305, loss: 1.538778997201007e-05\n",
      "step: 9306, loss: 2.832642348948866e-05\n",
      "step: 9307, loss: 1.4387955161510035e-05\n",
      "step: 9308, loss: 2.808534873111057e-06\n",
      "step: 9309, loss: 7.530902621510904e-06\n",
      "step: 9310, loss: 7.226094112411374e-06\n",
      "step: 9311, loss: 3.2590696719125845e-06\n",
      "step: 9312, loss: 2.6154537408729084e-05\n",
      "step: 9313, loss: 1.7178985217469744e-05\n",
      "step: 9314, loss: 1.2278462691028835e-06\n",
      "step: 9315, loss: 2.2438794985646382e-05\n",
      "step: 9316, loss: 2.956233402073849e-06\n",
      "step: 9317, loss: 4.8063320718938485e-06\n",
      "step: 9318, loss: 0.026280749589204788\n",
      "step: 9319, loss: 2.6773830086312955e-06\n",
      "step: 9320, loss: 9.314196177001577e-06\n",
      "step: 9321, loss: 1.6708143448340707e-05\n",
      "step: 9322, loss: 0.08983442187309265\n",
      "step: 9323, loss: 2.2434805941884406e-06\n",
      "step: 9324, loss: 9.14760948944604e-06\n",
      "step: 9325, loss: 3.2423861284769373e-06\n",
      "step: 9326, loss: 6.589479198737536e-06\n",
      "step: 9327, loss: 0.0001073927414836362\n",
      "step: 9328, loss: 1.1399248251109384e-05\n",
      "step: 9329, loss: 3.9408910197380465e-06\n",
      "step: 9330, loss: 1.0271577593812253e-05\n",
      "step: 9331, loss: 1.94071344594704e-06\n",
      "step: 9332, loss: 1.6879565691851894e-06\n",
      "step: 9333, loss: 4.239128975314088e-05\n",
      "step: 9334, loss: 1.562019497214351e-05\n",
      "step: 9335, loss: 5.604542820947245e-05\n",
      "step: 9336, loss: 2.648737790877931e-06\n",
      "step: 9337, loss: 2.3150150809669867e-06\n",
      "step: 9338, loss: 5.802933628729079e-06\n",
      "step: 9339, loss: 8.138964403769933e-06\n",
      "step: 9340, loss: 2.1071806258987635e-05\n",
      "step: 9341, loss: 1.8372389604337513e-05\n",
      "step: 9342, loss: 7.399888545478461e-06\n",
      "step: 9343, loss: 0.04283067584037781\n",
      "step: 9344, loss: 4.820557478524279e-06\n",
      "step: 9345, loss: 6.105513875809265e-06\n",
      "step: 9346, loss: 5.614520432573045e-06\n",
      "step: 9347, loss: 4.079226982867112e-06\n",
      "step: 9348, loss: 3.004060999955982e-07\n",
      "step: 9349, loss: 1.9168560356774833e-06\n",
      "step: 9350, loss: 2.7870469239132944e-06\n",
      "step: 9351, loss: 1.4646322597400285e-05\n",
      "step: 9352, loss: 1.569834421388805e-05\n",
      "step: 9353, loss: 0.12053952366113663\n",
      "step: 9354, loss: 8.698442252352834e-05\n",
      "step: 9355, loss: 2.5200233721989207e-06\n",
      "step: 9356, loss: 7.764812835375778e-06\n",
      "step: 9357, loss: 4.957573037245311e-05\n",
      "step: 9358, loss: 6.348294391500531e-06\n",
      "step: 9359, loss: 3.171034768456593e-05\n",
      "step: 9360, loss: 9.378902177559212e-06\n",
      "step: 9361, loss: 1.7094079112212057e-06\n",
      "step: 9362, loss: 1.3135634617356118e-05\n",
      "step: 9363, loss: 3.9577301436111156e-07\n",
      "step: 9364, loss: 0.00011221848399145529\n",
      "step: 9365, loss: 1.5178939975157846e-05\n",
      "step: 9366, loss: 4.958962108503329e-06\n",
      "step: 9367, loss: 1.9678995158756152e-05\n",
      "step: 9368, loss: 0.03302478417754173\n",
      "step: 9369, loss: 1.487801182520343e-05\n",
      "step: 9370, loss: 2.5081330932152923e-06\n",
      "step: 9371, loss: 3.92048605135642e-05\n",
      "step: 9372, loss: 1.0285383723385166e-05\n",
      "step: 9373, loss: 1.9112399968435057e-05\n",
      "step: 9374, loss: 1.3464265066431835e-05\n",
      "step: 9375, loss: 3.139915634164936e-06\n",
      "step: 9376, loss: 6.0673946791212074e-06\n",
      "step: 9377, loss: 2.5671772164059803e-05\n",
      "step: 9378, loss: 5.280575805954868e-06\n",
      "step: 9379, loss: 1.892404179670848e-05\n",
      "step: 9380, loss: 1.9240139863541117e-06\n",
      "step: 9381, loss: 4.9637810661806725e-06\n",
      "step: 9382, loss: 4.515518412517849e-06\n",
      "step: 9383, loss: 5.068573045718949e-06\n",
      "step: 9384, loss: 7.510159321100218e-07\n",
      "step: 9385, loss: 0.0003429025528021157\n",
      "step: 9386, loss: 2.5784373065107502e-05\n",
      "step: 9387, loss: 7.75285570853157e-06\n",
      "step: 9388, loss: 0.013465426862239838\n",
      "step: 9389, loss: 2.4079884042293997e-06\n",
      "step: 9390, loss: 4.677420292864554e-06\n",
      "step: 9391, loss: 4.126962267037015e-06\n",
      "step: 9392, loss: 7.342749086092226e-06\n",
      "step: 9393, loss: 2.362674649702967e-06\n",
      "step: 9394, loss: 1.0351399396313354e-05\n",
      "step: 9395, loss: 2.791782662825426e-06\n",
      "step: 9396, loss: 2.7643358407658525e-05\n",
      "step: 9397, loss: 4.064938366354909e-06\n",
      "step: 9398, loss: 5.9532703744480386e-05\n",
      "step: 9399, loss: 1.5210858919090242e-06\n",
      "step: 9400, loss: 1.8117863874067552e-05\n",
      "step: 9401, loss: 6.012487119733123e-06\n",
      "step: 9402, loss: 6.365433819155442e-06\n",
      "step: 9403, loss: 1.6903672985790763e-06\n",
      "step: 9404, loss: 4.908901701128343e-06\n",
      "step: 9405, loss: 5.3020739869680256e-06\n",
      "step: 9406, loss: 7.862265192670748e-06\n",
      "step: 9407, loss: 0.0501517690718174\n",
      "step: 9408, loss: 5.595772381639108e-05\n",
      "step: 9409, loss: 6.52766630082624e-06\n",
      "step: 9410, loss: 1.992175202758517e-05\n",
      "step: 9411, loss: 5.5429663916584104e-06\n",
      "step: 9412, loss: 0.026958467438817024\n",
      "step: 9413, loss: 1.2326170235610334e-06\n",
      "step: 9414, loss: 5.054096163803479e-06\n",
      "step: 9415, loss: 8.606871233496349e-07\n",
      "step: 9416, loss: 1.0297272638126742e-05\n",
      "step: 9417, loss: 1.8405681885269587e-06\n",
      "step: 9418, loss: 4.5060852471578983e-07\n",
      "step: 9419, loss: 7.311851732083596e-06\n",
      "step: 9420, loss: 1.0513716915738769e-05\n",
      "step: 9421, loss: 1.2278484291528002e-06\n",
      "step: 9422, loss: 9.388097168994136e-06\n",
      "step: 9423, loss: 0.00011838541104225442\n",
      "step: 9424, loss: 2.7845565000461647e-06\n",
      "step: 9425, loss: 4.93264133183402e-06\n",
      "step: 9426, loss: 4.117396201763768e-06\n",
      "step: 9427, loss: 1.6553969544474967e-05\n",
      "step: 9428, loss: 1.2588182016770588e-06\n",
      "step: 9429, loss: 1.0802632459672168e-05\n",
      "step: 9430, loss: 3.189900326105999e-06\n",
      "step: 9431, loss: 2.9968805392854847e-06\n",
      "step: 9432, loss: 2.0050611055921763e-06\n",
      "step: 9433, loss: 0.08089876174926758\n",
      "step: 9434, loss: 1.2443990272004157e-05\n",
      "step: 9435, loss: 6.801255494792713e-06\n",
      "step: 9436, loss: 1.6497666365467012e-05\n",
      "step: 9437, loss: 4.105462721781805e-06\n",
      "step: 9438, loss: 2.86463946395088e-05\n",
      "step: 9439, loss: 1.888556835183408e-05\n",
      "step: 9440, loss: 8.797567261353834e-07\n",
      "step: 9441, loss: 1.2883380804851186e-05\n",
      "step: 9442, loss: 1.214269741467433e-05\n",
      "step: 9443, loss: 0.00027145937201566994\n",
      "step: 9444, loss: 6.0842367020086385e-06\n",
      "step: 9445, loss: 1.8334023934585275e-06\n",
      "step: 9446, loss: 7.785970410623122e-06\n",
      "step: 9447, loss: 0.0002402768877800554\n",
      "step: 9448, loss: 5.888609393878141e-06\n",
      "step: 9449, loss: 2.2363406060321722e-06\n",
      "step: 9450, loss: 0.08436107635498047\n",
      "step: 9451, loss: 6.708899945806479e-06\n",
      "step: 9452, loss: 0.00039895199006423354\n",
      "step: 9453, loss: 1.8372222257312387e-05\n",
      "step: 9454, loss: 4.935249080517679e-07\n",
      "step: 9455, loss: 3.609492523537483e-06\n",
      "step: 9456, loss: 5.0781527534127235e-06\n",
      "step: 9457, loss: 1.01272971733124e-05\n",
      "step: 9458, loss: 2.8942552035005065e-06\n",
      "step: 9459, loss: 0.00014838212518952787\n",
      "step: 9460, loss: 7.595730494358577e-06\n",
      "step: 9461, loss: 8.41690634842962e-05\n",
      "step: 9462, loss: 1.0560513146629091e-05\n",
      "step: 9463, loss: 7.548779103672132e-05\n",
      "step: 9464, loss: 2.772731249933713e-06\n",
      "step: 9465, loss: 0.08394650369882584\n",
      "step: 9466, loss: 2.19720350287389e-05\n",
      "step: 9467, loss: 5.4476472541864496e-06\n",
      "step: 9468, loss: 3.292477913419134e-06\n",
      "step: 9469, loss: 6.28970010438934e-05\n",
      "step: 9470, loss: 0.00011631459346972406\n",
      "step: 9471, loss: 1.4121660569799133e-05\n",
      "step: 9472, loss: 1.0472409485373646e-05\n",
      "step: 9473, loss: 3.6738533708557952e-06\n",
      "step: 9474, loss: 1.5401628843392245e-06\n",
      "step: 9475, loss: 1.1014183655788656e-05\n",
      "step: 9476, loss: 2.8762477086274885e-05\n",
      "step: 9477, loss: 0.081153504550457\n",
      "step: 9478, loss: 1.4924875131328008e-06\n",
      "step: 9479, loss: 7.523206295445561e-05\n",
      "step: 9480, loss: 1.8167348798669991e-06\n",
      "step: 9481, loss: 0.0001743834582157433\n",
      "step: 9482, loss: 0.004282418172806501\n",
      "step: 9483, loss: 0.030390841886401176\n",
      "step: 9484, loss: 1.6179024896700867e-05\n",
      "step: 9485, loss: 0.00033729110145941377\n",
      "step: 9486, loss: 4.8610163503326476e-06\n",
      "step: 9487, loss: 9.246034460375085e-05\n",
      "step: 9488, loss: 3.0993312520877225e-06\n",
      "step: 9489, loss: 1.0036636012955569e-05\n",
      "step: 9490, loss: 5.857924770680256e-05\n",
      "step: 9491, loss: 2.1743408069596626e-06\n",
      "step: 9492, loss: 4.79045047541149e-06\n",
      "step: 9493, loss: 1.9001125792783569e-06\n",
      "step: 9494, loss: 2.4485074391122907e-05\n",
      "step: 9495, loss: 8.600538421887904e-06\n",
      "step: 9496, loss: 9.593245522410143e-06\n",
      "step: 9497, loss: 0.04543774947524071\n",
      "step: 9498, loss: 3.0826765851088567e-06\n",
      "step: 9499, loss: 0.0001783965271897614\n",
      "step: 9500, loss: 1.0394958280812716e-06\n",
      "step: 9501, loss: 9.244371540262364e-06\n",
      "step: 9502, loss: 1.255381812370615e-05\n",
      "step: 9503, loss: 2.2696692667523166e-06\n",
      "step: 9504, loss: 0.17729473114013672\n",
      "step: 9505, loss: 1.85145327122882e-05\n",
      "step: 9506, loss: 7.5883294812229e-06\n",
      "step: 9507, loss: 0.0002296079182997346\n",
      "step: 9508, loss: 7.664505574211944e-06\n",
      "step: 9509, loss: 1.6712901924620382e-06\n",
      "step: 9510, loss: 2.015513200603891e-05\n",
      "step: 9511, loss: 2.6485284251975827e-05\n",
      "step: 9512, loss: 3.067087891395204e-05\n",
      "step: 9513, loss: 9.878244600258768e-05\n",
      "step: 9514, loss: 1.8104423361364752e-05\n",
      "step: 9515, loss: 3.486201967461966e-05\n",
      "step: 9516, loss: 2.2959306988923345e-06\n",
      "step: 9517, loss: 3.4735909139271826e-05\n",
      "step: 9518, loss: 4.360524599178461e-06\n",
      "step: 9519, loss: 9.690540537121706e-06\n",
      "step: 9520, loss: 3.5929015211877413e-06\n",
      "step: 9521, loss: 9.318751835962757e-06\n",
      "step: 9522, loss: 4.533275932772085e-05\n",
      "step: 9523, loss: 5.483599920808047e-07\n",
      "step: 9524, loss: 0.034304674714803696\n",
      "step: 9525, loss: 4.002966761618154e-06\n",
      "step: 9526, loss: 2.379719626333099e-05\n",
      "step: 9527, loss: 1.7523485666970373e-06\n",
      "step: 9528, loss: 6.498432412627153e-06\n",
      "step: 9529, loss: 1.5917647033347748e-05\n",
      "step: 9530, loss: 0.00104139547329396\n",
      "step: 9531, loss: 6.277220109041082e-06\n",
      "step: 9532, loss: 1.3699414012080524e-05\n",
      "step: 9533, loss: 5.483439963427372e-06\n",
      "step: 9534, loss: 5.3163971642788965e-06\n",
      "step: 9535, loss: 1.1982429896306712e-05\n",
      "step: 9536, loss: 3.952873612433905e-06\n",
      "step: 9537, loss: 8.491723747283686e-06\n",
      "step: 9538, loss: 9.096615031012334e-06\n",
      "step: 9539, loss: 3.3639644243521616e-06\n",
      "step: 9540, loss: 0.04736783355474472\n",
      "step: 9541, loss: 9.880858124233782e-05\n",
      "step: 9542, loss: 4.286690455046482e-06\n",
      "step: 9543, loss: 0.13774973154067993\n",
      "step: 9544, loss: 0.00016467153909616172\n",
      "step: 9545, loss: 1.4642178939539008e-05\n",
      "step: 9546, loss: 2.642216168169398e-05\n",
      "step: 9547, loss: 1.0022453352576122e-05\n",
      "step: 9548, loss: 4.677599463320803e-06\n",
      "step: 9549, loss: 2.5438941975153284e-06\n",
      "step: 9550, loss: 0.00023683952167630196\n",
      "step: 9551, loss: 7.047096914902795e-06\n",
      "step: 9552, loss: 4.660879312723409e-06\n",
      "step: 9553, loss: 1.2588320714712609e-06\n",
      "step: 9554, loss: 6.305504939518869e-05\n",
      "step: 9555, loss: 0.00024467939510941505\n",
      "step: 9556, loss: 3.1470915473619243e-06\n",
      "step: 9557, loss: 2.374549694650341e-06\n",
      "step: 9558, loss: 1.3208227755967528e-06\n",
      "step: 9559, loss: 6.0317111092444975e-06\n",
      "step: 9560, loss: 0.007877480238676071\n",
      "step: 9561, loss: 2.106338070007041e-05\n",
      "step: 9562, loss: 7.2428233579557855e-06\n",
      "step: 9563, loss: 1.1038691809517331e-06\n",
      "step: 9564, loss: 2.4747134830249706e-06\n",
      "step: 9565, loss: 1.301755787608272e-06\n",
      "step: 9566, loss: 8.322150097228587e-05\n",
      "step: 9567, loss: 1.7680418750387616e-05\n",
      "step: 9568, loss: 0.09201826900243759\n",
      "step: 9569, loss: 8.702193099452415e-07\n",
      "step: 9570, loss: 3.259124923715717e-06\n",
      "step: 9571, loss: 3.659630010588444e-06\n",
      "step: 9572, loss: 7.931456821097527e-06\n",
      "step: 9573, loss: 0.0047119683586061\n",
      "step: 9574, loss: 1.8179489416070282e-05\n",
      "step: 9575, loss: 1.6689060657881782e-06\n",
      "step: 9576, loss: 5.4237934818957e-06\n",
      "step: 9577, loss: 1.387579573020048e-06\n",
      "step: 9578, loss: 1.7801574358600192e-05\n",
      "step: 9579, loss: 0.0001689766941126436\n",
      "step: 9580, loss: 4.86370026919758e-07\n",
      "step: 9581, loss: 2.6401366994832642e-05\n",
      "step: 9582, loss: 3.126983210677281e-05\n",
      "step: 9583, loss: 7.912009095889516e-06\n",
      "step: 9584, loss: 1.854873289630632e-06\n",
      "step: 9585, loss: 0.028109397739171982\n",
      "step: 9586, loss: 1.5568600701953983e-06\n",
      "step: 9587, loss: 5.578636319114594e-06\n",
      "step: 9588, loss: 0.0476200208067894\n",
      "step: 9589, loss: 2.1259713321342133e-05\n",
      "step: 9590, loss: 0.00011413257743697613\n",
      "step: 9591, loss: 3.51419794242247e-06\n",
      "step: 9592, loss: 2.581975195425912e-06\n",
      "step: 9593, loss: 2.7532199965207838e-05\n",
      "step: 9594, loss: 3.7330351915443316e-05\n",
      "step: 9595, loss: 1.1324787010380533e-06\n",
      "step: 9596, loss: 7.163613190641627e-06\n",
      "step: 9597, loss: 1.3422885558611597e-06\n",
      "step: 9598, loss: 8.52983066579327e-06\n",
      "step: 9599, loss: 2.5057304355868837e-06\n",
      "step: 9600, loss: 0.00022580262157134712\n",
      "step: 9601, loss: 1.4222501704352908e-05\n",
      "step: 9602, loss: 8.051037184486631e-06\n",
      "step: 9603, loss: 5.225915629125666e-06\n",
      "step: 9604, loss: 5.001873887522379e-06\n",
      "step: 9605, loss: 9.053708708961494e-06\n",
      "step: 9606, loss: 2.4814136850181967e-05\n",
      "step: 9607, loss: 4.194290886516683e-05\n",
      "step: 9608, loss: 8.089017683232669e-06\n",
      "step: 9609, loss: 3.5380444387556054e-06\n",
      "step: 9610, loss: 8.320760116475867e-07\n",
      "step: 9611, loss: 5.678587513102684e-06\n",
      "step: 9612, loss: 2.6582847567624412e-06\n",
      "step: 9613, loss: 1.370892277918756e-06\n",
      "step: 9614, loss: 1.3968246094009373e-05\n",
      "step: 9615, loss: 5.45459351997124e-06\n",
      "step: 9616, loss: 9.345893090539903e-07\n",
      "step: 9617, loss: 2.36267101172416e-06\n",
      "step: 9618, loss: 4.632626223610714e-05\n",
      "step: 9619, loss: 6.944225788174663e-06\n",
      "step: 9620, loss: 9.776200386113487e-06\n",
      "step: 9621, loss: 9.518612205283716e-05\n",
      "step: 9622, loss: 6.42725854049786e-06\n",
      "step: 9623, loss: 1.892999307528953e-06\n",
      "step: 9624, loss: 8.381305633520242e-06\n",
      "step: 9625, loss: 2.8609565561055206e-06\n",
      "step: 9626, loss: 0.00011508788884384558\n",
      "step: 9627, loss: 1.0514152108953567e-06\n",
      "step: 9628, loss: 5.686614531441592e-05\n",
      "step: 9629, loss: 0.0008305519586429\n",
      "step: 9630, loss: 1.4082104826229624e-05\n",
      "step: 9631, loss: 5.338613118510693e-05\n",
      "step: 9632, loss: 3.1948010814630834e-07\n",
      "step: 9633, loss: 4.045864443469327e-06\n",
      "step: 9634, loss: 2.913417802119511e-06\n",
      "step: 9635, loss: 6.1885994000476785e-06\n",
      "step: 9636, loss: 5.216383669903735e-06\n",
      "step: 9637, loss: 3.881156317220302e-06\n",
      "step: 9638, loss: 5.2829809646937065e-06\n",
      "step: 9639, loss: 2.1775751520181075e-05\n",
      "step: 9640, loss: 2.3173952286015265e-06\n",
      "step: 9641, loss: 3.2161797207663767e-06\n",
      "step: 9642, loss: 1.0061146440420998e-06\n",
      "step: 9643, loss: 9.002158549265005e-06\n",
      "step: 9644, loss: 4.470185103855329e-06\n",
      "step: 9645, loss: 1.7189772734127473e-06\n",
      "step: 9646, loss: 3.571427669157856e-06\n",
      "step: 9647, loss: 7.153691967687337e-06\n",
      "step: 9648, loss: 0.06859509646892548\n",
      "step: 9649, loss: 4.911204086965881e-06\n",
      "step: 9650, loss: 1.6688979940227e-06\n",
      "step: 9651, loss: 1.1181710988239502e-06\n",
      "step: 9652, loss: 1.0728768984336057e-06\n",
      "step: 9653, loss: 3.1826937174628256e-06\n",
      "step: 9654, loss: 2.6154075385420583e-06\n",
      "step: 9655, loss: 0.10500049591064453\n",
      "step: 9656, loss: 2.1743469460488996e-06\n",
      "step: 9657, loss: 7.039726187940687e-05\n",
      "step: 9658, loss: 8.916797469282756e-07\n",
      "step: 9659, loss: 3.299591526229051e-06\n",
      "step: 9660, loss: 1.8095723817168619e-06\n",
      "step: 9661, loss: 6.270383892115206e-07\n",
      "step: 9662, loss: 6.476246926467866e-05\n",
      "step: 9663, loss: 5.4570477914239746e-06\n",
      "step: 9664, loss: 1.6903634332265938e-06\n",
      "step: 9665, loss: 1.2901119589514565e-05\n",
      "step: 9666, loss: 7.867776048442465e-07\n",
      "step: 9667, loss: 3.056440391446813e-06\n",
      "step: 9668, loss: 6.3154275267152116e-06\n",
      "step: 9669, loss: 1.5497197125569073e-07\n",
      "step: 9670, loss: 7.325744809349999e-05\n",
      "step: 9671, loss: 2.0122270143474452e-06\n",
      "step: 9672, loss: 0.12211310118436813\n",
      "step: 9673, loss: 2.8681004096142715e-06\n",
      "step: 9674, loss: 6.699540904264722e-07\n",
      "step: 9675, loss: 3.363560972502455e-05\n",
      "step: 9676, loss: 6.267767275858205e-06\n",
      "step: 9677, loss: 1.113391022045107e-06\n",
      "step: 9678, loss: 0.00014114627265371382\n",
      "step: 9679, loss: 2.8854903575847857e-05\n",
      "step: 9680, loss: 2.7703481464413926e-06\n",
      "step: 9681, loss: 2.1119078155606985e-05\n",
      "step: 9682, loss: 2.6374123990535736e-05\n",
      "step: 9683, loss: 4.1672115003166255e-06\n",
      "step: 9684, loss: 4.387348599266261e-05\n",
      "step: 9685, loss: 3.809822374023497e-06\n",
      "step: 9686, loss: 2.571002369222697e-05\n",
      "step: 9687, loss: 2.811433296301402e-05\n",
      "step: 9688, loss: 2.9204891234257957e-06\n",
      "step: 9689, loss: 7.458908839907963e-06\n",
      "step: 9690, loss: 6.971101538510993e-05\n",
      "step: 9691, loss: 7.525745604652911e-05\n",
      "step: 9692, loss: 1.5544518419119413e-06\n",
      "step: 9693, loss: 8.551826613256708e-05\n",
      "step: 9694, loss: 0.04647359251976013\n",
      "step: 9695, loss: 1.6362870155717246e-05\n",
      "step: 9696, loss: 7.349983661697479e-06\n",
      "step: 9697, loss: 6.344047051243251e-06\n",
      "step: 9698, loss: 8.19131855678279e-06\n",
      "step: 9699, loss: 2.154992034775205e-05\n",
      "step: 9700, loss: 2.7131620754516916e-06\n",
      "step: 9701, loss: 1.1539375464053592e-06\n",
      "step: 9702, loss: 1.7189744312418043e-06\n",
      "step: 9703, loss: 5.6573560868855566e-05\n",
      "step: 9704, loss: 1.2564522648972343e-06\n",
      "step: 9705, loss: 2.27222317334963e-05\n",
      "step: 9706, loss: 7.912526052678004e-05\n",
      "step: 9707, loss: 7.259451194840949e-06\n",
      "step: 9708, loss: 2.725798185565509e-05\n",
      "step: 9709, loss: 4.5918309297121596e-06\n",
      "step: 9710, loss: 2.7322487312630983e-06\n",
      "step: 9711, loss: 2.2474232537206262e-05\n",
      "step: 9712, loss: 6.389580562427e-07\n",
      "step: 9713, loss: 8.02478189143585e-06\n",
      "step: 9714, loss: 9.239546670869458e-06\n",
      "step: 9715, loss: 2.1721501980209723e-05\n",
      "step: 9716, loss: 1.3404443961917423e-05\n",
      "step: 9717, loss: 2.9992027066327864e-06\n",
      "step: 9718, loss: 6.190932708705077e-06\n",
      "step: 9719, loss: 2.5028170057339594e-05\n",
      "step: 9720, loss: 5.092825449537486e-05\n",
      "step: 9721, loss: 1.2183060107417987e-06\n",
      "step: 9722, loss: 0.0005047866725362837\n",
      "step: 9723, loss: 5.490607691172045e-06\n",
      "step: 9724, loss: 1.8524829101806972e-06\n",
      "step: 9725, loss: 5.49491960555315e-06\n",
      "step: 9726, loss: 1.3565781955549028e-06\n",
      "step: 9727, loss: 4.617947070073569e-06\n",
      "step: 9728, loss: 4.427209933055565e-05\n",
      "step: 9729, loss: 2.8832526368205436e-05\n",
      "step: 9730, loss: 6.0914902860531583e-05\n",
      "step: 9731, loss: 3.6476294553722255e-06\n",
      "step: 9732, loss: 1.0592889339022804e-05\n",
      "step: 9733, loss: 0.0002929397742263973\n",
      "step: 9734, loss: 0.09258737415075302\n",
      "step: 9735, loss: 0.0014793929876759648\n",
      "step: 9736, loss: 1.055883967637783e-05\n",
      "step: 9737, loss: 4.756312591780443e-06\n",
      "step: 9738, loss: 1.1300901405775221e-06\n",
      "step: 9739, loss: 0.00031680590473115444\n",
      "step: 9740, loss: 4.0777322283247486e-05\n",
      "step: 9741, loss: 2.4868881155271083e-05\n",
      "step: 9742, loss: 1.7056579963536933e-05\n",
      "step: 9743, loss: 1.132479610532755e-06\n",
      "step: 9744, loss: 1.4061953152122442e-05\n",
      "step: 9745, loss: 0.00038699456490576267\n",
      "step: 9746, loss: 4.024222107545938e-06\n",
      "step: 9747, loss: 8.474251444567926e-06\n",
      "step: 9748, loss: 3.5714067507797154e-06\n",
      "step: 9749, loss: 1.5926467312965542e-05\n",
      "step: 9750, loss: 8.836987035465427e-06\n",
      "step: 9751, loss: 4.041344072902575e-05\n",
      "step: 9752, loss: 3.421123437874485e-06\n",
      "step: 9753, loss: 1.563192745379638e-05\n",
      "step: 9754, loss: 0.009202986024320126\n",
      "step: 9755, loss: 3.587299579521641e-05\n",
      "step: 9756, loss: 3.609768828027882e-05\n",
      "step: 9757, loss: 2.1571369870798662e-05\n",
      "step: 9758, loss: 9.569356188876554e-05\n",
      "step: 9759, loss: 2.5354785975650884e-05\n",
      "step: 9760, loss: 1.7531696357764304e-05\n",
      "step: 9761, loss: 0.00017522739653941244\n",
      "step: 9762, loss: 7.39090296519862e-07\n",
      "step: 9763, loss: 1.2326128171480377e-06\n",
      "step: 9764, loss: 6.055786343495129e-07\n",
      "step: 9765, loss: 9.79851665761089e-06\n",
      "step: 9766, loss: 6.692082479275996e-06\n",
      "step: 9767, loss: 1.5282454342013807e-06\n",
      "step: 9768, loss: 0.0001613934146007523\n",
      "step: 9769, loss: 8.70722287800163e-05\n",
      "step: 9770, loss: 2.125144237652421e-05\n",
      "step: 9771, loss: 6.922342436155304e-05\n",
      "step: 9772, loss: 4.422315214469563e-06\n",
      "step: 9773, loss: 2.6598496333463117e-05\n",
      "step: 9774, loss: 1.943885399668943e-05\n",
      "step: 9775, loss: 1.4017457033332903e-05\n",
      "step: 9776, loss: 1.260853332496481e-05\n",
      "step: 9777, loss: 5.1423662625893485e-06\n",
      "step: 9778, loss: 2.631528150232043e-05\n",
      "step: 9779, loss: 3.6856517908745445e-06\n",
      "step: 9780, loss: 3.959981768275611e-06\n",
      "step: 9781, loss: 4.998541044187732e-05\n",
      "step: 9782, loss: 1.5284400433301926e-05\n",
      "step: 9783, loss: 5.507703826879151e-05\n",
      "step: 9784, loss: 3.888455466949381e-06\n",
      "step: 9785, loss: 2.4981600290630013e-05\n",
      "step: 9786, loss: 8.321952918777242e-06\n",
      "step: 9787, loss: 0.013165204785764217\n",
      "step: 9788, loss: 3.5332905099494383e-06\n",
      "step: 9789, loss: 4.689225079346215e-06\n",
      "step: 9790, loss: 1.372408678435022e-05\n",
      "step: 9791, loss: 0.00011518914834596217\n",
      "step: 9792, loss: 8.856093700160272e-06\n",
      "step: 9793, loss: 0.0003558575117494911\n",
      "step: 9794, loss: 0.00033151661045849323\n",
      "step: 9795, loss: 0.01996123231947422\n",
      "step: 9796, loss: 6.382339779520407e-05\n",
      "step: 9797, loss: 5.850654997630045e-05\n",
      "step: 9798, loss: 0.0002566464536357671\n",
      "step: 9799, loss: 0.0001467008114559576\n",
      "step: 9800, loss: 4.093433744856156e-06\n",
      "step: 9801, loss: 8.096238161670044e-06\n",
      "step: 9802, loss: 1.3918486729380675e-05\n",
      "step: 9803, loss: 2.7480033168103546e-05\n",
      "step: 9804, loss: 8.980203347164206e-06\n",
      "step: 9805, loss: 0.0005100616835989058\n",
      "step: 9806, loss: 5.204325589147629e-06\n",
      "step: 9807, loss: 1.5410831110784784e-05\n",
      "step: 9808, loss: 0.06534531712532043\n",
      "step: 9809, loss: 4.362850631878246e-06\n",
      "step: 9810, loss: 1.0737375305325259e-05\n",
      "step: 9811, loss: 7.93638355389703e-06\n",
      "step: 9812, loss: 8.576539403293282e-05\n",
      "step: 9813, loss: 7.6169289968675e-06\n",
      "step: 9814, loss: 0.036076292395591736\n",
      "step: 9815, loss: 2.4627659058751306e-06\n",
      "step: 9816, loss: 8.972755495051388e-06\n",
      "step: 9817, loss: 3.0603554478148e-05\n",
      "step: 9818, loss: 7.653199531887367e-07\n",
      "step: 9819, loss: 1.5042031009215862e-05\n",
      "step: 9820, loss: 1.5616259361195262e-06\n",
      "step: 9821, loss: 2.4628063783893595e-06\n",
      "step: 9822, loss: 3.2018738238548394e-06\n",
      "step: 9823, loss: 0.0014220619341358542\n",
      "step: 9824, loss: 6.255476819205796e-06\n",
      "step: 9825, loss: 2.3816186512704007e-05\n",
      "step: 9826, loss: 4.043466105940752e-06\n",
      "step: 9827, loss: 1.7465450582676567e-05\n",
      "step: 9828, loss: 0.036770280450582504\n",
      "step: 9829, loss: 0.07817593216896057\n",
      "step: 9830, loss: 3.542819740687264e-06\n",
      "step: 9831, loss: 3.88416119676549e-05\n",
      "step: 9832, loss: 2.6967494704877026e-05\n",
      "step: 9833, loss: 5.8362013078294694e-05\n",
      "step: 9834, loss: 8.139013516483828e-06\n",
      "step: 9835, loss: 6.074599696148653e-06\n",
      "step: 9836, loss: 1.0038525033451151e-05\n",
      "step: 9837, loss: 3.137482053716667e-06\n",
      "step: 9838, loss: 0.0942203551530838\n",
      "step: 9839, loss: 4.324713700043503e-06\n",
      "step: 9840, loss: 4.24608606408583e-06\n",
      "step: 9841, loss: 2.0498211597441696e-05\n",
      "step: 9842, loss: 7.0616169978166e-06\n",
      "step: 9843, loss: 4.222266852593748e-06\n",
      "step: 9844, loss: 1.807185753932572e-06\n",
      "step: 9845, loss: 1.8024226164925494e-06\n",
      "step: 9846, loss: 0.06799038499593735\n",
      "step: 9847, loss: 2.8756920073647052e-05\n",
      "step: 9848, loss: 3.2185926102101803e-06\n",
      "step: 9849, loss: 3.2245927286567166e-05\n",
      "step: 9850, loss: 7.057151947265083e-07\n",
      "step: 9851, loss: 3.70726183973602e-06\n",
      "step: 9852, loss: 9.304534614784643e-06\n",
      "step: 9853, loss: 8.2913056758116e-06\n",
      "step: 9854, loss: 2.2721033019479364e-06\n",
      "step: 9855, loss: 3.642988758656429e-06\n",
      "step: 9856, loss: 3.4975337257492356e-06\n",
      "step: 9857, loss: 7.200206368906947e-07\n",
      "step: 9858, loss: 1.3147787285561208e-05\n",
      "step: 9859, loss: 7.858857861720026e-05\n",
      "step: 9860, loss: 8.873351362126414e-06\n",
      "step: 9861, loss: 1.009407606034074e-05\n",
      "step: 9862, loss: 2.57010105997324e-06\n",
      "step: 9863, loss: 2.2356030967785046e-05\n",
      "step: 9864, loss: 7.332751920330338e-06\n",
      "step: 9865, loss: 4.637126494344557e-06\n",
      "step: 9866, loss: 3.9705111703369766e-05\n",
      "step: 9867, loss: 1.461485567233467e-06\n",
      "step: 9868, loss: 1.9241109839640558e-05\n",
      "step: 9869, loss: 0.15062125027179718\n",
      "step: 9870, loss: 1.4686395388707751e-06\n",
      "step: 9871, loss: 0.00010154801566386595\n",
      "step: 9872, loss: 1.0871814311030903e-06\n",
      "step: 9873, loss: 5.006778565075365e-07\n",
      "step: 9874, loss: 9.566981316311285e-05\n",
      "step: 9875, loss: 2.86700433207443e-05\n",
      "step: 9876, loss: 0.0001900222123367712\n",
      "step: 9877, loss: 6.034163106960477e-06\n",
      "step: 9878, loss: 5.0399462452332955e-06\n",
      "step: 9879, loss: 7.180338798207231e-06\n",
      "step: 9880, loss: 1.4722381820320152e-05\n",
      "step: 9881, loss: 1.1992392501269933e-06\n",
      "step: 9882, loss: 1.8334076230530627e-06\n",
      "step: 9883, loss: 4.694358722190373e-06\n",
      "step: 9884, loss: 1.09136090031825e-05\n",
      "step: 9885, loss: 1.3762784874415956e-05\n",
      "step: 9886, loss: 4.143421392655e-06\n",
      "step: 9887, loss: 8.751045970711857e-05\n",
      "step: 9888, loss: 2.891992608056171e-06\n",
      "step: 9889, loss: 3.86697092835675e-06\n",
      "step: 9890, loss: 1.9240035271650413e-06\n",
      "step: 9891, loss: 6.415713869500905e-05\n",
      "step: 9892, loss: 0.0001882152573671192\n",
      "step: 9893, loss: 7.583657861687243e-06\n",
      "step: 9894, loss: 0.02708812803030014\n",
      "step: 9895, loss: 2.527133574403706e-06\n",
      "step: 9896, loss: 2.1219213408585347e-07\n",
      "step: 9897, loss: 3.006309952979791e-06\n",
      "step: 9898, loss: 0.14747999608516693\n",
      "step: 9899, loss: 2.2601373075303854e-06\n",
      "step: 9900, loss: 3.5165994631825015e-06\n",
      "step: 9901, loss: 4.74434318675776e-06\n",
      "step: 9902, loss: 2.133814859917038e-06\n",
      "step: 9903, loss: 2.3054467419569846e-06\n",
      "step: 9904, loss: 4.427057774591958e-06\n",
      "step: 9905, loss: 4.803946922038449e-06\n",
      "step: 9906, loss: 8.572852493671235e-06\n",
      "step: 9907, loss: 7.557080243714154e-05\n",
      "step: 9908, loss: 0.00010993275645887479\n",
      "step: 9909, loss: 2.6869969588005915e-05\n",
      "step: 9910, loss: 3.232806875530514e-06\n",
      "step: 9911, loss: 5.8180066844215617e-05\n",
      "step: 9912, loss: 7.116472261259332e-06\n",
      "step: 9913, loss: 3.3782810078264447e-06\n",
      "step: 9914, loss: 3.182804221069091e-06\n",
      "step: 9915, loss: 3.0468927434412763e-06\n",
      "step: 9916, loss: 7.120669579308014e-06\n",
      "step: 9917, loss: 4.186282239970751e-06\n",
      "step: 9918, loss: 4.246086973580532e-06\n",
      "step: 9919, loss: 4.1606846934882924e-05\n",
      "step: 9920, loss: 2.7059857075073523e-06\n",
      "step: 9921, loss: 1.7857070133686648e-06\n",
      "step: 9922, loss: 9.20290347039554e-07\n",
      "step: 9923, loss: 1.7881170606415253e-06\n",
      "step: 9924, loss: 6.656020559603348e-06\n",
      "step: 9925, loss: 3.65729147233651e-06\n",
      "step: 9926, loss: 6.508803949145658e-07\n",
      "step: 9927, loss: 1.509741741756443e-05\n",
      "step: 9928, loss: 1.723168679745868e-05\n",
      "step: 9929, loss: 1.5425470110130846e-06\n",
      "step: 9930, loss: 1.704670125945995e-06\n",
      "step: 9931, loss: 3.6261644709156826e-06\n",
      "step: 9932, loss: 1.8143509805668145e-06\n",
      "step: 9933, loss: 4.322338099882472e-06\n",
      "step: 9934, loss: 0.044072333723306656\n",
      "step: 9935, loss: 6.181947355798911e-06\n",
      "step: 9936, loss: 0.10121924430131912\n",
      "step: 9937, loss: 1.23975860333303e-06\n",
      "step: 9938, loss: 3.944219133700244e-05\n",
      "step: 9939, loss: 2.1691672372980975e-05\n",
      "step: 9940, loss: 2.9706143322982825e-06\n",
      "step: 9941, loss: 3.24240795634978e-06\n",
      "step: 9942, loss: 2.4338212824659422e-05\n",
      "step: 9943, loss: 5.242640781943919e-06\n",
      "step: 9944, loss: 6.362365638779011e-06\n",
      "step: 9945, loss: 7.113532774383202e-06\n",
      "step: 9946, loss: 9.560536682329257e-07\n",
      "step: 9947, loss: 2.0336799479991896e-06\n",
      "step: 9948, loss: 1.4257316252042074e-06\n",
      "step: 9949, loss: 0.05841556191444397\n",
      "step: 9950, loss: 0.0003103851922787726\n",
      "step: 9951, loss: 1.4114275472820736e-06\n",
      "step: 9952, loss: 1.7881257008411922e-06\n",
      "step: 9953, loss: 2.1505061340576503e-06\n",
      "step: 9954, loss: 7.897723116911948e-05\n",
      "step: 9955, loss: 9.103905540541746e-06\n",
      "step: 9956, loss: 1.2268817954463884e-05\n",
      "step: 9957, loss: 1.9287779196019983e-06\n",
      "step: 9958, loss: 5.912765459470393e-07\n",
      "step: 9959, loss: 4.434575373579719e-07\n",
      "step: 9960, loss: 5.395270363806048e-06\n",
      "step: 9961, loss: 4.0601171349408105e-06\n",
      "step: 9962, loss: 1.3899740451961407e-06\n",
      "step: 9963, loss: 6.78038577461848e-06\n",
      "step: 9964, loss: 1.8572530962046585e-06\n",
      "step: 9965, loss: 0.016754496842622757\n",
      "step: 9966, loss: 2.472358119121054e-06\n",
      "step: 9967, loss: 2.1934131382295163e-06\n",
      "step: 9968, loss: 7.750438271614257e-06\n",
      "step: 9969, loss: 9.465175594414177e-07\n",
      "step: 9970, loss: 1.342271730209177e-06\n",
      "step: 9971, loss: 3.9338971191682504e-07\n",
      "step: 9972, loss: 1.8715587657425203e-06\n",
      "step: 9973, loss: 4.219850325171137e-06\n",
      "step: 9974, loss: 4.347726280684583e-05\n",
      "step: 9975, loss: 1.118178147407889e-06\n",
      "step: 9976, loss: 5.650483103636361e-07\n",
      "step: 9977, loss: 4.8202084144577384e-05\n",
      "step: 9978, loss: 2.7894029699382372e-06\n",
      "step: 9979, loss: 7.821268809493631e-06\n",
      "step: 9980, loss: 7.20020921107789e-07\n",
      "step: 9981, loss: 2.524816409277264e-06\n",
      "step: 9982, loss: 2.8418953661457635e-06\n",
      "step: 9983, loss: 1.70466489635146e-06\n",
      "step: 9984, loss: 9.464323738939129e-06\n",
      "step: 9985, loss: 1.258833663086989e-06\n",
      "step: 9986, loss: 1.8930253418147913e-06\n",
      "step: 9987, loss: 1.0204158797932905e-06\n",
      "step: 9988, loss: 6.2841340877639595e-06\n",
      "step: 9989, loss: 1.0684403605409898e-05\n",
      "step: 9990, loss: 1.517608961876249e-05\n",
      "step: 9991, loss: 5.373490239435341e-06\n",
      "step: 9992, loss: 2.5749156407073315e-07\n",
      "step: 9993, loss: 2.9515736059693154e-06\n",
      "step: 9994, loss: 7.02999113855185e-06\n",
      "step: 9995, loss: 7.013865979388356e-05\n",
      "step: 9996, loss: 1.3805631169816479e-05\n",
      "step: 9997, loss: 3.2901091344683664e-06\n",
      "step: 9998, loss: 8.598630302003585e-06\n",
      "step: 9999, loss: 2.7417488581704674e-06\n",
      "step: 10000, loss: 8.153884323292004e-07\n",
      "step: 10001, loss: 1.3604634659714065e-05\n",
      "step: 10002, loss: 3.311034379294142e-05\n",
      "step: 10003, loss: 8.153217095241416e-06\n",
      "step: 10004, loss: 1.6426881757070078e-06\n",
      "step: 10005, loss: 9.468444659432862e-06\n",
      "step: 10006, loss: 1.1522073691594414e-05\n",
      "step: 10007, loss: 2.1171285879972856e-06\n",
      "step: 10008, loss: 8.225392207350524e-07\n",
      "step: 10009, loss: 8.229655577451922e-06\n",
      "step: 10010, loss: 3.4087232052115723e-05\n",
      "step: 10011, loss: 1.3661282309840317e-06\n",
      "step: 10012, loss: 8.463783274237358e-07\n",
      "step: 10013, loss: 0.0036849381867796183\n",
      "step: 10014, loss: 2.7629965188680217e-05\n",
      "step: 10015, loss: 8.606871233496349e-07\n",
      "step: 10016, loss: 5.232976036495529e-06\n",
      "step: 10017, loss: 0.004426156170666218\n",
      "step: 10018, loss: 6.318034593277844e-07\n",
      "step: 10019, loss: 8.916808269532339e-07\n",
      "step: 10020, loss: 0.10119631141424179\n",
      "step: 10021, loss: 1.4281141602623393e-06\n",
      "step: 10022, loss: 2.6161904315813445e-05\n",
      "step: 10023, loss: 1.025192887027515e-06\n",
      "step: 10024, loss: 4.1960797716456e-06\n",
      "step: 10025, loss: 4.436656126927119e-06\n",
      "step: 10026, loss: 2.419357406324707e-05\n",
      "step: 10027, loss: 1.2087688219253323e-06\n",
      "step: 10028, loss: 9.5843358849379e-07\n",
      "step: 10029, loss: 3.690621269925032e-06\n",
      "step: 10030, loss: 6.556476250807464e-07\n",
      "step: 10031, loss: 4.195852852717508e-06\n",
      "step: 10032, loss: 6.653653599641984e-06\n",
      "step: 10033, loss: 1.1330616871418897e-05\n",
      "step: 10034, loss: 1.585134850756731e-05\n",
      "step: 10035, loss: 3.7238228287606034e-06\n",
      "step: 10036, loss: 2.2768522285332438e-06\n",
      "step: 10037, loss: 2.176742782467045e-06\n",
      "step: 10038, loss: 1.2373834579193499e-06\n",
      "step: 10039, loss: 1.8918684872915037e-05\n",
      "step: 10040, loss: 2.8943563847860787e-06\n",
      "step: 10041, loss: 9.821314961300232e-06\n",
      "step: 10042, loss: 3.4069123557856074e-06\n",
      "step: 10043, loss: 1.8238500842926442e-06\n",
      "step: 10044, loss: 0.00024913993547670543\n",
      "step: 10045, loss: 2.0670581761805806e-06\n",
      "step: 10046, loss: 1.2275088010937907e-05\n",
      "step: 10047, loss: 7.918794290162623e-05\n",
      "step: 10048, loss: 8.869142789080797e-07\n",
      "step: 10049, loss: 1.1141041795781348e-05\n",
      "step: 10050, loss: 2.7182359190192074e-05\n",
      "step: 10051, loss: 9.561691513226833e-06\n",
      "step: 10052, loss: 0.08674092590808868\n",
      "step: 10053, loss: 0.073999784886837\n",
      "step: 10054, loss: 2.198502807004843e-05\n",
      "step: 10055, loss: 1.3173224942875095e-05\n",
      "step: 10056, loss: 2.4736411432968453e-05\n",
      "step: 10057, loss: 3.5858844057656825e-05\n",
      "step: 10058, loss: 6.017513442202471e-05\n",
      "step: 10059, loss: 5.841240522386215e-07\n",
      "step: 10060, loss: 1.5377737554445048e-06\n",
      "step: 10061, loss: 2.624897661007708e-06\n",
      "step: 10062, loss: 6.65184700210375e-07\n",
      "step: 10063, loss: 1.0633397096171393e-06\n",
      "step: 10064, loss: 0.00012050992518197745\n",
      "step: 10065, loss: 2.663086661414127e-06\n",
      "step: 10066, loss: 2.8370602649374632e-06\n",
      "step: 10067, loss: 1.9454660105111543e-06\n",
      "step: 10068, loss: 1.6689087942722836e-06\n",
      "step: 10069, loss: 1.545699342386797e-05\n",
      "step: 10070, loss: 7.986955665728601e-07\n",
      "step: 10071, loss: 0.042784061282873154\n",
      "step: 10072, loss: 6.04607112109079e-06\n",
      "step: 10073, loss: 9.679755521574407e-07\n",
      "step: 10074, loss: 3.3520198030601023e-06\n",
      "step: 10075, loss: 0.1333390325307846\n",
      "step: 10076, loss: 6.599045718758134e-06\n",
      "step: 10077, loss: 1.022805918182712e-06\n",
      "step: 10078, loss: 2.3125905954657355e-06\n",
      "step: 10079, loss: 7.806677604094148e-05\n",
      "step: 10080, loss: 1.1979681403317954e-05\n",
      "step: 10081, loss: 5.006780838812119e-07\n",
      "step: 10082, loss: 2.6702180093707284e-06\n",
      "step: 10083, loss: 2.4342173219338292e-06\n",
      "step: 10084, loss: 1.1600203833950218e-05\n",
      "step: 10085, loss: 6.42968007014133e-06\n",
      "step: 10086, loss: 1.030440216709394e-05\n",
      "step: 10087, loss: 0.0332796536386013\n",
      "step: 10088, loss: 0.00036739849019795656\n",
      "step: 10089, loss: 1.2815920854336582e-05\n",
      "step: 10090, loss: 7.803039807185996e-06\n",
      "step: 10091, loss: 0.00010795612615766004\n",
      "step: 10092, loss: 3.203692540409975e-05\n",
      "step: 10093, loss: 9.109107850235887e-06\n",
      "step: 10094, loss: 4.2269989535270724e-06\n",
      "step: 10095, loss: 2.181456693506334e-05\n",
      "step: 10096, loss: 2.1385699255915824e-06\n",
      "step: 10097, loss: 1.0752921298262663e-05\n",
      "step: 10098, loss: 4.229930345900357e-05\n",
      "step: 10099, loss: 9.511588359600864e-06\n",
      "step: 10100, loss: 1.9987150153610855e-05\n",
      "step: 10101, loss: 0.0004386655637063086\n",
      "step: 10102, loss: 1.5115574569790624e-06\n",
      "step: 10103, loss: 6.484709501819452e-06\n",
      "step: 10104, loss: 0.06280412524938583\n",
      "step: 10105, loss: 0.025141078978776932\n",
      "step: 10106, loss: 6.2343483477889095e-06\n",
      "step: 10107, loss: 4.980419817002257e-06\n",
      "step: 10108, loss: 7.147022188291885e-06\n",
      "step: 10109, loss: 0.00028501826454885304\n",
      "step: 10110, loss: 4.9801823479356244e-05\n",
      "step: 10111, loss: 1.8596406334836502e-06\n",
      "step: 10112, loss: 4.753832854476059e-06\n",
      "step: 10113, loss: 1.052040715876501e-05\n",
      "step: 10114, loss: 4.470276962820208e-06\n",
      "step: 10115, loss: 3.0218816391425207e-05\n",
      "step: 10116, loss: 0.00019788196368608624\n",
      "step: 10117, loss: 1.1730110145435901e-06\n",
      "step: 10118, loss: 0.08139898627996445\n",
      "step: 10119, loss: 7.552257557108533e-06\n",
      "step: 10120, loss: 3.6548422031046357e-06\n",
      "step: 10121, loss: 8.463801464131393e-07\n",
      "step: 10122, loss: 2.739385763561586e-06\n",
      "step: 10123, loss: 0.045382410287857056\n",
      "step: 10124, loss: 7.980899681570008e-06\n",
      "step: 10125, loss: 1.5473186749659362e-06\n",
      "step: 10126, loss: 7.776102393108886e-06\n",
      "step: 10127, loss: 1.3074858543404844e-05\n",
      "step: 10128, loss: 1.9478113699733512e-06\n",
      "step: 10129, loss: 3.2757586723164422e-06\n",
      "step: 10130, loss: 5.092293577035889e-06\n",
      "step: 10131, loss: 2.717964093790215e-07\n",
      "step: 10132, loss: 2.435531496303156e-05\n",
      "step: 10133, loss: 1.726132609292108e-06\n",
      "step: 10134, loss: 8.841724593366962e-06\n",
      "step: 10135, loss: 3.383096191100776e-05\n",
      "step: 10136, loss: 7.4238237175450195e-06\n",
      "step: 10137, loss: 6.211103027453646e-05\n",
      "step: 10138, loss: 2.1290200038492912e-06\n",
      "step: 10139, loss: 4.708528649643995e-06\n",
      "step: 10140, loss: 0.12572340667247772\n",
      "step: 10141, loss: 4.601464809184108e-07\n",
      "step: 10142, loss: 1.5735226952529047e-06\n",
      "step: 10143, loss: 1.4706104593642522e-05\n",
      "step: 10144, loss: 1.897796209959779e-06\n",
      "step: 10145, loss: 1.3756632597505813e-06\n",
      "step: 10146, loss: 1.1633280337264296e-05\n",
      "step: 10147, loss: 0.017501723021268845\n",
      "step: 10148, loss: 1.7547464494782616e-06\n",
      "step: 10149, loss: 2.4365187982766656e-06\n",
      "step: 10150, loss: 0.0726635605096817\n",
      "step: 10151, loss: 2.4556800326536177e-06\n",
      "step: 10152, loss: 3.0731116567039862e-06\n",
      "step: 10153, loss: 0.0010965680703520775\n",
      "step: 10154, loss: 3.2895080948947e-05\n",
      "step: 10155, loss: 1.4734166597918374e-06\n",
      "step: 10156, loss: 9.703491059553926e-07\n",
      "step: 10157, loss: 4.527446890278952e-06\n",
      "step: 10158, loss: 2.7854563086293638e-05\n",
      "step: 10159, loss: 0.00011951154738198966\n",
      "step: 10160, loss: 6.7279033828526735e-06\n",
      "step: 10161, loss: 3.7955423977109604e-06\n",
      "step: 10162, loss: 2.07183893508045e-06\n",
      "step: 10163, loss: 1.1634750762823387e-06\n",
      "step: 10164, loss: 9.155158409157593e-07\n",
      "step: 10165, loss: 1.188353962788824e-05\n",
      "step: 10166, loss: 2.9055190680082887e-05\n",
      "step: 10167, loss: 6.627617494814331e-06\n",
      "step: 10168, loss: 2.7479289201437496e-05\n",
      "step: 10169, loss: 9.388410035171546e-06\n",
      "step: 10170, loss: 5.2446139306994155e-06\n",
      "step: 10171, loss: 4.816034788746038e-07\n",
      "step: 10172, loss: 1.79765333996329e-06\n",
      "step: 10173, loss: 1.9216336113458965e-06\n",
      "step: 10174, loss: 9.703550176709541e-07\n",
      "step: 10175, loss: 1.9097087715636007e-06\n",
      "step: 10176, loss: 3.0682660963066155e-06\n",
      "step: 10177, loss: 2.6320776669308543e-06\n",
      "step: 10178, loss: 1.5403828001581132e-05\n",
      "step: 10179, loss: 4.699068540503504e-06\n",
      "step: 10180, loss: 5.681190032191807e-06\n",
      "step: 10181, loss: 4.7942799028533045e-06\n",
      "step: 10182, loss: 2.285264417878352e-05\n",
      "step: 10183, loss: 1.4400372947420692e-06\n",
      "step: 10184, loss: 9.580957339494489e-06\n",
      "step: 10185, loss: 1.1084892321377993e-05\n",
      "step: 10186, loss: 3.8477181078633294e-05\n",
      "step: 10187, loss: 0.00013307771587278694\n",
      "step: 10188, loss: 4.611009717336856e-05\n",
      "step: 10189, loss: 3.0802746096014744e-06\n",
      "step: 10190, loss: 1.4662634839623934e-06\n",
      "step: 10191, loss: 2.0759734979947098e-05\n",
      "step: 10192, loss: 1.2562968549900688e-05\n",
      "step: 10193, loss: 3.3378518082827213e-07\n",
      "step: 10194, loss: 6.236304216145072e-06\n",
      "step: 10195, loss: 5.28789223608328e-06\n",
      "step: 10196, loss: 1.069856807589531e-05\n",
      "step: 10197, loss: 1.3565938843385084e-06\n",
      "step: 10198, loss: 5.638410584651865e-06\n",
      "step: 10199, loss: 1.5170230653893668e-05\n",
      "step: 10200, loss: 5.81740152938437e-07\n",
      "step: 10201, loss: 0.014141765423119068\n",
      "step: 10202, loss: 5.986001724522794e-06\n",
      "step: 10203, loss: 1.621632145543117e-05\n",
      "step: 10204, loss: 4.987261490896344e-06\n",
      "step: 10205, loss: 1.2779154303643736e-06\n",
      "step: 10206, loss: 1.0281451977789402e-05\n",
      "step: 10207, loss: 2.0384507024573395e-06\n",
      "step: 10208, loss: 4.022943903692067e-05\n",
      "step: 10209, loss: 1.4004924196342472e-05\n",
      "step: 10210, loss: 1.6045433994804625e-06\n",
      "step: 10211, loss: 6.150437911855988e-06\n",
      "step: 10212, loss: 3.9100569892980275e-07\n",
      "step: 10213, loss: 0.05258088931441307\n",
      "step: 10214, loss: 1.926402546814643e-06\n",
      "step: 10215, loss: 0.0040992628782987595\n",
      "step: 10216, loss: 5.847765805810923e-06\n",
      "step: 10217, loss: 2.0575271264533512e-06\n",
      "step: 10218, loss: 2.1971005480736494e-05\n",
      "step: 10219, loss: 6.329632014967501e-06\n",
      "step: 10220, loss: 1.6522110399819212e-06\n",
      "step: 10221, loss: 1.90494404250785e-06\n",
      "step: 10222, loss: 1.5239417734846938e-05\n",
      "step: 10223, loss: 8.894538041204214e-06\n",
      "step: 10224, loss: 4.827703378396109e-06\n",
      "step: 10225, loss: 1.8098833606927656e-05\n",
      "step: 10226, loss: 1.6951363477346604e-06\n",
      "step: 10227, loss: 3.0683368095196784e-06\n",
      "step: 10228, loss: 1.6784531453595264e-06\n",
      "step: 10229, loss: 0.00010650850890669972\n",
      "step: 10230, loss: 2.8526828828034922e-05\n",
      "step: 10231, loss: 3.6428652947506635e-06\n",
      "step: 10232, loss: 3.027906814168091e-07\n",
      "step: 10233, loss: 2.6177544896199834e-06\n",
      "step: 10234, loss: 8.038741725613363e-06\n",
      "step: 10235, loss: 4.064725726493634e-05\n",
      "step: 10236, loss: 8.72448526934022e-06\n",
      "step: 10237, loss: 1.7475877029937692e-06\n",
      "step: 10238, loss: 2.2676089429296553e-05\n",
      "step: 10239, loss: 0.00011887038999702781\n",
      "step: 10240, loss: 7.963077450767742e-07\n",
      "step: 10241, loss: 1.7138470866484568e-05\n",
      "step: 10242, loss: 1.74120650626719e-05\n",
      "step: 10243, loss: 3.7947145756334066e-05\n",
      "step: 10244, loss: 3.499907052173512e-06\n",
      "step: 10245, loss: 8.47533592605032e-05\n",
      "step: 10246, loss: 7.311861281777965e-06\n",
      "step: 10247, loss: 2.9420239116006996e-06\n",
      "step: 10248, loss: 0.034756340086460114\n",
      "step: 10249, loss: 4.6491527427861e-07\n",
      "step: 10250, loss: 9.846629609455704e-07\n",
      "step: 10251, loss: 3.118871245533228e-05\n",
      "step: 10252, loss: 0.00015861507563386112\n",
      "step: 10253, loss: 2.1609097530017607e-05\n",
      "step: 10254, loss: 1.3494368431565817e-06\n",
      "step: 10255, loss: 1.110790890379576e-05\n",
      "step: 10256, loss: 1.4543513771059224e-07\n",
      "step: 10257, loss: 4.513793464866467e-05\n",
      "step: 10258, loss: 1.3294741620484274e-05\n",
      "step: 10259, loss: 5.590693945123348e-06\n",
      "step: 10260, loss: 7.53215717850253e-05\n",
      "step: 10261, loss: 8.832832463667728e-06\n",
      "step: 10262, loss: 1.8596178961161058e-06\n",
      "step: 10263, loss: 5.790720479126321e-06\n",
      "step: 10264, loss: 4.127242573304102e-05\n",
      "step: 10265, loss: 3.433006440900499e-06\n",
      "step: 10266, loss: 4.7250759962480515e-06\n",
      "step: 10267, loss: 2.129054337274283e-06\n",
      "step: 10268, loss: 1.3947407069281326e-06\n",
      "step: 10269, loss: 9.903152204060461e-06\n",
      "step: 10270, loss: 3.3448811791458866e-06\n",
      "step: 10271, loss: 2.1004441350669367e-06\n",
      "step: 10272, loss: 2.3841434995119926e-06\n",
      "step: 10273, loss: 7.70755377743626e-06\n",
      "step: 10274, loss: 9.512792757959687e-07\n",
      "step: 10275, loss: 2.0545478037092835e-05\n",
      "step: 10276, loss: 2.5271938284276985e-06\n",
      "step: 10277, loss: 9.525110363028944e-05\n",
      "step: 10278, loss: 4.9207196752831805e-06\n",
      "step: 10279, loss: 7.677051598875551e-07\n",
      "step: 10280, loss: 1.0411480616312474e-05\n",
      "step: 10281, loss: 4.147221625316888e-05\n",
      "step: 10282, loss: 0.006278190761804581\n",
      "step: 10283, loss: 2.706013674469432e-06\n",
      "step: 10284, loss: 8.164760401996318e-06\n",
      "step: 10285, loss: 2.7727487577067222e-06\n",
      "step: 10286, loss: 0.0022408480290323496\n",
      "step: 10287, loss: 1.3316400327312294e-05\n",
      "step: 10288, loss: 1.2922170071760775e-06\n",
      "step: 10289, loss: 2.4699379537196364e-06\n",
      "step: 10290, loss: 3.113374987151474e-05\n",
      "step: 10291, loss: 1.299363475482096e-06\n",
      "step: 10292, loss: 1.5783200524310814e-06\n",
      "step: 10293, loss: 2.22650542127667e-05\n",
      "step: 10294, loss: 9.456989937461913e-05\n",
      "step: 10295, loss: 5.7027900766115636e-06\n",
      "step: 10296, loss: 6.429303903132677e-05\n",
      "step: 10297, loss: 5.46339615539182e-05\n",
      "step: 10298, loss: 3.248482971685007e-05\n",
      "step: 10299, loss: 1.673682959335565e-06\n",
      "step: 10300, loss: 1.4281090443546418e-06\n",
      "step: 10301, loss: 1.0312836820958182e-05\n",
      "step: 10302, loss: 1.581641481607221e-05\n",
      "step: 10303, loss: 7.843929097361979e-07\n",
      "step: 10304, loss: 1.2969736644663499e-06\n",
      "step: 10305, loss: 1.4400366126210429e-06\n",
      "step: 10306, loss: 3.609475015764474e-06\n",
      "step: 10307, loss: 0.1419535130262375\n",
      "step: 10308, loss: 0.007669935002923012\n",
      "step: 10309, loss: 2.6129862362722633e-06\n",
      "step: 10310, loss: 2.7536877951206407e-06\n",
      "step: 10311, loss: 2.431776465527946e-06\n",
      "step: 10312, loss: 2.169577328459127e-06\n",
      "step: 10313, loss: 3.035021563846385e-06\n",
      "step: 10314, loss: 3.3004689612425864e-05\n",
      "step: 10315, loss: 6.64664912619628e-06\n",
      "step: 10316, loss: 4.145987531956052e-06\n",
      "step: 10317, loss: 1.2779117923855665e-06\n",
      "step: 10318, loss: 3.609427267292631e-06\n",
      "step: 10319, loss: 4.803955562238116e-06\n",
      "step: 10320, loss: 0.1752985119819641\n",
      "step: 10321, loss: 3.893248049280373e-06\n",
      "step: 10322, loss: 0.044823288917541504\n",
      "step: 10323, loss: 3.5976329400000395e-06\n",
      "step: 10324, loss: 3.020586518687196e-06\n",
      "step: 10325, loss: 9.298223631049041e-07\n",
      "step: 10326, loss: 1.5919067664071918e-05\n",
      "step: 10327, loss: 2.2339352199196583e-06\n",
      "step: 10328, loss: 2.887199798351503e-06\n",
      "step: 10329, loss: 0.05541252717375755\n",
      "step: 10330, loss: 1.5282462300092448e-06\n",
      "step: 10331, loss: 0.0001381175097776577\n",
      "step: 10332, loss: 5.094752395962132e-06\n",
      "step: 10333, loss: 8.003209586604498e-06\n",
      "step: 10334, loss: 6.581849902431713e-06\n",
      "step: 10335, loss: 8.058515277298284e-07\n",
      "step: 10336, loss: 2.094771662086714e-05\n",
      "step: 10337, loss: 0.00016416906146332622\n",
      "step: 10338, loss: 1.5997521813915228e-06\n",
      "step: 10339, loss: 1.739199251460377e-05\n",
      "step: 10340, loss: 5.776631951448508e-06\n",
      "step: 10341, loss: 1.4400372947420692e-06\n",
      "step: 10342, loss: 3.993345671915449e-06\n",
      "step: 10343, loss: 1.689745840849355e-05\n",
      "step: 10344, loss: 4.620469553628936e-06\n",
      "step: 10345, loss: 1.838487332861405e-05\n",
      "step: 10346, loss: 1.5157867892412469e-05\n",
      "step: 10347, loss: 1.1136489774798974e-05\n",
      "step: 10348, loss: 7.19135714462027e-05\n",
      "step: 10349, loss: 8.457707735942677e-06\n",
      "step: 10350, loss: 1.8119337710231775e-06\n",
      "step: 10351, loss: 5.555117468247772e-07\n",
      "step: 10352, loss: 6.496242349385284e-06\n",
      "step: 10353, loss: 1.3708890946872998e-06\n",
      "step: 10354, loss: 0.06119655445218086\n",
      "step: 10355, loss: 2.2588383217225783e-05\n",
      "step: 10356, loss: 2.9014493065915303e-06\n",
      "step: 10357, loss: 8.869113798937178e-07\n",
      "step: 10358, loss: 5.755255642725388e-06\n",
      "step: 10359, loss: 0.03885402902960777\n",
      "step: 10360, loss: 0.00034883845364674926\n",
      "step: 10361, loss: 1.397321557305986e-05\n",
      "step: 10362, loss: 2.9753618946415372e-06\n",
      "step: 10363, loss: 6.103499003984325e-07\n",
      "step: 10364, loss: 1.1146756150992587e-05\n",
      "step: 10365, loss: 1.849696673161816e-05\n",
      "step: 10366, loss: 1.2214843081892468e-05\n",
      "step: 10367, loss: 7.518249185523018e-05\n",
      "step: 10368, loss: 5.106698154122569e-06\n",
      "step: 10369, loss: 2.655908429005649e-05\n",
      "step: 10370, loss: 4.26520000473829e-06\n",
      "step: 10371, loss: 2.722680847000447e-06\n",
      "step: 10372, loss: 2.2816116143076215e-06\n",
      "step: 10373, loss: 7.71447867009556e-06\n",
      "step: 10374, loss: 1.3375143907978781e-06\n",
      "step: 10375, loss: 1.0943344932456966e-06\n",
      "step: 10376, loss: 8.79861181601882e-06\n",
      "step: 10377, loss: 3.471662421361543e-05\n",
      "step: 10378, loss: 3.3591754799999762e-06\n",
      "step: 10379, loss: 1.0079800631501712e-05\n",
      "step: 10380, loss: 0.1000392884016037\n",
      "step: 10381, loss: 3.6450583138503134e-05\n",
      "step: 10382, loss: 1.447915656171972e-05\n",
      "step: 10383, loss: 0.011722931638360023\n",
      "step: 10384, loss: 6.939503236935707e-06\n",
      "step: 10385, loss: 8.617248386144638e-05\n",
      "step: 10386, loss: 0.15829090774059296\n",
      "step: 10387, loss: 5.533109742827946e-06\n",
      "step: 10388, loss: 8.249242569036142e-07\n",
      "step: 10389, loss: 1.8691805507842219e-06\n",
      "step: 10390, loss: 4.553640792437363e-06\n",
      "step: 10391, loss: 1.5019952570582973e-06\n",
      "step: 10392, loss: 6.675699637526122e-07\n",
      "step: 10393, loss: 1.0371088592364686e-06\n",
      "step: 10394, loss: 1.8643855810296373e-06\n",
      "step: 10395, loss: 1.6355345451302128e-06\n",
      "step: 10396, loss: 1.4174576108416659e-06\n",
      "step: 10397, loss: 1.049035063260817e-06\n",
      "step: 10398, loss: 1.015656380332075e-06\n",
      "step: 10399, loss: 3.0354824048117734e-05\n",
      "step: 10400, loss: 2.2696444830216933e-06\n",
      "step: 10401, loss: 1.0784860933199525e-05\n",
      "step: 10402, loss: 2.9753634862572653e-06\n",
      "step: 10403, loss: 1.3044672414253e-05\n",
      "step: 10404, loss: 1.9025541178052663e-06\n",
      "step: 10405, loss: 6.181905064295279e-06\n",
      "step: 10406, loss: 4.682357939600479e-06\n",
      "step: 10407, loss: 6.3176066760206595e-06\n",
      "step: 10408, loss: 2.9277298381202854e-06\n",
      "step: 10409, loss: 1.7165954204756417e-06\n",
      "step: 10410, loss: 8.262917617685162e-06\n",
      "step: 10411, loss: 2.2888148976107914e-07\n",
      "step: 10412, loss: 3.342554464325076e-06\n",
      "step: 10413, loss: 6.508801675408904e-07\n",
      "step: 10414, loss: 1.4829237215963076e-06\n",
      "step: 10415, loss: 3.738331997737987e-06\n",
      "step: 10416, loss: 3.213842092009145e-06\n",
      "step: 10417, loss: 2.419883685433888e-06\n",
      "step: 10418, loss: 0.0001818050950532779\n",
      "step: 10419, loss: 0.03728444501757622\n",
      "step: 10420, loss: 3.9487542380811647e-05\n",
      "step: 10421, loss: 1.4421916603168938e-05\n",
      "step: 10422, loss: 6.720638339174911e-06\n",
      "step: 10423, loss: 2.465206989654689e-06\n",
      "step: 10424, loss: 3.6572280350810615e-06\n",
      "step: 10425, loss: 5.4402898967964575e-06\n",
      "step: 10426, loss: 1.9788412828347646e-06\n",
      "step: 10427, loss: 1.2898364047941868e-06\n",
      "step: 10428, loss: 4.172305523297837e-07\n",
      "step: 10429, loss: 1.4185768577590352e-06\n",
      "step: 10430, loss: 1.7761860817699926e-06\n",
      "step: 10431, loss: 6.665772616543109e-06\n",
      "step: 10432, loss: 4.833964703720994e-05\n",
      "step: 10433, loss: 3.174315133946948e-05\n",
      "step: 10434, loss: 3.041891432076227e-05\n",
      "step: 10435, loss: 1.2171823982498609e-05\n",
      "step: 10436, loss: 0.00337651907466352\n",
      "step: 10437, loss: 1.0151037713512778e-05\n",
      "step: 10438, loss: 3.850287612294778e-06\n",
      "step: 10439, loss: 0.0026229831855744123\n",
      "step: 10440, loss: 2.9205809823906748e-06\n",
      "step: 10441, loss: 2.0861234588664956e-06\n",
      "step: 10442, loss: 8.74992281296727e-07\n",
      "step: 10443, loss: 5.914803296036553e-06\n",
      "step: 10444, loss: 3.1661456887377426e-06\n",
      "step: 10445, loss: 3.0181920010363683e-05\n",
      "step: 10446, loss: 0.0005547315231524408\n",
      "step: 10447, loss: 0.0002646766370162368\n",
      "step: 10448, loss: 3.361697338277736e-07\n",
      "step: 10449, loss: 1.1164059287693817e-05\n",
      "step: 10450, loss: 1.1857790923386347e-05\n",
      "step: 10451, loss: 1.829052416724153e-05\n",
      "step: 10452, loss: 3.476126948953606e-05\n",
      "step: 10453, loss: 2.336498852173463e-07\n",
      "step: 10454, loss: 1.485150096414145e-05\n",
      "step: 10455, loss: 3.862371613649884e-07\n",
      "step: 10456, loss: 1.5353897424574825e-06\n",
      "step: 10457, loss: 5.769713311565283e-07\n",
      "step: 10458, loss: 0.018025090917944908\n",
      "step: 10459, loss: 1.680823515926022e-06\n",
      "step: 10460, loss: 2.8776512408512644e-06\n",
      "step: 10461, loss: 4.949273261445342e-06\n",
      "step: 10462, loss: 0.16621039807796478\n",
      "step: 10463, loss: 1.4757979442947544e-06\n",
      "step: 10464, loss: 1.1205609098396963e-06\n",
      "step: 10465, loss: 4.291404820833122e-06\n",
      "step: 10466, loss: 4.269886630936526e-06\n",
      "step: 10467, loss: 9.54104179982096e-05\n",
      "step: 10468, loss: 1.2612268847078667e-06\n",
      "step: 10469, loss: 9.798928886084468e-07\n",
      "step: 10470, loss: 0.022081159055233\n",
      "step: 10471, loss: 2.219643192802323e-06\n",
      "step: 10472, loss: 9.465164225730405e-07\n",
      "step: 10473, loss: 1.0514185078136506e-06\n",
      "step: 10474, loss: 0.00295237242244184\n",
      "step: 10475, loss: 2.3841813856506633e-07\n",
      "step: 10476, loss: 0.05031633377075195\n",
      "step: 10477, loss: 1.046650481839606e-06\n",
      "step: 10478, loss: 1.3157285138731822e-05\n",
      "step: 10479, loss: 1.4365372408065014e-05\n",
      "step: 10480, loss: 7.581661520816851e-07\n",
      "step: 10481, loss: 4.67299202000504e-07\n",
      "step: 10482, loss: 2.53191660704033e-06\n",
      "step: 10483, loss: 1.0621488399920054e-05\n",
      "step: 10484, loss: 1.134864533014479e-06\n",
      "step: 10485, loss: 3.314009404675744e-07\n",
      "step: 10486, loss: 4.636972334992606e-06\n",
      "step: 10487, loss: 1.9287560917291557e-06\n",
      "step: 10488, loss: 1.5997533182599e-06\n",
      "step: 10489, loss: 3.910047325916821e-07\n",
      "step: 10490, loss: 6.699481309624389e-05\n",
      "step: 10491, loss: 1.0013507107942132e-06\n",
      "step: 10492, loss: 7.056947652017698e-06\n",
      "step: 10493, loss: 1.4651131095888559e-05\n",
      "step: 10494, loss: 1.0993764590239152e-05\n",
      "step: 10495, loss: 4.398584223963553e-06\n",
      "step: 10496, loss: 5.626228357868968e-06\n",
      "step: 10497, loss: 1.5061024896567687e-05\n",
      "step: 10498, loss: 1.2059417713317089e-05\n",
      "step: 10499, loss: 4.684781288233353e-06\n",
      "step: 10500, loss: 5.5692853493383154e-06\n",
      "step: 10501, loss: 0.000819983019027859\n",
      "step: 10502, loss: 2.5533995540172327e-06\n",
      "step: 10503, loss: 5.5881182561279275e-06\n",
      "step: 10504, loss: 1.8429557258059504e-06\n",
      "step: 10505, loss: 9.821884304983541e-05\n",
      "step: 10506, loss: 1.2298896763240919e-05\n",
      "step: 10507, loss: 8.81453524925746e-05\n",
      "step: 10508, loss: 2.72267720902164e-06\n",
      "step: 10509, loss: 4.861138677370036e-06\n",
      "step: 10510, loss: 4.973140221409267e-06\n",
      "step: 10511, loss: 0.0001643046416575089\n",
      "step: 10512, loss: 1.1276086297584698e-05\n",
      "step: 10513, loss: 0.021191952750086784\n",
      "step: 10514, loss: 3.4581709769554436e-05\n",
      "step: 10515, loss: 6.4345458667958155e-06\n",
      "step: 10516, loss: 9.102160220209043e-06\n",
      "step: 10517, loss: 1.6903734376683133e-06\n",
      "step: 10518, loss: 1.8810984556694166e-06\n",
      "step: 10519, loss: 8.496204827679321e-06\n",
      "step: 10520, loss: 2.828870856319554e-05\n",
      "step: 10521, loss: 2.167184220525087e-06\n",
      "step: 10522, loss: 0.00014734602882526815\n",
      "step: 10523, loss: 1.2993759810342453e-06\n",
      "step: 10524, loss: 3.0850351322442293e-06\n",
      "step: 10525, loss: 1.7967784515349194e-05\n",
      "step: 10526, loss: 2.2887898012413643e-06\n",
      "step: 10527, loss: 1.4764952538826037e-05\n",
      "step: 10528, loss: 1.2966345821041614e-05\n",
      "step: 10529, loss: 2.7822848096548114e-06\n",
      "step: 10530, loss: 3.995729002781445e-06\n",
      "step: 10531, loss: 9.780898108147085e-05\n",
      "step: 10532, loss: 1.0701706742111128e-05\n",
      "step: 10533, loss: 1.978798309210106e-06\n",
      "step: 10534, loss: 2.603441771498183e-06\n",
      "step: 10535, loss: 1.00612112419185e-06\n",
      "step: 10536, loss: 0.08518719673156738\n",
      "step: 10537, loss: 1.2305635209486354e-05\n",
      "step: 10538, loss: 5.180407242733054e-05\n",
      "step: 10539, loss: 5.340560278455087e-07\n",
      "step: 10540, loss: 0.06647036969661713\n",
      "step: 10541, loss: 3.7644856547558447e-06\n",
      "step: 10542, loss: 6.937951297913969e-07\n",
      "step: 10543, loss: 3.859857315546833e-06\n",
      "step: 10544, loss: 2.4795053832349367e-06\n",
      "step: 10545, loss: 3.557120862751617e-06\n",
      "step: 10546, loss: 3.418695087020751e-06\n",
      "step: 10547, loss: 3.3281401101703523e-06\n",
      "step: 10548, loss: 2.140955530194333e-06\n",
      "step: 10549, loss: 1.197199435409857e-05\n",
      "step: 10550, loss: 3.1279678296414204e-06\n",
      "step: 10551, loss: 8.539566806575749e-06\n",
      "step: 10552, loss: 5.25210589330527e-06\n",
      "step: 10553, loss: 1.7332863535557408e-06\n",
      "step: 10554, loss: 1.8691756622501998e-06\n",
      "step: 10555, loss: 3.6239030123397242e-06\n",
      "step: 10556, loss: 2.2387164335668785e-06\n",
      "step: 10557, loss: 2.7939333449467085e-05\n",
      "step: 10558, loss: 2.3888730993348872e-06\n",
      "step: 10559, loss: 3.6261617424315773e-06\n",
      "step: 10560, loss: 4.133959919272456e-06\n",
      "step: 10561, loss: 6.005356681271223e-06\n",
      "step: 10562, loss: 2.394992407062091e-05\n",
      "step: 10563, loss: 0.0003904525947291404\n",
      "step: 10564, loss: 2.361092083447147e-05\n",
      "step: 10565, loss: 1.3422894653558615e-06\n",
      "step: 10566, loss: 1.282680273106962e-06\n",
      "step: 10567, loss: 4.682430699176621e-06\n",
      "step: 10568, loss: 6.024471986165736e-06\n",
      "step: 10569, loss: 6.508801106974715e-07\n",
      "step: 10570, loss: 0.040433626621961594\n",
      "step: 10571, loss: 1.2878585948783439e-05\n",
      "step: 10572, loss: 7.009467140051129e-07\n",
      "step: 10573, loss: 9.60970101004932e-06\n",
      "step: 10574, loss: 3.857277988572605e-06\n",
      "step: 10575, loss: 5.3575615311274305e-05\n",
      "step: 10576, loss: 4.3366726458771154e-06\n",
      "step: 10577, loss: 0.09827031195163727\n",
      "step: 10578, loss: 3.236314660171047e-05\n",
      "step: 10579, loss: 3.603867662604898e-05\n",
      "step: 10580, loss: 2.8657416351052234e-06\n",
      "step: 10581, loss: 8.522929419996217e-06\n",
      "step: 10582, loss: 0.20581232011318207\n",
      "step: 10583, loss: 2.360340971563346e-07\n",
      "step: 10584, loss: 4.6085056055744644e-06\n",
      "step: 10585, loss: 5.444920589070534e-06\n",
      "step: 10586, loss: 3.143871799693443e-05\n",
      "step: 10587, loss: 2.381752210567356e-06\n",
      "step: 10588, loss: 9.584324516254128e-07\n",
      "step: 10589, loss: 0.04513855278491974\n",
      "step: 10590, loss: 3.2304192245646846e-06\n",
      "step: 10591, loss: 7.168754109443398e-06\n",
      "step: 10592, loss: 2.0691673853434622e-05\n",
      "step: 10593, loss: 8.797570103524777e-07\n",
      "step: 10594, loss: 0.019756661728024483\n",
      "step: 10595, loss: 2.839522494468838e-06\n",
      "step: 10596, loss: 8.362818334717304e-06\n",
      "step: 10597, loss: 0.3105611503124237\n",
      "step: 10598, loss: 3.0421174415096175e-06\n",
      "step: 10599, loss: 8.761027856962755e-06\n",
      "step: 10600, loss: 6.510684215754736e-06\n",
      "step: 10601, loss: 5.621704076474998e-06\n",
      "step: 10602, loss: 0.10502871870994568\n",
      "step: 10603, loss: 4.6218130592023954e-05\n",
      "step: 10604, loss: 4.9033180403057486e-05\n",
      "step: 10605, loss: 3.013569312315667e-06\n",
      "step: 10606, loss: 2.1934486937880138e-07\n",
      "step: 10607, loss: 1.3161417882656679e-05\n",
      "step: 10608, loss: 0.0002811714366544038\n",
      "step: 10609, loss: 6.274683983065188e-06\n",
      "step: 10610, loss: 8.13191945781e-06\n",
      "step: 10611, loss: 2.527203832869418e-06\n",
      "step: 10612, loss: 6.364625733112916e-05\n",
      "step: 10613, loss: 9.432833030587062e-05\n",
      "step: 10614, loss: 0.0003450264048296958\n",
      "step: 10615, loss: 1.1776257451856509e-05\n",
      "step: 10616, loss: 4.572991747409105e-05\n",
      "step: 10617, loss: 1.7955897419597022e-05\n",
      "step: 10618, loss: 2.696475121410913e-06\n",
      "step: 10619, loss: 7.367108878497675e-07\n",
      "step: 10620, loss: 0.0006789572653360665\n",
      "step: 10621, loss: 0.04346632584929466\n",
      "step: 10622, loss: 7.000990990491118e-06\n",
      "step: 10623, loss: 5.98636916038231e-06\n",
      "step: 10624, loss: 1.1208139767404646e-05\n",
      "step: 10625, loss: 5.981579761282774e-06\n",
      "step: 10626, loss: 8.787195838522166e-06\n",
      "step: 10627, loss: 2.813202627294231e-06\n",
      "step: 10628, loss: 9.476370905758813e-06\n",
      "step: 10629, loss: 9.131996193900704e-05\n",
      "step: 10630, loss: 0.0009250558214262128\n",
      "step: 10631, loss: 1.3780311292066472e-06\n",
      "step: 10632, loss: 0.0003126352676190436\n",
      "step: 10633, loss: 1.1269277820247225e-05\n",
      "step: 10634, loss: 1.4019597983860876e-05\n",
      "step: 10635, loss: 0.00026531005278229713\n",
      "step: 10636, loss: 1.6807940710350522e-06\n",
      "step: 10637, loss: 8.679570782987867e-06\n",
      "step: 10638, loss: 0.004243309609591961\n",
      "step: 10639, loss: 2.31021772378881e-06\n",
      "step: 10640, loss: 9.98961809273169e-07\n",
      "step: 10641, loss: 6.9710863499494735e-06\n",
      "step: 10642, loss: 9.388238140672911e-06\n",
      "step: 10643, loss: 9.18778187042335e-06\n",
      "step: 10644, loss: 1.3829394447384402e-05\n",
      "step: 10645, loss: 1.367370623484021e-05\n",
      "step: 10646, loss: 0.0020205508917570114\n",
      "step: 10647, loss: 5.8458681451156735e-06\n",
      "step: 10648, loss: 3.106462145296973e-06\n",
      "step: 10649, loss: 1.3512825717043597e-05\n",
      "step: 10650, loss: 6.103179202909814e-06\n",
      "step: 10651, loss: 1.54570589074865e-05\n",
      "step: 10652, loss: 6.079653189772216e-07\n",
      "step: 10653, loss: 5.035250069340691e-06\n",
      "step: 10654, loss: 0.09322061389684677\n",
      "step: 10655, loss: 5.5991080444073305e-05\n",
      "step: 10656, loss: 2.777535883069504e-06\n",
      "step: 10657, loss: 3.6423112760530785e-05\n",
      "step: 10658, loss: 0.0009924240875989199\n",
      "step: 10659, loss: 6.427154858101858e-06\n",
      "step: 10660, loss: 4.157929197390331e-06\n",
      "step: 10661, loss: 6.587633106391877e-05\n",
      "step: 10662, loss: 1.4604045645683073e-05\n",
      "step: 10663, loss: 8.010251804080326e-06\n",
      "step: 10664, loss: 1.4253130757424515e-05\n",
      "step: 10665, loss: 2.2112248188932426e-05\n",
      "step: 10666, loss: 3.239534635213204e-05\n",
      "step: 10667, loss: 1.3685124713447294e-06\n",
      "step: 10668, loss: 1.8128812371287495e-05\n",
      "step: 10669, loss: 4.56304223916959e-06\n",
      "step: 10670, loss: 8.069788236753084e-06\n",
      "step: 10671, loss: 1.9708180843736045e-05\n",
      "step: 10672, loss: 2.918183963629417e-05\n",
      "step: 10673, loss: 1.0383252629253548e-05\n",
      "step: 10674, loss: 3.485417619231157e-05\n",
      "step: 10675, loss: 2.0838775526499376e-05\n",
      "step: 10676, loss: 1.3994919072501943e-06\n",
      "step: 10677, loss: 0.007908097468316555\n",
      "step: 10678, loss: 4.172073204244953e-06\n",
      "step: 10679, loss: 3.5976561321149347e-06\n",
      "step: 10680, loss: 6.510859748232178e-06\n",
      "step: 10681, loss: 3.3853063996502897e-06\n",
      "step: 10682, loss: 2.4684017262188718e-05\n",
      "step: 10683, loss: 2.9276257009769324e-06\n",
      "step: 10684, loss: 0.000917262805160135\n",
      "step: 10685, loss: 5.108982350066071e-06\n",
      "step: 10686, loss: 1.5337267541326582e-05\n",
      "step: 10687, loss: 2.2625652036367683e-06\n",
      "step: 10688, loss: 2.5986626042140415e-06\n",
      "step: 10689, loss: 7.45675279176794e-05\n",
      "step: 10690, loss: 0.006927272770553827\n",
      "step: 10691, loss: 8.990124115371145e-06\n",
      "step: 10692, loss: 1.716578935884172e-06\n",
      "step: 10693, loss: 1.9352299204911105e-05\n",
      "step: 10694, loss: 5.98373753746273e-06\n",
      "step: 10695, loss: 4.0553545659349766e-06\n",
      "step: 10696, loss: 2.3268789846042637e-06\n",
      "step: 10697, loss: 4.16740886066691e-06\n",
      "step: 10698, loss: 3.609574559959583e-05\n",
      "step: 10699, loss: 6.890248869240168e-07\n",
      "step: 10700, loss: 0.05213829129934311\n",
      "step: 10701, loss: 1.633130295886076e-06\n",
      "step: 10702, loss: 1.8326560166315176e-05\n",
      "step: 10703, loss: 3.0111173145996872e-06\n",
      "step: 10704, loss: 0.0007065156241878867\n",
      "step: 10705, loss: 5.187839633435942e-06\n",
      "step: 10706, loss: 4.713405360234901e-06\n",
      "step: 10707, loss: 3.8742100514355116e-06\n",
      "step: 10708, loss: 1.1302369784971233e-05\n",
      "step: 10709, loss: 2.4261413273052312e-05\n",
      "step: 10710, loss: 1.5986252037691884e-05\n",
      "step: 10711, loss: 2.0742179458466126e-06\n",
      "step: 10712, loss: 1.2927614079671912e-05\n",
      "step: 10713, loss: 5.291614797897637e-05\n",
      "step: 10714, loss: 8.17966883914778e-06\n",
      "step: 10715, loss: 1.4955741789890453e-05\n",
      "step: 10716, loss: 3.85025759896962e-06\n",
      "step: 10717, loss: 1.5950054148561321e-06\n",
      "step: 10718, loss: 2.0003053577966057e-06\n",
      "step: 10719, loss: 7.225354693218833e-06\n",
      "step: 10720, loss: 9.976623005059082e-06\n",
      "step: 10721, loss: 0.00012225290993228555\n",
      "step: 10722, loss: 1.6498311197210569e-06\n",
      "step: 10723, loss: 6.8850199568259995e-06\n",
      "step: 10724, loss: 1.5683292076573707e-05\n",
      "step: 10725, loss: 1.6498338482051622e-06\n",
      "step: 10726, loss: 5.1162392082915176e-06\n",
      "step: 10727, loss: 2.057523488474544e-06\n",
      "step: 10728, loss: 4.505795004661195e-06\n",
      "step: 10729, loss: 6.916136499057757e-06\n",
      "step: 10730, loss: 3.444458343437873e-05\n",
      "step: 10731, loss: 1.2687324669968802e-05\n",
      "step: 10732, loss: 7.176316216828127e-07\n",
      "step: 10733, loss: 3.3461456041550264e-05\n",
      "step: 10734, loss: 4.217460627842229e-06\n",
      "step: 10735, loss: 3.3783080652938224e-06\n",
      "step: 10736, loss: 1.9836165847664233e-06\n",
      "step: 10737, loss: 1.3553841199609451e-05\n",
      "step: 10738, loss: 5.967131073703058e-06\n",
      "step: 10739, loss: 4.312867531552911e-06\n",
      "step: 10740, loss: 7.366130012087524e-05\n",
      "step: 10741, loss: 3.166714304825291e-05\n",
      "step: 10742, loss: 4.759513831231743e-05\n",
      "step: 10743, loss: 2.5693534553283826e-05\n",
      "step: 10744, loss: 1.3565781955549028e-06\n",
      "step: 10745, loss: 7.319416681639268e-07\n",
      "step: 10746, loss: 0.0001516157208243385\n",
      "step: 10747, loss: 7.016236850176938e-06\n",
      "step: 10748, loss: 0.00018765227287076414\n",
      "step: 10749, loss: 0.0848323181271553\n",
      "step: 10750, loss: 5.798689016955905e-05\n",
      "step: 10751, loss: 4.464156518224627e-05\n",
      "step: 10752, loss: 1.1038674756491673e-06\n",
      "step: 10753, loss: 0.0001286058541154489\n",
      "step: 10754, loss: 4.095863914699294e-06\n",
      "step: 10755, loss: 6.69206428938196e-06\n",
      "step: 10756, loss: 3.3449305192334577e-06\n",
      "step: 10757, loss: 5.566576874116436e-06\n",
      "step: 10758, loss: 5.504463842953555e-05\n",
      "step: 10759, loss: 2.052753643511096e-06\n",
      "step: 10760, loss: 2.059908410956268e-06\n",
      "step: 10761, loss: 3.382930799489259e-06\n",
      "step: 10762, loss: 4.291517257115629e-07\n",
      "step: 10763, loss: 2.0963263523299247e-05\n",
      "step: 10764, loss: 2.9420305054372875e-06\n",
      "step: 10765, loss: 8.201566856769205e-07\n",
      "step: 10766, loss: 8.062515007623006e-06\n",
      "step: 10767, loss: 1.0601780559227336e-05\n",
      "step: 10768, loss: 8.272988907265244e-07\n",
      "step: 10769, loss: 1.543699727335479e-05\n",
      "step: 10770, loss: 1.3684959867532598e-06\n",
      "step: 10771, loss: 4.4277181586949155e-05\n",
      "step: 10772, loss: 8.797601935839339e-07\n",
      "step: 10773, loss: 8.174309186870232e-06\n",
      "step: 10774, loss: 4.972965598426526e-06\n",
      "step: 10775, loss: 3.511736622385797e-06\n",
      "step: 10776, loss: 1.230903944815509e-05\n",
      "step: 10777, loss: 2.679700855878764e-06\n",
      "step: 10778, loss: 5.183085249882424e-06\n",
      "step: 10779, loss: 3.423641828703694e-06\n",
      "step: 10780, loss: 1.376659292873228e-05\n",
      "step: 10781, loss: 0.039721228182315826\n",
      "step: 10782, loss: 5.250961476122029e-05\n",
      "step: 10783, loss: 9.47364060266409e-06\n",
      "step: 10784, loss: 5.483492259372724e-06\n",
      "step: 10785, loss: 6.022064098942792e-06\n",
      "step: 10786, loss: 9.077887625608128e-06\n",
      "step: 10787, loss: 1.9294588128104806e-05\n",
      "step: 10788, loss: 2.142090306733735e-05\n",
      "step: 10789, loss: 5.724168659071438e-06\n",
      "step: 10790, loss: 2.7735133699025027e-05\n",
      "step: 10791, loss: 2.2029394131095614e-06\n",
      "step: 10792, loss: 3.38544759870274e-06\n",
      "step: 10793, loss: 1.7332540664938278e-06\n",
      "step: 10794, loss: 8.654521934658987e-07\n",
      "step: 10795, loss: 3.2638665743434103e-06\n",
      "step: 10796, loss: 1.1905383871635422e-05\n",
      "step: 10797, loss: 1.0275792874381295e-06\n",
      "step: 10798, loss: 6.611102890019538e-06\n",
      "step: 10799, loss: 1.4161943227009033e-06\n",
      "step: 10800, loss: 4.798265945282765e-05\n",
      "step: 10801, loss: 1.020335821522167e-05\n",
      "step: 10802, loss: 6.77966236253269e-05\n",
      "step: 10803, loss: 0.04453295096755028\n",
      "step: 10804, loss: 3.743163574654318e-07\n",
      "step: 10805, loss: 3.2631443900754675e-05\n",
      "step: 10806, loss: 6.343090353766456e-05\n",
      "step: 10807, loss: 4.053104305512534e-07\n",
      "step: 10808, loss: 2.025532921834383e-05\n",
      "step: 10809, loss: 1.7881185385704157e-06\n",
      "step: 10810, loss: 0.0679255798459053\n",
      "step: 10811, loss: 5.704708019038662e-05\n",
      "step: 10812, loss: 4.560797606245615e-06\n",
      "step: 10813, loss: 0.09819222241640091\n",
      "step: 10814, loss: 0.0010885688243433833\n",
      "step: 10815, loss: 1.1948331120947842e-05\n",
      "step: 10816, loss: 4.069672286277637e-06\n",
      "step: 10817, loss: 0.000608018774073571\n",
      "step: 10818, loss: 1.5239114873111248e-05\n",
      "step: 10819, loss: 0.038057390600442886\n",
      "step: 10820, loss: 4.863723574999312e-07\n",
      "step: 10821, loss: 6.647501140832901e-05\n",
      "step: 10822, loss: 4.56786438007839e-06\n",
      "step: 10823, loss: 0.00016266739112325013\n",
      "step: 10824, loss: 0.0011636980343610048\n",
      "step: 10825, loss: 8.331659046234563e-06\n",
      "step: 10826, loss: 3.477532300166786e-05\n",
      "step: 10827, loss: 1.4448075944528682e-06\n",
      "step: 10828, loss: 3.906800702679902e-05\n",
      "step: 10829, loss: 8.908437303034589e-05\n",
      "step: 10830, loss: 0.04186820983886719\n",
      "step: 10831, loss: 2.7703738396667177e-06\n",
      "step: 10832, loss: 0.07151667773723602\n",
      "step: 10833, loss: 0.00016979772772174329\n",
      "step: 10834, loss: 8.081012310867663e-06\n",
      "step: 10835, loss: 1.2038475688314065e-05\n",
      "step: 10836, loss: 0.06225116550922394\n",
      "step: 10837, loss: 1.5473169696633704e-06\n",
      "step: 10838, loss: 0.00017404940444976091\n",
      "step: 10839, loss: 3.1458224839298055e-05\n",
      "step: 10840, loss: 0.00025637305225245655\n",
      "step: 10841, loss: 1.419611999153858e-05\n",
      "step: 10842, loss: 2.052762965831789e-06\n",
      "step: 10843, loss: 0.03900914266705513\n",
      "step: 10844, loss: 5.7759778428589925e-05\n",
      "step: 10845, loss: 2.167173988709692e-06\n",
      "step: 10846, loss: 5.206920377531787e-06\n",
      "step: 10847, loss: 8.253023224824574e-06\n",
      "step: 10848, loss: 2.2973044906393625e-05\n",
      "step: 10849, loss: 9.035967423187685e-07\n",
      "step: 10850, loss: 9.584367717252462e-07\n",
      "step: 10851, loss: 7.247040230140556e-06\n",
      "step: 10852, loss: 6.591716555703897e-06\n",
      "step: 10853, loss: 3.557073569027125e-06\n",
      "step: 10854, loss: 9.271435374103021e-06\n",
      "step: 10855, loss: 0.00012873619562014937\n",
      "step: 10856, loss: 1.9293402147013694e-05\n",
      "step: 10857, loss: 2.6123272618860938e-05\n",
      "step: 10858, loss: 9.445118848816492e-06\n",
      "step: 10859, loss: 0.08819696307182312\n",
      "step: 10860, loss: 1.7662092432146892e-05\n",
      "step: 10861, loss: 6.265208412514767e-06\n",
      "step: 10862, loss: 5.9043126384494826e-05\n",
      "step: 10863, loss: 8.583033945797069e-07\n",
      "step: 10864, loss: 5.971736300125485e-06\n",
      "step: 10865, loss: 0.0001349290832877159\n",
      "step: 10866, loss: 9.488989576311724e-07\n",
      "step: 10867, loss: 6.571775884367526e-05\n",
      "step: 10868, loss: 3.0300818252726458e-05\n",
      "step: 10869, loss: 5.8613804867491126e-05\n",
      "step: 10870, loss: 8.559187563150772e-07\n",
      "step: 10871, loss: 2.7179666517440637e-07\n",
      "step: 10872, loss: 0.00013881690392736346\n",
      "step: 10873, loss: 3.404514018257032e-06\n",
      "step: 10874, loss: 6.556473977070709e-07\n",
      "step: 10875, loss: 2.4698811103007756e-06\n",
      "step: 10876, loss: 1.5830612483114237e-06\n",
      "step: 10877, loss: 0.1399298906326294\n",
      "step: 10878, loss: 9.894331469695317e-07\n",
      "step: 10879, loss: 1.0053368896478787e-05\n",
      "step: 10880, loss: 2.8528029361041263e-05\n",
      "step: 10881, loss: 3.4822911402443424e-05\n",
      "step: 10882, loss: 1.4042475413589273e-06\n",
      "step: 10883, loss: 1.1420392183936201e-05\n",
      "step: 10884, loss: 2.253025058962521e-06\n",
      "step: 10885, loss: 1.068602068698965e-05\n",
      "step: 10886, loss: 1.1539414117578417e-06\n",
      "step: 10887, loss: 4.055619865539484e-05\n",
      "step: 10888, loss: 4.911412361252587e-07\n",
      "step: 10889, loss: 4.306272603571415e-05\n",
      "step: 10890, loss: 0.001909713027998805\n",
      "step: 10891, loss: 5.283210612105904e-06\n",
      "step: 10892, loss: 1.695128730716533e-06\n",
      "step: 10893, loss: 3.697642750921659e-05\n",
      "step: 10894, loss: 0.00026547990273684263\n",
      "step: 10895, loss: 4.406894731801003e-05\n",
      "step: 10896, loss: 1.5099024494702462e-05\n",
      "step: 10897, loss: 0.08871191740036011\n",
      "step: 10898, loss: 0.1700182408094406\n",
      "step: 10899, loss: 1.9720220734598115e-05\n",
      "step: 10900, loss: 8.257502486230806e-05\n",
      "step: 10901, loss: 1.4018710317031946e-06\n",
      "step: 10902, loss: 1.741184496495407e-05\n",
      "step: 10903, loss: 0.0008677879814058542\n",
      "step: 10904, loss: 9.199931810144335e-06\n",
      "step: 10905, loss: 5.149537628312828e-06\n",
      "step: 10906, loss: 4.513103704084642e-06\n",
      "step: 10907, loss: 6.717961696267594e-06\n",
      "step: 10908, loss: 2.3055134079186246e-05\n",
      "step: 10909, loss: 1.0275740578435943e-06\n",
      "step: 10910, loss: 2.4341572952835122e-06\n",
      "step: 10911, loss: 8.21289995656116e-06\n",
      "step: 10912, loss: 1.0582471986708697e-05\n",
      "step: 10913, loss: 1.920215981954243e-05\n",
      "step: 10914, loss: 8.308195901918225e-06\n",
      "step: 10915, loss: 1.867018363554962e-05\n",
      "step: 10916, loss: 4.749004347104346e-06\n",
      "step: 10917, loss: 1.8429605006531347e-06\n",
      "step: 10918, loss: 5.399845576903317e-06\n",
      "step: 10919, loss: 1.1883437764481641e-05\n",
      "step: 10920, loss: 2.146623592125252e-05\n",
      "step: 10921, loss: 1.542557470202155e-06\n",
      "step: 10922, loss: 2.967938235087786e-05\n",
      "step: 10923, loss: 8.601363333582412e-06\n",
      "step: 10924, loss: 4.0027048271440435e-06\n",
      "step: 10925, loss: 9.338160452898592e-06\n",
      "step: 10926, loss: 1.3542085071094334e-06\n",
      "step: 10927, loss: 1.1387392987671774e-05\n",
      "step: 10928, loss: 4.6768574975430965e-05\n",
      "step: 10929, loss: 1.1651241038634907e-05\n",
      "step: 10930, loss: 1.8092539903591387e-05\n",
      "step: 10931, loss: 4.873045782005647e-06\n",
      "step: 10932, loss: 8.89296643435955e-07\n",
      "step: 10933, loss: 7.652150088688359e-06\n",
      "step: 10934, loss: 1.5239225831464864e-05\n",
      "step: 10935, loss: 8.29083364806138e-06\n",
      "step: 10936, loss: 5.221329502091976e-07\n",
      "step: 10937, loss: 2.357302037125919e-05\n",
      "step: 10938, loss: 0.0002489937178324908\n",
      "step: 10939, loss: 0.0005810639122501016\n",
      "step: 10940, loss: 8.17772729533317e-07\n",
      "step: 10941, loss: 2.298321987836971e-06\n",
      "step: 10942, loss: 0.02294054441154003\n",
      "step: 10943, loss: 4.3153562501174747e-07\n",
      "step: 10944, loss: 1.3279792483444908e-06\n",
      "step: 10945, loss: 2.310236141056521e-06\n",
      "step: 10946, loss: 1.0276960892952047e-05\n",
      "step: 10947, loss: 9.752442565513775e-06\n",
      "step: 10948, loss: 1.9049402908422053e-06\n",
      "step: 10949, loss: 1.2944622540089767e-05\n",
      "step: 10950, loss: 1.8071841623168439e-06\n",
      "step: 10951, loss: 1.2222782970638946e-05\n",
      "step: 10952, loss: 1.3208265272623976e-06\n",
      "step: 10953, loss: 6.474139809142798e-05\n",
      "step: 10954, loss: 6.14158998359926e-05\n",
      "step: 10955, loss: 1.051985327649163e-05\n",
      "step: 10956, loss: 2.0050633793289308e-06\n",
      "step: 10957, loss: 5.626664005831117e-07\n",
      "step: 10958, loss: 2.2506276309286477e-06\n",
      "step: 10959, loss: 4.12684130424168e-06\n",
      "step: 10960, loss: 5.464182322612032e-06\n",
      "step: 10961, loss: 1.0347316674597096e-06\n",
      "step: 10962, loss: 1.0961102816509083e-05\n",
      "step: 10963, loss: 1.132480406340619e-06\n",
      "step: 10964, loss: 1.26122472465795e-06\n",
      "step: 10965, loss: 2.2959256966714747e-06\n",
      "step: 10966, loss: 0.00010890885459957644\n",
      "step: 10967, loss: 2.0062063413206488e-05\n",
      "step: 10968, loss: 1.7615238903090358e-05\n",
      "step: 10969, loss: 6.856091204099357e-05\n",
      "step: 10970, loss: 0.15910273790359497\n",
      "step: 10971, loss: 5.720545959775336e-05\n",
      "step: 10972, loss: 6.267620847211219e-06\n",
      "step: 10973, loss: 1.0370484233135357e-05\n",
      "step: 10974, loss: 5.075739409221569e-06\n",
      "step: 10975, loss: 1.615764449525159e-05\n",
      "step: 10976, loss: 1.7094234863179736e-06\n",
      "step: 10977, loss: 2.5200160962413065e-06\n",
      "step: 10978, loss: 1.2683676686719991e-06\n",
      "step: 10979, loss: 3.630980017987895e-06\n",
      "step: 10980, loss: 2.55577083407843e-06\n",
      "step: 10981, loss: 7.489948984584771e-06\n",
      "step: 10982, loss: 1.568757761560846e-05\n",
      "step: 10983, loss: 3.0205985694919946e-06\n",
      "step: 10984, loss: 1.106254217120295e-06\n",
      "step: 10985, loss: 5.483612994794385e-07\n",
      "step: 10986, loss: 2.8633669444388943e-06\n",
      "step: 10987, loss: 0.00035569682950153947\n",
      "step: 10988, loss: 2.5508436010568403e-05\n",
      "step: 10989, loss: 3.864666723529808e-06\n",
      "step: 10990, loss: 0.06856140494346619\n",
      "step: 10991, loss: 0.06720533967018127\n",
      "step: 10992, loss: 0.09481379389762878\n",
      "step: 10993, loss: 1.9311669348098803e-06\n",
      "step: 10994, loss: 4.853973678109469e-06\n",
      "step: 10995, loss: 9.226728252542671e-07\n",
      "step: 10996, loss: 2.6072593755088747e-05\n",
      "step: 10997, loss: 1.4329352779895999e-05\n",
      "step: 10998, loss: 2.488968220859533e-06\n",
      "step: 10999, loss: 5.928803602728294e-06\n",
      "step: 11000, loss: 0.06216239929199219\n",
      "step: 11001, loss: 5.4428969633590896e-06\n",
      "step: 11002, loss: 7.790493327775039e-06\n",
      "step: 11003, loss: 2.162435521313455e-06\n",
      "step: 11004, loss: 6.723387855345209e-07\n",
      "step: 11005, loss: 2.1838809516339097e-06\n",
      "step: 11006, loss: 2.0289198801037855e-06\n",
      "step: 11007, loss: 4.6043176553212106e-05\n",
      "step: 11008, loss: 1.6474415360789862e-06\n",
      "step: 11009, loss: 8.653354598209262e-06\n",
      "step: 11010, loss: 5.07344702782575e-06\n",
      "step: 11011, loss: 2.0765742192452308e-06\n",
      "step: 11012, loss: 0.00014413121971301734\n",
      "step: 11013, loss: 2.7083808618044714e-06\n",
      "step: 11014, loss: 5.974585292278789e-06\n",
      "step: 11015, loss: 9.585148291080259e-06\n",
      "step: 11016, loss: 5.8830530178966e-05\n",
      "step: 11017, loss: 3.0064081784075825e-06\n",
      "step: 11018, loss: 0.023395642638206482\n",
      "step: 11019, loss: 2.6654631710698595e-06\n",
      "step: 11020, loss: 5.55710403205012e-06\n",
      "step: 11021, loss: 0.055335186421871185\n",
      "step: 11022, loss: 3.1613558348908555e-06\n",
      "step: 11023, loss: 3.004066684297868e-07\n",
      "step: 11024, loss: 2.8262795240152627e-05\n",
      "step: 11025, loss: 1.7923255654750392e-05\n",
      "step: 11026, loss: 3.2757479857536964e-06\n",
      "step: 11027, loss: 8.062867891567294e-06\n",
      "step: 11028, loss: 6.301053872448392e-06\n",
      "step: 11029, loss: 0.000512988306581974\n",
      "step: 11030, loss: 0.04388199374079704\n",
      "step: 11031, loss: 1.560953023727052e-05\n",
      "step: 11032, loss: 8.821454002827522e-07\n",
      "step: 11033, loss: 3.058466245420277e-05\n",
      "step: 11034, loss: 4.7533489123452455e-05\n",
      "step: 11035, loss: 7.524418469984084e-05\n",
      "step: 11036, loss: 2.7465293896966614e-06\n",
      "step: 11037, loss: 0.00015771668404340744\n",
      "step: 11038, loss: 2.1656111130141653e-05\n",
      "step: 11039, loss: 0.004566115792840719\n",
      "step: 11040, loss: 3.3258481835218845e-06\n",
      "step: 11041, loss: 1.3615387615573127e-05\n",
      "step: 11042, loss: 2.529582161514554e-06\n",
      "step: 11043, loss: 1.586061443958897e-05\n",
      "step: 11044, loss: 1.7785915815693443e-06\n",
      "step: 11045, loss: 2.7249936920270557e-06\n",
      "step: 11046, loss: 0.13567399978637695\n",
      "step: 11047, loss: 2.4103701434796676e-06\n",
      "step: 11048, loss: 0.0001570006861584261\n",
      "step: 11049, loss: 3.8302139728330076e-05\n",
      "step: 11050, loss: 4.019548214273527e-06\n",
      "step: 11051, loss: 6.108502566348761e-05\n",
      "step: 11052, loss: 2.5745188395376317e-05\n",
      "step: 11053, loss: 0.0029083117842674255\n",
      "step: 11054, loss: 0.0019731635693460703\n",
      "step: 11055, loss: 3.53328891833371e-06\n",
      "step: 11056, loss: 1.9645492557174293e-06\n",
      "step: 11057, loss: 0.00013997583300806582\n",
      "step: 11058, loss: 1.1341464414726943e-05\n",
      "step: 11059, loss: 2.4260480131488293e-05\n",
      "step: 11060, loss: 2.231895996374078e-05\n",
      "step: 11061, loss: 0.04792872071266174\n",
      "step: 11062, loss: 0.000739648996386677\n",
      "step: 11063, loss: 0.00010841119365068153\n",
      "step: 11064, loss: 0.00719282915815711\n",
      "step: 11065, loss: 6.343880613712827e-06\n",
      "step: 11066, loss: 4.191289917798713e-06\n",
      "step: 11067, loss: 5.84306008022395e-06\n",
      "step: 11068, loss: 9.15011769393459e-05\n",
      "step: 11069, loss: 2.9826060199411586e-05\n",
      "step: 11070, loss: 1.1830770745291375e-05\n",
      "step: 11071, loss: 1.3309222595125902e-05\n",
      "step: 11072, loss: 1.0228088740404928e-06\n",
      "step: 11073, loss: 4.5773933379678056e-05\n",
      "step: 11074, loss: 8.527785212208983e-06\n",
      "step: 11075, loss: 2.269722244818695e-06\n",
      "step: 11076, loss: 5.924434390180977e-06\n",
      "step: 11077, loss: 1.0371092002969817e-06\n",
      "step: 11078, loss: 4.849243396165548e-06\n",
      "step: 11079, loss: 6.414007657440379e-05\n",
      "step: 11080, loss: 8.555766726203728e-06\n",
      "step: 11081, loss: 3.7382158097898355e-06\n",
      "step: 11082, loss: 5.559469627769431e-06\n",
      "step: 11083, loss: 1.107510797737632e-05\n",
      "step: 11084, loss: 1.9550266472379008e-07\n",
      "step: 11085, loss: 9.25059453038557e-07\n",
      "step: 11086, loss: 4.054619785165414e-05\n",
      "step: 11087, loss: 5.102143632029765e-07\n",
      "step: 11088, loss: 2.8218968509463593e-05\n",
      "step: 11089, loss: 1.6617530036455719e-06\n",
      "step: 11090, loss: 2.0309991668909788e-05\n",
      "step: 11091, loss: 1.3053858310740907e-05\n",
      "step: 11092, loss: 1.4090385320741916e-06\n",
      "step: 11093, loss: 1.4138116739559337e-06\n",
      "step: 11094, loss: 0.022579414770007133\n",
      "step: 11095, loss: 6.188942643348128e-05\n",
      "step: 11096, loss: 2.4508415208401857e-06\n",
      "step: 11097, loss: 4.703774720837828e-06\n",
      "step: 11098, loss: 1.4114306168266921e-06\n",
      "step: 11099, loss: 2.5796209683903726e-06\n",
      "step: 11100, loss: 0.01928880251944065\n",
      "step: 11101, loss: 8.72733198775677e-06\n",
      "step: 11102, loss: 2.036039404629264e-06\n",
      "step: 11103, loss: 1.6784417766757542e-06\n",
      "step: 11104, loss: 7.939303827697586e-07\n",
      "step: 11105, loss: 3.9697857573628426e-05\n",
      "step: 11106, loss: 1.5997691207303433e-06\n",
      "step: 11107, loss: 1.3112770602674573e-06\n",
      "step: 11108, loss: 2.6368331873527495e-06\n",
      "step: 11109, loss: 7.137814463931136e-06\n",
      "step: 11110, loss: 1.227843995366129e-06\n",
      "step: 11111, loss: 0.0769498273730278\n",
      "step: 11112, loss: 0.00019335553224664181\n",
      "step: 11113, loss: 7.185416325228289e-05\n",
      "step: 11114, loss: 1.2588269555635634e-06\n",
      "step: 11115, loss: 0.00040419449214823544\n",
      "step: 11116, loss: 2.331925497855991e-05\n",
      "step: 11117, loss: 5.717062322219135e-06\n",
      "step: 11118, loss: 7.820091809662699e-07\n",
      "step: 11119, loss: 0.18709726631641388\n",
      "step: 11120, loss: 1.456720610804041e-06\n",
      "step: 11121, loss: 9.679711183707695e-07\n",
      "step: 11122, loss: 2.31265573802375e-07\n",
      "step: 11123, loss: 4.2126839616685174e-06\n",
      "step: 11124, loss: 0.0005372199229896069\n",
      "step: 11125, loss: 0.000156558642629534\n",
      "step: 11126, loss: 0.049937814474105835\n",
      "step: 11127, loss: 4.553613962343661e-06\n",
      "step: 11128, loss: 2.8323213427938754e-06\n",
      "step: 11129, loss: 7.015128358034417e-05\n",
      "step: 11130, loss: 7.007059321040288e-05\n",
      "step: 11131, loss: 0.10549729317426682\n",
      "step: 11132, loss: 2.169906656490639e-05\n",
      "step: 11133, loss: 6.389585678334697e-07\n",
      "step: 11134, loss: 2.0741890693898313e-06\n",
      "step: 11135, loss: 0.000166006269864738\n",
      "step: 11136, loss: 6.389586815203074e-07\n",
      "step: 11137, loss: 1.1251323485339526e-05\n",
      "step: 11138, loss: 2.560568646003958e-06\n",
      "step: 11139, loss: 3.402139782338054e-06\n",
      "step: 11140, loss: 9.075372872757725e-06\n",
      "step: 11141, loss: 2.679788167370134e-06\n",
      "step: 11142, loss: 4.939932478009723e-05\n",
      "step: 11143, loss: 0.0028255251236259937\n",
      "step: 11144, loss: 7.674133485124912e-06\n",
      "step: 11145, loss: 3.3376545616192743e-06\n",
      "step: 11146, loss: 4.4819976210419554e-06\n",
      "step: 11147, loss: 5.483609015755064e-07\n",
      "step: 11148, loss: 1.7809697965276428e-06\n",
      "step: 11149, loss: 3.704181290231645e-05\n",
      "step: 11150, loss: 4.527375949692214e-06\n",
      "step: 11151, loss: 1.0338058928027749e-05\n",
      "step: 11152, loss: 1.1539389106474118e-06\n",
      "step: 11153, loss: 1.5950007536957855e-06\n",
      "step: 11154, loss: 0.0003044265613425523\n",
      "step: 11155, loss: 9.155133966487483e-07\n",
      "step: 11156, loss: 3.5723260225495324e-05\n",
      "step: 11157, loss: 7.297506726899883e-06\n",
      "step: 11158, loss: 1.2826806141674751e-06\n",
      "step: 11159, loss: 6.675695658486802e-07\n",
      "step: 11160, loss: 2.6916707156487973e-06\n",
      "step: 11161, loss: 7.548877329099923e-05\n",
      "step: 11162, loss: 1.3302836123330053e-05\n",
      "step: 11163, loss: 0.017106838524341583\n",
      "step: 11164, loss: 9.477668209001422e-05\n",
      "step: 11165, loss: 4.160282514931168e-06\n",
      "step: 11166, loss: 0.14455671608448029\n",
      "step: 11167, loss: 1.2946868992003147e-05\n",
      "step: 11168, loss: 4.172313197159383e-07\n",
      "step: 11169, loss: 5.044332738179946e-06\n",
      "step: 11170, loss: 4.4867833821626846e-06\n",
      "step: 11171, loss: 4.887346676696325e-06\n",
      "step: 11172, loss: 2.0193824639136437e-06\n",
      "step: 11173, loss: 3.424941678531468e-05\n",
      "step: 11174, loss: 4.70898266939912e-05\n",
      "step: 11175, loss: 6.794887212890899e-07\n",
      "step: 11176, loss: 5.621501259156503e-06\n",
      "step: 11177, loss: 2.0889438019366935e-05\n",
      "step: 11178, loss: 6.3340685301227495e-06\n",
      "step: 11179, loss: 2.596044032543432e-05\n",
      "step: 11180, loss: 1.3034851690463256e-05\n",
      "step: 11181, loss: 4.7658500079705846e-06\n",
      "step: 11182, loss: 2.5152739908662625e-06\n",
      "step: 11183, loss: 3.592817165554152e-06\n",
      "step: 11184, loss: 2.267282070533838e-06\n",
      "step: 11185, loss: 2.0361510905786417e-05\n",
      "step: 11186, loss: 1.7905134654938593e-06\n",
      "step: 11187, loss: 1.4420051229535602e-05\n",
      "step: 11188, loss: 8.982156032288913e-06\n",
      "step: 11189, loss: 3.3473388612037525e-06\n",
      "step: 11190, loss: 9.846100510912947e-06\n",
      "step: 11191, loss: 2.9938726584077813e-05\n",
      "step: 11192, loss: 1.616454483155394e-06\n",
      "step: 11193, loss: 2.7846697321365355e-06\n",
      "step: 11194, loss: 2.841095920302905e-05\n",
      "step: 11195, loss: 1.4042614111531293e-06\n",
      "step: 11196, loss: 2.4979246518341824e-05\n",
      "step: 11197, loss: 0.01901865564286709\n",
      "step: 11198, loss: 0.0013155188644304872\n",
      "step: 11199, loss: 2.455707317494671e-07\n",
      "step: 11200, loss: 4.701494617620483e-06\n",
      "step: 11201, loss: 9.15892596822232e-05\n",
      "step: 11202, loss: 4.3630450363707496e-07\n",
      "step: 11203, loss: 0.04111546650528908\n",
      "step: 11204, loss: 5.337767106539104e-06\n",
      "step: 11205, loss: 7.886621460784227e-05\n",
      "step: 11206, loss: 4.515456566878129e-06\n",
      "step: 11207, loss: 2.003682493523229e-05\n",
      "step: 11208, loss: 9.574325304129161e-06\n",
      "step: 11209, loss: 5.147019328433089e-05\n",
      "step: 11210, loss: 2.5578223358024843e-05\n",
      "step: 11211, loss: 2.192225838371087e-05\n",
      "step: 11212, loss: 1.701598012004979e-05\n",
      "step: 11213, loss: 3.840681893052533e-05\n",
      "step: 11214, loss: 7.147189535317011e-06\n",
      "step: 11215, loss: 2.137912633770611e-05\n",
      "step: 11216, loss: 3.890711013809778e-06\n",
      "step: 11217, loss: 0.0005055533838458359\n",
      "step: 11218, loss: 6.958478934393497e-06\n",
      "step: 11219, loss: 9.17541328817606e-05\n",
      "step: 11220, loss: 1.6546068764000665e-06\n",
      "step: 11221, loss: 3.6738683775183745e-06\n",
      "step: 11222, loss: 0.043344009667634964\n",
      "step: 11223, loss: 6.515614131785696e-06\n",
      "step: 11224, loss: 4.377238838060293e-06\n",
      "step: 11225, loss: 1.6689059521013405e-06\n",
      "step: 11226, loss: 6.842561788289458e-07\n",
      "step: 11227, loss: 2.0620662326109596e-05\n",
      "step: 11228, loss: 9.202884143633128e-07\n",
      "step: 11229, loss: 8.606854180470691e-07\n",
      "step: 11230, loss: 5.850194065715186e-05\n",
      "step: 11231, loss: 2.794225565594388e-06\n",
      "step: 11232, loss: 4.777519279741682e-06\n",
      "step: 11233, loss: 8.317644642374944e-06\n",
      "step: 11234, loss: 1.1732521670637652e-05\n",
      "step: 11235, loss: 0.09315046668052673\n",
      "step: 11236, loss: 9.706789569463581e-05\n",
      "step: 11237, loss: 1.4543425095325802e-06\n",
      "step: 11238, loss: 1.2093249097233638e-05\n",
      "step: 11239, loss: 1.5806764395165374e-06\n",
      "step: 11240, loss: 3.7215236261545215e-06\n",
      "step: 11241, loss: 1.96928704099264e-06\n",
      "step: 11242, loss: 2.1357423975132406e-05\n",
      "step: 11243, loss: 4.777763479069108e-06\n",
      "step: 11244, loss: 6.715488780173473e-06\n",
      "step: 11245, loss: 9.831418537942227e-06\n",
      "step: 11246, loss: 0.00015222765796352178\n",
      "step: 11247, loss: 1.4304939668363659e-06\n",
      "step: 11248, loss: 1.9747265469050035e-05\n",
      "step: 11249, loss: 6.54653376841452e-06\n",
      "step: 11250, loss: 0.09336972236633301\n",
      "step: 11251, loss: 1.0348518117098138e-05\n",
      "step: 11252, loss: 3.361640210641781e-06\n",
      "step: 11253, loss: 4.405853815114824e-06\n",
      "step: 11254, loss: 1.2764860912284348e-05\n",
      "step: 11255, loss: 8.50353444548091e-06\n",
      "step: 11256, loss: 6.034011676092632e-06\n",
      "step: 11257, loss: 2.2459327738033608e-05\n",
      "step: 11258, loss: 8.038956730160862e-05\n",
      "step: 11259, loss: 8.630711931800761e-07\n",
      "step: 11260, loss: 5.8933474065270275e-06\n",
      "step: 11261, loss: 2.810821797538665e-06\n",
      "step: 11262, loss: 2.710769194891327e-06\n",
      "step: 11263, loss: 2.572425046309945e-06\n",
      "step: 11264, loss: 0.09616990387439728\n",
      "step: 11265, loss: 2.741718844845309e-06\n",
      "step: 11266, loss: 2.4246403427241603e-06\n",
      "step: 11267, loss: 3.678556822706014e-06\n",
      "step: 11268, loss: 4.169760813965695e-06\n",
      "step: 11269, loss: 4.26259430241771e-06\n",
      "step: 11270, loss: 1.5902369341347367e-06\n",
      "step: 11271, loss: 7.343210199906025e-07\n",
      "step: 11272, loss: 9.52394930209266e-06\n",
      "step: 11273, loss: 2.9085902042425005e-06\n",
      "step: 11274, loss: 6.391368970071198e-06\n",
      "step: 11275, loss: 1.6045389656937914e-06\n",
      "step: 11276, loss: 1.4238391486287583e-05\n",
      "step: 11277, loss: 0.06703457236289978\n",
      "step: 11278, loss: 2.0146094357187394e-06\n",
      "step: 11279, loss: 0.15302424132823944\n",
      "step: 11280, loss: 1.76429551856927e-07\n",
      "step: 11281, loss: 2.186269512094441e-06\n",
      "step: 11282, loss: 1.7308883570876787e-06\n",
      "step: 11283, loss: 2.6439499833941227e-06\n",
      "step: 11284, loss: 4.133818492846331e-06\n",
      "step: 11285, loss: 6.82049130773521e-06\n",
      "step: 11286, loss: 1.127702375924855e-06\n",
      "step: 11287, loss: 3.743161585134658e-07\n",
      "step: 11288, loss: 4.043477474624524e-06\n",
      "step: 11289, loss: 3.3448775411670795e-06\n",
      "step: 11290, loss: 4.959099442203296e-07\n",
      "step: 11291, loss: 7.50898016121937e-06\n",
      "step: 11292, loss: 2.5248075417039217e-06\n",
      "step: 11293, loss: 1.558835901960265e-05\n",
      "step: 11294, loss: 5.054454277342302e-07\n",
      "step: 11295, loss: 4.494180757319555e-05\n",
      "step: 11296, loss: 1.037109882418008e-06\n",
      "step: 11297, loss: 2.6892857931670733e-06\n",
      "step: 11298, loss: 4.501152034208644e-06\n",
      "step: 11299, loss: 6.848990324215265e-06\n",
      "step: 11300, loss: 2.4735431907174643e-06\n",
      "step: 11301, loss: 1.3970264262752607e-05\n",
      "step: 11302, loss: 4.911394171358552e-07\n",
      "step: 11303, loss: 3.1851925541559467e-06\n",
      "step: 11304, loss: 1.196849098050734e-06\n",
      "step: 11305, loss: 7.16898603059235e-06\n",
      "step: 11306, loss: 1.2922100722789764e-06\n",
      "step: 11307, loss: 3.361688243330718e-07\n",
      "step: 11308, loss: 1.2757202057400718e-05\n",
      "step: 11309, loss: 0.0003746565489564091\n",
      "step: 11310, loss: 9.107553182730044e-07\n",
      "step: 11311, loss: 1.8167131656809943e-06\n",
      "step: 11312, loss: 0.00016928589320741594\n",
      "step: 11313, loss: 1.5282348613254726e-06\n",
      "step: 11314, loss: 1.8739207234830246e-06\n",
      "step: 11315, loss: 0.0817960724234581\n",
      "step: 11316, loss: 4.386885450458067e-07\n",
      "step: 11317, loss: 0.041217684745788574\n",
      "step: 11318, loss: 7.142620233935304e-06\n",
      "step: 11319, loss: 8.360622814507224e-06\n",
      "step: 11320, loss: 0.00012676302867475897\n",
      "step: 11321, loss: 5.618954673991539e-06\n",
      "step: 11322, loss: 8.550980965083e-06\n",
      "step: 11323, loss: 2.1552821181103354e-06\n",
      "step: 11324, loss: 0.003409850411117077\n",
      "step: 11325, loss: 7.843908065297001e-07\n",
      "step: 11326, loss: 1.8620053197082598e-06\n",
      "step: 11327, loss: 1.5044010979181621e-06\n",
      "step: 11328, loss: 4.856511168327415e-06\n",
      "step: 11329, loss: 0.00025247674784623086\n",
      "step: 11330, loss: 2.061623308691196e-05\n",
      "step: 11331, loss: 2.408379441476427e-05\n",
      "step: 11332, loss: 1.3613538385470747e-06\n",
      "step: 11333, loss: 1.5020192449810565e-06\n",
      "step: 11334, loss: 3.3616811379033606e-07\n",
      "step: 11335, loss: 4.040313797304407e-05\n",
      "step: 11336, loss: 7.820431346772239e-05\n",
      "step: 11337, loss: 1.3585982742370106e-05\n",
      "step: 11338, loss: 8.630637466922053e-07\n",
      "step: 11339, loss: 2.477089765307028e-06\n",
      "step: 11340, loss: 5.245196916803252e-07\n",
      "step: 11341, loss: 1.6840878743096255e-05\n",
      "step: 11342, loss: 1.3965835933049675e-05\n",
      "step: 11343, loss: 8.58301348216628e-07\n",
      "step: 11344, loss: 1.1491736131574726e-06\n",
      "step: 11345, loss: 7.2373081820842344e-06\n",
      "step: 11346, loss: 9.9508288258221e-05\n",
      "step: 11347, loss: 1.9311872279104136e-07\n",
      "step: 11348, loss: 7.09687992639374e-06\n",
      "step: 11349, loss: 7.588034350192174e-06\n",
      "step: 11350, loss: 5.051476910011843e-05\n",
      "step: 11351, loss: 1.1906278814421967e-05\n",
      "step: 11352, loss: 1.6112169760162942e-05\n",
      "step: 11353, loss: 1.542549284749839e-06\n",
      "step: 11354, loss: 5.957807388767833e-06\n",
      "step: 11355, loss: 1.2859173693868797e-05\n",
      "step: 11356, loss: 7.302207905013347e-06\n",
      "step: 11357, loss: 0.00020107306772843003\n",
      "step: 11358, loss: 7.471410299331183e-06\n",
      "step: 11359, loss: 2.242740629299078e-05\n",
      "step: 11360, loss: 3.6954807569600234e-07\n",
      "step: 11361, loss: 1.156307462224504e-06\n",
      "step: 11362, loss: 1.4662588228020468e-06\n",
      "step: 11363, loss: 2.1075654785818188e-06\n",
      "step: 11364, loss: 8.343375156982802e-06\n",
      "step: 11365, loss: 2.3483926270273514e-06\n",
      "step: 11366, loss: 1.2236851944180671e-05\n",
      "step: 11367, loss: 0.00038395452429540455\n",
      "step: 11368, loss: 2.260167320855544e-06\n",
      "step: 11369, loss: 6.785545701859519e-05\n",
      "step: 11370, loss: 2.2196102236193838e-06\n",
      "step: 11371, loss: 1.6474557469337014e-06\n",
      "step: 11372, loss: 4.131672540097497e-06\n",
      "step: 11373, loss: 1.4829369092694833e-06\n",
      "step: 11374, loss: 7.171013294282602e-06\n",
      "step: 11375, loss: 1.228545170306461e-05\n",
      "step: 11376, loss: 1.320827550443937e-06\n",
      "step: 11377, loss: 7.566887688881252e-06\n",
      "step: 11378, loss: 1.1563238331291359e-06\n",
      "step: 11379, loss: 7.702363291173242e-06\n",
      "step: 11380, loss: 4.889692263532197e-06\n",
      "step: 11381, loss: 1.0561877843429102e-06\n",
      "step: 11382, loss: 4.954157247993862e-06\n",
      "step: 11383, loss: 7.459745575033594e-06\n",
      "step: 11384, loss: 1.9049102775170468e-06\n",
      "step: 11385, loss: 9.934386071108747e-06\n",
      "step: 11386, loss: 0.01757104881107807\n",
      "step: 11387, loss: 2.626605964906048e-05\n",
      "step: 11388, loss: 0.00016124671674333513\n",
      "step: 11389, loss: 5.256917575025e-06\n",
      "step: 11390, loss: 3.9100589788176876e-07\n",
      "step: 11391, loss: 8.515750778315123e-06\n",
      "step: 11392, loss: 8.07254582468886e-06\n",
      "step: 11393, loss: 3.266243083999143e-06\n",
      "step: 11394, loss: 1.9645497104647802e-06\n",
      "step: 11395, loss: 2.6583136332192225e-06\n",
      "step: 11396, loss: 4.7557041398249567e-05\n",
      "step: 11397, loss: 0.000136614398797974\n",
      "step: 11398, loss: 5.955306733085308e-06\n",
      "step: 11399, loss: 0.2191767692565918\n",
      "step: 11400, loss: 1.3661189086633385e-06\n",
      "step: 11401, loss: 4.710846496891463e-06\n",
      "step: 11402, loss: 3.3997512218775228e-06\n",
      "step: 11403, loss: 9.795476216822863e-06\n",
      "step: 11404, loss: 0.00016494528972543776\n",
      "step: 11405, loss: 7.771603122819215e-06\n",
      "step: 11406, loss: 4.863724143433501e-07\n",
      "step: 11407, loss: 3.228695641155355e-05\n",
      "step: 11408, loss: 6.949042017367901e-06\n",
      "step: 11409, loss: 9.297605174651835e-06\n",
      "step: 11410, loss: 5.912758069825941e-07\n",
      "step: 11411, loss: 5.063314893050119e-05\n",
      "step: 11412, loss: 1.0228062592432252e-06\n",
      "step: 11413, loss: 4.289057869755197e-06\n",
      "step: 11414, loss: 7.308777639991604e-06\n",
      "step: 11415, loss: 2.691674126253929e-06\n",
      "step: 11416, loss: 4.5060957631903875e-07\n",
      "step: 11417, loss: 1.4853367247269489e-06\n",
      "step: 11418, loss: 4.4141357648186386e-05\n",
      "step: 11419, loss: 5.924295237491606e-06\n",
      "step: 11420, loss: 7.986049240571447e-06\n",
      "step: 11421, loss: 3.68088626601093e-06\n",
      "step: 11422, loss: 3.4355668958596652e-06\n",
      "step: 11423, loss: 1.593076558492612e-05\n",
      "step: 11424, loss: 1.5401545852000709e-06\n",
      "step: 11425, loss: 2.0477438738453202e-05\n",
      "step: 11426, loss: 3.84307058993727e-05\n",
      "step: 11427, loss: 3.075589347645291e-07\n",
      "step: 11428, loss: 1.6593625105087995e-06\n",
      "step: 11429, loss: 2.50339240892572e-07\n",
      "step: 11430, loss: 5.421445166575722e-06\n",
      "step: 11431, loss: 4.798966983798891e-06\n",
      "step: 11432, loss: 7.28061786503531e-06\n",
      "step: 11433, loss: 4.744502462017408e-07\n",
      "step: 11434, loss: 8.439892553724349e-07\n",
      "step: 11435, loss: 7.1492668212158605e-06\n",
      "step: 11436, loss: 3.449793439358473e-06\n",
      "step: 11437, loss: 1.9408062144066207e-05\n",
      "step: 11438, loss: 3.4926838452520315e-06\n",
      "step: 11439, loss: 5.269020562082005e-07\n",
      "step: 11440, loss: 1.3994944083606242e-06\n",
      "step: 11441, loss: 9.155096449831035e-07\n",
      "step: 11442, loss: 0.08215638250112534\n",
      "step: 11443, loss: 1.4622230992245022e-05\n",
      "step: 11444, loss: 1.6450712791993283e-06\n",
      "step: 11445, loss: 3.4878946735261707e-06\n",
      "step: 11446, loss: 8.434541086899117e-06\n",
      "step: 11447, loss: 8.29637428978458e-06\n",
      "step: 11448, loss: 9.490829143032897e-06\n",
      "step: 11449, loss: 5.125971256347839e-07\n",
      "step: 11450, loss: 9.131288720709563e-07\n",
      "step: 11451, loss: 3.4332234122302907e-07\n",
      "step: 11452, loss: 1.361865179205779e-05\n",
      "step: 11453, loss: 0.005816513206809759\n",
      "step: 11454, loss: 1.7332713468931615e-06\n",
      "step: 11455, loss: 1.4503273632726632e-05\n",
      "step: 11456, loss: 2.2172861235958408e-07\n",
      "step: 11457, loss: 7.271739832503954e-07\n",
      "step: 11458, loss: 2.9614211598527618e-05\n",
      "step: 11459, loss: 8.448349944956135e-06\n",
      "step: 11460, loss: 0.10875857621431351\n",
      "step: 11461, loss: 3.914728949894197e-06\n",
      "step: 11462, loss: 1.5735431588836946e-06\n",
      "step: 11463, loss: 1.018042553369014e-06\n",
      "step: 11464, loss: 6.4484165704925545e-06\n",
      "step: 11465, loss: 1.677523323451169e-05\n",
      "step: 11466, loss: 7.486315212190675e-07\n",
      "step: 11467, loss: 2.3936685465741903e-05\n",
      "step: 11468, loss: 5.881837569177151e-05\n",
      "step: 11469, loss: 5.814600172016071e-06\n",
      "step: 11470, loss: 4.269936198397772e-06\n",
      "step: 11471, loss: 3.3568410344742006e-06\n",
      "step: 11472, loss: 1.041532050294336e-05\n",
      "step: 11473, loss: 6.739794116583653e-06\n",
      "step: 11474, loss: 5.674324370374961e-07\n",
      "step: 11475, loss: 0.0023135761730372906\n",
      "step: 11476, loss: 5.0065018513123505e-06\n",
      "step: 11477, loss: 7.266292413987685e-06\n",
      "step: 11478, loss: 7.69087091612164e-06\n",
      "step: 11479, loss: 2.202968516940018e-06\n",
      "step: 11480, loss: 2.7512387532624416e-06\n",
      "step: 11481, loss: 2.103667975461576e-05\n",
      "step: 11482, loss: 3.988676326116547e-06\n",
      "step: 11483, loss: 0.000169333114172332\n",
      "step: 11484, loss: 5.364377102523576e-07\n",
      "step: 11485, loss: 1.2779081544067594e-06\n",
      "step: 11486, loss: 2.6654317935026484e-06\n",
      "step: 11487, loss: 6.72338728691102e-07\n",
      "step: 11488, loss: 5.75747799302917e-06\n",
      "step: 11489, loss: 6.243696407182142e-05\n",
      "step: 11490, loss: 4.996929874323541e-06\n",
      "step: 11491, loss: 6.0148727243358735e-06\n",
      "step: 11492, loss: 1.0701432074711192e-05\n",
      "step: 11493, loss: 0.04035501554608345\n",
      "step: 11494, loss: 8.562839866499417e-06\n",
      "step: 11495, loss: 1.008504796118359e-06\n",
      "step: 11496, loss: 1.1854706826852635e-05\n",
      "step: 11497, loss: 0.055430226027965546\n",
      "step: 11498, loss: 1.4495725508822943e-06\n",
      "step: 11499, loss: 1.85872231668327e-05\n",
      "step: 11500, loss: 7.764681868138723e-06\n",
      "step: 11501, loss: 4.675256604969036e-06\n",
      "step: 11502, loss: 1.77619119767769e-06\n",
      "step: 11503, loss: 0.00010520795331103727\n",
      "step: 11504, loss: 7.486293043257319e-07\n",
      "step: 11505, loss: 2.4282708181999624e-05\n",
      "step: 11506, loss: 4.157679086347343e-06\n",
      "step: 11507, loss: 2.2423062546295114e-05\n",
      "step: 11508, loss: 1.2620766938198358e-05\n",
      "step: 11509, loss: 5.7595416365074925e-06\n",
      "step: 11510, loss: 0.12432987242937088\n",
      "step: 11511, loss: 3.1493682399741374e-06\n",
      "step: 11512, loss: 2.1791070139443036e-06\n",
      "step: 11513, loss: 0.00021271698642522097\n",
      "step: 11514, loss: 1.276371767744422e-05\n",
      "step: 11515, loss: 3.0039909688639455e-06\n",
      "step: 11516, loss: 1.3279533277454902e-06\n",
      "step: 11517, loss: 0.00010992755414918065\n",
      "step: 11518, loss: 6.937679245311301e-06\n",
      "step: 11519, loss: 3.55713268618274e-06\n",
      "step: 11520, loss: 2.0274866983527318e-05\n",
      "step: 11521, loss: 2.761519317573402e-05\n",
      "step: 11522, loss: 8.654462817503372e-07\n",
      "step: 11523, loss: 0.00018775896751321852\n",
      "step: 11524, loss: 1.1671088032016996e-05\n",
      "step: 11525, loss: 3.498874502838589e-05\n",
      "step: 11526, loss: 1.3663594472745899e-05\n",
      "step: 11527, loss: 0.005984657909721136\n",
      "step: 11528, loss: 3.2185762393055484e-06\n",
      "step: 11529, loss: 6.574706731043989e-06\n",
      "step: 11530, loss: 1.6988347852020524e-05\n",
      "step: 11531, loss: 0.06768834590911865\n",
      "step: 11532, loss: 2.64690606854856e-05\n",
      "step: 11533, loss: 8.172300113074016e-06\n",
      "step: 11534, loss: 8.221924872486852e-06\n",
      "step: 11535, loss: 0.08453565835952759\n",
      "step: 11536, loss: 1.2877178050985094e-05\n",
      "step: 11537, loss: 0.032091930508613586\n",
      "step: 11538, loss: 2.4150691388058476e-05\n",
      "step: 11539, loss: 3.1112576834857464e-06\n",
      "step: 11540, loss: 2.1194960027060006e-06\n",
      "step: 11541, loss: 3.17768135573715e-05\n",
      "step: 11542, loss: 4.966032520314911e-06\n",
      "step: 11543, loss: 7.528647074650507e-06\n",
      "step: 11544, loss: 2.555779929025448e-06\n",
      "step: 11545, loss: 1.750498813635204e-05\n",
      "step: 11546, loss: 1.7571187527209986e-06\n",
      "step: 11547, loss: 1.4662451803815202e-06\n",
      "step: 11548, loss: 9.304923878517002e-06\n",
      "step: 11549, loss: 1.1110223567811772e-06\n",
      "step: 11550, loss: 1.0609581977405469e-06\n",
      "step: 11551, loss: 1.8953901417262387e-06\n",
      "step: 11552, loss: 1.3208262998887221e-06\n",
      "step: 11553, loss: 5.555138500312751e-07\n",
      "step: 11554, loss: 1.6569676972721936e-06\n",
      "step: 11555, loss: 3.6716329532282543e-07\n",
      "step: 11556, loss: 0.12536078691482544\n",
      "step: 11557, loss: 2.3562757633044384e-05\n",
      "step: 11558, loss: 3.5522743928595446e-06\n",
      "step: 11559, loss: 7.917302355053835e-06\n",
      "step: 11560, loss: 1.537779780846904e-06\n",
      "step: 11561, loss: 7.224059572763508e-07\n",
      "step: 11562, loss: 6.32718456472503e-06\n",
      "step: 11563, loss: 0.12806499004364014\n",
      "step: 11564, loss: 1.673679776104109e-06\n",
      "step: 11565, loss: 3.468850081844721e-06\n",
      "step: 11566, loss: 1.5974021039255604e-07\n",
      "step: 11567, loss: 7.426731463056058e-05\n",
      "step: 11568, loss: 3.933894845431496e-07\n",
      "step: 11569, loss: 3.638093176050461e-06\n",
      "step: 11570, loss: 3.6009376344736665e-05\n",
      "step: 11571, loss: 2.724948672039318e-06\n",
      "step: 11572, loss: 4.284231181372888e-06\n",
      "step: 11573, loss: 2.4127559299813583e-05\n",
      "step: 11574, loss: 1.065725996340916e-06\n",
      "step: 11575, loss: 4.792196932612569e-07\n",
      "step: 11576, loss: 2.6320726647099946e-06\n",
      "step: 11577, loss: 0.07167572528123856\n",
      "step: 11578, loss: 5.9123244682268705e-06\n",
      "step: 11579, loss: 9.536648803987191e-07\n",
      "step: 11580, loss: 3.6046885725227185e-06\n",
      "step: 11581, loss: 1.4471545455307933e-06\n",
      "step: 11582, loss: 6.694295279885409e-06\n",
      "step: 11583, loss: 1.165853745987988e-06\n",
      "step: 11584, loss: 1.94307062884036e-06\n",
      "step: 11585, loss: 4.219996583287866e-07\n",
      "step: 11586, loss: 1.502026407251833e-06\n",
      "step: 11587, loss: 0.012913371436297894\n",
      "step: 11588, loss: 7.309533430088777e-06\n",
      "step: 11589, loss: 1.7404356640327023e-06\n",
      "step: 11590, loss: 1.6307574242091505e-06\n",
      "step: 11591, loss: 1.4996342088124948e-06\n",
      "step: 11592, loss: 2.9802259859934566e-07\n",
      "step: 11593, loss: 4.710166103905067e-05\n",
      "step: 11594, loss: 1.189369686471764e-05\n",
      "step: 11595, loss: 1.7118218238465488e-06\n",
      "step: 11596, loss: 0.00010108120477525517\n",
      "step: 11597, loss: 5.4021593314246275e-06\n",
      "step: 11598, loss: 0.00026322828489355743\n",
      "step: 11599, loss: 5.6118415159289725e-06\n",
      "step: 11600, loss: 2.7060114007326774e-06\n",
      "step: 11601, loss: 2.6797458758665016e-06\n",
      "step: 11602, loss: 4.524714768194826e-06\n",
      "step: 11603, loss: 1.2469183729990618e-06\n",
      "step: 11604, loss: 2.145719236068544e-06\n",
      "step: 11605, loss: 1.4829510064373608e-06\n",
      "step: 11606, loss: 1.5282378171832534e-06\n",
      "step: 11607, loss: 8.703263119969051e-06\n",
      "step: 11608, loss: 1.6396867067669518e-05\n",
      "step: 11609, loss: 0.00010957375343423337\n",
      "step: 11610, loss: 0.0010343367466703057\n",
      "step: 11611, loss: 1.5353945173046668e-06\n",
      "step: 11612, loss: 1.8391334378975444e-05\n",
      "step: 11613, loss: 1.6760699281803682e-06\n",
      "step: 11614, loss: 7.287636435648892e-06\n",
      "step: 11615, loss: 3.5974799175164662e-06\n",
      "step: 11616, loss: 3.926530553144403e-06\n",
      "step: 11617, loss: 1.8930219312096597e-06\n",
      "step: 11618, loss: 3.457058426192816e-07\n",
      "step: 11619, loss: 1.1870039088535123e-05\n",
      "step: 11620, loss: 0.00010334455146221444\n",
      "step: 11621, loss: 6.923082310095197e-06\n",
      "step: 11622, loss: 3.435494363657199e-06\n",
      "step: 11623, loss: 3.149363919874304e-06\n",
      "step: 11624, loss: 0.14402295649051666\n",
      "step: 11625, loss: 4.839885718865844e-07\n",
      "step: 11626, loss: 8.625135706097353e-06\n",
      "step: 11627, loss: 3.9886099330033176e-06\n",
      "step: 11628, loss: 1.4304989690572256e-06\n",
      "step: 11629, loss: 5.14365965500474e-05\n",
      "step: 11630, loss: 2.7154760573466774e-06\n",
      "step: 11631, loss: 2.0860014046775177e-05\n",
      "step: 11632, loss: 4.44632678409107e-06\n",
      "step: 11633, loss: 1.878695002233144e-06\n",
      "step: 11634, loss: 2.217250539615634e-06\n",
      "step: 11635, loss: 1.3542047554437886e-06\n",
      "step: 11636, loss: 4.74675925943302e-06\n",
      "step: 11637, loss: 1.6903503592402558e-06\n",
      "step: 11638, loss: 1.6069030834842124e-06\n",
      "step: 11639, loss: 9.468990356253926e-06\n",
      "step: 11640, loss: 1.3708915957977297e-06\n",
      "step: 11641, loss: 7.520747021771967e-05\n",
      "step: 11642, loss: 7.506214751629159e-05\n",
      "step: 11643, loss: 6.00813166329317e-07\n",
      "step: 11644, loss: 2.4031780867517227e-06\n",
      "step: 11645, loss: 1.6712956494302489e-06\n",
      "step: 11646, loss: 7.24788776551577e-07\n",
      "step: 11647, loss: 1.2683797194767976e-06\n",
      "step: 11648, loss: 4.1697949200170115e-06\n",
      "step: 11649, loss: 3.38384706992656e-05\n",
      "step: 11650, loss: 3.2495613595528994e-06\n",
      "step: 11651, loss: 1.7630894944886677e-05\n",
      "step: 11652, loss: 6.982412742218003e-06\n",
      "step: 11653, loss: 6.484583082055906e-06\n",
      "step: 11654, loss: 2.069439688057173e-06\n",
      "step: 11655, loss: 2.3197642349259695e-06\n",
      "step: 11656, loss: 0.10337287187576294\n",
      "step: 11657, loss: 1.2683739214480738e-06\n",
      "step: 11658, loss: 1.3947334309705184e-06\n",
      "step: 11659, loss: 8.557936780562159e-06\n",
      "step: 11660, loss: 1.2963998415216338e-05\n",
      "step: 11661, loss: 6.091238446970237e-06\n",
      "step: 11662, loss: 2.129011136275949e-06\n",
      "step: 11663, loss: 2.2315603018796537e-06\n",
      "step: 11664, loss: 2.0861207303823903e-06\n",
      "step: 11665, loss: 3.2948439638857963e-06\n",
      "step: 11666, loss: 7.367056014118134e-07\n",
      "step: 11667, loss: 6.846976248198189e-06\n",
      "step: 11668, loss: 1.496036202297546e-05\n",
      "step: 11669, loss: 5.459768317450653e-07\n",
      "step: 11670, loss: 0.0012758431257680058\n",
      "step: 11671, loss: 8.124622581817675e-06\n",
      "step: 11672, loss: 1.4503925740427803e-05\n",
      "step: 11673, loss: 5.886142389499582e-06\n",
      "step: 11674, loss: 6.81085202813847e-06\n",
      "step: 11675, loss: 2.1147452571312897e-06\n",
      "step: 11676, loss: 1.4587672922061756e-05\n",
      "step: 11677, loss: 1.206390152219683e-06\n",
      "step: 11678, loss: 1.0224299330729991e-05\n",
      "step: 11679, loss: 5.531295528271585e-07\n",
      "step: 11680, loss: 0.0335678905248642\n",
      "step: 11681, loss: 3.466519501671428e-06\n",
      "step: 11682, loss: 1.027580310619669e-06\n",
      "step: 11683, loss: 2.291174496349413e-06\n",
      "step: 11684, loss: 2.524727051422815e-06\n",
      "step: 11685, loss: 1.6830386812216602e-05\n",
      "step: 11686, loss: 9.146079719357658e-06\n",
      "step: 11687, loss: 7.628041203133762e-06\n",
      "step: 11688, loss: 6.491498879768187e-06\n",
      "step: 11689, loss: 0.0001876453316072002\n",
      "step: 11690, loss: 1.00336255854927e-05\n",
      "step: 11691, loss: 3.125549483229406e-06\n",
      "step: 11692, loss: 1.5464880561921746e-05\n",
      "step: 11693, loss: 2.5534086489642505e-06\n",
      "step: 11694, loss: 2.3820868591428734e-05\n",
      "step: 11695, loss: 2.298317895110813e-06\n",
      "step: 11696, loss: 5.8386790442455094e-06\n",
      "step: 11697, loss: 1.7237449583262787e-06\n",
      "step: 11698, loss: 0.08301858603954315\n",
      "step: 11699, loss: 3.4970064007211477e-05\n",
      "step: 11700, loss: 1.7332524748780997e-06\n",
      "step: 11701, loss: 3.9874936192063615e-05\n",
      "step: 11702, loss: 9.560483249515528e-07\n",
      "step: 11703, loss: 1.3756651924268226e-06\n",
      "step: 11704, loss: 3.1279864742828067e-06\n",
      "step: 11705, loss: 8.092965799733065e-06\n",
      "step: 11706, loss: 5.988468274154002e-06\n",
      "step: 11707, loss: 1.6379252656406607e-06\n",
      "step: 11708, loss: 7.265739986905828e-05\n",
      "step: 11709, loss: 2.077954195556231e-05\n",
      "step: 11710, loss: 0.04716360569000244\n",
      "step: 11711, loss: 6.270395260798978e-07\n",
      "step: 11712, loss: 4.2222281990689225e-06\n",
      "step: 11713, loss: 1.6379208318539895e-06\n",
      "step: 11714, loss: 4.3392020643295837e-07\n",
      "step: 11715, loss: 6.343688710330753e-06\n",
      "step: 11716, loss: 7.868972716096323e-06\n",
      "step: 11717, loss: 4.505861397774424e-06\n",
      "step: 11718, loss: 1.831569170462899e-05\n",
      "step: 11719, loss: 0.0001639270776649937\n",
      "step: 11720, loss: 4.315908518037759e-05\n",
      "step: 11721, loss: 4.43916542280931e-06\n",
      "step: 11722, loss: 8.372730007977225e-06\n",
      "step: 11723, loss: 2.1260326320771128e-05\n",
      "step: 11724, loss: 7.4262229645682964e-06\n",
      "step: 11725, loss: 1.8872156942961738e-05\n",
      "step: 11726, loss: 6.791648502257885e-06\n",
      "step: 11727, loss: 8.305812116304878e-06\n",
      "step: 11728, loss: 8.92095977178542e-06\n",
      "step: 11729, loss: 1.9069533664151095e-05\n",
      "step: 11730, loss: 1.9931655970140127e-06\n",
      "step: 11731, loss: 5.697008236893453e-05\n",
      "step: 11732, loss: 6.252668390516192e-05\n",
      "step: 11733, loss: 0.00010429337271489203\n",
      "step: 11734, loss: 3.0946092010708526e-05\n",
      "step: 11735, loss: 5.197502446208091e-07\n",
      "step: 11736, loss: 6.846876203780994e-06\n",
      "step: 11737, loss: 2.36890646192478e-05\n",
      "step: 11738, loss: 0.09127651900053024\n",
      "step: 11739, loss: 0.00019065657397732139\n",
      "step: 11740, loss: 7.963142820699431e-07\n",
      "step: 11741, loss: 3.6522909795166925e-05\n",
      "step: 11742, loss: 8.225410397244559e-07\n",
      "step: 11743, loss: 1.9375685951672494e-05\n",
      "step: 11744, loss: 6.927503818587866e-06\n",
      "step: 11745, loss: 7.152116722863866e-06\n",
      "step: 11746, loss: 1.885819438030012e-05\n",
      "step: 11747, loss: 1.9131226508761756e-05\n",
      "step: 11748, loss: 0.0001414842699887231\n",
      "step: 11749, loss: 2.245880523332744e-06\n",
      "step: 11750, loss: 5.826732831337722e-06\n",
      "step: 11751, loss: 3.279933662270196e-05\n",
      "step: 11752, loss: 2.9521870601456612e-06\n",
      "step: 11753, loss: 5.225889708526665e-06\n",
      "step: 11754, loss: 1.7880987570606521e-06\n",
      "step: 11755, loss: 7.414201263600262e-06\n",
      "step: 11756, loss: 0.04251093044877052\n",
      "step: 11757, loss: 1.4386727343662642e-05\n",
      "step: 11758, loss: 9.131388196692569e-07\n",
      "step: 11759, loss: 0.0002862826513592154\n",
      "step: 11760, loss: 3.6191204344504513e-06\n",
      "step: 11761, loss: 7.830623508198187e-05\n",
      "step: 11762, loss: 0.0016656151274219155\n",
      "step: 11763, loss: 2.3435807179339463e-06\n",
      "step: 11764, loss: 4.933057425660081e-05\n",
      "step: 11765, loss: 2.552444493630901e-05\n",
      "step: 11766, loss: 8.67840697083011e-07\n",
      "step: 11767, loss: 1.6760645848989952e-06\n",
      "step: 11768, loss: 5.1498244602044e-07\n",
      "step: 11769, loss: 1.835794932958379e-06\n",
      "step: 11770, loss: 0.0009250122820958495\n",
      "step: 11771, loss: 3.6333929074316984e-06\n",
      "step: 11772, loss: 2.6251944291288964e-05\n",
      "step: 11773, loss: 1.430508973498945e-07\n",
      "step: 11774, loss: 2.906455847551115e-05\n",
      "step: 11775, loss: 2.622598458401626e-07\n",
      "step: 11776, loss: 3.8385317679967557e-07\n",
      "step: 11777, loss: 1.565908496559132e-05\n",
      "step: 11778, loss: 0.06930401921272278\n",
      "step: 11779, loss: 6.079175363993272e-06\n",
      "step: 11780, loss: 5.032763965573395e-06\n",
      "step: 11781, loss: 4.434402853803476e-06\n",
      "step: 11782, loss: 7.118158009689068e-06\n",
      "step: 11783, loss: 7.034692316665314e-06\n",
      "step: 11784, loss: 5.647374564432539e-06\n",
      "step: 11785, loss: 1.619318027223926e-05\n",
      "step: 11786, loss: 1.2850665598307387e-06\n",
      "step: 11787, loss: 1.0013392284236033e-06\n",
      "step: 11788, loss: 5.42139787285123e-06\n",
      "step: 11789, loss: 2.2670148609904572e-05\n",
      "step: 11790, loss: 1.6712951037334278e-05\n",
      "step: 11791, loss: 5.690041871275753e-05\n",
      "step: 11792, loss: 5.316717306413921e-07\n",
      "step: 11793, loss: 5.282915481075179e-06\n",
      "step: 11794, loss: 0.03584269434213638\n",
      "step: 11795, loss: 2.9849045404262142e-06\n",
      "step: 11796, loss: 2.0575175767589826e-06\n",
      "step: 11797, loss: 1.4614907968280022e-06\n",
      "step: 11798, loss: 1.988370740946266e-06\n",
      "step: 11799, loss: 1.572715882502962e-05\n",
      "step: 11800, loss: 2.3960592443472706e-06\n",
      "step: 11801, loss: 1.2850412076659268e-06\n",
      "step: 11802, loss: 2.596297463242081e-06\n",
      "step: 11803, loss: 4.415355306264246e-06\n",
      "step: 11804, loss: 1.0637175364536233e-05\n",
      "step: 11805, loss: 8.749824473852641e-07\n",
      "step: 11806, loss: 1.673522092460189e-05\n",
      "step: 11807, loss: 2.922829480667133e-06\n",
      "step: 11808, loss: 3.3424428238504333e-06\n",
      "step: 11809, loss: 5.8267828535463195e-06\n",
      "step: 11810, loss: 6.508777801172982e-07\n",
      "step: 11811, loss: 3.7907705063844332e-06\n",
      "step: 11812, loss: 8.184215403161943e-06\n",
      "step: 11813, loss: 2.5538825866533443e-05\n",
      "step: 11814, loss: 0.005283038597553968\n",
      "step: 11815, loss: 1.537773300697154e-06\n",
      "step: 11816, loss: 3.924135671695694e-05\n",
      "step: 11817, loss: 9.312581096310169e-05\n",
      "step: 11818, loss: 4.51552386948606e-06\n",
      "step: 11819, loss: 1.4567204971172032e-06\n",
      "step: 11820, loss: 4.649139668799762e-07\n",
      "step: 11821, loss: 8.683850319357589e-05\n",
      "step: 11822, loss: 5.733281341235852e-06\n",
      "step: 11823, loss: 9.72742896010459e-07\n",
      "step: 11824, loss: 1.5611522030667402e-05\n",
      "step: 11825, loss: 2.2458830244431738e-06\n",
      "step: 11826, loss: 4.527398232312407e-06\n",
      "step: 11827, loss: 5.701919144485146e-05\n",
      "step: 11828, loss: 1.0565335287537891e-05\n",
      "step: 11829, loss: 5.316718443282298e-07\n",
      "step: 11830, loss: 0.00046073441626504064\n",
      "step: 11831, loss: 1.0090415344166104e-05\n",
      "step: 11832, loss: 1.0569322512310464e-05\n",
      "step: 11833, loss: 5.71439704799559e-06\n",
      "step: 11834, loss: 1.0685609595384449e-05\n",
      "step: 11835, loss: 6.939521699678153e-05\n",
      "step: 11836, loss: 4.896820883004693e-06\n",
      "step: 11837, loss: 0.005148157011717558\n",
      "step: 11838, loss: 1.2373777735774638e-06\n",
      "step: 11839, loss: 9.82865549303824e-06\n",
      "step: 11840, loss: 0.10644922405481339\n",
      "step: 11841, loss: 1.0537999060034053e-06\n",
      "step: 11842, loss: 1.4686378335682093e-06\n",
      "step: 11843, loss: 1.8057620764011517e-05\n",
      "step: 11844, loss: 2.2410965812014183e-06\n",
      "step: 11845, loss: 2.455709307014331e-07\n",
      "step: 11846, loss: 5.6788626352499705e-06\n",
      "step: 11847, loss: 0.08158765733242035\n",
      "step: 11848, loss: 3.0110616080492036e-06\n",
      "step: 11849, loss: 9.91008710116148e-06\n",
      "step: 11850, loss: 1.7287857190240175e-05\n",
      "step: 11851, loss: 1.4066555422687088e-06\n",
      "step: 11852, loss: 2.4787517759250477e-05\n",
      "step: 11853, loss: 1.0388287591922563e-05\n",
      "step: 11854, loss: 1.809582158784906e-06\n",
      "step: 11855, loss: 3.511740260364604e-06\n",
      "step: 11856, loss: 7.057159336909535e-07\n",
      "step: 11857, loss: 4.100666501472006e-06\n",
      "step: 11858, loss: 3.240042133256793e-06\n",
      "step: 11859, loss: 8.010204510355834e-06\n",
      "step: 11860, loss: 0.09155717492103577\n",
      "step: 11861, loss: 1.554210757603869e-05\n",
      "step: 11862, loss: 6.000709163345164e-06\n",
      "step: 11863, loss: 5.12598717250512e-07\n",
      "step: 11864, loss: 5.85499765293207e-06\n",
      "step: 11865, loss: 4.601467651355051e-07\n",
      "step: 11866, loss: 2.1997608200763352e-05\n",
      "step: 11867, loss: 1.3198940905567724e-05\n",
      "step: 11868, loss: 7.23801895219367e-06\n",
      "step: 11869, loss: 0.0007935658213682473\n",
      "step: 11870, loss: 5.2946936193620786e-05\n",
      "step: 11871, loss: 4.672993725307606e-07\n",
      "step: 11872, loss: 8.076915037236176e-06\n",
      "step: 11873, loss: 1.7070260582841001e-06\n",
      "step: 11874, loss: 0.00015664577949792147\n",
      "step: 11875, loss: 9.922984645527322e-06\n",
      "step: 11876, loss: 1.969914410437923e-05\n",
      "step: 11877, loss: 2.534587838454172e-05\n",
      "step: 11878, loss: 1.00850525086571e-06\n",
      "step: 11879, loss: 8.670714123582002e-06\n",
      "step: 11880, loss: 0.09089711308479309\n",
      "step: 11881, loss: 1.8071696104016155e-06\n",
      "step: 11882, loss: 3.1613661121809855e-05\n",
      "step: 11883, loss: 9.06867535377387e-06\n",
      "step: 11884, loss: 9.241417501471005e-06\n",
      "step: 11885, loss: 2.675020368769765e-06\n",
      "step: 11886, loss: 2.6606728624756215e-06\n",
      "step: 11887, loss: 7.094848115229979e-06\n",
      "step: 11888, loss: 1.1932718734897207e-05\n",
      "step: 11889, loss: 1.5973886320352904e-06\n",
      "step: 11890, loss: 8.122154213197064e-06\n",
      "step: 11891, loss: 2.1807234588777646e-05\n",
      "step: 11892, loss: 5.793554578303883e-07\n",
      "step: 11893, loss: 1.55386260303203e-05\n",
      "step: 11894, loss: 1.2787661944457795e-05\n",
      "step: 11895, loss: 0.00017070038302335888\n",
      "step: 11896, loss: 1.6487871107528917e-05\n",
      "step: 11897, loss: 6.484931986960873e-07\n",
      "step: 11898, loss: 0.08672425150871277\n",
      "step: 11899, loss: 1.9311476080474677e-06\n",
      "step: 11900, loss: 1.773771600710461e-06\n",
      "step: 11901, loss: 1.5041832739370875e-05\n",
      "step: 11902, loss: 5.93661184211669e-07\n",
      "step: 11903, loss: 2.993088673974853e-05\n",
      "step: 11904, loss: 9.083166514756158e-05\n",
      "step: 11905, loss: 3.83356518796063e-06\n",
      "step: 11906, loss: 0.1007237359881401\n",
      "step: 11907, loss: 6.890242616464093e-07\n",
      "step: 11908, loss: 7.843922276151716e-07\n",
      "step: 11909, loss: 4.698794327850919e-06\n",
      "step: 11910, loss: 6.968157322262414e-06\n",
      "step: 11911, loss: 1.2302297136557172e-06\n",
      "step: 11912, loss: 6.984952051425353e-06\n",
      "step: 11913, loss: 6.896540980960708e-06\n",
      "step: 11914, loss: 4.226944292895496e-05\n",
      "step: 11915, loss: 5.10890458826907e-06\n",
      "step: 11916, loss: 1.6855815374583472e-06\n",
      "step: 11917, loss: 6.413438882191258e-07\n",
      "step: 11918, loss: 1.1420098644521204e-06\n",
      "step: 11919, loss: 3.373448180354899e-06\n",
      "step: 11920, loss: 0.000113448899355717\n",
      "step: 11921, loss: 1.397914002154721e-05\n",
      "step: 11922, loss: 6.957519508432597e-05\n",
      "step: 11923, loss: 2.8799868232454173e-06\n",
      "step: 11924, loss: 1.2540689340312383e-06\n",
      "step: 11925, loss: 8.331726348842494e-06\n",
      "step: 11926, loss: 7.986002856341656e-06\n",
      "step: 11927, loss: 3.1255849535227753e-06\n",
      "step: 11928, loss: 1.5973849940564833e-06\n",
      "step: 11929, loss: 0.06735355406999588\n",
      "step: 11930, loss: 7.057165021251421e-07\n",
      "step: 11931, loss: 5.273583155940287e-06\n",
      "step: 11932, loss: 1.3112912711221725e-06\n",
      "step: 11933, loss: 2.0265338207536843e-06\n",
      "step: 11934, loss: 4.713323050964391e-06\n",
      "step: 11935, loss: 3.2423267839476466e-06\n",
      "step: 11936, loss: 6.415521966118831e-06\n",
      "step: 11937, loss: 1.106254217120295e-06\n",
      "step: 11938, loss: 3.7557143514277413e-05\n",
      "step: 11939, loss: 9.047417734109331e-06\n",
      "step: 11940, loss: 0.15332376956939697\n",
      "step: 11941, loss: 3.6716210161102936e-07\n",
      "step: 11942, loss: 2.2601695945922984e-06\n",
      "step: 11943, loss: 1.077645038094488e-06\n",
      "step: 11944, loss: 6.567891887243604e-06\n",
      "step: 11945, loss: 0.11604288220405579\n",
      "step: 11946, loss: 4.4584143665815645e-07\n",
      "step: 11947, loss: 0.040684398263692856\n",
      "step: 11948, loss: 5.07830463902792e-07\n",
      "step: 11949, loss: 5.221335754868051e-07\n",
      "step: 11950, loss: 2.3841843699301535e-07\n",
      "step: 11951, loss: 2.0599065919668647e-06\n",
      "step: 11952, loss: 8.726000828573888e-07\n",
      "step: 11953, loss: 3.31401309949797e-07\n",
      "step: 11954, loss: 2.9928651201771572e-05\n",
      "step: 11955, loss: 4.7683570869594405e-07\n",
      "step: 11956, loss: 6.079636136746558e-07\n",
      "step: 11957, loss: 2.4318637770193163e-07\n",
      "step: 11958, loss: 1.1420162309150328e-06\n",
      "step: 11959, loss: 4.1183739085681736e-05\n",
      "step: 11960, loss: 1.4138108781480696e-06\n",
      "step: 11961, loss: 1.4605141586798709e-05\n",
      "step: 11962, loss: 4.291515551813063e-07\n",
      "step: 11963, loss: 1.7166127008749754e-07\n",
      "step: 11964, loss: 1.3663590834767092e-05\n",
      "step: 11965, loss: 4.005424045772088e-07\n",
      "step: 11966, loss: 1.7070429976229207e-06\n",
      "step: 11967, loss: 9.226640145243437e-07\n",
      "step: 11968, loss: 1.9478197827993426e-06\n",
      "step: 11969, loss: 1.7404531149622926e-07\n",
      "step: 11970, loss: 2.0169575236650417e-06\n",
      "step: 11971, loss: 3.459395657046116e-06\n",
      "step: 11972, loss: 6.103491045905685e-07\n",
      "step: 11973, loss: 4.071986040798947e-06\n",
      "step: 11974, loss: 4.245953277859371e-06\n",
      "step: 11975, loss: 3.530689355102368e-05\n",
      "step: 11976, loss: 2.7489184049045434e-06\n",
      "step: 11977, loss: 1.42570604566572e-06\n",
      "step: 11978, loss: 1.570182803334319e-06\n",
      "step: 11979, loss: 1.8739508504950209e-06\n",
      "step: 11980, loss: 6.474934252764797e-06\n",
      "step: 11981, loss: 1.3799290172755718e-05\n",
      "step: 11982, loss: 1.432882527296897e-06\n",
      "step: 11983, loss: 2.9626989999087527e-05\n",
      "step: 11984, loss: 8.773757258495607e-07\n",
      "step: 11985, loss: 1.7213562841789098e-06\n",
      "step: 11986, loss: 1.7818903870647773e-05\n",
      "step: 11987, loss: 2.5295094019384123e-06\n",
      "step: 11988, loss: 1.1539336810528766e-06\n",
      "step: 11989, loss: 1.0323432206860161e-06\n",
      "step: 11990, loss: 4.65849916508887e-06\n",
      "step: 11991, loss: 1.527428685221821e-05\n",
      "step: 11992, loss: 4.29371721111238e-06\n",
      "step: 11993, loss: 2.67021891886543e-06\n",
      "step: 11994, loss: 3.211421471860376e-06\n",
      "step: 11995, loss: 4.005163646070287e-06\n",
      "step: 11996, loss: 2.610599040053785e-06\n",
      "step: 11997, loss: 6.031958150742867e-07\n",
      "step: 11998, loss: 1.2182999853393994e-06\n",
      "step: 11999, loss: 5.919344857829856e-06\n",
      "step: 12000, loss: 7.871401976444758e-06\n",
      "step: 12001, loss: 2.6725997486209963e-06\n",
      "step: 12002, loss: 2.348395582885132e-06\n",
      "step: 12003, loss: 0.0027375933714210987\n",
      "step: 12004, loss: 3.2281004678225145e-06\n",
      "step: 12005, loss: 2.0700999812106602e-05\n",
      "step: 12006, loss: 9.012151735987572e-07\n",
      "step: 12007, loss: 9.322121172772313e-07\n",
      "step: 12008, loss: 0.0012018914567306638\n",
      "step: 12009, loss: 2.7822720767289866e-06\n",
      "step: 12010, loss: 1.5878171325311996e-05\n",
      "step: 12011, loss: 1.802412157303479e-06\n",
      "step: 12012, loss: 7.700865580773097e-07\n",
      "step: 12013, loss: 3.0372711989912204e-06\n",
      "step: 12014, loss: 1.0204236104982556e-06\n",
      "step: 12015, loss: 1.3079818018013611e-05\n",
      "step: 12016, loss: 2.6296950181858847e-06\n",
      "step: 12017, loss: 5.60279261208052e-07\n",
      "step: 12018, loss: 1.1992333384114318e-06\n",
      "step: 12019, loss: 1.5210887340799673e-06\n",
      "step: 12020, loss: 2.3841813856506633e-07\n",
      "step: 12021, loss: 9.822711035667453e-07\n",
      "step: 12022, loss: 6.172153007355519e-06\n",
      "step: 12023, loss: 5.912744427405414e-07\n",
      "step: 12024, loss: 2.3388386125589022e-06\n",
      "step: 12025, loss: 1.2034321116516367e-05\n",
      "step: 12026, loss: 9.775153841928841e-08\n",
      "step: 12027, loss: 1.9287697341496823e-06\n",
      "step: 12028, loss: 2.5772740173124475e-06\n",
      "step: 12029, loss: 1.7636042684898712e-05\n",
      "step: 12030, loss: 1.5058405551826581e-05\n",
      "step: 12031, loss: 1.4875669876346365e-05\n",
      "step: 12032, loss: 1.211156359204324e-06\n",
      "step: 12033, loss: 8.99221959116403e-06\n",
      "step: 12034, loss: 8.141131729644258e-06\n",
      "step: 12035, loss: 4.319694198784418e-06\n",
      "step: 12036, loss: 1.3613530427392107e-06\n",
      "step: 12037, loss: 4.5534161472460255e-06\n",
      "step: 12038, loss: 3.4616355151229072e-06\n",
      "step: 12039, loss: 4.331844593252754e-06\n",
      "step: 12040, loss: 0.06527787446975708\n",
      "step: 12041, loss: 2.978151860588696e-05\n",
      "step: 12042, loss: 0.00011035875650122762\n",
      "step: 12043, loss: 4.752790482598357e-05\n",
      "step: 12044, loss: 1.2469025705286185e-06\n",
      "step: 12045, loss: 3.1540810141450493e-06\n",
      "step: 12046, loss: 1.4552592801919673e-05\n",
      "step: 12047, loss: 2.5768498744582757e-05\n",
      "step: 12048, loss: 4.427341536938911e-06\n",
      "step: 12049, loss: 0.00010222612763755023\n",
      "step: 12050, loss: 3.602219294407405e-06\n",
      "step: 12051, loss: 1.3375210983213037e-06\n",
      "step: 12052, loss: 3.831101821560878e-06\n",
      "step: 12053, loss: 6.8133772401779424e-06\n",
      "step: 12054, loss: 1.5425542869706987e-06\n",
      "step: 12055, loss: 2.813334560869407e-07\n",
      "step: 12056, loss: 2.16478247239138e-06\n",
      "step: 12057, loss: 4.3392000748099235e-07\n",
      "step: 12058, loss: 0.1130385622382164\n",
      "step: 12059, loss: 6.151183242764091e-07\n",
      "step: 12060, loss: 0.007530437782406807\n",
      "step: 12061, loss: 1.5807000863787835e-06\n",
      "step: 12062, loss: 3.7359016005211743e-06\n",
      "step: 12063, loss: 2.0980783688173688e-07\n",
      "step: 12064, loss: 2.5781095246202312e-05\n",
      "step: 12065, loss: 9.298292411585862e-07\n",
      "step: 12066, loss: 3.8719379517715424e-05\n",
      "step: 12067, loss: 3.0779292501392774e-06\n",
      "step: 12068, loss: 1.0442685152156628e-06\n",
      "step: 12069, loss: 1.1944694051635452e-06\n",
      "step: 12070, loss: 2.3364991363905574e-07\n",
      "step: 12071, loss: 2.0026789115945576e-06\n",
      "step: 12072, loss: 8.654531598040194e-07\n",
      "step: 12073, loss: 0.0003513081755954772\n",
      "step: 12074, loss: 5.888909413442889e-07\n",
      "step: 12075, loss: 4.2915272047139297e-07\n",
      "step: 12076, loss: 1.239756556969951e-06\n",
      "step: 12077, loss: 2.0527286324067973e-06\n",
      "step: 12078, loss: 2.065831358777359e-05\n",
      "step: 12079, loss: 1.6331530332536204e-06\n",
      "step: 12080, loss: 3.266330850237864e-07\n",
      "step: 12081, loss: 0.06961934268474579\n",
      "step: 12082, loss: 1.1444080882938579e-07\n",
      "step: 12083, loss: 8.582989607930358e-07\n",
      "step: 12084, loss: 4.6321524678205606e-06\n",
      "step: 12085, loss: 2.932537483957276e-07\n",
      "step: 12086, loss: 4.982923655916238e-07\n",
      "step: 12087, loss: 3.5285862054479367e-07\n",
      "step: 12088, loss: 6.747202974111133e-07\n",
      "step: 12089, loss: 7.224059572763508e-07\n",
      "step: 12090, loss: 8.482154953526333e-06\n",
      "step: 12091, loss: 8.892940286386875e-07\n",
      "step: 12092, loss: 1.9621618321252754e-06\n",
      "step: 12093, loss: 3.1255435715138447e-06\n",
      "step: 12094, loss: 2.813331718698464e-07\n",
      "step: 12095, loss: 1.8405544324195944e-06\n",
      "step: 12096, loss: 7.986990340214106e-07\n",
      "step: 12097, loss: 8.845297543302877e-07\n",
      "step: 12098, loss: 2.0053377738804556e-05\n",
      "step: 12099, loss: 8.719889592612162e-06\n",
      "step: 12100, loss: 8.94060860900936e-07\n",
      "step: 12101, loss: 0.07148467004299164\n",
      "step: 12102, loss: 1.4781933543872583e-07\n",
      "step: 12103, loss: 2.0336624402261805e-06\n",
      "step: 12104, loss: 9.202894375448523e-07\n",
      "step: 12105, loss: 6.46558919470408e-06\n",
      "step: 12106, loss: 2.479498789398349e-06\n",
      "step: 12107, loss: 2.028906237683259e-06\n",
      "step: 12108, loss: 1.2270136721781455e-05\n",
      "step: 12109, loss: 2.913410980909248e-06\n",
      "step: 12110, loss: 1.800013365027553e-06\n",
      "step: 12111, loss: 4.792197501046758e-07\n",
      "step: 12112, loss: 1.7952706912183203e-06\n",
      "step: 12113, loss: 1.2540672287286725e-06\n",
      "step: 12114, loss: 7.86777832217922e-07\n",
      "step: 12115, loss: 6.7892651713918895e-06\n",
      "step: 12116, loss: 2.8108393053116743e-06\n",
      "step: 12117, loss: 7.62934064368892e-07\n",
      "step: 12118, loss: 3.4664367376535665e-06\n",
      "step: 12119, loss: 8.058461276050366e-07\n",
      "step: 12120, loss: 4.7683494130978943e-07\n",
      "step: 12121, loss: 3.7193183288763976e-07\n",
      "step: 12122, loss: 1.0537909247432253e-06\n",
      "step: 12123, loss: 6.752515764674172e-05\n",
      "step: 12124, loss: 4.1723157551132317e-07\n",
      "step: 12125, loss: 0.019525092095136642\n",
      "step: 12126, loss: 1.5282446383935167e-06\n",
      "step: 12127, loss: 4.877589162788354e-06\n",
      "step: 12128, loss: 4.2915129938592145e-07\n",
      "step: 12129, loss: 0.0004135597846470773\n",
      "step: 12130, loss: 5.1499671826604754e-05\n",
      "step: 12131, loss: 1.8067457858705893e-05\n",
      "step: 12132, loss: 2.35468123719329e-05\n",
      "step: 12133, loss: 1.1706255236276775e-06\n",
      "step: 12134, loss: 1.4161857961880742e-06\n",
      "step: 12135, loss: 8.892943696992006e-07\n",
      "step: 12136, loss: 1.1755314517358784e-05\n",
      "step: 12137, loss: 1.0347309853386832e-06\n",
      "step: 12138, loss: 1.4829322481091367e-06\n",
      "step: 12139, loss: 6.417856184270931e-06\n",
      "step: 12140, loss: 8.646080459584482e-06\n",
      "step: 12141, loss: 7.47567719372455e-06\n",
      "step: 12142, loss: 1.0803578334162012e-05\n",
      "step: 12143, loss: 2.6487052764423424e-06\n",
      "step: 12144, loss: 1.3692318134417292e-05\n",
      "step: 12145, loss: 3.5641874092107173e-06\n",
      "step: 12146, loss: 1.7428071714675752e-06\n",
      "step: 12147, loss: 9.077107279154006e-06\n",
      "step: 12148, loss: 5.275930561765563e-06\n",
      "step: 12149, loss: 2.7878568289452232e-05\n",
      "step: 12150, loss: 3.871688477374846e-06\n",
      "step: 12151, loss: 1.2683710792771308e-06\n",
      "step: 12152, loss: 5.1065858315269e-06\n",
      "step: 12153, loss: 2.0002764813398244e-06\n",
      "step: 12154, loss: 9.322113783127861e-07\n",
      "step: 12155, loss: 2.829166805895511e-05\n",
      "step: 12156, loss: 5.006770606996724e-07\n",
      "step: 12157, loss: 2.3221500669023953e-06\n",
      "step: 12158, loss: 1.2373834579193499e-06\n",
      "step: 12159, loss: 1.9144665657222504e-06\n",
      "step: 12160, loss: 1.7738087763063959e-06\n",
      "step: 12161, loss: 1.4734167734786752e-06\n",
      "step: 12162, loss: 3.096967020610464e-06\n",
      "step: 12163, loss: 6.055818744243879e-07\n",
      "step: 12164, loss: 2.2410777091863565e-06\n",
      "step: 12165, loss: 4.701166290033143e-06\n",
      "step: 12166, loss: 3.88621060665173e-07\n",
      "step: 12167, loss: 0.006635301746428013\n",
      "step: 12168, loss: 7.295522550521127e-07\n",
      "step: 12169, loss: 1.1348440693836892e-06\n",
      "step: 12170, loss: 2.6892678306467133e-06\n",
      "step: 12171, loss: 1.4526694940286689e-05\n",
      "step: 12172, loss: 0.03882111236453056\n",
      "step: 12173, loss: 5.435657840280328e-06\n",
      "step: 12174, loss: 1.4517372619593516e-05\n",
      "step: 12175, loss: 1.499626364420692e-06\n",
      "step: 12176, loss: 2.1481205294549e-06\n",
      "step: 12177, loss: 1.4798793927184306e-05\n",
      "step: 12178, loss: 4.927864210912958e-06\n",
      "step: 12179, loss: 3.185162995578139e-06\n",
      "step: 12180, loss: 1.4114141322352225e-06\n",
      "step: 12181, loss: 1.0967086154778372e-06\n",
      "step: 12182, loss: 1.9120884644507896e-06\n",
      "step: 12183, loss: 0.08386155962944031\n",
      "step: 12184, loss: 0.06836580485105515\n",
      "step: 12185, loss: 2.152832848878461e-06\n",
      "step: 12186, loss: 5.674350518347637e-07\n",
      "step: 12187, loss: 0.00021253680461086333\n",
      "step: 12188, loss: 6.270385597417771e-07\n",
      "step: 12189, loss: 3.9861752156866714e-06\n",
      "step: 12190, loss: 6.246545467547548e-07\n",
      "step: 12191, loss: 3.457055299804779e-07\n",
      "step: 12192, loss: 2.4294436116178986e-06\n",
      "step: 12193, loss: 3.9815796526454506e-07\n",
      "step: 12194, loss: 3.1471216743739205e-07\n",
      "step: 12195, loss: 3.3854664707178017e-06\n",
      "step: 12196, loss: 1.5496957530558575e-06\n",
      "step: 12197, loss: 3.3996755064436e-06\n",
      "step: 12198, loss: 1.1706272289302433e-06\n",
      "step: 12199, loss: 0.0004187732993159443\n",
      "step: 12200, loss: 0.04645131900906563\n",
      "step: 12201, loss: 1.9263920876255725e-06\n",
      "step: 12202, loss: 1.451953153264185e-06\n",
      "step: 12203, loss: 0.08391410112380981\n",
      "step: 12204, loss: 5.464356945594773e-06\n",
      "step: 12205, loss: 1.7319165635854006e-05\n",
      "step: 12206, loss: 1.4677019862574525e-05\n",
      "step: 12207, loss: 1.282661514778738e-06\n",
      "step: 12208, loss: 7.843926255191036e-07\n",
      "step: 12209, loss: 2.7775258786277846e-06\n",
      "step: 12210, loss: 1.7213433238794096e-06\n",
      "step: 12211, loss: 7.104839596649981e-07\n",
      "step: 12212, loss: 2.4699872938072076e-06\n",
      "step: 12213, loss: 6.35605028946884e-05\n",
      "step: 12214, loss: 2.0980519366275985e-06\n",
      "step: 12215, loss: 7.907581675681286e-06\n",
      "step: 12216, loss: 4.383304258226417e-05\n",
      "step: 12217, loss: 3.8050584407756105e-06\n",
      "step: 12218, loss: 2.3269126359082293e-06\n",
      "step: 12219, loss: 8.201520813599927e-07\n",
      "step: 12220, loss: 1.0037388165073935e-06\n",
      "step: 12221, loss: 7.456610546796583e-06\n",
      "step: 12222, loss: 1.3923483948019566e-06\n",
      "step: 12223, loss: 5.814592441311106e-06\n",
      "step: 12224, loss: 9.488978207627952e-07\n",
      "step: 12225, loss: 6.126901098468807e-06\n",
      "step: 12226, loss: 0.08784829825162888\n",
      "step: 12227, loss: 3.7167199025134323e-06\n",
      "step: 12228, loss: 2.708365855141892e-06\n",
      "step: 12229, loss: 3.8862074802636926e-07\n",
      "step: 12230, loss: 3.630515493568964e-05\n",
      "step: 12231, loss: 2.257786036352627e-06\n",
      "step: 12232, loss: 0.003089552279561758\n",
      "step: 12233, loss: 1.0728749657573644e-06\n",
      "step: 12234, loss: 3.83835140382871e-06\n",
      "step: 12235, loss: 7.319413271034136e-07\n",
      "step: 12236, loss: 1.6069268440332962e-06\n",
      "step: 12237, loss: 2.2239741156226955e-05\n",
      "step: 12238, loss: 5.245177590040839e-07\n",
      "step: 12239, loss: 2.7250423499936005e-06\n",
      "step: 12240, loss: 1.9848477677442133e-05\n",
      "step: 12241, loss: 0.005208345130085945\n",
      "step: 12242, loss: 3.1993590710044373e-06\n",
      "step: 12243, loss: 4.467709004529752e-06\n",
      "step: 12244, loss: 3.125614966847934e-06\n",
      "step: 12245, loss: 8.79760136740515e-07\n",
      "step: 12246, loss: 7.871699381212238e-06\n",
      "step: 12247, loss: 2.0289146505092504e-06\n",
      "step: 12248, loss: 4.050453298987122e-06\n",
      "step: 12249, loss: 0.002161801792681217\n",
      "step: 12250, loss: 8.487657510158897e-07\n",
      "step: 12251, loss: 2.315005986019969e-06\n",
      "step: 12252, loss: 1.8691634977585636e-06\n",
      "step: 12253, loss: 3.1136560210143216e-06\n",
      "step: 12254, loss: 3.027910793207411e-07\n",
      "step: 12255, loss: 4.675088348449208e-06\n",
      "step: 12256, loss: 2.8226464564795606e-05\n",
      "step: 12257, loss: 1.730907001729065e-06\n",
      "step: 12258, loss: 1.1634759857770405e-06\n",
      "step: 12259, loss: 6.19172424194403e-05\n",
      "step: 12260, loss: 6.5654558056849055e-06\n",
      "step: 12261, loss: 4.651303243008442e-06\n",
      "step: 12262, loss: 5.578955892815429e-07\n",
      "step: 12263, loss: 1.96354667423293e-05\n",
      "step: 12264, loss: 2.1814903448102996e-06\n",
      "step: 12265, loss: 8.014897503016982e-06\n",
      "step: 12266, loss: 3.480901398233982e-07\n",
      "step: 12267, loss: 0.04302511364221573\n",
      "step: 12268, loss: 7.688223377044778e-06\n",
      "step: 12269, loss: 2.0336501620477065e-06\n",
      "step: 12270, loss: 2.980161298182793e-06\n",
      "step: 12271, loss: 3.0279034035629593e-07\n",
      "step: 12272, loss: 3.940948317904258e-06\n",
      "step: 12273, loss: 0.09619149565696716\n",
      "step: 12274, loss: 2.6497529688640498e-05\n",
      "step: 12275, loss: 1.8835049786503077e-07\n",
      "step: 12276, loss: 4.0292593439517077e-07\n",
      "step: 12277, loss: 5.9127501117473e-07\n",
      "step: 12278, loss: 4.744516388655029e-07\n",
      "step: 12279, loss: 4.729740339826094e-06\n",
      "step: 12280, loss: 2.0717725419672206e-06\n",
      "step: 12281, loss: 6.899458185216645e-06\n",
      "step: 12282, loss: 3.847269545076415e-05\n",
      "step: 12283, loss: 9.15836699277861e-06\n",
      "step: 12284, loss: 2.455643880239222e-06\n",
      "step: 12285, loss: 4.171969976596301e-06\n",
      "step: 12286, loss: 5.692998456652276e-06\n",
      "step: 12287, loss: 4.5056567614665255e-06\n",
      "step: 12288, loss: 8.678377980686491e-07\n",
      "step: 12289, loss: 7.716852451267187e-06\n",
      "step: 12290, loss: 7.351692147494759e-06\n",
      "step: 12291, loss: 4.560695742839016e-06\n",
      "step: 12292, loss: 3.918462971341796e-05\n",
      "step: 12293, loss: 6.584706625289982e-06\n",
      "step: 12294, loss: 0.00016584266268182546\n",
      "step: 12295, loss: 2.46758258981572e-06\n",
      "step: 12296, loss: 4.241234364599222e-06\n",
      "step: 12297, loss: 1.8309952793060802e-05\n",
      "step: 12298, loss: 1.0466537787579e-06\n",
      "step: 12299, loss: 1.1777823374359286e-06\n",
      "step: 12300, loss: 4.1246249793402967e-07\n",
      "step: 12301, loss: 1.6712838259991258e-06\n",
      "step: 12302, loss: 4.806880679097958e-05\n",
      "step: 12303, loss: 0.00013663583376910537\n",
      "step: 12304, loss: 8.106193831736164e-07\n",
      "step: 12305, loss: 1.0871744962059893e-06\n",
      "step: 12306, loss: 9.258464160666335e-06\n",
      "step: 12307, loss: 7.82845017965883e-06\n",
      "step: 12308, loss: 1.4543101087838295e-06\n",
      "step: 12309, loss: 2.074218173220288e-06\n",
      "step: 12310, loss: 3.690597850436461e-06\n",
      "step: 12311, loss: 9.148457138508093e-06\n",
      "step: 12312, loss: 4.639230610337108e-06\n",
      "step: 12313, loss: 3.7168827020650497e-06\n",
      "step: 12314, loss: 2.4413650407950627e-06\n",
      "step: 12315, loss: 2.2649697939414182e-07\n",
      "step: 12316, loss: 3.4091558518412057e-06\n",
      "step: 12317, loss: 3.4854967907449463e-06\n",
      "step: 12318, loss: 9.453921666136011e-05\n",
      "step: 12319, loss: 1.068720030161785e-05\n",
      "step: 12320, loss: 2.655938260431867e-06\n",
      "step: 12321, loss: 4.379562597023323e-06\n",
      "step: 12322, loss: 4.372488547232933e-06\n",
      "step: 12323, loss: 1.178700495074736e-05\n",
      "step: 12324, loss: 5.120791229273891e-06\n",
      "step: 12325, loss: 2.524800947867334e-06\n",
      "step: 12326, loss: 1.4448088450080832e-06\n",
      "step: 12327, loss: 2.4964041585917585e-05\n",
      "step: 12328, loss: 1.559249653837469e-06\n",
      "step: 12329, loss: 8.8691376731731e-07\n",
      "step: 12330, loss: 2.560505208748509e-06\n",
      "step: 12331, loss: 0.08238361030817032\n",
      "step: 12332, loss: 2.2387303033610806e-06\n",
      "step: 12333, loss: 3.2208899938268587e-06\n",
      "step: 12334, loss: 2.8347221814328805e-06\n",
      "step: 12335, loss: 1.6688735513525899e-06\n",
      "step: 12336, loss: 1.3446659750115941e-06\n",
      "step: 12337, loss: 2.8133348450865014e-07\n",
      "step: 12338, loss: 2.3102436443878105e-06\n",
      "step: 12339, loss: 1.3994972505315673e-06\n",
      "step: 12340, loss: 9.560526450513862e-07\n",
      "step: 12341, loss: 1.0180400522585842e-06\n",
      "step: 12342, loss: 4.263541632099077e-05\n",
      "step: 12343, loss: 5.798996426165104e-05\n",
      "step: 12344, loss: 0.12995003163814545\n",
      "step: 12345, loss: 1.697519792287494e-06\n",
      "step: 12346, loss: 3.108840701315785e-06\n",
      "step: 12347, loss: 6.246534098863776e-07\n",
      "step: 12348, loss: 6.672964445897378e-06\n",
      "step: 12349, loss: 0.000151047992403619\n",
      "step: 12350, loss: 3.0230150969146052e-06\n",
      "step: 12351, loss: 2.6765548682305962e-05\n",
      "step: 12352, loss: 1.061175498762168e-05\n",
      "step: 12353, loss: 1.3414208297035657e-05\n",
      "step: 12354, loss: 0.07255895435810089\n",
      "step: 12355, loss: 1.0538018386796466e-06\n",
      "step: 12356, loss: 1.3041269539826317e-06\n",
      "step: 12357, loss: 4.363047310107504e-07\n",
      "step: 12358, loss: 9.632044566387776e-07\n",
      "step: 12359, loss: 2.734531790338224e-06\n",
      "step: 12360, loss: 5.1684246500371955e-06\n",
      "step: 12361, loss: 2.57491620914152e-07\n",
      "step: 12362, loss: 3.433220854276442e-07\n",
      "step: 12363, loss: 3.924277734768111e-06\n",
      "step: 12364, loss: 7.71165741753066e-06\n",
      "step: 12365, loss: 8.720564437680878e-06\n",
      "step: 12366, loss: 1.854856122918136e-06\n",
      "step: 12367, loss: 5.315618182066828e-05\n",
      "step: 12368, loss: 6.389598752321035e-07\n",
      "step: 12369, loss: 6.099855090724304e-05\n",
      "step: 12370, loss: 4.339210875059507e-07\n",
      "step: 12371, loss: 0.1324736326932907\n",
      "step: 12372, loss: 0.05524320900440216\n",
      "step: 12373, loss: 7.247875259963621e-07\n",
      "step: 12374, loss: 9.144360774371307e-06\n",
      "step: 12375, loss: 0.0001635497756069526\n",
      "step: 12376, loss: 2.5509946226520697e-06\n",
      "step: 12377, loss: 4.982705377187813e-06\n",
      "step: 12378, loss: 7.585922048747307e-06\n",
      "step: 12379, loss: 0.0003898802387993783\n",
      "step: 12380, loss: 6.6994971348322e-07\n",
      "step: 12381, loss: 1.0790543456096202e-05\n",
      "step: 12382, loss: 1.6973845049506053e-05\n",
      "step: 12383, loss: 2.3650613911740948e-06\n",
      "step: 12384, loss: 2.7226101337873843e-06\n",
      "step: 12385, loss: 5.931473424425349e-06\n",
      "step: 12386, loss: 1.1758784239646047e-05\n",
      "step: 12387, loss: 5.769619747297838e-05\n",
      "step: 12388, loss: 1.0800281415868085e-06\n",
      "step: 12389, loss: 2.8585557174665155e-06\n",
      "step: 12390, loss: 0.10214387625455856\n",
      "step: 12391, loss: 7.597567673656158e-06\n",
      "step: 12392, loss: 1.914483391374233e-06\n",
      "step: 12393, loss: 0.11066801846027374\n",
      "step: 12394, loss: 1.003724719339516e-06\n",
      "step: 12395, loss: 5.550013156607747e-06\n",
      "step: 12396, loss: 9.375092304253485e-06\n",
      "step: 12397, loss: 0.03430464118719101\n",
      "step: 12398, loss: 0.08954399079084396\n",
      "step: 12399, loss: 4.8156643970287405e-06\n",
      "step: 12400, loss: 2.455706749060482e-07\n",
      "step: 12401, loss: 3.814687374870118e-07\n",
      "step: 12402, loss: 1.6212449338581791e-07\n",
      "step: 12403, loss: 2.6368625185568817e-06\n",
      "step: 12404, loss: 1.3279490076456568e-06\n",
      "step: 12405, loss: 1.8835044102161191e-07\n",
      "step: 12406, loss: 1.5163310536081553e-06\n",
      "step: 12407, loss: 5.437921572593041e-06\n",
      "step: 12408, loss: 9.655865369495586e-07\n",
      "step: 12409, loss: 4.353150870883837e-06\n",
      "step: 12410, loss: 3.278143367424491e-06\n",
      "step: 12411, loss: 2.7894918730453355e-07\n",
      "step: 12412, loss: 1.3637348956763162e-06\n",
      "step: 12413, loss: 5.745869771089929e-07\n",
      "step: 12414, loss: 2.6296527266822523e-06\n",
      "step: 12415, loss: 6.055815333638748e-07\n",
      "step: 12416, loss: 2.0742389494898816e-07\n",
      "step: 12417, loss: 7.576072675874457e-06\n",
      "step: 12418, loss: 1.9758757844101638e-05\n",
      "step: 12419, loss: 8.058448770498217e-07\n",
      "step: 12420, loss: 3.018200459337095e-06\n",
      "step: 12421, loss: 4.50609064728269e-07\n",
      "step: 12422, loss: 2.729778543653083e-06\n",
      "step: 12423, loss: 4.506101731749368e-07\n",
      "step: 12424, loss: 0.04750664159655571\n",
      "step: 12425, loss: 4.6968364131316775e-07\n",
      "step: 12426, loss: 0.0008600575383752584\n",
      "step: 12427, loss: 1.091950252884999e-06\n",
      "step: 12428, loss: 3.085539356106892e-05\n",
      "step: 12429, loss: 6.977965767873684e-06\n",
      "step: 12430, loss: 1.7489704760009772e-06\n",
      "step: 12431, loss: 8.08868499007076e-05\n",
      "step: 12432, loss: 0.002805115655064583\n",
      "step: 12433, loss: 8.916833849070827e-07\n",
      "step: 12434, loss: 1.6879721442819573e-06\n",
      "step: 12435, loss: 5.364394723983423e-07\n",
      "step: 12436, loss: 6.985594609432155e-07\n",
      "step: 12437, loss: 1.6212258060477325e-06\n",
      "step: 12438, loss: 5.435931598185562e-07\n",
      "step: 12439, loss: 3.433221706927725e-07\n",
      "step: 12440, loss: 3.16978657792788e-05\n",
      "step: 12441, loss: 7.534009114351647e-07\n",
      "step: 12442, loss: 1.726132268231595e-06\n",
      "step: 12443, loss: 2.2577867184736533e-06\n",
      "step: 12444, loss: 1.4495470850306447e-06\n",
      "step: 12445, loss: 9.393643836119736e-07\n",
      "step: 12446, loss: 2.4080227944978105e-07\n",
      "step: 12447, loss: 1.6402652818214847e-06\n",
      "step: 12448, loss: 4.920476840197807e-06\n",
      "step: 12449, loss: 0.0002394932380411774\n",
      "step: 12450, loss: 4.284217084205011e-06\n",
      "step: 12451, loss: 1.7461547031416558e-05\n",
      "step: 12452, loss: 3.8312091419356875e-06\n",
      "step: 12453, loss: 5.531275633074983e-07\n",
      "step: 12454, loss: 8.098289981717244e-06\n",
      "step: 12455, loss: 1.8739435745374067e-06\n",
      "step: 12456, loss: 1.2731450169667369e-06\n",
      "step: 12457, loss: 1.978871893015821e-07\n",
      "step: 12458, loss: 6.461121415668458e-07\n",
      "step: 12459, loss: 1.2206893416077946e-06\n",
      "step: 12460, loss: 4.238961537339492e-06\n",
      "step: 12461, loss: 1.5663994190617814e-06\n",
      "step: 12462, loss: 1.5760269889142364e-05\n",
      "step: 12463, loss: 4.529929071850347e-07\n",
      "step: 12464, loss: 7.773933248245157e-06\n",
      "step: 12465, loss: 9.942010592567385e-07\n",
      "step: 12466, loss: 1.13248086108797e-06\n",
      "step: 12467, loss: 1.9478393369354308e-06\n",
      "step: 12468, loss: 2.4417588065261953e-05\n",
      "step: 12469, loss: 7.265822659974219e-06\n",
      "step: 12470, loss: 2.7513115128385834e-06\n",
      "step: 12471, loss: 1.4042710745343356e-06\n",
      "step: 12472, loss: 1.2250220606802031e-05\n",
      "step: 12473, loss: 0.00045111519284546375\n",
      "step: 12474, loss: 1.656961785556632e-06\n",
      "step: 12475, loss: 1.2469088233046932e-06\n",
      "step: 12476, loss: 9.775154552471577e-08\n",
      "step: 12477, loss: 6.842589073130512e-07\n",
      "step: 12478, loss: 1.3497275176632684e-05\n",
      "step: 12479, loss: 6.198857818162651e-07\n",
      "step: 12480, loss: 2.6511779651627876e-06\n",
      "step: 12481, loss: 2.2891043045092374e-05\n",
      "step: 12482, loss: 2.272036454087356e-06\n",
      "step: 12483, loss: 3.404546305318945e-06\n",
      "step: 12484, loss: 4.179200914222747e-06\n",
      "step: 12485, loss: 5.29507406099583e-06\n",
      "step: 12486, loss: 3.397409500394133e-06\n",
      "step: 12487, loss: 2.4814180505927652e-05\n",
      "step: 12488, loss: 8.115039236145094e-06\n",
      "step: 12489, loss: 1.8429395822749939e-06\n",
      "step: 12490, loss: 1.619217982806731e-05\n",
      "step: 12491, loss: 6.632549047935754e-05\n",
      "step: 12492, loss: 4.458419482489262e-07\n",
      "step: 12493, loss: 2.052706349786604e-06\n",
      "step: 12494, loss: 6.198881408181478e-08\n",
      "step: 12495, loss: 4.748877472593449e-06\n",
      "step: 12496, loss: 0.08453717827796936\n",
      "step: 12497, loss: 9.588628745405003e-05\n",
      "step: 12498, loss: 1.227827283400984e-06\n",
      "step: 12499, loss: 2.8610199365175504e-07\n",
      "step: 12500, loss: 5.878954652871471e-06\n",
      "step: 12501, loss: 1.945436679307022e-06\n",
      "step: 12502, loss: 5.09725487063406e-06\n",
      "step: 12503, loss: 5.771646556240739e-06\n",
      "step: 12504, loss: 0.05342172458767891\n",
      "step: 12505, loss: 1.4209640539775137e-06\n",
      "step: 12506, loss: 1.8262155663251178e-06\n",
      "step: 12507, loss: 5.094558218843304e-06\n",
      "step: 12508, loss: 1.5306181921914686e-06\n",
      "step: 12509, loss: 1.3008448149776086e-05\n",
      "step: 12510, loss: 2.2172898184180667e-07\n",
      "step: 12511, loss: 9.0021558207809e-06\n",
      "step: 12512, loss: 1.788120016499306e-06\n",
      "step: 12513, loss: 6.1338946579780895e-06\n",
      "step: 12514, loss: 6.27234066996607e-06\n",
      "step: 12515, loss: 1.733281806082232e-06\n",
      "step: 12516, loss: 5.56161330678151e-06\n",
      "step: 12517, loss: 0.004219420254230499\n",
      "step: 12518, loss: 2.326924914086703e-06\n",
      "step: 12519, loss: 4.446258390089497e-05\n",
      "step: 12520, loss: 4.291416644264245e-06\n",
      "step: 12521, loss: 1.3267400390759576e-05\n",
      "step: 12522, loss: 2.2411306588310254e-07\n",
      "step: 12523, loss: 7.957440175232477e-06\n",
      "step: 12524, loss: 4.510298458626494e-05\n",
      "step: 12525, loss: 9.27435394260101e-07\n",
      "step: 12526, loss: 2.171963842556579e-06\n",
      "step: 12527, loss: 7.820040082151536e-07\n",
      "step: 12528, loss: 1.2130517461628187e-05\n",
      "step: 12529, loss: 1.2612244972842745e-06\n",
      "step: 12530, loss: 0.10256627947092056\n",
      "step: 12531, loss: 8.106221116577217e-08\n",
      "step: 12532, loss: 2.5033309611899313e-06\n",
      "step: 12533, loss: 4.634617198462365e-06\n",
      "step: 12534, loss: 5.276294905343093e-05\n",
      "step: 12535, loss: 6.193424269440584e-06\n",
      "step: 12536, loss: 2.5987569074459316e-07\n",
      "step: 12537, loss: 4.029262470339745e-07\n",
      "step: 12538, loss: 1.8835045523246663e-07\n",
      "step: 12539, loss: 1.2326039495746954e-06\n",
      "step: 12540, loss: 1.9274166334071197e-05\n",
      "step: 12541, loss: 2.7559724458114943e-06\n",
      "step: 12542, loss: 1.97646636479476e-06\n",
      "step: 12543, loss: 8.67838252816e-07\n",
      "step: 12544, loss: 0.11271271854639053\n",
      "step: 12545, loss: 8.806003279460128e-06\n",
      "step: 12546, loss: 3.862086941808229e-06\n",
      "step: 12547, loss: 6.580331728400779e-07\n",
      "step: 12548, loss: 2.012225422731717e-06\n",
      "step: 12549, loss: 1.6450284192615072e-06\n",
      "step: 12550, loss: 7.218759856186807e-05\n",
      "step: 12551, loss: 2.0980807846626703e-07\n",
      "step: 12552, loss: 1.0299547739123227e-06\n",
      "step: 12553, loss: 5.609502477454953e-05\n",
      "step: 12554, loss: 0.088079072535038\n",
      "step: 12555, loss: 1.575922169649857e-06\n",
      "step: 12556, loss: 2.39607174989942e-06\n",
      "step: 12557, loss: 1.224566494784085e-05\n",
      "step: 12558, loss: 6.521755858557299e-05\n",
      "step: 12559, loss: 2.22659273276804e-05\n",
      "step: 12560, loss: 0.014810347929596901\n",
      "step: 12561, loss: 6.747201268808567e-07\n",
      "step: 12562, loss: 5.487689122674055e-06\n",
      "step: 12563, loss: 5.902464181417599e-06\n",
      "step: 12564, loss: 9.899793076328933e-05\n",
      "step: 12565, loss: 2.9301029371708864e-06\n",
      "step: 12566, loss: 1.1836325029435102e-05\n",
      "step: 12567, loss: 3.4332177278884046e-07\n",
      "step: 12568, loss: 5.8456639635551255e-06\n",
      "step: 12569, loss: 9.77504214461078e-07\n",
      "step: 12570, loss: 2.140980177500751e-05\n",
      "step: 12571, loss: 4.739594714919804e-06\n",
      "step: 12572, loss: 1.716609432378391e-07\n",
      "step: 12573, loss: 1.5711432297393912e-06\n",
      "step: 12574, loss: 0.172899067401886\n",
      "step: 12575, loss: 6.879667580506066e-06\n",
      "step: 12576, loss: 1.2659963886108017e-06\n",
      "step: 12577, loss: 0.10941430926322937\n",
      "step: 12578, loss: 4.245938725944143e-06\n",
      "step: 12579, loss: 1.552082949274336e-06\n",
      "step: 12580, loss: 4.961075774190249e-06\n",
      "step: 12581, loss: 2.5853896659100428e-05\n",
      "step: 12582, loss: 4.5940378186060116e-06\n",
      "step: 12583, loss: 1.4686316944789723e-06\n",
      "step: 12584, loss: 4.744508714793483e-07\n",
      "step: 12585, loss: 1.3422754818748217e-06\n",
      "step: 12586, loss: 1.3285591194289736e-05\n",
      "step: 12587, loss: 2.4508265141776064e-06\n",
      "step: 12588, loss: 2.0027077596296294e-07\n",
      "step: 12589, loss: 4.5060937736707274e-07\n",
      "step: 12590, loss: 5.602819328487385e-07\n",
      "step: 12591, loss: 1.0275732620357303e-06\n",
      "step: 12592, loss: 4.86372869090701e-07\n",
      "step: 12593, loss: 2.3126577275434101e-07\n",
      "step: 12594, loss: 1.6855869944265578e-06\n",
      "step: 12595, loss: 7.271709137057769e-07\n",
      "step: 12596, loss: 2.3364957257854257e-07\n",
      "step: 12597, loss: 4.851587164012017e-06\n",
      "step: 12598, loss: 0.0008169221691787243\n",
      "step: 12599, loss: 2.600999096102896e-06\n",
      "step: 12600, loss: 2.3816655811970122e-06\n",
      "step: 12601, loss: 3.082699777223752e-06\n",
      "step: 12602, loss: 2.903902213802212e-06\n",
      "step: 12603, loss: 7.997869033715688e-06\n",
      "step: 12604, loss: 9.94198444459471e-07\n",
      "step: 12605, loss: 4.222003553877585e-06\n",
      "step: 12606, loss: 2.1547537471633404e-05\n",
      "step: 12607, loss: 5.555119741984527e-07\n",
      "step: 12608, loss: 1.5115319911274128e-06\n",
      "step: 12609, loss: 0.002324372064322233\n",
      "step: 12610, loss: 4.8847882681002375e-06\n",
      "step: 12611, loss: 1.2254608918738086e-06\n",
      "step: 12612, loss: 7.986989771779918e-07\n",
      "step: 12613, loss: 2.8060239856131375e-06\n",
      "step: 12614, loss: 1.0779152034956496e-05\n",
      "step: 12615, loss: 1.6879926079127472e-06\n",
      "step: 12616, loss: 1.4066678488688922e-07\n",
      "step: 12617, loss: 0.09072273224592209\n",
      "step: 12618, loss: 2.9921200166427298e-06\n",
      "step: 12619, loss: 2.7750456865760498e-06\n",
      "step: 12620, loss: 9.775118314792053e-07\n",
      "step: 12621, loss: 1.8429233250571997e-06\n",
      "step: 12622, loss: 9.46513750932354e-07\n",
      "step: 12623, loss: 5.006780270377931e-07\n",
      "step: 12624, loss: 2.014615347434301e-06\n",
      "step: 12625, loss: 4.966018423147034e-06\n",
      "step: 12626, loss: 0.00395615678280592\n",
      "step: 12627, loss: 1.327983682131162e-06\n",
      "step: 12628, loss: 1.0180392564507201e-06\n",
      "step: 12629, loss: 1.2469224657252198e-06\n",
      "step: 12630, loss: 1.4281056337495102e-06\n",
      "step: 12631, loss: 2.837171848568687e-07\n",
      "step: 12632, loss: 8.869102998687595e-07\n",
      "step: 12633, loss: 7.390954124275595e-07\n",
      "step: 12634, loss: 1.308899982177536e-06\n",
      "step: 12635, loss: 3.8217285691644065e-06\n",
      "step: 12636, loss: 2.391089037701022e-05\n",
      "step: 12637, loss: 8.916803153624642e-07\n",
      "step: 12638, loss: 0.047200802713632584\n",
      "step: 12639, loss: 4.024356712761801e-06\n",
      "step: 12640, loss: 4.656128112401348e-06\n",
      "step: 12641, loss: 9.393661457579583e-07\n",
      "step: 12642, loss: 1.4519220030706492e-06\n",
      "step: 12643, loss: 3.959818968723994e-06\n",
      "step: 12644, loss: 7.2422785706294235e-06\n",
      "step: 12645, loss: 2.3173449790192535e-06\n",
      "step: 12646, loss: 9.564456377120223e-06\n",
      "step: 12647, loss: 1.1347548934281804e-05\n",
      "step: 12648, loss: 6.389593067979149e-07\n",
      "step: 12649, loss: 6.04683518758975e-05\n",
      "step: 12650, loss: 2.839517946995329e-06\n",
      "step: 12651, loss: 2.4032158307818463e-06\n",
      "step: 12652, loss: 1.901348150568083e-05\n",
      "step: 12653, loss: 1.959783503480139e-06\n",
      "step: 12654, loss: 1.3136789220880019e-06\n",
      "step: 12655, loss: 7.724728448010865e-07\n",
      "step: 12656, loss: 3.04313452943461e-05\n",
      "step: 12657, loss: 2.8848592137364903e-07\n",
      "step: 12658, loss: 3.537104930728674e-05\n",
      "step: 12659, loss: 0.1611901819705963\n",
      "step: 12660, loss: 1.8500749092709157e-06\n",
      "step: 12661, loss: 1.1515544429130387e-06\n",
      "step: 12662, loss: 5.62665434244991e-07\n",
      "step: 12663, loss: 7.319395081140101e-07\n",
      "step: 12664, loss: 1.3657086128660012e-05\n",
      "step: 12665, loss: 8.511471492056444e-07\n",
      "step: 12666, loss: 0.03487289324402809\n",
      "step: 12667, loss: 3.218645758806815e-07\n",
      "step: 12668, loss: 3.7310830975911813e-06\n",
      "step: 12669, loss: 1.3160620255803224e-06\n",
      "step: 12670, loss: 3.0540797979483614e-06\n",
      "step: 12671, loss: 5.787855479866266e-05\n",
      "step: 12672, loss: 6.546588338096626e-06\n",
      "step: 12673, loss: 2.522438080632128e-06\n",
      "step: 12674, loss: 2.291119699293631e-06\n",
      "step: 12675, loss: 3.4760055314109195e-06\n",
      "step: 12676, loss: 1.5592341924275388e-06\n",
      "step: 12677, loss: 5.538024652196327e-06\n",
      "step: 12678, loss: 6.508788032988377e-07\n",
      "step: 12679, loss: 0.00018237683980260044\n",
      "step: 12680, loss: 5.724133188778069e-06\n",
      "step: 12681, loss: 3.404520157346269e-06\n",
      "step: 12682, loss: 7.867059139243793e-06\n",
      "step: 12683, loss: 9.417440764991625e-07\n",
      "step: 12684, loss: 1.0418846159154782e-06\n",
      "step: 12685, loss: 1.6092000805656426e-05\n",
      "step: 12686, loss: 1.835821024087636e-07\n",
      "step: 12687, loss: 1.6379123053411604e-06\n",
      "step: 12688, loss: 6.365758622450812e-07\n",
      "step: 12689, loss: 1.618628630239982e-05\n",
      "step: 12690, loss: 5.364399839891121e-07\n",
      "step: 12691, loss: 5.242308816377772e-06\n",
      "step: 12692, loss: 4.520067250268767e-06\n",
      "step: 12693, loss: 2.32692059398687e-06\n",
      "step: 12694, loss: 3.361696485626453e-07\n",
      "step: 12695, loss: 2.8633301099034725e-06\n",
      "step: 12696, loss: 2.1958094293950126e-06\n",
      "step: 12697, loss: 5.349462298909202e-06\n",
      "step: 12698, loss: 9.41750784022588e-07\n",
      "step: 12699, loss: 2.347438203287311e-05\n",
      "step: 12700, loss: 6.723377623529814e-07\n",
      "step: 12701, loss: 3.7883155528106727e-06\n",
      "step: 12702, loss: 2.770358150883112e-06\n",
      "step: 12703, loss: 3.469938019406982e-05\n",
      "step: 12704, loss: 5.245190664027177e-07\n",
      "step: 12705, loss: 3.218639790247835e-07\n",
      "step: 12706, loss: 3.7908466765657067e-07\n",
      "step: 12707, loss: 4.980414360034047e-06\n",
      "step: 12708, loss: 1.7356665011902805e-06\n",
      "step: 12709, loss: 1.4332905266201124e-05\n",
      "step: 12710, loss: 7.752892997814342e-06\n",
      "step: 12711, loss: 2.4079783997876802e-06\n",
      "step: 12712, loss: 1.7404106529284036e-06\n",
      "step: 12713, loss: 3.120749852314475e-06\n",
      "step: 12714, loss: 1.0992273018928245e-05\n",
      "step: 12715, loss: 2.1576115614152513e-06\n",
      "step: 12716, loss: 2.7679482172970893e-06\n",
      "step: 12717, loss: 1.378032038701349e-06\n",
      "step: 12718, loss: 5.626667984870437e-07\n",
      "step: 12719, loss: 5.633372438751394e-06\n",
      "step: 12720, loss: 4.026368696941063e-05\n",
      "step: 12721, loss: 5.948049874859862e-06\n",
      "step: 12722, loss: 1.4948586795071606e-06\n",
      "step: 12723, loss: 2.1456723970914027e-06\n",
      "step: 12724, loss: 4.040958629047964e-06\n",
      "step: 12725, loss: 5.173642421141267e-07\n",
      "step: 12726, loss: 1.440032519894885e-06\n",
      "step: 12727, loss: 3.259732329752296e-05\n",
      "step: 12728, loss: 2.047914676950313e-06\n",
      "step: 12729, loss: 1.6975085372905596e-06\n",
      "step: 12730, loss: 1.6689286042037565e-07\n",
      "step: 12731, loss: 5.585792678175494e-06\n",
      "step: 12732, loss: 1.2421528481354471e-06\n",
      "step: 12733, loss: 0.0027658783365041018\n",
      "step: 12734, loss: 0.00019750611681956798\n",
      "step: 12735, loss: 1.1372476365067996e-06\n",
      "step: 12736, loss: 5.313816927809967e-06\n",
      "step: 12737, loss: 2.384145318501396e-06\n",
      "step: 12738, loss: 2.515210098863463e-06\n",
      "step: 12739, loss: 1.5568596154480474e-06\n",
      "step: 12740, loss: 1.6855810827109963e-06\n",
      "step: 12741, loss: 2.1314131117833313e-06\n",
      "step: 12742, loss: 3.831212325167144e-06\n",
      "step: 12743, loss: 0.09343215823173523\n",
      "step: 12744, loss: 3.9122364796639886e-06\n",
      "step: 12745, loss: 1.3732778825215064e-06\n",
      "step: 12746, loss: 1.1074386748077814e-05\n",
      "step: 12747, loss: 7.24790027106792e-07\n",
      "step: 12748, loss: 6.91410150466254e-07\n",
      "step: 12749, loss: 8.439970429208188e-07\n",
      "step: 12750, loss: 4.7320936573669314e-05\n",
      "step: 12751, loss: 4.959074999533186e-07\n",
      "step: 12752, loss: 3.6477834441939194e-07\n",
      "step: 12753, loss: 7.585898401885061e-06\n",
      "step: 12754, loss: 1.4424158507608809e-06\n",
      "step: 12755, loss: 4.369842372398125e-06\n",
      "step: 12756, loss: 4.474834895518143e-06\n",
      "step: 12757, loss: 7.510165573876293e-07\n",
      "step: 12758, loss: 2.2118545530247502e-05\n",
      "step: 12759, loss: 4.2199877725579427e-07\n",
      "step: 12760, loss: 5.650492767017568e-07\n",
      "step: 12761, loss: 9.036032793119375e-07\n",
      "step: 12762, loss: 2.6511686428420944e-06\n",
      "step: 12763, loss: 2.4461387511109933e-06\n",
      "step: 12764, loss: 1.282685957448848e-06\n",
      "step: 12765, loss: 1.5735520264570368e-06\n",
      "step: 12766, loss: 2.0851170120295137e-05\n",
      "step: 12767, loss: 1.1420038390497211e-06\n",
      "step: 12768, loss: 0.05753791332244873\n",
      "step: 12769, loss: 6.229026439541485e-06\n",
      "step: 12770, loss: 2.698830257941154e-06\n",
      "step: 12771, loss: 2.6868788154388312e-06\n",
      "step: 12772, loss: 2.7249946015217574e-06\n",
      "step: 12773, loss: 7.277728400367778e-06\n",
      "step: 12774, loss: 7.820094651833642e-07\n",
      "step: 12775, loss: 1.378035676680156e-06\n",
      "step: 12776, loss: 1.5020164028101135e-06\n",
      "step: 12777, loss: 5.316716737979732e-07\n",
      "step: 12778, loss: 2.429437245154986e-06\n",
      "step: 12779, loss: 2.5486317554168636e-06\n",
      "step: 12780, loss: 1.0108827837029821e-06\n",
      "step: 12781, loss: 6.151159936962358e-07\n",
      "step: 12782, loss: 2.660612608451629e-06\n",
      "step: 12783, loss: 6.8486315285554156e-06\n",
      "step: 12784, loss: 1.6617447045064182e-06\n",
      "step: 12785, loss: 1.049029719979444e-06\n",
      "step: 12786, loss: 7.087038284225855e-06\n",
      "step: 12787, loss: 6.389598752321035e-07\n",
      "step: 12788, loss: 6.580305011993914e-07\n",
      "step: 12789, loss: 0.0031508419197052717\n",
      "step: 12790, loss: 5.221334617999673e-07\n",
      "step: 12791, loss: 1.14440773302249e-07\n",
      "step: 12792, loss: 4.1246153159590904e-07\n",
      "step: 12793, loss: 1.4304916930996114e-06\n",
      "step: 12794, loss: 9.179012181448343e-07\n",
      "step: 12795, loss: 7.462473945452075e-07\n",
      "step: 12796, loss: 1.5304594853660092e-05\n",
      "step: 12797, loss: 1.9859860458382173e-06\n",
      "step: 12798, loss: 4.816046725863998e-07\n",
      "step: 12799, loss: 3.661968776214053e-06\n",
      "step: 12800, loss: 1.1062478506573825e-06\n",
      "step: 12801, loss: 1.1134096666864934e-06\n",
      "step: 12802, loss: 2.908603846663027e-06\n",
      "step: 12803, loss: 4.017244918941287e-06\n",
      "step: 12804, loss: 1.4161901162879076e-06\n",
      "step: 12805, loss: 5.173655495127605e-07\n",
      "step: 12806, loss: 2.670279855010449e-07\n",
      "step: 12807, loss: 5.078291565041582e-07\n",
      "step: 12808, loss: 2.1881100110476837e-05\n",
      "step: 12809, loss: 1.487996087234933e-05\n",
      "step: 12810, loss: 9.965752951757167e-07\n",
      "step: 12811, loss: 6.622576165682403e-06\n",
      "step: 12812, loss: 1.7189830714414711e-06\n",
      "step: 12813, loss: 7.671153980481904e-06\n",
      "step: 12814, loss: 1.718979888210015e-06\n",
      "step: 12815, loss: 5.269035341370909e-07\n",
      "step: 12816, loss: 3.695479620091646e-07\n",
      "step: 12817, loss: 1.9311594314785907e-06\n",
      "step: 12818, loss: 1.1586976143007632e-06\n",
      "step: 12819, loss: 7.08754350853269e-06\n",
      "step: 12820, loss: 1.4209525716069038e-06\n",
      "step: 12821, loss: 2.1624018700094894e-06\n",
      "step: 12822, loss: 7.939309512039472e-07\n",
      "step: 12823, loss: 0.05394022911787033\n",
      "step: 12824, loss: 1.2731406968669035e-06\n",
      "step: 12825, loss: 2.2887791146786185e-06\n",
      "step: 12826, loss: 7.065983481879812e-06\n",
      "step: 12827, loss: 1.5520619172093575e-06\n",
      "step: 12828, loss: 0.004348268266767263\n",
      "step: 12829, loss: 7.82009578870202e-07\n",
      "step: 12830, loss: 3.0731184779142495e-06\n",
      "step: 12831, loss: 2.432260225759819e-05\n",
      "step: 12832, loss: 1.1753954822779633e-06\n",
      "step: 12833, loss: 2.1743376237282064e-06\n",
      "step: 12834, loss: 6.127340270722925e-07\n",
      "step: 12835, loss: 1.8071847307510325e-06\n",
      "step: 12836, loss: 9.34591469103907e-07\n",
      "step: 12837, loss: 0.04725348576903343\n",
      "step: 12838, loss: 5.674340286532242e-07\n",
      "step: 12839, loss: 9.012182999867946e-07\n",
      "step: 12840, loss: 6.50879087515932e-07\n",
      "step: 12841, loss: 1.9311064534122124e-06\n",
      "step: 12842, loss: 9.083631198336661e-07\n",
      "step: 12843, loss: 0.00010758755524875596\n",
      "step: 12844, loss: 2.427060508125578e-06\n",
      "step: 12845, loss: 0.0005134096136316657\n",
      "step: 12846, loss: 2.7822648007713724e-06\n",
      "step: 12847, loss: 1.7790527635952458e-05\n",
      "step: 12848, loss: 8.627672286820598e-06\n",
      "step: 12849, loss: 5.340558004718332e-07\n",
      "step: 12850, loss: 7.02299394106376e-06\n",
      "step: 12851, loss: 1.025021992973052e-05\n",
      "step: 12852, loss: 0.08095914870500565\n",
      "step: 12853, loss: 3.0755938951188e-07\n",
      "step: 12854, loss: 1.9168555809301324e-06\n",
      "step: 12855, loss: 2.4652076717757154e-06\n",
      "step: 12856, loss: 5.148917261976749e-05\n",
      "step: 12857, loss: 9.499898624198977e-06\n",
      "step: 12858, loss: 5.304196292854613e-06\n",
      "step: 12859, loss: 5.125983761899988e-07\n",
      "step: 12860, loss: 1.7595115195945255e-06\n",
      "step: 12861, loss: 2.0861264147242764e-06\n",
      "step: 12862, loss: 2.70599480245437e-06\n",
      "step: 12863, loss: 7.104849828465376e-07\n",
      "step: 12864, loss: 0.39693450927734375\n",
      "step: 12865, loss: 8.600455657870043e-06\n",
      "step: 12866, loss: 0.06054149195551872\n",
      "step: 12867, loss: 1.859661153957859e-07\n",
      "step: 12868, loss: 8.105425877147354e-06\n",
      "step: 12869, loss: 1.9822920876322314e-05\n",
      "step: 12870, loss: 4.720666879620694e-07\n",
      "step: 12871, loss: 8.460793651465792e-06\n",
      "step: 12872, loss: 5.936589673183335e-07\n",
      "step: 12873, loss: 1.9287640498077963e-06\n",
      "step: 12874, loss: 0.010644061490893364\n",
      "step: 12875, loss: 2.062285602733027e-06\n",
      "step: 12876, loss: 1.075257614502334e-06\n",
      "step: 12877, loss: 4.1246283899454284e-07\n",
      "step: 12878, loss: 3.0755938951188e-07\n",
      "step: 12879, loss: 6.720193596265744e-06\n",
      "step: 12880, loss: 1.2421370456650038e-06\n",
      "step: 12881, loss: 1.0621678484312724e-05\n",
      "step: 12882, loss: 1.4528623637488636e-07\n",
      "step: 12883, loss: 7.54128850530833e-05\n",
      "step: 12884, loss: 1.1186498340975959e-05\n",
      "step: 12885, loss: 1.7951098925550468e-05\n",
      "step: 12886, loss: 1.2894744031655136e-05\n",
      "step: 12887, loss: 4.458410955976433e-07\n",
      "step: 12888, loss: 2.217242354163318e-06\n",
      "step: 12889, loss: 3.6001131320517743e-07\n",
      "step: 12890, loss: 6.165095328469761e-06\n",
      "step: 12891, loss: 6.699532946186082e-07\n",
      "step: 12892, loss: 2.3364232220046688e-06\n",
      "step: 12893, loss: 1.2993644986636355e-06\n",
      "step: 12894, loss: 9.775005764822708e-07\n",
      "step: 12895, loss: 9.155183988696081e-07\n",
      "step: 12896, loss: 0.0017987532773986459\n",
      "step: 12897, loss: 2.2124534098111326e-06\n",
      "step: 12898, loss: 8.535265010323201e-07\n",
      "step: 12899, loss: 3.383046760063735e-06\n",
      "step: 12900, loss: 2.0027131597544212e-07\n",
      "step: 12901, loss: 0.06500128656625748\n",
      "step: 12902, loss: 4.410729843584704e-07\n",
      "step: 12903, loss: 1.0967099797198898e-06\n",
      "step: 12904, loss: 1.7881379221762472e-07\n",
      "step: 12905, loss: 1.8739323195404722e-06\n",
      "step: 12906, loss: 4.434558888988249e-07\n",
      "step: 12907, loss: 0.05596903711557388\n",
      "step: 12908, loss: 6.628011988141225e-07\n",
      "step: 12909, loss: 2.384185648907078e-08\n",
      "step: 12910, loss: 1.0776475392049178e-06\n",
      "step: 12911, loss: 1.273126144951675e-06\n",
      "step: 12912, loss: 5.745845896854007e-07\n",
      "step: 12913, loss: 1.9383139715500874e-06\n",
      "step: 12914, loss: 3.576267033622571e-07\n",
      "step: 12915, loss: 8.511461260241049e-07\n",
      "step: 12916, loss: 1.716611279789504e-07\n",
      "step: 12917, loss: 1.9931496808567317e-06\n",
      "step: 12918, loss: 5.557167241931893e-06\n",
      "step: 12919, loss: 1.5973719200701453e-06\n",
      "step: 12920, loss: 0.09733984619379044\n",
      "step: 12921, loss: 1.299360633311153e-06\n",
      "step: 12922, loss: 7.55782878059108e-07\n",
      "step: 12923, loss: 6.019514330546372e-06\n",
      "step: 12924, loss: 5.230808528722264e-05\n",
      "step: 12925, loss: 0.00028464003116823733\n",
      "step: 12926, loss: 9.086779755307361e-06\n",
      "step: 12927, loss: 2.448451596137602e-06\n",
      "step: 12928, loss: 4.720673416613863e-07\n",
      "step: 12929, loss: 9.560525313645485e-07\n",
      "step: 12930, loss: 2.54862993642746e-06\n",
      "step: 12931, loss: 1.4614788597100414e-06\n",
      "step: 12932, loss: 1.7475717868364882e-06\n",
      "step: 12933, loss: 4.801231170858955e-06\n",
      "step: 12934, loss: 5.37850155524211e-06\n",
      "step: 12935, loss: 5.364397566154366e-07\n",
      "step: 12936, loss: 5.578977493314596e-07\n",
      "step: 12937, loss: 9.226683914675959e-07\n",
      "step: 12938, loss: 3.743162722003035e-07\n",
      "step: 12939, loss: 3.859669504890917e-06\n",
      "step: 12940, loss: 9.465114771955996e-07\n",
      "step: 12941, loss: 1.5024979802547023e-05\n",
      "step: 12942, loss: 1.0013409337261692e-06\n",
      "step: 12943, loss: 4.460457603272516e-06\n",
      "step: 12944, loss: 3.1471208217226376e-07\n",
      "step: 12945, loss: 1.8501056047171005e-06\n",
      "step: 12946, loss: 4.2604033296811394e-06\n",
      "step: 12947, loss: 5.006788228456571e-08\n",
      "step: 12948, loss: 4.1723137655935716e-07\n",
      "step: 12949, loss: 0.07081243395805359\n",
      "step: 12950, loss: 9.751279321790207e-07\n",
      "step: 12951, loss: 6.511663377750665e-05\n",
      "step: 12952, loss: 3.194797955075046e-07\n",
      "step: 12953, loss: 1.5687802488173475e-06\n",
      "step: 12954, loss: 2.9529037419706583e-05\n",
      "step: 12955, loss: 6.031966677255696e-07\n",
      "step: 12956, loss: 1.2922670975967776e-05\n",
      "step: 12957, loss: 2.467578497089562e-06\n",
      "step: 12958, loss: 2.2720428205502685e-06\n",
      "step: 12959, loss: 1.2468631211959291e-05\n",
      "step: 12960, loss: 1.2894290193798952e-05\n",
      "step: 12961, loss: 8.320706683662138e-07\n",
      "step: 12962, loss: 0.00020771881099790335\n",
      "step: 12963, loss: 2.1189383915043436e-05\n",
      "step: 12964, loss: 8.034611482798937e-07\n",
      "step: 12965, loss: 8.27308838324825e-07\n",
      "step: 12966, loss: 4.052962594869314e-06\n",
      "step: 12967, loss: 5.244663498160662e-06\n",
      "step: 12968, loss: 5.650492198583379e-07\n",
      "step: 12969, loss: 2.6535669803706696e-06\n",
      "step: 12970, loss: 2.112353286065627e-06\n",
      "step: 12971, loss: 5.483613563228573e-07\n",
      "step: 12972, loss: 7.581667205158737e-07\n",
      "step: 12973, loss: 2.3364927415059356e-07\n",
      "step: 12974, loss: 9.798949349715258e-07\n",
      "step: 12975, loss: 0.09028464555740356\n",
      "step: 12976, loss: 8.487673994750367e-07\n",
      "step: 12977, loss: 6.508785190817434e-07\n",
      "step: 12978, loss: 0.11238343268632889\n",
      "step: 12979, loss: 9.202864816870715e-07\n",
      "step: 12980, loss: 7.741065928712487e-06\n",
      "step: 12981, loss: 1.2397750026593712e-07\n",
      "step: 12982, loss: 9.703475143396645e-07\n",
      "step: 12983, loss: 4.3868823240700294e-07\n",
      "step: 12984, loss: 2.6654672637960175e-06\n",
      "step: 12985, loss: 5.402324404712999e-06\n",
      "step: 12986, loss: 8.51543081807904e-05\n",
      "step: 12987, loss: 7.028117124718847e-06\n",
      "step: 12988, loss: 1.3923431652074214e-06\n",
      "step: 12989, loss: 2.598759181182686e-07\n",
      "step: 12990, loss: 0.005325756035745144\n",
      "step: 12991, loss: 5.809878530271817e-06\n",
      "step: 12992, loss: 1.1396356285331422e-06\n",
      "step: 12993, loss: 0.0003654964966699481\n",
      "step: 12994, loss: 1.1396289210097166e-06\n",
      "step: 12995, loss: 3.552426051101065e-07\n",
      "step: 12996, loss: 2.1552493763010716e-06\n",
      "step: 12997, loss: 7.754777470836416e-06\n",
      "step: 12998, loss: 3.1566055440634955e-06\n",
      "step: 12999, loss: 7.963107577779738e-07\n",
      "step: 13000, loss: 3.3140099731099326e-07\n",
      "step: 13001, loss: 0.00012752090697176754\n",
      "step: 13002, loss: 3.690596258820733e-06\n",
      "step: 13003, loss: 8.416120635956759e-07\n",
      "step: 13004, loss: 5.490783223649487e-05\n",
      "step: 13005, loss: 1.9261680790805258e-05\n",
      "step: 13006, loss: 4.8659512685844675e-06\n",
      "step: 13007, loss: 1.8048042420559796e-06\n",
      "step: 13008, loss: 7.3688825068529695e-06\n",
      "step: 13009, loss: 6.365754074977303e-07\n",
      "step: 13010, loss: 3.291152461315505e-05\n",
      "step: 13011, loss: 2.129010226781247e-06\n",
      "step: 13012, loss: 7.939291890579625e-07\n",
      "step: 13013, loss: 3.3568194339750335e-06\n",
      "step: 13014, loss: 1.2516716196842026e-06\n",
      "step: 13015, loss: 4.899064151686616e-05\n",
      "step: 13016, loss: 3.07559190559914e-07\n",
      "step: 13017, loss: 9.846590955930878e-07\n",
      "step: 13018, loss: 4.796756911673583e-06\n",
      "step: 13019, loss: 1.220949343405664e-05\n",
      "step: 13020, loss: 1.9550310526028625e-07\n",
      "step: 13021, loss: 7.761805136397015e-06\n",
      "step: 13022, loss: 1.050206719810376e-05\n",
      "step: 13023, loss: 3.320947826068732e-06\n",
      "step: 13024, loss: 1.8143339275411563e-06\n",
      "step: 13025, loss: 2.920534370787209e-06\n",
      "step: 13026, loss: 3.497507805150235e-06\n",
      "step: 13027, loss: 8.392278232349781e-07\n",
      "step: 13028, loss: 1.108635501623212e-06\n",
      "step: 13029, loss: 4.033876848552609e-06\n",
      "step: 13030, loss: 3.3378555031049473e-07\n",
      "step: 13031, loss: 3.9338937085631187e-07\n",
      "step: 13032, loss: 3.1232767128130945e-07\n",
      "step: 13033, loss: 4.863729259341198e-07\n",
      "step: 13034, loss: 1.013271457850351e-06\n",
      "step: 13035, loss: 7.395279681077227e-06\n",
      "step: 13036, loss: 0.09688826650381088\n",
      "step: 13037, loss: 0.09243777394294739\n",
      "step: 13038, loss: 1.0610446224745829e-05\n",
      "step: 13039, loss: 2.517639586585574e-06\n",
      "step: 13040, loss: 1.644052281335462e-05\n",
      "step: 13041, loss: 2.6464408620086033e-07\n",
      "step: 13042, loss: 5.67434142340062e-07\n",
      "step: 13043, loss: 4.1484688040327455e-07\n",
      "step: 13044, loss: 1.1086348195021856e-06\n",
      "step: 13045, loss: 2.6988407171302242e-06\n",
      "step: 13046, loss: 1.0760828445199877e-05\n",
      "step: 13047, loss: 8.119245649140794e-06\n",
      "step: 13048, loss: 3.38553292067445e-07\n",
      "step: 13049, loss: 7.966486009536311e-06\n",
      "step: 13050, loss: 4.529937598363176e-07\n",
      "step: 13051, loss: 1.6069286630226998e-06\n",
      "step: 13052, loss: 5.239861366135301e-06\n",
      "step: 13053, loss: 4.720675406133523e-07\n",
      "step: 13054, loss: 4.2676819589360093e-07\n",
      "step: 13055, loss: 0.05447933077812195\n",
      "step: 13056, loss: 5.19749448812945e-07\n",
      "step: 13057, loss: 6.628008577536093e-07\n",
      "step: 13058, loss: 6.604151963074401e-07\n",
      "step: 13059, loss: 4.86371220631554e-07\n",
      "step: 13060, loss: 1.2063837857567705e-06\n",
      "step: 13061, loss: 7.128635388653493e-07\n",
      "step: 13062, loss: 1.344663132840651e-06\n",
      "step: 13063, loss: 4.959091484124656e-07\n",
      "step: 13064, loss: 1.4017487046658061e-05\n",
      "step: 13065, loss: 9.465127277508145e-07\n",
      "step: 13066, loss: 2.131438577634981e-06\n",
      "step: 13067, loss: 1.7857353213912575e-06\n",
      "step: 13068, loss: 3.4332205700593477e-07\n",
      "step: 13069, loss: 3.5524300301403855e-07\n",
      "step: 13070, loss: 2.6988009267370217e-06\n",
      "step: 13071, loss: 2.210065758845303e-06\n",
      "step: 13072, loss: 0.027398250997066498\n",
      "step: 13073, loss: 1.3113015029375674e-07\n",
      "step: 13074, loss: 1.776180624801782e-06\n",
      "step: 13075, loss: 3.0754924864595523e-06\n",
      "step: 13076, loss: 1.3828261558046506e-07\n",
      "step: 13077, loss: 5.364399839891121e-07\n",
      "step: 13078, loss: 7.843898401915794e-07\n",
      "step: 13079, loss: 6.914133621194196e-08\n",
      "step: 13080, loss: 4.577617858103622e-07\n",
      "step: 13081, loss: 6.890254553582054e-07\n",
      "step: 13082, loss: 1.2302276672926382e-06\n",
      "step: 13083, loss: 0.0102371321991086\n",
      "step: 13084, loss: 1.0561781209617038e-06\n",
      "step: 13085, loss: 8.73754015628947e-06\n",
      "step: 13086, loss: 2.5915380774677033e-06\n",
      "step: 13087, loss: 2.2148403786559356e-06\n",
      "step: 13088, loss: 1.1062495559599483e-06\n",
      "step: 13089, loss: 6.818718247814104e-07\n",
      "step: 13090, loss: 5.245204093284883e-08\n",
      "step: 13091, loss: 1.5401691371152992e-06\n",
      "step: 13092, loss: 1.254067001354997e-06\n",
      "step: 13093, loss: 2.767857722574263e-06\n",
      "step: 13094, loss: 2.429351070531993e-06\n",
      "step: 13095, loss: 5.912764891036204e-07\n",
      "step: 13096, loss: 2.098081353096859e-07\n",
      "step: 13097, loss: 4.2438435343683523e-07\n",
      "step: 13098, loss: 2.0431716620805673e-06\n",
      "step: 13099, loss: 1.0129459951713216e-05\n",
      "step: 13100, loss: 4.753560915560229e-06\n",
      "step: 13101, loss: 1.5539211744908243e-05\n",
      "step: 13102, loss: 4.303323294152506e-06\n",
      "step: 13103, loss: 0.12457991391420364\n",
      "step: 13104, loss: 7.176378744588874e-07\n",
      "step: 13105, loss: 3.7669923358407686e-07\n",
      "step: 13106, loss: 1.0037358606496127e-06\n",
      "step: 13107, loss: 0.04313354566693306\n",
      "step: 13108, loss: 3.464511451056751e-07\n",
      "step: 13109, loss: 1.3255843214210472e-06\n",
      "step: 13110, loss: 1.3065181292404304e-06\n",
      "step: 13111, loss: 1.239765879290644e-06\n",
      "step: 13112, loss: 1.5878282511039288e-06\n",
      "step: 13113, loss: 7.621609711350175e-06\n",
      "step: 13114, loss: 1.7642963712205528e-07\n",
      "step: 13115, loss: 7.723715498286765e-06\n",
      "step: 13116, loss: 1.2111529485991923e-06\n",
      "step: 13117, loss: 0.0002139567950507626\n",
      "step: 13118, loss: 4.815693955606548e-06\n",
      "step: 13119, loss: 8.22533252176072e-07\n",
      "step: 13120, loss: 1.5187184772003093e-06\n",
      "step: 13121, loss: 1.1062533076255932e-06\n",
      "step: 13122, loss: 8.773749868851155e-07\n",
      "step: 13123, loss: 2.6702852551352407e-07\n",
      "step: 13124, loss: 9.816674719331786e-06\n",
      "step: 13125, loss: 1.7547339439261123e-06\n",
      "step: 13126, loss: 2.3126137875806307e-06\n",
      "step: 13127, loss: 1.4221799574443139e-05\n",
      "step: 13128, loss: 8.63070397372212e-07\n",
      "step: 13129, loss: 0.07157294452190399\n",
      "step: 13130, loss: 7.170829121605493e-06\n",
      "step: 13131, loss: 3.123280123418226e-07\n",
      "step: 13132, loss: 1.9073469559316436e-07\n",
      "step: 13133, loss: 1.0490410318197974e-07\n",
      "step: 13134, loss: 1.2985385183128528e-05\n",
      "step: 13135, loss: 3.8623670661763754e-07\n",
      "step: 13136, loss: 0.011944588273763657\n",
      "step: 13137, loss: 2.7418093395681353e-07\n",
      "step: 13138, loss: 1.1444051324360771e-06\n",
      "step: 13139, loss: 1.227647499035811e-05\n",
      "step: 13140, loss: 2.8610182312149846e-07\n",
      "step: 13141, loss: 3.5545695027394686e-06\n",
      "step: 13142, loss: 3.6475016713666264e-06\n",
      "step: 13143, loss: 1.3961890545033384e-05\n",
      "step: 13144, loss: 5.936573757026053e-07\n",
      "step: 13145, loss: 3.780749466386624e-05\n",
      "step: 13146, loss: 9.436639811610803e-05\n",
      "step: 13147, loss: 0.0020326448138803244\n",
      "step: 13148, loss: 2.4484938876412343e-06\n",
      "step: 13149, loss: 0.0040587796829640865\n",
      "step: 13150, loss: 2.5939400529750856e-06\n",
      "step: 13151, loss: 1.1873120229211054e-06\n",
      "step: 13152, loss: 7.4853655860351864e-06\n",
      "step: 13153, loss: 3.2685338737792335e-06\n",
      "step: 13154, loss: 4.911406108476513e-07\n",
      "step: 13155, loss: 4.911413498120965e-07\n",
      "step: 13156, loss: 2.861014820609853e-07\n",
      "step: 13157, loss: 2.429554660920985e-05\n",
      "step: 13158, loss: 2.915274308179505e-05\n",
      "step: 13159, loss: 7.52578080209787e-06\n",
      "step: 13160, loss: 8.988325816972065e-07\n",
      "step: 13161, loss: 2.8848592137364903e-07\n",
      "step: 13162, loss: 5.060028706793673e-05\n",
      "step: 13163, loss: 1.0895629429796827e-06\n",
      "step: 13164, loss: 0.13691602647304535\n",
      "step: 13165, loss: 1.5664927559555508e-05\n",
      "step: 13166, loss: 1.3398970395428478e-06\n",
      "step: 13167, loss: 1.874862027761992e-05\n",
      "step: 13168, loss: 2.336498852173463e-07\n",
      "step: 13169, loss: 4.6491481953125913e-07\n",
      "step: 13170, loss: 1.7776546883396804e-05\n",
      "step: 13171, loss: 1.0418838201076142e-06\n",
      "step: 13172, loss: 7.510100203944603e-07\n",
      "step: 13173, loss: 7.39087454348919e-07\n",
      "step: 13174, loss: 9.751236120791873e-07\n",
      "step: 13175, loss: 1.0013575035827671e-07\n",
      "step: 13176, loss: 1.6212100035772892e-06\n",
      "step: 13177, loss: 2.837167016878084e-07\n",
      "step: 13178, loss: 3.800213562499266e-06\n",
      "step: 13179, loss: 1.8500670648791129e-06\n",
      "step: 13180, loss: 2.551074089751637e-07\n",
      "step: 13181, loss: 1.7404538255050284e-07\n",
      "step: 13182, loss: 9.083679515242693e-07\n",
      "step: 13183, loss: 2.813336266171973e-07\n",
      "step: 13184, loss: 2.455709307014331e-07\n",
      "step: 13185, loss: 1.0967246311111012e-07\n",
      "step: 13186, loss: 5.459759222503635e-07\n",
      "step: 13187, loss: 3.862357402795169e-07\n",
      "step: 13188, loss: 2.715512891882099e-06\n",
      "step: 13189, loss: 4.625307781225274e-07\n",
      "step: 13190, loss: 2.0980319277441595e-06\n",
      "step: 13191, loss: 4.792194658875815e-07\n",
      "step: 13192, loss: 2.7894907361769583e-07\n",
      "step: 13193, loss: 3.194802786765649e-07\n",
      "step: 13194, loss: 1.2469142802729039e-06\n",
      "step: 13195, loss: 2.548624706832925e-06\n",
      "step: 13196, loss: 4.053101747558685e-07\n",
      "step: 13197, loss: 0.0018303011311218143\n",
      "step: 13198, loss: 2.5272314019275655e-07\n",
      "step: 13199, loss: 3.123278986549849e-07\n",
      "step: 13200, loss: 2.3603318766163284e-07\n",
      "step: 13201, loss: 1.597402814468296e-07\n",
      "step: 13202, loss: 2.720214297369239e-06\n",
      "step: 13203, loss: 1.7213253613590496e-06\n",
      "step: 13204, loss: 2.097404285450466e-05\n",
      "step: 13205, loss: 0.09441863745450974\n",
      "step: 13206, loss: 2.028898506978294e-06\n",
      "step: 13207, loss: 4.761024229082977e-06\n",
      "step: 13208, loss: 6.413389996851038e-07\n",
      "step: 13209, loss: 5.054463940723508e-07\n",
      "step: 13210, loss: 2.0265242710593157e-06\n",
      "step: 13211, loss: 5.221348828854389e-07\n",
      "step: 13212, loss: 1.2779158851117245e-06\n",
      "step: 13213, loss: 4.1075913031818345e-05\n",
      "step: 13214, loss: 2.679762019397458e-06\n",
      "step: 13215, loss: 5.364378807826142e-07\n",
      "step: 13216, loss: 1.6569510989938863e-06\n",
      "step: 13217, loss: 9.536737621829161e-08\n",
      "step: 13218, loss: 1.461492502130568e-06\n",
      "step: 13219, loss: 7.104856081241451e-07\n",
      "step: 13220, loss: 6.151172442514508e-07\n",
      "step: 13221, loss: 4.3868908505828585e-07\n",
      "step: 13222, loss: 1.6045320307966904e-06\n",
      "step: 13223, loss: 2.150504315068247e-06\n",
      "step: 13224, loss: 2.2934709704713896e-06\n",
      "step: 13225, loss: 0.1919950246810913\n",
      "step: 13226, loss: 0.0487368144094944\n",
      "step: 13227, loss: 2.0064981072209775e-05\n",
      "step: 13228, loss: 4.1484688040327455e-07\n",
      "step: 13229, loss: 5.60281307571131e-07\n",
      "step: 13230, loss: 1.289833448936406e-06\n",
      "step: 13231, loss: 6.818726205892744e-07\n",
      "step: 13232, loss: 5.626641836897761e-07\n",
      "step: 13233, loss: 2.2553676899406128e-06\n",
      "step: 13234, loss: 1.2707590713034733e-06\n",
      "step: 13235, loss: 0.05593100190162659\n",
      "step: 13236, loss: 1.5188887118711136e-05\n",
      "step: 13237, loss: 1.7094317854571273e-06\n",
      "step: 13238, loss: 8.964467497207806e-07\n",
      "step: 13239, loss: 1.0847957128135022e-06\n",
      "step: 13240, loss: 4.264959443389671e-06\n",
      "step: 13241, loss: 1.4688603187096305e-05\n",
      "step: 13242, loss: 1.2516879905888345e-06\n",
      "step: 13243, loss: 8.773759532232361e-07\n",
      "step: 13244, loss: 1.0895655577769503e-06\n",
      "step: 13245, loss: 5.221355081630463e-07\n",
      "step: 13246, loss: 0.09554038196802139\n",
      "step: 13247, loss: 3.3234057355002733e-06\n",
      "step: 13248, loss: 1.1276991926933988e-06\n",
      "step: 13249, loss: 2.7488481464388315e-06\n",
      "step: 13250, loss: 6.604179816349642e-07\n",
      "step: 13251, loss: 2.0503493942669593e-06\n",
      "step: 13252, loss: 2.565260501796729e-06\n",
      "step: 13253, loss: 3.32098784383561e-06\n",
      "step: 13254, loss: 4.696833002526546e-07\n",
      "step: 13255, loss: 1.7237085785382078e-06\n",
      "step: 13256, loss: 9.832700015977025e-06\n",
      "step: 13257, loss: 1.8787037561196485e-06\n",
      "step: 13258, loss: 1.8620273749547778e-06\n",
      "step: 13259, loss: 9.441219503969478e-07\n",
      "step: 13260, loss: 1.2850662187702255e-06\n",
      "step: 13261, loss: 2.603468601591885e-06\n",
      "step: 13262, loss: 7.997487955435645e-06\n",
      "step: 13263, loss: 3.936019766115351e-06\n",
      "step: 13264, loss: 3.862362802919961e-07\n",
      "step: 13265, loss: 1.9788723193414626e-07\n",
      "step: 13266, loss: 3.0994357302915887e-07\n",
      "step: 13267, loss: 2.391310772509314e-06\n",
      "step: 13268, loss: 8.392249810640351e-07\n",
      "step: 13269, loss: 8.988263289211318e-07\n",
      "step: 13270, loss: 2.837177532910573e-07\n",
      "step: 13271, loss: 5.14982843924372e-07\n",
      "step: 13272, loss: 1.0114682481798809e-05\n",
      "step: 13273, loss: 2.5629442461649887e-06\n",
      "step: 13274, loss: 1.6784351828391664e-06\n",
      "step: 13275, loss: 0.0005393856554292142\n",
      "step: 13276, loss: 1.8835044102161191e-07\n",
      "step: 13277, loss: 1.4848655155219603e-05\n",
      "step: 13278, loss: 3.88850639865268e-06\n",
      "step: 13279, loss: 3.2827672384883044e-06\n",
      "step: 13280, loss: 1.1729907782864757e-06\n",
      "step: 13281, loss: 1.5687619452364743e-06\n",
      "step: 13282, loss: 3.2901670010687667e-07\n",
      "step: 13283, loss: 2.319773102499312e-06\n",
      "step: 13284, loss: 2.2840072233520914e-06\n",
      "step: 13285, loss: 3.163629298796877e-05\n",
      "step: 13286, loss: 0.0001884820667328313\n",
      "step: 13287, loss: 1.6776371921878308e-05\n",
      "step: 13288, loss: 1.4161819308355916e-06\n",
      "step: 13289, loss: 5.3045578169985674e-06\n",
      "step: 13290, loss: 5.793556283606449e-07\n",
      "step: 13291, loss: 2.479548868450365e-07\n",
      "step: 13292, loss: 3.671633521662443e-07\n",
      "step: 13293, loss: 1.4233322644940927e-06\n",
      "step: 13294, loss: 1.5735611214040546e-07\n",
      "step: 13295, loss: 1.3232040600996697e-06\n",
      "step: 13296, loss: 1.759503106768534e-06\n",
      "step: 13297, loss: 3.2471452868776396e-06\n",
      "step: 13298, loss: 0.0018150086980313063\n",
      "step: 13299, loss: 1.5452029401785694e-05\n",
      "step: 13300, loss: 3.2352095331589226e-06\n",
      "step: 13301, loss: 2.6941242481370864e-07\n",
      "step: 13302, loss: 0.10619793087244034\n",
      "step: 13303, loss: 8.892911296243255e-07\n",
      "step: 13304, loss: 6.670398761343677e-06\n",
      "step: 13305, loss: 9.615925591788255e-06\n",
      "step: 13306, loss: 1.3088913419778692e-06\n",
      "step: 13307, loss: 5.623637662210967e-06\n",
      "step: 13308, loss: 2.145764597116795e-07\n",
      "step: 13309, loss: 3.2519385513296584e-06\n",
      "step: 13310, loss: 4.7489515964116435e-06\n",
      "step: 13311, loss: 1.5067947742863907e-06\n",
      "step: 13312, loss: 1.2993612017453415e-06\n",
      "step: 13313, loss: 1.8328812075196765e-05\n",
      "step: 13314, loss: 1.1682401463986025e-06\n",
      "step: 13315, loss: 1.0085042276841705e-06\n",
      "step: 13316, loss: 1.7237389329238795e-06\n",
      "step: 13317, loss: 4.6729951463930774e-07\n",
      "step: 13318, loss: 7.843931939532922e-07\n",
      "step: 13319, loss: 4.100784281035885e-07\n",
      "step: 13320, loss: 0.018319571390748024\n",
      "step: 13321, loss: 1.0228058044958743e-06\n",
      "step: 13322, loss: 1.7380568806402152e-06\n",
      "step: 13323, loss: 6.890276722515409e-07\n",
      "step: 13324, loss: 5.495130153576611e-06\n",
      "step: 13325, loss: 0.05732030048966408\n",
      "step: 13326, loss: 3.5524291774891026e-07\n",
      "step: 13327, loss: 1.2516881042756722e-06\n",
      "step: 13328, loss: 1.1443893299656338e-06\n",
      "step: 13329, loss: 1.1029964298359118e-05\n",
      "step: 13330, loss: 5.516315468412358e-06\n",
      "step: 13331, loss: 1.8095632867698441e-06\n",
      "step: 13332, loss: 1.8834355159924598e-06\n",
      "step: 13333, loss: 2.019354951698915e-06\n",
      "step: 13334, loss: 0.04443886876106262\n",
      "step: 13335, loss: 8.225374017456488e-07\n",
      "step: 13336, loss: 0.003769519040361047\n",
      "step: 13337, loss: 1.187315433526237e-06\n",
      "step: 13338, loss: 1.3986013073008507e-05\n",
      "step: 13339, loss: 1.9311886489958852e-07\n",
      "step: 13340, loss: 2.834749238900258e-06\n",
      "step: 13341, loss: 3.7431595956149977e-07\n",
      "step: 13342, loss: 3.1255717658495996e-06\n",
      "step: 13343, loss: 2.479550573752931e-07\n",
      "step: 13344, loss: 5.507454829967173e-07\n",
      "step: 13345, loss: 3.754937552002957e-06\n",
      "step: 13346, loss: 2.1504606593225617e-06\n",
      "step: 13347, loss: 1.9216201962990453e-06\n",
      "step: 13348, loss: 2.1020292479079217e-05\n",
      "step: 13349, loss: 0.08554380387067795\n",
      "step: 13350, loss: 1.3804159380015335e-06\n",
      "step: 13351, loss: 2.062428575300146e-05\n",
      "step: 13352, loss: 1.6403089375671698e-06\n",
      "step: 13353, loss: 6.72338728691102e-07\n",
      "step: 13354, loss: 7.915480182418833e-07\n",
      "step: 13355, loss: 6.629963081650203e-06\n",
      "step: 13356, loss: 0.12040088325738907\n",
      "step: 13357, loss: 3.847881544061238e-06\n",
      "step: 13358, loss: 0.08814272284507751\n",
      "step: 13359, loss: 3.406825953788939e-06\n",
      "step: 13360, loss: 8.058478329076024e-07\n",
      "step: 13361, loss: 1.1873049743371666e-06\n",
      "step: 13362, loss: 6.890276722515409e-07\n",
      "step: 13363, loss: 0.15691561996936798\n",
      "step: 13364, loss: 1.9810871890513226e-05\n",
      "step: 13365, loss: 5.316696842783131e-07\n",
      "step: 13366, loss: 3.189868039044086e-06\n",
      "step: 13367, loss: 2.4652276806591544e-06\n",
      "step: 13368, loss: 0.0702698826789856\n",
      "step: 13369, loss: 1.8548598745837808e-06\n",
      "step: 13370, loss: 3.0040700949029997e-07\n",
      "step: 13371, loss: 1.0514144150874927e-06\n",
      "step: 13372, loss: 1.3684850728168385e-06\n",
      "step: 13373, loss: 2.3126551695895614e-07\n",
      "step: 13374, loss: 7.057143989186443e-07\n",
      "step: 13375, loss: 2.555722403485561e-06\n",
      "step: 13376, loss: 3.0397577575058676e-06\n",
      "step: 13377, loss: 3.4807246720447438e-06\n",
      "step: 13378, loss: 6.175013709253108e-07\n",
      "step: 13379, loss: 0.0002165369805879891\n",
      "step: 13380, loss: 8.579600034863688e-06\n",
      "step: 13381, loss: 1.573560552969866e-07\n",
      "step: 13382, loss: 4.963498213328421e-06\n",
      "step: 13383, loss: 3.8385297784770955e-07\n",
      "step: 13384, loss: 3.1208410291583277e-06\n",
      "step: 13385, loss: 1.1610862884481321e-06\n",
      "step: 13386, loss: 8.603895366832148e-06\n",
      "step: 13387, loss: 4.0531085687689483e-07\n",
      "step: 13388, loss: 2.2196572899702005e-06\n",
      "step: 13389, loss: 2.5416233256692067e-05\n",
      "step: 13390, loss: 1.1210156117158476e-05\n",
      "step: 13391, loss: 2.989657332364004e-06\n",
      "step: 13392, loss: 1.79848702828167e-05\n",
      "step: 13393, loss: 9.339106327388436e-06\n",
      "step: 13394, loss: 7.629390097463329e-08\n",
      "step: 13395, loss: 2.4556563857913716e-06\n",
      "step: 13396, loss: 1.5902299992376356e-06\n",
      "step: 13397, loss: 1.921626108014607e-06\n",
      "step: 13398, loss: 9.012184705170512e-07\n",
      "step: 13399, loss: 0.003724473062902689\n",
      "step: 13400, loss: 2.384182948844682e-07\n",
      "step: 13401, loss: 6.818738711444894e-07\n",
      "step: 13402, loss: 4.982936729902576e-07\n",
      "step: 13403, loss: 2.005052238018834e-06\n",
      "step: 13404, loss: 4.482257054405636e-07\n",
      "step: 13405, loss: 6.937937087059254e-07\n",
      "step: 13406, loss: 0.04095721244812012\n",
      "step: 13407, loss: 1.9488779798848554e-05\n",
      "step: 13408, loss: 8.654553198539361e-07\n",
      "step: 13409, loss: 1.0514121413507382e-06\n",
      "step: 13410, loss: 8.65450033415982e-07\n",
      "step: 13411, loss: 3.5165148801752366e-06\n",
      "step: 13412, loss: 2.861022529998536e-08\n",
      "step: 13413, loss: 1.656994868426409e-06\n",
      "step: 13414, loss: 1.0371095413574949e-06\n",
      "step: 13415, loss: 8.01084240720229e-07\n",
      "step: 13416, loss: 9.512146789347753e-05\n",
      "step: 13417, loss: 1.4161906847220962e-06\n",
      "step: 13418, loss: 2.992044301208807e-06\n",
      "step: 13419, loss: 5.523866093426477e-06\n",
      "step: 13420, loss: 5.769716722170415e-07\n",
      "step: 13421, loss: 1.4138154256215785e-06\n",
      "step: 13422, loss: 2.8537583602883387e-06\n",
      "step: 13423, loss: 1.8244183593196794e-05\n",
      "step: 13424, loss: 4.894482117379084e-06\n",
      "step: 13425, loss: 1.5306277418858372e-06\n",
      "step: 13426, loss: 1.4279606148193125e-05\n",
      "step: 13427, loss: 1.580692469360656e-06\n",
      "step: 13428, loss: 1.330350187345175e-06\n",
      "step: 13429, loss: 1.9788723193414626e-07\n",
      "step: 13430, loss: 1.7881262692753808e-06\n",
      "step: 13431, loss: 2.016980715779937e-06\n",
      "step: 13432, loss: 1.2469209877963294e-06\n",
      "step: 13433, loss: 0.06433247029781342\n",
      "step: 13434, loss: 8.940645557231619e-07\n",
      "step: 13435, loss: 5.078309754935617e-07\n",
      "step: 13436, loss: 2.2411049940274097e-06\n",
      "step: 13437, loss: 9.703551313577918e-07\n",
      "step: 13438, loss: 0.07111209630966187\n",
      "step: 13439, loss: 2.0599024992407067e-06\n",
      "step: 13440, loss: 5.531297233574151e-07\n",
      "step: 13441, loss: 8.726082683097047e-07\n",
      "step: 13442, loss: 4.3795098463306203e-05\n",
      "step: 13443, loss: 0.00869324803352356\n",
      "step: 13444, loss: 6.198856112860085e-07\n",
      "step: 13445, loss: 2.7373042030376382e-05\n",
      "step: 13446, loss: 2.0956767912139185e-06\n",
      "step: 13447, loss: 6.651853823314013e-07\n",
      "step: 13448, loss: 4.3153664819328696e-07\n",
      "step: 13449, loss: 3.1948030709827435e-07\n",
      "step: 13450, loss: 1.957350832526572e-06\n",
      "step: 13451, loss: 9.63195702752273e-07\n",
      "step: 13452, loss: 5.626662300528551e-07\n",
      "step: 13453, loss: 2.2434267066273605e-06\n",
      "step: 13454, loss: 1.3089087360640406e-06\n",
      "step: 13455, loss: 3.0279113616416e-07\n",
      "step: 13456, loss: 1.120566608392437e-07\n",
      "step: 13457, loss: 2.980226554427645e-07\n",
      "step: 13458, loss: 3.3996705042227404e-06\n",
      "step: 13459, loss: 2.932542031430785e-07\n",
      "step: 13460, loss: 3.743163574654318e-07\n",
      "step: 13461, loss: 4.4107312646701757e-07\n",
      "step: 13462, loss: 5.085255452286219e-06\n",
      "step: 13463, loss: 7.033323186078633e-07\n",
      "step: 13464, loss: 1.6450763951070257e-06\n",
      "step: 13465, loss: 1.1646519851638004e-05\n",
      "step: 13466, loss: 4.880232609139057e-06\n",
      "step: 13467, loss: 2.098033291986212e-06\n",
      "step: 13468, loss: 1.9550290630832023e-07\n",
      "step: 13469, loss: 9.155197631116607e-07\n",
      "step: 13470, loss: 3.1948039236340264e-07\n",
      "step: 13471, loss: 9.345912417302316e-07\n",
      "step: 13472, loss: 2.376983957219636e-06\n",
      "step: 13473, loss: 5.868318839929998e-05\n",
      "step: 13474, loss: 5.602811370408745e-07\n",
      "step: 13475, loss: 1.4996023764979327e-06\n",
      "step: 13476, loss: 3.409377598018182e-07\n",
      "step: 13477, loss: 2.1909647784923436e-06\n",
      "step: 13478, loss: 4.577618994971999e-07\n",
      "step: 13479, loss: 2.694125953439652e-07\n",
      "step: 13480, loss: 7.057113293740258e-07\n",
      "step: 13481, loss: 3.204118002031464e-06\n",
      "step: 13482, loss: 3.528578247369296e-07\n",
      "step: 13483, loss: 3.199488219252089e-06\n",
      "step: 13484, loss: 0.03884091600775719\n",
      "step: 13485, loss: 9.533479897072539e-06\n",
      "step: 13486, loss: 3.394903615117073e-06\n",
      "step: 13487, loss: 3.218644906155532e-07\n",
      "step: 13488, loss: 9.646680155128706e-06\n",
      "step: 13489, loss: 0.000675615796353668\n",
      "step: 13490, loss: 1.516332417850208e-06\n",
      "step: 13491, loss: 0.031167082488536835\n",
      "step: 13492, loss: 6.205644240253605e-06\n",
      "step: 13493, loss: 1.7642960870034585e-07\n",
      "step: 13494, loss: 4.436734798218822e-06\n",
      "step: 13495, loss: 1.0394977607575129e-06\n",
      "step: 13496, loss: 3.4020718885585666e-05\n",
      "step: 13497, loss: 3.910054431344179e-07\n",
      "step: 13498, loss: 6.985607683418493e-07\n",
      "step: 13499, loss: 2.02655499492721e-07\n",
      "step: 13500, loss: 9.129625141213182e-06\n",
      "step: 13501, loss: 2.088463361360482e-06\n",
      "step: 13502, loss: 3.266328008066921e-07\n",
      "step: 13503, loss: 8.511509577147081e-07\n",
      "step: 13504, loss: 9.536691436551337e-07\n",
      "step: 13505, loss: 7.343240326918021e-07\n",
      "step: 13506, loss: 1.4328801398733049e-06\n",
      "step: 13507, loss: 6.723374781358871e-07\n",
      "step: 13508, loss: 1.0466530966368737e-06\n",
      "step: 13509, loss: 1.3785666851617862e-05\n",
      "step: 13510, loss: 4.005424898423371e-07\n",
      "step: 13511, loss: 3.192371423210716e-06\n",
      "step: 13512, loss: 5.054455414210679e-07\n",
      "step: 13513, loss: 1.306512558585382e-06\n",
      "step: 13514, loss: 2.0027104596920253e-07\n",
      "step: 13515, loss: 4.506100879098085e-07\n",
      "step: 13516, loss: 1.2087674576832796e-06\n",
      "step: 13517, loss: 4.220002267629752e-07\n",
      "step: 13518, loss: 3.799451587838121e-05\n",
      "step: 13519, loss: 3.4093747558472387e-07\n",
      "step: 13520, loss: 0.04639613255858421\n",
      "step: 13521, loss: 2.956385287689045e-07\n",
      "step: 13522, loss: 0.07720710337162018\n",
      "step: 13523, loss: 2.813333708218124e-07\n",
      "step: 13524, loss: 4.4822613176620507e-07\n",
      "step: 13525, loss: 1.637896161810204e-06\n",
      "step: 13526, loss: 3.0755936109017057e-07\n",
      "step: 13527, loss: 5.817386181661277e-07\n",
      "step: 13528, loss: 1.811979757349036e-07\n",
      "step: 13529, loss: 1.1658505627565319e-06\n",
      "step: 13530, loss: 2.1361388462537434e-06\n",
      "step: 13531, loss: 1.8453389429851086e-06\n",
      "step: 13532, loss: 5.865056209586328e-07\n",
      "step: 13533, loss: 3.680714871734381e-05\n",
      "step: 13534, loss: 1.0776340786833316e-05\n",
      "step: 13535, loss: 0.0015641807112842798\n",
      "step: 13536, loss: 4.696453288488556e-06\n",
      "step: 13537, loss: 1.1157873132106033e-06\n",
      "step: 13538, loss: 1.7546246454003267e-05\n",
      "step: 13539, loss: 2.264870545332087e-06\n",
      "step: 13540, loss: 9.372983186040074e-06\n",
      "step: 13541, loss: 8.548300684196874e-06\n",
      "step: 13542, loss: 2.7418093395681353e-07\n",
      "step: 13543, loss: 1.920471913763322e-05\n",
      "step: 13544, loss: 1.2492876066971803e-06\n",
      "step: 13545, loss: 8.058517551035038e-07\n",
      "step: 13546, loss: 2.2434785478253616e-06\n",
      "step: 13547, loss: 1.0228064866169007e-06\n",
      "step: 13548, loss: 1.0633353895173059e-06\n",
      "step: 13549, loss: 7.271733011293691e-07\n",
      "step: 13550, loss: 7.629390097463329e-08\n",
      "step: 13551, loss: 4.806167453352828e-06\n",
      "step: 13552, loss: 1.466256321691617e-06\n",
      "step: 13553, loss: 7.867809159733952e-08\n",
      "step: 13554, loss: 8.53534288580704e-07\n",
      "step: 13555, loss: 1.9525725747371325e-06\n",
      "step: 13556, loss: 9.202859700963018e-07\n",
      "step: 13557, loss: 5.971570317342412e-06\n",
      "step: 13558, loss: 7.013439699221635e-06\n",
      "step: 13559, loss: 0.08309424668550491\n",
      "step: 13560, loss: 6.146726860833951e-08\n",
      "step: 13561, loss: 2.317401140317088e-06\n",
      "step: 13562, loss: 1.7881379221762472e-07\n",
      "step: 13563, loss: 8.920006621337961e-06\n",
      "step: 13564, loss: 0.017815779894590378\n",
      "step: 13565, loss: 1.0132608849744429e-06\n",
      "step: 13566, loss: 7.057163884383044e-07\n",
      "step: 13567, loss: 5.399529527494451e-06\n",
      "step: 13568, loss: 3.518799985613441e-06\n",
      "step: 13569, loss: 2.7656514589580183e-07\n",
      "step: 13570, loss: 1.0704848136811052e-06\n",
      "step: 13571, loss: 4.400792022352107e-06\n",
      "step: 13572, loss: 7.626016667927615e-06\n",
      "step: 13573, loss: 2.0718259747809498e-06\n",
      "step: 13574, loss: 1.02280353075912e-06\n",
      "step: 13575, loss: 7.67703966175759e-07\n",
      "step: 13576, loss: 2.8466072308219736e-06\n",
      "step: 13577, loss: 5.912702363275457e-07\n",
      "step: 13578, loss: 8.964483413365087e-07\n",
      "step: 13579, loss: 0.0004679194826167077\n",
      "step: 13580, loss: 0.010418453253805637\n",
      "step: 13581, loss: 5.3580770327243954e-05\n",
      "step: 13582, loss: 1.0514148698348436e-06\n",
      "step: 13583, loss: 3.7193174762251147e-07\n",
      "step: 13584, loss: 9.401410352438688e-06\n",
      "step: 13585, loss: 1.723736659187125e-06\n",
      "step: 13586, loss: 0.01617293991148472\n",
      "step: 13587, loss: 5.6573217079858296e-06\n",
      "step: 13588, loss: 7.176368512773479e-07\n",
      "step: 13589, loss: 1.7169137208838947e-05\n",
      "step: 13590, loss: 2.6153993530897424e-06\n",
      "step: 13591, loss: 1.269729091291083e-05\n",
      "step: 13592, loss: 4.791907940671081e-06\n",
      "step: 13593, loss: 3.7383072140073637e-06\n",
      "step: 13594, loss: 1.2758523553202394e-05\n",
      "step: 13595, loss: 3.7181995139690116e-05\n",
      "step: 13596, loss: 3.692979134939378e-06\n",
      "step: 13597, loss: 0.0020591069478541613\n",
      "step: 13598, loss: 7.200206368906947e-07\n",
      "step: 13599, loss: 0.05019967630505562\n",
      "step: 13600, loss: 3.6976916817366146e-06\n",
      "step: 13601, loss: 3.314008552024461e-07\n",
      "step: 13602, loss: 3.004069242251717e-07\n",
      "step: 13603, loss: 0.099077969789505\n",
      "step: 13604, loss: 1.2779131566276192e-06\n",
      "step: 13605, loss: 3.5762684547080426e-07\n",
      "step: 13606, loss: 3.5642362945509376e-06\n",
      "step: 13607, loss: 4.529929071850347e-07\n",
      "step: 13608, loss: 3.0340179364429787e-05\n",
      "step: 13609, loss: 9.310584573540837e-06\n",
      "step: 13610, loss: 2.898997990996577e-06\n",
      "step: 13611, loss: 4.498621365200961e-06\n",
      "step: 13612, loss: 2.7155008410773007e-06\n",
      "step: 13613, loss: 4.541587713902118e-06\n",
      "step: 13614, loss: 3.1709620884612377e-07\n",
      "step: 13615, loss: 5.489161776495166e-05\n",
      "step: 13616, loss: 3.528587626533408e-07\n",
      "step: 13617, loss: 1.506792614236474e-06\n",
      "step: 13618, loss: 1.53774954014807e-06\n",
      "step: 13619, loss: 3.2066286621557083e-06\n",
      "step: 13620, loss: 5.7213310356019065e-06\n",
      "step: 13621, loss: 0.0013102166121825576\n",
      "step: 13622, loss: 2.4246696739282925e-06\n",
      "step: 13623, loss: 3.2542441203986527e-06\n",
      "step: 13624, loss: 0.00014549448678735644\n",
      "step: 13625, loss: 2.9824201192241162e-05\n",
      "step: 13626, loss: 0.00042079511331394315\n",
      "step: 13627, loss: 5.681027687387541e-06\n",
      "step: 13628, loss: 9.11966708372347e-05\n",
      "step: 13629, loss: 1.959722339961445e-06\n",
      "step: 13630, loss: 1.4415219084185082e-05\n",
      "step: 13631, loss: 2.0622333067876752e-06\n",
      "step: 13632, loss: 2.694123679702898e-07\n",
      "step: 13633, loss: 2.6034231268567964e-06\n",
      "step: 13634, loss: 7.414787432935555e-07\n",
      "step: 13635, loss: 1.3731229955737945e-05\n",
      "step: 13636, loss: 4.002858531748643e-06\n",
      "step: 13637, loss: 1.306524040955992e-06\n",
      "step: 13638, loss: 1.1945522601308767e-05\n",
      "step: 13639, loss: 1.6498337345183245e-06\n",
      "step: 13640, loss: 1.802431370379054e-06\n",
      "step: 13641, loss: 5.611142114503309e-05\n",
      "step: 13642, loss: 7.170691787905525e-06\n",
      "step: 13643, loss: 4.196148779556097e-07\n",
      "step: 13644, loss: 7.200201821433438e-07\n",
      "step: 13645, loss: 4.887265276920516e-06\n",
      "step: 13646, loss: 2.360340971563346e-07\n",
      "step: 13647, loss: 3.821538484771736e-06\n",
      "step: 13648, loss: 7.510162163271161e-07\n",
      "step: 13649, loss: 0.0864153578877449\n",
      "step: 13650, loss: 5.943039468547795e-06\n",
      "step: 13651, loss: 1.4495578852802282e-06\n",
      "step: 13652, loss: 7.915467676866683e-07\n",
      "step: 13653, loss: 6.656017376371892e-06\n",
      "step: 13654, loss: 8.153837711688539e-07\n",
      "step: 13655, loss: 1.2612176760740113e-06\n",
      "step: 13656, loss: 1.2874410231233924e-06\n",
      "step: 13657, loss: 3.4809008297997934e-07\n",
      "step: 13658, loss: 2.622602153223852e-07\n",
      "step: 13659, loss: 1.3089052117720712e-06\n",
      "step: 13660, loss: 5.153982328920392e-06\n",
      "step: 13661, loss: 2.438943283777917e-06\n",
      "step: 13662, loss: 5.078306912764674e-07\n",
      "step: 13663, loss: 1.7356614989694208e-06\n",
      "step: 13664, loss: 0.0001824156497605145\n",
      "step: 13665, loss: 7.390973877363649e-08\n",
      "step: 13666, loss: 1.003731654236617e-06\n",
      "step: 13667, loss: 4.649136826628819e-07\n",
      "step: 13668, loss: 5.79354775709362e-07\n",
      "step: 13669, loss: 1.9383262497285614e-06\n",
      "step: 13670, loss: 5.364400976759498e-07\n",
      "step: 13671, loss: 9.179055382446677e-07\n",
      "step: 13672, loss: 1.4281037010732689e-06\n",
      "step: 13673, loss: 2.319696704944363e-06\n",
      "step: 13674, loss: 1.0061199873234727e-06\n",
      "step: 13675, loss: 9.1313717121011e-07\n",
      "step: 13676, loss: 3.838527504740341e-07\n",
      "step: 13677, loss: 1.6450867690309678e-07\n",
      "step: 13678, loss: 5.054462803855131e-07\n",
      "step: 13679, loss: 2.233944769614027e-06\n",
      "step: 13680, loss: 5.79531115363352e-06\n",
      "step: 13681, loss: 1.7094283748519956e-06\n",
      "step: 13682, loss: 1.8500667238185997e-06\n",
      "step: 13683, loss: 6.27034523859038e-07\n",
      "step: 13684, loss: 1.8214996089227498e-06\n",
      "step: 13685, loss: 0.11154040694236755\n",
      "step: 13686, loss: 2.0408249383763177e-06\n",
      "step: 13687, loss: 6.0622073760896455e-06\n",
      "step: 13688, loss: 1.5091774230313604e-06\n",
      "step: 13689, loss: 1.6402850633312482e-06\n",
      "step: 13690, loss: 7.390941050289257e-07\n",
      "step: 13691, loss: 1.1420180499044363e-06\n",
      "step: 13692, loss: 1.780963202691055e-06\n",
      "step: 13693, loss: 1.7046439779733191e-06\n",
      "step: 13694, loss: 4.557946522254497e-05\n",
      "step: 13695, loss: 3.1637359825253952e-06\n",
      "step: 13696, loss: 2.1242856291792123e-06\n",
      "step: 13697, loss: 4.977860498911468e-06\n",
      "step: 13698, loss: 4.625303802185954e-07\n",
      "step: 13699, loss: 1.5115281257749302e-06\n",
      "step: 13700, loss: 1.7213667433679802e-06\n",
      "step: 13701, loss: 1.7475931599619798e-06\n",
      "step: 13702, loss: 1.4924729612175724e-06\n",
      "step: 13703, loss: 3.3400478969269898e-06\n",
      "step: 13704, loss: 0.0704987421631813\n",
      "step: 13705, loss: 3.9528149500256404e-06\n",
      "step: 13706, loss: 1.2206915016577113e-06\n",
      "step: 13707, loss: 5.841223469360557e-07\n",
      "step: 13708, loss: 1.3089027106616413e-06\n",
      "step: 13709, loss: 2.4376746296184137e-05\n",
      "step: 13710, loss: 9.536738332371897e-08\n",
      "step: 13711, loss: 1.2898367458547e-06\n",
      "step: 13712, loss: 3.7167076243349584e-06\n",
      "step: 13713, loss: 1.9788212739513256e-06\n",
      "step: 13714, loss: 1.1753895705624018e-06\n",
      "step: 13715, loss: 1.2397448472256656e-06\n",
      "step: 13716, loss: 1.974049155251123e-06\n",
      "step: 13717, loss: 4.7445197992601607e-07\n",
      "step: 13718, loss: 1.1401383744669147e-05\n",
      "step: 13719, loss: 1.2540533589344705e-06\n",
      "step: 13720, loss: 1.160850024461979e-05\n",
      "step: 13721, loss: 1.8477044250175823e-06\n",
      "step: 13722, loss: 3.053997943425202e-06\n",
      "step: 13723, loss: 3.6095059385843342e-06\n",
      "step: 13724, loss: 2.734587269515032e-06\n",
      "step: 13725, loss: 5.483613563228573e-07\n",
      "step: 13726, loss: 0.02255973406136036\n",
      "step: 13727, loss: 5.754721769335447e-06\n",
      "step: 13728, loss: 7.451985766238067e-06\n",
      "step: 13729, loss: 1.8405548871669453e-06\n",
      "step: 13730, loss: 4.169683052168693e-06\n",
      "step: 13731, loss: 7.867777185310842e-07\n",
      "step: 13732, loss: 1.4257292377806152e-06\n",
      "step: 13733, loss: 1.947829105120036e-06\n",
      "step: 13734, loss: 6.413438882191258e-07\n",
      "step: 13735, loss: 1.3619296623801347e-05\n",
      "step: 13736, loss: 4.763372999150306e-06\n",
      "step: 13737, loss: 7.656819798285142e-05\n",
      "step: 13738, loss: 6.0259379097260535e-05\n",
      "step: 13739, loss: 3.0683204386150464e-06\n",
      "step: 13740, loss: 2.179114972022944e-06\n",
      "step: 13741, loss: 4.7467087824770715e-06\n",
      "step: 13742, loss: 2.0217601104377536e-06\n",
      "step: 13743, loss: 5.769683752987476e-07\n",
      "step: 13744, loss: 2.0027131597544212e-07\n",
      "step: 13745, loss: 1.5592056570312707e-06\n",
      "step: 13746, loss: 6.392807699739933e-05\n",
      "step: 13747, loss: 1.8953921880893176e-06\n",
      "step: 13748, loss: 8.201517402994796e-07\n",
      "step: 13749, loss: 8.487659215461463e-07\n",
      "step: 13750, loss: 3.523578925523907e-06\n",
      "step: 13751, loss: 0.11530972272157669\n",
      "step: 13752, loss: 1.3232061064627487e-06\n",
      "step: 13753, loss: 4.922851985611487e-06\n",
      "step: 13754, loss: 8.296914870697947e-07\n",
      "step: 13755, loss: 3.933897687602439e-07\n",
      "step: 13756, loss: 9.012159125632024e-07\n",
      "step: 13757, loss: 0.00019144474936183542\n",
      "step: 13758, loss: 7.831505172362085e-06\n",
      "step: 13759, loss: 5.655009317706572e-06\n",
      "step: 13760, loss: 1.292209049097437e-06\n",
      "step: 13761, loss: 1.0299612540620728e-06\n",
      "step: 13762, loss: 1.6021550663936068e-06\n",
      "step: 13763, loss: 3.290168990588427e-07\n",
      "step: 13764, loss: 4.241321221343242e-06\n",
      "step: 13765, loss: 0.00021020091662649065\n",
      "step: 13766, loss: 1.7165730241686106e-06\n",
      "step: 13767, loss: 2.055146296697785e-06\n",
      "step: 13768, loss: 1.4924835340934806e-06\n",
      "step: 13769, loss: 5.281058474793099e-05\n",
      "step: 13770, loss: 1.4042724387763883e-06\n",
      "step: 13771, loss: 1.709444177322439e-06\n",
      "step: 13772, loss: 2.6177792733506067e-06\n",
      "step: 13773, loss: 0.016684630885720253\n",
      "step: 13774, loss: 1.797642312340031e-06\n",
      "step: 13775, loss: 0.0001778967707650736\n",
      "step: 13776, loss: 8.964490803009539e-07\n",
      "step: 13777, loss: 1.3255821613711305e-06\n",
      "step: 13778, loss: 0.008815289475023746\n",
      "step: 13779, loss: 2.6486698061489733e-06\n",
      "step: 13780, loss: 0.16013528406620026\n",
      "step: 13781, loss: 8.689392416272312e-05\n",
      "step: 13782, loss: 1.101483462662145e-06\n",
      "step: 13783, loss: 7.748475923108344e-07\n",
      "step: 13784, loss: 0.0800863653421402\n",
      "step: 13785, loss: 6.914114010214689e-07\n",
      "step: 13786, loss: 1.3160124581190757e-05\n",
      "step: 13787, loss: 2.753668468358228e-06\n",
      "step: 13788, loss: 8.346301001438405e-06\n",
      "step: 13789, loss: 1.0347271199862007e-06\n",
      "step: 13790, loss: 2.956381592866819e-07\n",
      "step: 13791, loss: 3.385540310318902e-07\n",
      "step: 13792, loss: 2.408024215583282e-07\n",
      "step: 13793, loss: 3.0420501389016863e-06\n",
      "step: 13794, loss: 5.578971240538522e-07\n",
      "step: 13795, loss: 3.898461727658287e-05\n",
      "step: 13796, loss: 1.3112844499119092e-06\n",
      "step: 13797, loss: 0.0001817620504880324\n",
      "step: 13798, loss: 1.6522154737685923e-06\n",
      "step: 13799, loss: 1.3470478279486997e-06\n",
      "step: 13800, loss: 8.201537866625586e-07\n",
      "step: 13801, loss: 4.2915141307275917e-07\n",
      "step: 13802, loss: 8.273053140328557e-07\n",
      "step: 13803, loss: 5.545250587601913e-06\n",
      "step: 13804, loss: 1.6164453882083762e-06\n",
      "step: 13805, loss: 2.848952362910495e-06\n",
      "step: 13806, loss: 4.029265312510688e-07\n",
      "step: 13807, loss: 5.412080099631567e-07\n",
      "step: 13808, loss: 2.505727934476454e-06\n",
      "step: 13809, loss: 5.650496177622699e-07\n",
      "step: 13810, loss: 1.4328818451758707e-06\n",
      "step: 13811, loss: 5.149809112481307e-07\n",
      "step: 13812, loss: 0.0004067749541718513\n",
      "step: 13813, loss: 4.512817668000935e-06\n",
      "step: 13814, loss: 7.915449486972648e-07\n",
      "step: 13815, loss: 1.3483113434631377e-05\n",
      "step: 13816, loss: 6.908484010637039e-06\n",
      "step: 13817, loss: 3.0755961688555544e-07\n",
      "step: 13818, loss: 1.3160550906832214e-06\n",
      "step: 13819, loss: 5.245194074632309e-07\n",
      "step: 13820, loss: 3.576257654458459e-07\n",
      "step: 13821, loss: 3.7431658483910724e-07\n",
      "step: 13822, loss: 2.694125953439652e-07\n",
      "step: 13823, loss: 5.1922565944551025e-06\n",
      "step: 13824, loss: 3.814685385350458e-07\n",
      "step: 13825, loss: 1.7019168808474205e-05\n",
      "step: 13826, loss: 4.0054263195088424e-07\n",
      "step: 13827, loss: 1.9311870858018665e-07\n",
      "step: 13828, loss: 2.2053252450859873e-06\n",
      "step: 13829, loss: 1.3613484952657018e-06\n",
      "step: 13830, loss: 1.7737595499056624e-06\n",
      "step: 13831, loss: 8.83450593391899e-06\n",
      "step: 13832, loss: 9.149362995231058e-06\n",
      "step: 13833, loss: 2.6349833206040785e-05\n",
      "step: 13834, loss: 1.6998986893668189e-06\n",
      "step: 13835, loss: 4.1981966205639765e-06\n",
      "step: 13836, loss: 1.2612248383447877e-06\n",
      "step: 13837, loss: 3.576277762817881e-08\n",
      "step: 13838, loss: 2.8418660349416314e-06\n",
      "step: 13839, loss: 1.2278443364266423e-06\n",
      "step: 13840, loss: 3.080185251747025e-06\n",
      "step: 13841, loss: 0.07110089063644409\n",
      "step: 13842, loss: 4.0769424458630965e-07\n",
      "step: 13843, loss: 2.455705612192105e-07\n",
      "step: 13844, loss: 1.9144348698318936e-05\n",
      "step: 13845, loss: 9.703564955998445e-07\n",
      "step: 13846, loss: 6.985623599575774e-07\n",
      "step: 13847, loss: 1.957428321475163e-05\n",
      "step: 13848, loss: 0.09217186272144318\n",
      "step: 13849, loss: 1.409039214195218e-06\n",
      "step: 13850, loss: 1.5234493275784189e-06\n",
      "step: 13851, loss: 0.0011812075972557068\n",
      "step: 13852, loss: 1.7499362456874223e-06\n",
      "step: 13853, loss: 1.5473141274924274e-06\n",
      "step: 13854, loss: 1.244537429556658e-06\n",
      "step: 13855, loss: 4.363033099252789e-07\n",
      "step: 13856, loss: 3.60010886879536e-07\n",
      "step: 13857, loss: 2.1095900592627004e-05\n",
      "step: 13858, loss: 0.1037299856543541\n",
      "step: 13859, loss: 2.1242851744318614e-06\n",
      "step: 13860, loss: 0.09238076210021973\n",
      "step: 13861, loss: 0.015132793225347996\n",
      "step: 13862, loss: 1.2707541827694513e-06\n",
      "step: 13863, loss: 1.6180218153749593e-05\n",
      "step: 13864, loss: 3.91005585242965e-07\n",
      "step: 13865, loss: 0.03678204491734505\n",
      "step: 13866, loss: 2.946656877611531e-06\n",
      "step: 13867, loss: 8.273985258711036e-06\n",
      "step: 13868, loss: 3.4093784506694647e-07\n",
      "step: 13869, loss: 7.367093530774582e-07\n",
      "step: 13870, loss: 1.4042644806977478e-06\n",
      "step: 13871, loss: 1.1682299145832076e-06\n",
      "step: 13872, loss: 0.03450958430767059\n",
      "step: 13873, loss: 1.5497187177970773e-07\n",
      "step: 13874, loss: 1.2397758553106542e-07\n",
      "step: 13875, loss: 0.11369326710700989\n",
      "step: 13876, loss: 2.1790986011183122e-06\n",
      "step: 13877, loss: 0.08695569634437561\n",
      "step: 13878, loss: 7.850382644392084e-06\n",
      "step: 13879, loss: 6.079617378418334e-07\n",
      "step: 13880, loss: 9.012128998620028e-07\n",
      "step: 13881, loss: 1.5329911775552318e-06\n",
      "step: 13882, loss: 7.748547545816109e-07\n",
      "step: 13883, loss: 1.3113016450461146e-07\n",
      "step: 13884, loss: 1.6419042367488146e-05\n",
      "step: 13885, loss: 3.8146893643897783e-07\n",
      "step: 13886, loss: 1.4781943491470884e-07\n",
      "step: 13887, loss: 0.000573033350519836\n",
      "step: 13888, loss: 6.911179298185743e-06\n",
      "step: 13889, loss: 1.7785752106647124e-06\n",
      "step: 13890, loss: 3.147120253288449e-07\n",
      "step: 13891, loss: 6.604155373679532e-07\n",
      "step: 13892, loss: 7.123575414880179e-06\n",
      "step: 13893, loss: 2.005028591156588e-06\n",
      "step: 13894, loss: 1.0551539162406698e-05\n",
      "step: 13895, loss: 3.194802786765649e-07\n",
      "step: 13896, loss: 4.563071343000047e-06\n",
      "step: 13897, loss: 1.3947292245575227e-06\n",
      "step: 13898, loss: 1.883502420696459e-07\n",
      "step: 13899, loss: 6.939931608940242e-06\n",
      "step: 13900, loss: 1.0185813152929768e-05\n",
      "step: 13901, loss: 4.0357383113587275e-05\n",
      "step: 13902, loss: 1.0776323051686632e-06\n",
      "step: 13903, loss: 6.055789185666072e-07\n",
      "step: 13904, loss: 1.9144711131957592e-06\n",
      "step: 13905, loss: 5.50744402971759e-07\n",
      "step: 13906, loss: 0.0692085325717926\n",
      "step: 13907, loss: 3.9694928091194015e-06\n",
      "step: 13908, loss: 5.633204636978917e-06\n",
      "step: 13909, loss: 0.0002374805189901963\n",
      "step: 13910, loss: 5.626662300528551e-07\n",
      "step: 13911, loss: 2.3275391868082806e-05\n",
      "step: 13912, loss: 6.119550562289078e-06\n",
      "step: 13913, loss: 1.2111509022361133e-06\n",
      "step: 13914, loss: 1.7690315416984959e-06\n",
      "step: 13915, loss: 1.3113005081777374e-07\n",
      "step: 13916, loss: 0.08321942389011383\n",
      "step: 13917, loss: 4.315365060847398e-07\n",
      "step: 13918, loss: 5.4836249319123453e-08\n",
      "step: 13919, loss: 6.937929128980613e-07\n",
      "step: 13920, loss: 1.1969539627898484e-05\n",
      "step: 13921, loss: 0.06796058267354965\n",
      "step: 13922, loss: 2.3841823804104934e-07\n",
      "step: 13923, loss: 2.813333708218124e-07\n",
      "step: 13924, loss: 2.427006165817147e-06\n",
      "step: 13925, loss: 9.441303063795203e-07\n",
      "step: 13926, loss: 4.872777481068624e-06\n",
      "step: 13927, loss: 3.862358539663546e-07\n",
      "step: 13928, loss: 2.622601300572569e-07\n",
      "step: 13929, loss: 5.602815349448065e-07\n",
      "step: 13930, loss: 1.9788731719927455e-07\n",
      "step: 13931, loss: 1.2302288041610154e-06\n",
      "step: 13932, loss: 2.679759973034379e-06\n",
      "step: 13933, loss: 4.386881755635841e-07\n",
      "step: 13934, loss: 2.977695885419962e-06\n",
      "step: 13935, loss: 1.5091533214217634e-06\n",
      "step: 13936, loss: 1.2302309642109321e-06\n",
      "step: 13937, loss: 1.0776357157737948e-06\n",
      "step: 13938, loss: 4.5776249635309796e-07\n",
      "step: 13939, loss: 0.046899449080228806\n",
      "step: 13940, loss: 1.199227540382708e-06\n",
      "step: 13941, loss: 2.0027137281886098e-07\n",
      "step: 13942, loss: 5.6073677114909515e-06\n",
      "step: 13943, loss: 5.292877176543698e-07\n",
      "step: 13944, loss: 1.0967245600568276e-07\n",
      "step: 13945, loss: 5.817378223582637e-07\n",
      "step: 13946, loss: 8.416143941758492e-07\n",
      "step: 13947, loss: 6.270380481510074e-07\n",
      "step: 13948, loss: 5.106323442305438e-06\n",
      "step: 13949, loss: 7.081009130160965e-07\n",
      "step: 13950, loss: 3.147120253288449e-07\n",
      "step: 13951, loss: 8.439937460025249e-07\n",
      "step: 13952, loss: 1.2302299410293926e-06\n",
      "step: 13953, loss: 4.722887297248235e-06\n",
      "step: 13954, loss: 4.744506156839634e-07\n",
      "step: 13955, loss: 0.07749423384666443\n",
      "step: 13956, loss: 1.077630713552935e-06\n",
      "step: 13957, loss: 6.723357159899024e-07\n",
      "step: 13958, loss: 3.2663254501130723e-07\n",
      "step: 13959, loss: 6.93795072947978e-07\n",
      "step: 13960, loss: 6.818721089985047e-07\n",
      "step: 13961, loss: 1.573561547729696e-07\n",
      "step: 13962, loss: 7.700055903114844e-06\n",
      "step: 13963, loss: 5.201663043408189e-06\n",
      "step: 13964, loss: 2.0265564160126814e-07\n",
      "step: 13965, loss: 7.176348049142689e-07\n",
      "step: 13966, loss: 7.438563898176653e-07\n",
      "step: 13967, loss: 1.1110214472864754e-06\n",
      "step: 13968, loss: 7.677041367060156e-07\n",
      "step: 13969, loss: 2.343578444197192e-06\n",
      "step: 13970, loss: 1.697389416221995e-05\n",
      "step: 13971, loss: 2.479549721101648e-07\n",
      "step: 13972, loss: 3.433218012105499e-07\n",
      "step: 13973, loss: 7.033322049210255e-07\n",
      "step: 13974, loss: 8.296932492157794e-07\n",
      "step: 13975, loss: 8.225400165429164e-07\n",
      "step: 13976, loss: 5.507449714059476e-07\n",
      "step: 13977, loss: 1.931188506887338e-07\n",
      "step: 13978, loss: 5.102140221424634e-07\n",
      "step: 13979, loss: 4.625304654837237e-07\n",
      "step: 13980, loss: 8.392270842705329e-07\n",
      "step: 13981, loss: 3.7670062624783895e-07\n",
      "step: 13982, loss: 6.580324338756327e-07\n",
      "step: 13983, loss: 2.767935256997589e-06\n",
      "step: 13984, loss: 1.2636175483748957e-07\n",
      "step: 13985, loss: 9.846569355431711e-07\n",
      "step: 13986, loss: 4.107827408006415e-06\n",
      "step: 13987, loss: 0.0514928475022316\n",
      "step: 13988, loss: 2.984889079016284e-06\n",
      "step: 13989, loss: 9.608111213310622e-07\n",
      "step: 13990, loss: 6.079645231693576e-07\n",
      "step: 13991, loss: 2.436531303828815e-06\n",
      "step: 13992, loss: 8.344554771611001e-07\n",
      "step: 13993, loss: 1.5163183206823305e-06\n",
      "step: 13994, loss: 3.337859055818626e-08\n",
      "step: 13995, loss: 1.692770297267998e-07\n",
      "step: 13996, loss: 3.5047469282289967e-07\n",
      "step: 13997, loss: 4.887561431132781e-07\n",
      "step: 13998, loss: 1.5663973726987024e-06\n",
      "step: 13999, loss: 3.518842504490749e-06\n",
      "step: 14000, loss: 1.14677914098138e-06\n",
      "step: 14001, loss: 5.125984898768365e-07\n",
      "step: 14002, loss: 1.1920916165308881e-07\n",
      "step: 14003, loss: 2.83717469073963e-07\n",
      "step: 14004, loss: 2.2363162770489e-06\n",
      "step: 14005, loss: 5.435925913843676e-07\n",
      "step: 14006, loss: 1.8763269054034026e-06\n",
      "step: 14007, loss: 1.2493026133597596e-06\n",
      "step: 14008, loss: 3.2947457384580048e-06\n",
      "step: 14009, loss: 2.741810760653607e-07\n",
      "step: 14010, loss: 5.730849352403311e-06\n",
      "step: 14011, loss: 2.162389819204691e-06\n",
      "step: 14012, loss: 0.02315567620098591\n",
      "step: 14013, loss: 1.8024127257376676e-06\n",
      "step: 14014, loss: 1.668929030529398e-07\n",
      "step: 14015, loss: 1.3184366025598138e-06\n",
      "step: 14016, loss: 6.81875235386542e-07\n",
      "step: 14017, loss: 7.033323186078633e-07\n",
      "step: 14018, loss: 1.1205663952296163e-07\n",
      "step: 14019, loss: 3.8623670661763754e-07\n",
      "step: 14020, loss: 3.60011284783468e-07\n",
      "step: 14021, loss: 8.344587740793941e-07\n",
      "step: 14022, loss: 0.0461919866502285\n",
      "step: 14023, loss: 2.310361924173776e-05\n",
      "step: 14024, loss: 1.111019969357585e-06\n",
      "step: 14025, loss: 1.9597416667238576e-06\n",
      "step: 14026, loss: 1.406668701520175e-07\n",
      "step: 14027, loss: 0.0007250562193803489\n",
      "step: 14028, loss: 0.0006156160961836576\n",
      "step: 14029, loss: 9.787109775061253e-06\n",
      "step: 14030, loss: 6.701242000417551e-06\n",
      "step: 14031, loss: 6.055797143744712e-07\n",
      "step: 14032, loss: 5.245150305199786e-07\n",
      "step: 14033, loss: 1.4781937807128998e-07\n",
      "step: 14034, loss: 3.4332202858422534e-07\n",
      "step: 14035, loss: 0.00018037859990727156\n",
      "step: 14036, loss: 7.224045361908793e-07\n",
      "step: 14037, loss: 3.9815796526454506e-07\n",
      "step: 14038, loss: 1.5687550103393733e-06\n",
      "step: 14039, loss: 2.62259675309906e-07\n",
      "step: 14040, loss: 2.694125669222558e-07\n",
      "step: 14041, loss: 2.741807918482664e-07\n",
      "step: 14042, loss: 1.311301076611926e-07\n",
      "step: 14043, loss: 4.911406108476513e-07\n",
      "step: 14044, loss: 0.00010759635915746912\n",
      "step: 14045, loss: 0.07334865629673004\n",
      "step: 14046, loss: 7.80228219809942e-06\n",
      "step: 14047, loss: 0.00020175149256829172\n",
      "step: 14048, loss: 1.1444084435652258e-07\n",
      "step: 14049, loss: 5.555133952839242e-07\n",
      "step: 14050, loss: 3.7262771002133377e-06\n",
      "step: 14051, loss: 4.761098807648523e-06\n",
      "step: 14052, loss: 3.719321739481529e-07\n",
      "step: 14053, loss: 1.1324691513436846e-06\n",
      "step: 14054, loss: 1.6689286042037565e-07\n",
      "step: 14055, loss: 5.745857833971968e-07\n",
      "step: 14056, loss: 1.9979265744041186e-06\n",
      "step: 14057, loss: 2.2697076929034665e-06\n",
      "step: 14058, loss: 5.340563689060218e-07\n",
      "step: 14059, loss: 2.932545726253011e-07\n",
      "step: 14060, loss: 2.350734803258092e-06\n",
      "step: 14061, loss: 2.3603413978889876e-07\n",
      "step: 14062, loss: 3.147120253288449e-07\n",
      "step: 14063, loss: 4.944536613038508e-06\n",
      "step: 14064, loss: 3.266326871198544e-07\n",
      "step: 14065, loss: 3.952844508603448e-06\n",
      "step: 14066, loss: 1.1920923270736239e-07\n",
      "step: 14067, loss: 1.1086415270256111e-06\n",
      "step: 14068, loss: 1.2445574611774646e-05\n",
      "step: 14069, loss: 2.5510752266200143e-07\n",
      "step: 14070, loss: 0.01623629964888096\n",
      "step: 14071, loss: 6.620942440349609e-05\n",
      "step: 14072, loss: 2.3507570858782856e-06\n",
      "step: 14073, loss: 3.204277891200036e-05\n",
      "step: 14074, loss: 4.696399628301151e-06\n",
      "step: 14075, loss: 7.2955788255058e-07\n",
      "step: 14076, loss: 0.12281599640846252\n",
      "step: 14077, loss: 2.853693786164513e-06\n",
      "step: 14078, loss: 4.839876055484638e-07\n",
      "step: 14079, loss: 1.940651372933644e-06\n",
      "step: 14080, loss: 3.266329144935298e-07\n",
      "step: 14081, loss: 7.867805607020273e-08\n",
      "step: 14082, loss: 1.6927270962696639e-06\n",
      "step: 14083, loss: 0.05478338897228241\n",
      "step: 14084, loss: 4.982392511010403e-06\n",
      "step: 14085, loss: 7.605516429975978e-07\n",
      "step: 14086, loss: 5.459771159621596e-07\n",
      "step: 14087, loss: 9.393638720212039e-07\n",
      "step: 14088, loss: 7.1043391471903306e-06\n",
      "step: 14089, loss: 2.574917061792803e-07\n",
      "step: 14090, loss: 1.6080135537777096e-05\n",
      "step: 14091, loss: 8.249185157183092e-07\n",
      "step: 14092, loss: 5.745871476392495e-07\n",
      "step: 14093, loss: 9.059869512384466e-07\n",
      "step: 14094, loss: 6.179044703458203e-06\n",
      "step: 14095, loss: 0.04264313355088234\n",
      "step: 14096, loss: 2.3674544991081348e-06\n",
      "step: 14097, loss: 1.5179514775809366e-05\n",
      "step: 14098, loss: 8.749188964429777e-06\n",
      "step: 14099, loss: 1.1181690524608712e-06\n",
      "step: 14100, loss: 1.3184362614993006e-06\n",
      "step: 14101, loss: 0.0003975643194280565\n",
      "step: 14102, loss: 3.910056705080933e-07\n",
      "step: 14103, loss: 1.1139089110656641e-05\n",
      "step: 14104, loss: 1.6595633496763185e-05\n",
      "step: 14105, loss: 9.059899497287915e-08\n",
      "step: 14106, loss: 1.4257261682359967e-06\n",
      "step: 14107, loss: 1.4792820365983061e-05\n",
      "step: 14108, loss: 0.00011797519255196676\n",
      "step: 14109, loss: 9.798974360819557e-07\n",
      "step: 14110, loss: 1.0156552434636978e-06\n",
      "step: 14111, loss: 7.980181544553488e-05\n",
      "step: 14112, loss: 2.9716464268858545e-05\n",
      "step: 14113, loss: 1.3279635595608852e-06\n",
      "step: 14114, loss: 0.10607081651687622\n",
      "step: 14115, loss: 1.5603958672727458e-05\n",
      "step: 14116, loss: 1.5234813872666564e-06\n",
      "step: 14117, loss: 3.37581368512474e-05\n",
      "step: 14118, loss: 3.995822225988377e-06\n",
      "step: 14119, loss: 4.579386586556211e-05\n",
      "step: 14120, loss: 6.34190200798912e-07\n",
      "step: 14121, loss: 9.43726445257198e-06\n",
      "step: 14122, loss: 4.052885287819663e-06\n",
      "step: 14123, loss: 3.549898337951163e-06\n",
      "step: 14124, loss: 1.621216597413877e-06\n",
      "step: 14125, loss: 8.845209436003643e-07\n",
      "step: 14126, loss: 2.846596089511877e-06\n",
      "step: 14127, loss: 1.3232119044914725e-06\n",
      "step: 14128, loss: 1.502017425991653e-06\n",
      "step: 14129, loss: 1.0967165735564777e-06\n",
      "step: 14130, loss: 4.558209184324369e-06\n",
      "step: 14131, loss: 3.623949282882677e-07\n",
      "step: 14132, loss: 2.4557030542382563e-07\n",
      "step: 14133, loss: 2.4890052827686304e-06\n",
      "step: 14134, loss: 1.95021402760176e-06\n",
      "step: 14135, loss: 2.060877341136802e-05\n",
      "step: 14136, loss: 8.644179615657777e-06\n",
      "step: 14137, loss: 1.2063879921697662e-06\n",
      "step: 14138, loss: 5.161140961718047e-06\n",
      "step: 14139, loss: 1.2826836837120936e-06\n",
      "step: 14140, loss: 2.2411315114823083e-07\n",
      "step: 14141, loss: 8.249220400102786e-07\n",
      "step: 14142, loss: 3.235191343264887e-06\n",
      "step: 14143, loss: 4.39847508459934e-06\n",
      "step: 14144, loss: 1.5997726450223126e-06\n",
      "step: 14145, loss: 1.8972679754369892e-05\n",
      "step: 14146, loss: 1.0490403212770616e-07\n",
      "step: 14147, loss: 1.451942125640926e-06\n",
      "step: 14148, loss: 1.7618867786950432e-06\n",
      "step: 14149, loss: 4.927690042677568e-06\n",
      "step: 14150, loss: 9.703486512080417e-07\n",
      "step: 14151, loss: 6.914135042279668e-08\n",
      "step: 14152, loss: 1.28745710981093e-07\n",
      "step: 14153, loss: 4.69178712592111e-06\n",
      "step: 14154, loss: 5.769696826973814e-07\n",
      "step: 14155, loss: 2.670282697181392e-07\n",
      "step: 14156, loss: 6.8727340476471e-06\n",
      "step: 14157, loss: 0.17021191120147705\n",
      "step: 14158, loss: 1.8548738580648205e-06\n",
      "step: 14159, loss: 5.197467771722586e-07\n",
      "step: 14160, loss: 6.842583388788626e-07\n",
      "step: 14161, loss: 1.1491715667943936e-06\n",
      "step: 14162, loss: 4.682246526499512e-06\n",
      "step: 14163, loss: 4.410728706716327e-07\n",
      "step: 14164, loss: 1.068103188117675e-06\n",
      "step: 14165, loss: 4.195943347440334e-06\n",
      "step: 14166, loss: 2.789493009913713e-07\n",
      "step: 14167, loss: 6.436868716264144e-06\n",
      "step: 14168, loss: 7.152485181904922e-07\n",
      "step: 14169, loss: 6.031966677255696e-07\n",
      "step: 14170, loss: 3.027913635378354e-07\n",
      "step: 14171, loss: 2.1457636023569648e-07\n",
      "step: 14172, loss: 8.535317306268553e-07\n",
      "step: 14173, loss: 1.5974033829024847e-07\n",
      "step: 14174, loss: 1.799121491785627e-05\n",
      "step: 14175, loss: 6.174998361530015e-07\n",
      "step: 14176, loss: 4.2438344394213345e-07\n",
      "step: 14177, loss: 2.312656590675033e-07\n",
      "step: 14178, loss: 0.08973512053489685\n",
      "step: 14179, loss: 4.98292706652137e-07\n",
      "step: 14180, loss: 7.033255542410188e-07\n",
      "step: 14181, loss: 1.0877916793106124e-05\n",
      "step: 14182, loss: 1.2397592854540562e-06\n",
      "step: 14183, loss: 9.155197062682419e-07\n",
      "step: 14184, loss: 3.3543581139383605e-06\n",
      "step: 14185, loss: 1.3589846048489562e-07\n",
      "step: 14186, loss: 1.0347248462494463e-06\n",
      "step: 14187, loss: 1.7427821603632765e-06\n",
      "step: 14188, loss: 3.266329713369487e-07\n",
      "step: 14189, loss: 6.103485929997987e-07\n",
      "step: 14190, loss: 2.4080244998003764e-07\n",
      "step: 14191, loss: 1.2636174062663486e-07\n",
      "step: 14192, loss: 1.1539332263055257e-06\n",
      "step: 14193, loss: 3.251826001360314e-06\n",
      "step: 14194, loss: 1.4829598512733355e-05\n",
      "step: 14195, loss: 1.2397742921166355e-07\n",
      "step: 14196, loss: 3.6477956655289745e-07\n",
      "step: 14197, loss: 4.250811343808891e-06\n",
      "step: 14198, loss: 2.3149689241108717e-06\n",
      "step: 14199, loss: 1.1276936220383504e-06\n",
      "step: 14200, loss: 5.865055641152139e-07\n",
      "step: 14201, loss: 0.04722299426794052\n",
      "step: 14202, loss: 1.3756675798504148e-06\n",
      "step: 14203, loss: 3.498409569147043e-05\n",
      "step: 14204, loss: 6.484945060947211e-07\n",
      "step: 14205, loss: 1.9788711824730854e-07\n",
      "step: 14206, loss: 2.651077011250891e-06\n",
      "step: 14207, loss: 5.650470029650023e-07\n",
      "step: 14208, loss: 3.290168422154238e-07\n",
      "step: 14209, loss: 3.910036809884332e-07\n",
      "step: 14210, loss: 2.956385003471951e-07\n",
      "step: 14211, loss: 4.315341470828571e-07\n",
      "step: 14212, loss: 7.200198979262495e-07\n",
      "step: 14213, loss: 1.420948365193908e-06\n",
      "step: 14214, loss: 7.080985255925043e-07\n",
      "step: 14215, loss: 0.06934641301631927\n",
      "step: 14216, loss: 2.455708454363048e-07\n",
      "step: 14217, loss: 1.1634712109298562e-06\n",
      "step: 14218, loss: 5.285184670356102e-06\n",
      "step: 14219, loss: 1.318441832154349e-06\n",
      "step: 14220, loss: 0.075823575258255\n",
      "step: 14221, loss: 2.431865198104788e-07\n",
      "step: 14222, loss: 8.463809990644222e-07\n",
      "step: 14223, loss: 3.073004791076528e-06\n",
      "step: 14224, loss: 3.1255456178769236e-06\n",
      "step: 14225, loss: 6.6988891376240645e-06\n",
      "step: 14226, loss: 2.9801503842463717e-06\n",
      "step: 14227, loss: 1.0251991966470086e-07\n",
      "step: 14228, loss: 2.753584340098314e-06\n",
      "step: 14229, loss: 2.901444986491697e-06\n",
      "step: 14230, loss: 1.7141901480499655e-06\n",
      "step: 14231, loss: 2.989712129419786e-06\n",
      "step: 14232, loss: 5.054457119513245e-07\n",
      "step: 14233, loss: 1.7404539676135755e-07\n",
      "step: 14234, loss: 8.821481856102764e-08\n",
      "step: 14235, loss: 1.3661181128554745e-06\n",
      "step: 14236, loss: 7.783631190250162e-06\n",
      "step: 14237, loss: 6.0554266383405775e-06\n",
      "step: 14238, loss: 8.139671194840048e-07\n",
      "step: 14239, loss: 2.1886398826609366e-06\n",
      "step: 14240, loss: 8.08234176474798e-07\n",
      "step: 14241, loss: 1.0285506505169906e-05\n",
      "step: 14242, loss: 4.935254764859565e-07\n",
      "step: 14243, loss: 4.517577053775312e-06\n",
      "step: 14244, loss: 1.8358194608936174e-07\n",
      "step: 14245, loss: 1.4686165741295554e-06\n",
      "step: 14246, loss: 2.0074369331268826e-06\n",
      "step: 14247, loss: 9.141524060396478e-06\n",
      "step: 14248, loss: 5.698182121705031e-07\n",
      "step: 14249, loss: 1.2498288015194703e-05\n",
      "step: 14250, loss: 1.516322640782164e-06\n",
      "step: 14251, loss: 5.483580025611445e-07\n",
      "step: 14252, loss: 4.961049853591248e-06\n",
      "step: 14253, loss: 7.939309512039472e-07\n",
      "step: 14254, loss: 2.8501066481112503e-05\n",
      "step: 14255, loss: 1.516321390226949e-06\n",
      "step: 14256, loss: 2.994367605424486e-06\n",
      "step: 14257, loss: 1.454350098128998e-07\n",
      "step: 14258, loss: 1.8358213083047303e-07\n",
      "step: 14259, loss: 3.218644621938438e-07\n",
      "step: 14260, loss: 9.146876982413232e-05\n",
      "step: 14261, loss: 7.772383128212823e-07\n",
      "step: 14262, loss: 4.792188974533929e-07\n",
      "step: 14263, loss: 1.2248867278685793e-05\n",
      "step: 14264, loss: 1.7738110500431503e-06\n",
      "step: 14265, loss: 0.00041984577546827495\n",
      "step: 14266, loss: 5.459766612148087e-07\n",
      "step: 14267, loss: 1.2469192824937636e-06\n",
      "step: 14268, loss: 4.5776164370181505e-07\n",
      "step: 14269, loss: 2.408020236543962e-07\n",
      "step: 14270, loss: 1.6689284620952094e-07\n",
      "step: 14271, loss: 1.3089085086903651e-06\n",
      "step: 14272, loss: 1.2015992751912563e-06\n",
      "step: 14273, loss: 1.3113003660691902e-07\n",
      "step: 14274, loss: 6.924998160684481e-06\n",
      "step: 14275, loss: 1.3827910834152135e-06\n",
      "step: 14276, loss: 9.345873195343302e-07\n",
      "step: 14277, loss: 2.7059190870204475e-06\n",
      "step: 14278, loss: 5.285276529320981e-06\n",
      "step: 14279, loss: 0.00010137935169041157\n",
      "step: 14280, loss: 0.10955732315778732\n",
      "step: 14281, loss: 4.816032515009283e-07\n",
      "step: 14282, loss: 6.175011435516353e-07\n",
      "step: 14283, loss: 8.296893838632968e-07\n",
      "step: 14284, loss: 5.769691142631928e-07\n",
      "step: 14285, loss: 1.7166115640065982e-07\n",
      "step: 14286, loss: 3.0373410027095815e-06\n",
      "step: 14287, loss: 2.2888153239364328e-07\n",
      "step: 14288, loss: 2.0384204617585056e-06\n",
      "step: 14289, loss: 6.294234253800823e-07\n",
      "step: 14290, loss: 2.429418145766249e-06\n",
      "step: 14291, loss: 2.1934472727025423e-07\n",
      "step: 14292, loss: 3.6716389217872347e-07\n",
      "step: 14293, loss: 0.018343640491366386\n",
      "step: 14294, loss: 2.9061277473374503e-06\n",
      "step: 14295, loss: 2.4557087385801424e-07\n",
      "step: 14296, loss: 2.3126577275434101e-07\n",
      "step: 14297, loss: 2.5272333914472256e-07\n",
      "step: 14298, loss: 2.145762891814229e-07\n",
      "step: 14299, loss: 2.3364981416307273e-07\n",
      "step: 14300, loss: 4.434576226231002e-07\n",
      "step: 14301, loss: 7.438526381520205e-07\n",
      "step: 14302, loss: 2.542831498431042e-05\n",
      "step: 14303, loss: 1.0037184665634413e-06\n",
      "step: 14304, loss: 1.6760575363150565e-06\n",
      "step: 14305, loss: 0.15129774808883667\n",
      "step: 14306, loss: 6.604162763323984e-07\n",
      "step: 14307, loss: 5.793545483356866e-07\n",
      "step: 14308, loss: 4.929961505695246e-06\n",
      "step: 14309, loss: 1.0490411028740709e-07\n",
      "step: 14310, loss: 9.822739457376883e-07\n",
      "step: 14311, loss: 4.477057245821925e-06\n",
      "step: 14312, loss: 9.488883279118454e-07\n",
      "step: 14313, loss: 8.583012345297902e-07\n",
      "step: 14314, loss: 0.00011305308726150542\n",
      "step: 14315, loss: 1.239775713202107e-07\n",
      "step: 14316, loss: 5.316684337230981e-07\n",
      "step: 14317, loss: 1.430509684041681e-07\n",
      "step: 14318, loss: 9.925750418915413e-06\n",
      "step: 14319, loss: 3.91005386290999e-07\n",
      "step: 14320, loss: 2.708371994231129e-06\n",
      "step: 14321, loss: 0.07306747883558273\n",
      "step: 14322, loss: 3.0040632736927364e-07\n",
      "step: 14323, loss: 2.5033867245838337e-07\n",
      "step: 14324, loss: 1.2874593835476844e-07\n",
      "step: 14325, loss: 8.34464515264699e-08\n",
      "step: 14326, loss: 0.022680217400193214\n",
      "step: 14327, loss: 0.05646193027496338\n",
      "step: 14328, loss: 8.130016908580728e-07\n",
      "step: 14329, loss: 5.1439012167975307e-05\n",
      "step: 14330, loss: 4.696834992046206e-07\n",
      "step: 14331, loss: 4.863721301262558e-07\n",
      "step: 14332, loss: 1.206368096973165e-06\n",
      "step: 14333, loss: 1.5711234482296277e-06\n",
      "step: 14334, loss: 2.8490101158240577e-06\n",
      "step: 14335, loss: 1.2588301387950196e-06\n",
      "step: 14336, loss: 3.91005386290999e-07\n",
      "step: 14337, loss: 1.4477036529569887e-05\n",
      "step: 14338, loss: 2.8371715643515927e-07\n",
      "step: 14339, loss: 1.6951100860751467e-06\n",
      "step: 14340, loss: 1.1369652384018991e-05\n",
      "step: 14341, loss: 6.914135042279668e-08\n",
      "step: 14342, loss: 4.5061040054861223e-07\n",
      "step: 14343, loss: 3.0517520599460113e-07\n",
      "step: 14344, loss: 5.125972393216216e-07\n",
      "step: 14345, loss: 1.0919377473328495e-06\n",
      "step: 14346, loss: 5.0708872549876105e-06\n",
      "step: 14347, loss: 3.910049031219387e-07\n",
      "step: 14348, loss: 4.24824065703433e-06\n",
      "step: 14349, loss: 1.0085034318763064e-06\n",
      "step: 14350, loss: 4.73448335469584e-06\n",
      "step: 14351, loss: 5.092236278869677e-06\n",
      "step: 14352, loss: 4.0531057265980053e-07\n",
      "step: 14353, loss: 1.3708668120671064e-06\n",
      "step: 14354, loss: 1.5258767405157414e-07\n",
      "step: 14355, loss: 1.034721321957477e-06\n",
      "step: 14356, loss: 1.2516678680185578e-06\n",
      "step: 14357, loss: 8.91679974301951e-07\n",
      "step: 14358, loss: 2.83717668025929e-07\n",
      "step: 14359, loss: 5.102137379253691e-07\n",
      "step: 14360, loss: 2.20057131627982e-06\n",
      "step: 14361, loss: 1.118158934332314e-06\n",
      "step: 14362, loss: 4.5061042897032166e-07\n",
      "step: 14363, loss: 3.576254812287516e-07\n",
      "step: 14364, loss: 7.390943324026011e-07\n",
      "step: 14365, loss: 3.3730659197317436e-05\n",
      "step: 14366, loss: 0.12942363321781158\n",
      "step: 14367, loss: 8.273053140328557e-07\n",
      "step: 14368, loss: 3.68804262507183e-06\n",
      "step: 14369, loss: 1.3589840364147676e-07\n",
      "step: 14370, loss: 0.08884069323539734\n",
      "step: 14371, loss: 4.529951169729429e-08\n",
      "step: 14372, loss: 4.148470225118217e-07\n",
      "step: 14373, loss: 1.5029406313260552e-05\n",
      "step: 14374, loss: 6.771031735297584e-07\n",
      "step: 14375, loss: 7.104833912308095e-07\n",
      "step: 14376, loss: 4.4107378016633447e-07\n",
      "step: 14377, loss: 1.0633350484567927e-06\n",
      "step: 14378, loss: 4.141068075114163e-06\n",
      "step: 14379, loss: 4.959090347256279e-07\n",
      "step: 14380, loss: 3.98156771552749e-07\n",
      "step: 14381, loss: 5.984258564240008e-07\n",
      "step: 14382, loss: 2.2411309430481197e-07\n",
      "step: 14383, loss: 1.877180693554692e-05\n",
      "step: 14384, loss: 7.343263064285566e-07\n",
      "step: 14385, loss: 3.693547478178516e-05\n",
      "step: 14386, loss: 4.017086666863179e-06\n",
      "step: 14387, loss: 1.1539382285263855e-06\n",
      "step: 14388, loss: 0.03246886283159256\n",
      "step: 14389, loss: 0.001312102423980832\n",
      "step: 14390, loss: 1.3327511396710179e-06\n",
      "step: 14391, loss: 1.4666567039967049e-05\n",
      "step: 14392, loss: 1.1602161976043135e-05\n",
      "step: 14393, loss: 1.0728824406669446e-07\n",
      "step: 14394, loss: 2.1457643128997006e-07\n",
      "step: 14395, loss: 5.245184979685291e-07\n",
      "step: 14396, loss: 1.5735616898382432e-07\n",
      "step: 14397, loss: 2.1219221935098176e-07\n",
      "step: 14398, loss: 1.7166125587664283e-07\n",
      "step: 14399, loss: 7.03329590123758e-07\n",
      "step: 14400, loss: 7.867810580819423e-08\n",
      "step: 14401, loss: 3.623958662046789e-07\n",
      "step: 14402, loss: 5.149826733941154e-07\n",
      "step: 14403, loss: 0.09439751505851746\n",
      "step: 14404, loss: 4.672991167353757e-07\n",
      "step: 14405, loss: 5.922320269746706e-05\n",
      "step: 14406, loss: 3.671640342872706e-07\n",
      "step: 14407, loss: 5.841231995873386e-07\n",
      "step: 14408, loss: 2.102806092807441e-06\n",
      "step: 14409, loss: 7.843912044336321e-07\n",
      "step: 14410, loss: 3.1471205375055433e-07\n",
      "step: 14411, loss: 8.821485408816443e-08\n",
      "step: 14412, loss: 2.908700764692185e-07\n",
      "step: 14413, loss: 3.409380724406219e-07\n",
      "step: 14414, loss: 3.657125716927112e-06\n",
      "step: 14415, loss: 6.9177754085103516e-06\n",
      "step: 14416, loss: 2.469972514518304e-06\n",
      "step: 14417, loss: 4.29153317327291e-08\n",
      "step: 14418, loss: 6.556472271768143e-07\n",
      "step: 14419, loss: 3.218642063984589e-07\n",
      "step: 14420, loss: 5.12597466695297e-07\n",
      "step: 14421, loss: 8.916699130168126e-07\n",
      "step: 14422, loss: 2.0027117386689497e-07\n",
      "step: 14423, loss: 1.3309081623447128e-05\n",
      "step: 14424, loss: 0.12281171977519989\n",
      "step: 14425, loss: 1.2469100738599082e-06\n",
      "step: 14426, loss: 4.219998572807526e-07\n",
      "step: 14427, loss: 3.2446582736156415e-06\n",
      "step: 14428, loss: 8.05850845608802e-07\n",
      "step: 14429, loss: 3.149410076730419e-06\n",
      "step: 14430, loss: 0.07912730425596237\n",
      "step: 14431, loss: 4.1484744883746316e-07\n",
      "step: 14432, loss: 1.3208241398388054e-06\n",
      "step: 14433, loss: 0.06964720040559769\n",
      "step: 14434, loss: 2.140940750905429e-06\n",
      "step: 14435, loss: 1.2206930932734394e-06\n",
      "step: 14436, loss: 2.9657362574653234e-06\n",
      "step: 14437, loss: 6.908300747454632e-06\n",
      "step: 14438, loss: 9.465132961850031e-07\n",
      "step: 14439, loss: 2.0265525790819083e-07\n",
      "step: 14440, loss: 2.0027113123433082e-07\n",
      "step: 14441, loss: 7.247876965266187e-07\n",
      "step: 14442, loss: 2.2411268219002523e-07\n",
      "step: 14443, loss: 7.438609372911742e-07\n",
      "step: 14444, loss: 1.740696097840555e-05\n",
      "step: 14445, loss: 3.051752912597294e-07\n",
      "step: 14446, loss: 2.241132079916497e-07\n",
      "step: 14447, loss: 3.3140125310637814e-07\n",
      "step: 14448, loss: 1.2606428754224908e-05\n",
      "step: 14449, loss: 2.8942290555278305e-06\n",
      "step: 14450, loss: 3.4808988402801333e-07\n",
      "step: 14451, loss: 7.843891012271342e-07\n",
      "step: 14452, loss: 8.082326985459076e-07\n",
      "step: 14453, loss: 2.0431780285434797e-06\n",
      "step: 14454, loss: 6.651861781392654e-07\n",
      "step: 14455, loss: 2.338785634492524e-06\n",
      "step: 14456, loss: 3.8862148699081445e-07\n",
      "step: 14457, loss: 1.0108871038028155e-06\n",
      "step: 14458, loss: 6.127336860117794e-07\n",
      "step: 14459, loss: 3.910046189048444e-07\n",
      "step: 14460, loss: 6.556481366715161e-07\n",
      "step: 14461, loss: 1.5579913451801986e-05\n",
      "step: 14462, loss: 8.721934136701748e-06\n",
      "step: 14463, loss: 1.5330117093981244e-05\n",
      "step: 14464, loss: 5.513384735422733e-07\n",
      "step: 14465, loss: 1.7642942395923455e-07\n",
      "step: 14466, loss: 6.253095762076555e-06\n",
      "step: 14467, loss: 1.1658589755825233e-06\n",
      "step: 14468, loss: 1.1443956964285462e-06\n",
      "step: 14469, loss: 7.370876119239256e-06\n",
      "step: 14470, loss: 1.2349889857432572e-06\n",
      "step: 14471, loss: 9.298307190874766e-08\n",
      "step: 14472, loss: 3.4809028193194536e-07\n",
      "step: 14473, loss: 3.189916924384306e-06\n",
      "step: 14474, loss: 2.260104338347446e-06\n",
      "step: 14475, loss: 6.484957566499361e-07\n",
      "step: 14476, loss: 5.244562544248765e-06\n",
      "step: 14477, loss: 1.8166746258430067e-06\n",
      "step: 14478, loss: 7.904765880084597e-06\n",
      "step: 14479, loss: 1.411415155416762e-06\n",
      "step: 14480, loss: 8.841417729854584e-05\n",
      "step: 14481, loss: 1.091950593945512e-06\n",
      "step: 14482, loss: 2.837177532910573e-07\n",
      "step: 14483, loss: 2.837170995917404e-07\n",
      "step: 14484, loss: 8.348457413376309e-06\n",
      "step: 14485, loss: 4.935225774715946e-07\n",
      "step: 14486, loss: 3.816839125647675e-06\n",
      "step: 14487, loss: 2.956383582386479e-07\n",
      "step: 14488, loss: 1.1682436706905719e-06\n",
      "step: 14489, loss: 3.683447630464798e-06\n",
      "step: 14490, loss: 1.3314082025317475e-05\n",
      "step: 14491, loss: 3.814696825088504e-08\n",
      "step: 14492, loss: 8.940642146626487e-07\n",
      "step: 14493, loss: 4.959080683875072e-07\n",
      "step: 14494, loss: 2.7656514589580183e-07\n",
      "step: 14495, loss: 0.0006763038109056652\n",
      "step: 14496, loss: 2.3603402610206103e-07\n",
      "step: 14497, loss: 1.5805946532054804e-05\n",
      "step: 14498, loss: 0.11093282699584961\n",
      "step: 14499, loss: 8.13001236110722e-07\n",
      "step: 14500, loss: 1.1086369795521023e-06\n",
      "step: 14501, loss: 1.2789527318091132e-05\n",
      "step: 14502, loss: 1.9764711396419443e-06\n",
      "step: 14503, loss: 1.4757504686713219e-05\n",
      "step: 14504, loss: 1.3565909284807276e-06\n",
      "step: 14505, loss: 2.1886089598410763e-06\n",
      "step: 14506, loss: 5.287639214657247e-05\n",
      "step: 14507, loss: 2.064675527435611e-06\n",
      "step: 14508, loss: 8.021222129173111e-06\n",
      "step: 14509, loss: 2.861017662780796e-07\n",
      "step: 14510, loss: 8.821476171760878e-08\n",
      "step: 14511, loss: 3.290168422154238e-07\n",
      "step: 14512, loss: 1.4543094266628032e-06\n",
      "step: 14513, loss: 1.914476115416619e-06\n",
      "step: 14514, loss: 2.3364548269455554e-06\n",
      "step: 14515, loss: 3.457064678968891e-07\n",
      "step: 14516, loss: 2.765648332569981e-07\n",
      "step: 14517, loss: 6.4372630959042e-07\n",
      "step: 14518, loss: 9.158025932265446e-06\n",
      "step: 14519, loss: 1.0717439181462396e-05\n",
      "step: 14520, loss: 3.4259473977726884e-06\n",
      "step: 14521, loss: 8.633835022919811e-06\n",
      "step: 14522, loss: 1.0516755537537392e-05\n",
      "step: 14523, loss: 7.080985255925043e-07\n",
      "step: 14524, loss: 4.549261211650446e-05\n",
      "step: 14525, loss: 5.841226879965689e-07\n",
      "step: 14526, loss: 1.2397758553106542e-07\n",
      "step: 14527, loss: 0.0028160151559859514\n",
      "step: 14528, loss: 8.67830522111035e-07\n",
      "step: 14529, loss: 1.7404372556484304e-06\n",
      "step: 14530, loss: 1.280286710425571e-06\n",
      "step: 14531, loss: 2.4318649138876935e-07\n",
      "step: 14532, loss: 2.1790997379866894e-06\n",
      "step: 14533, loss: 2.2053104657970835e-06\n",
      "step: 14534, loss: 5.388219506130554e-07\n",
      "step: 14535, loss: 0.11722006648778915\n",
      "step: 14536, loss: 2.7418067816142866e-07\n",
      "step: 14537, loss: 1.4757920325791929e-06\n",
      "step: 14538, loss: 2.1457651655509835e-07\n",
      "step: 14539, loss: 5.106564003654057e-06\n",
      "step: 14540, loss: 2.956385003471951e-07\n",
      "step: 14541, loss: 1.4138116739559337e-06\n",
      "step: 14542, loss: 1.2755251646012766e-06\n",
      "step: 14543, loss: 2.300703727087239e-06\n",
      "step: 14544, loss: 5.936586262578203e-07\n",
      "step: 14545, loss: 2.9087033226460335e-07\n",
      "step: 14546, loss: 3.2446655495732557e-06\n",
      "step: 14547, loss: 1.113411030928546e-06\n",
      "step: 14548, loss: 7.057121820253087e-07\n",
      "step: 14549, loss: 1.4066679909774393e-07\n",
      "step: 14550, loss: 5.412091468315339e-07\n",
      "step: 14551, loss: 0.0011914432980120182\n",
      "step: 14552, loss: 5.221345418249257e-07\n",
      "step: 14553, loss: 2.655821162989014e-06\n",
      "step: 14554, loss: 1.4543522297572054e-07\n",
      "step: 14555, loss: 5.960421844974917e-07\n",
      "step: 14556, loss: 2.8133339924352185e-07\n",
      "step: 14557, loss: 4.9086520448327065e-06\n",
      "step: 14558, loss: 0.000582893961109221\n",
      "step: 14559, loss: 1.2540737088784226e-06\n",
      "step: 14560, loss: 0.06612223386764526\n",
      "step: 14561, loss: 8.312848876812495e-06\n",
      "step: 14562, loss: 8.320655524585163e-07\n",
      "step: 14563, loss: 7.96314338913362e-07\n",
      "step: 14564, loss: 0.17052403092384338\n",
      "step: 14565, loss: 2.717968641263724e-07\n",
      "step: 14566, loss: 1.8667591348275892e-06\n",
      "step: 14567, loss: 1.668928177878115e-07\n",
      "step: 14568, loss: 0.017258353531360626\n",
      "step: 14569, loss: 1.5258768826242886e-07\n",
      "step: 14570, loss: 3.51649873664428e-06\n",
      "step: 14571, loss: 6.127335723249416e-07\n",
      "step: 14572, loss: 1.3327203305379953e-06\n",
      "step: 14573, loss: 8.606784831499681e-07\n",
      "step: 14574, loss: 9.255828445020597e-06\n",
      "step: 14575, loss: 2.884858929519396e-07\n",
      "step: 14576, loss: 1.4447994090005523e-06\n",
      "step: 14577, loss: 1.0743478924268857e-05\n",
      "step: 14578, loss: 7.96309052475408e-07\n",
      "step: 14579, loss: 4.1790601244429126e-05\n",
      "step: 14580, loss: 1.1396252830309095e-06\n",
      "step: 14581, loss: 7.747868039587047e-06\n",
      "step: 14582, loss: 1.6092651549115544e-06\n",
      "step: 14583, loss: 5.745857833971968e-07\n",
      "step: 14584, loss: 3.2186403586820234e-07\n",
      "step: 14585, loss: 6.67571669055178e-08\n",
      "step: 14586, loss: 5.03060164191993e-07\n",
      "step: 14587, loss: 9.536581160318747e-07\n",
      "step: 14588, loss: 0.09042193740606308\n",
      "step: 14589, loss: 5.0067875179138355e-08\n",
      "step: 14590, loss: 9.298317138473067e-08\n",
      "step: 14591, loss: 9.10748951810092e-07\n",
      "step: 14592, loss: 7.629390097463329e-08\n",
      "step: 14593, loss: 1.335143338110356e-07\n",
      "step: 14594, loss: 0.00029765532235614955\n",
      "step: 14595, loss: 0.02078665979206562\n",
      "step: 14596, loss: 6.96180052273121e-07\n",
      "step: 14597, loss: 1.8691732748266077e-06\n",
      "step: 14598, loss: 3.5524291774891026e-07\n",
      "step: 14599, loss: 1.7404526886366511e-07\n",
      "step: 14600, loss: 7.343234074141947e-07\n",
      "step: 14601, loss: 2.1934494043307495e-07\n",
      "step: 14602, loss: 2.8371744065225357e-07\n",
      "step: 14603, loss: 1.9502026589179877e-06\n",
      "step: 14604, loss: 5.118455646879738e-06\n",
      "step: 14605, loss: 3.767001146570692e-07\n",
      "step: 14606, loss: 2.5151810405077413e-05\n",
      "step: 14607, loss: 1.0084979749080958e-06\n",
      "step: 14608, loss: 3.9815861896386195e-07\n",
      "step: 14609, loss: 1.275521753996145e-06\n",
      "step: 14610, loss: 2.4913788365665823e-05\n",
      "step: 14611, loss: 1.9311883647787909e-07\n",
      "step: 14612, loss: 6.651808348578925e-07\n",
      "step: 14613, loss: 6.198881408181478e-08\n",
      "step: 14614, loss: 5.531299507310905e-07\n",
      "step: 14615, loss: 4.911412361252587e-07\n",
      "step: 14616, loss: 0.07681278884410858\n",
      "step: 14617, loss: 4.291515267595969e-07\n",
      "step: 14618, loss: 7.740132787148468e-06\n",
      "step: 14619, loss: 8.916774731915211e-07\n",
      "step: 14620, loss: 2.8275735530769452e-06\n",
      "step: 14621, loss: 2.2888077921834338e-07\n",
      "step: 14622, loss: 2.1884683519601822e-05\n",
      "step: 14623, loss: 1.0919507076323498e-06\n",
      "step: 14624, loss: 1.7881216081150342e-06\n",
      "step: 14625, loss: 0.08896210789680481\n",
      "step: 14626, loss: 1.9788723193414626e-07\n",
      "step: 14627, loss: 3.185081368428655e-06\n",
      "step: 14628, loss: 1.3255776138976216e-06\n",
      "step: 14629, loss: 1.716611279789504e-07\n",
      "step: 14630, loss: 2.198690526711289e-05\n",
      "step: 14631, loss: 3.886216290993616e-07\n",
      "step: 14632, loss: 9.775155263014312e-08\n",
      "step: 14633, loss: 4.148475909460103e-07\n",
      "step: 14634, loss: 5.769715016867849e-07\n",
      "step: 14635, loss: 4.10079252333162e-07\n",
      "step: 14636, loss: 5.912735332458396e-07\n",
      "step: 14637, loss: 1.0776411727420054e-06\n",
      "step: 14638, loss: 1.3080908502161037e-05\n",
      "step: 14639, loss: 1.6975016023934586e-06\n",
      "step: 14640, loss: 1.4328794577522785e-06\n",
      "step: 14641, loss: 1.592609464751149e-06\n",
      "step: 14642, loss: 1.4948640227885335e-06\n",
      "step: 14643, loss: 1.8786618056765292e-06\n",
      "step: 14644, loss: 3.6954676829736854e-07\n",
      "step: 14645, loss: 7.390945597762766e-07\n",
      "step: 14646, loss: 3.270883553341264e-06\n",
      "step: 14647, loss: 5.960442308605707e-07\n",
      "step: 14648, loss: 2.632044697747915e-06\n",
      "step: 14649, loss: 3.8862123119542957e-07\n",
      "step: 14650, loss: 1.0013541213993449e-06\n",
      "step: 14651, loss: 6.675717401094516e-08\n",
      "step: 14652, loss: 5.411638994701207e-06\n",
      "step: 14653, loss: 8.392276527047215e-07\n",
      "step: 14654, loss: 1.2397758553106542e-07\n",
      "step: 14655, loss: 2.9325397576940304e-07\n",
      "step: 14656, loss: 2.2625523342867382e-05\n",
      "step: 14657, loss: 6.771032872165961e-07\n",
      "step: 14658, loss: 1.883505262867402e-07\n",
      "step: 14659, loss: 1.740453399179387e-07\n",
      "step: 14660, loss: 2.145764597116795e-07\n",
      "step: 14661, loss: 2.241131369373761e-07\n",
      "step: 14662, loss: 1.2874596677647787e-07\n",
      "step: 14663, loss: 4.336575329944026e-06\n",
      "step: 14664, loss: 1.9024969333258923e-06\n",
      "step: 14665, loss: 3.0040698106859054e-07\n",
      "step: 14666, loss: 1.0371065854997141e-06\n",
      "step: 14667, loss: 4.672988893617003e-07\n",
      "step: 14668, loss: 1.318424665441853e-06\n",
      "step: 14669, loss: 5.459758085635258e-07\n",
      "step: 14670, loss: 0.0815834254026413\n",
      "step: 14671, loss: 2.9802251333421736e-07\n",
      "step: 14672, loss: 2.5510763634883915e-07\n",
      "step: 14673, loss: 1.0609493301672046e-06\n",
      "step: 14674, loss: 1.0967248442739219e-07\n",
      "step: 14675, loss: 1.029945678965305e-06\n",
      "step: 14676, loss: 1.2874592414391373e-07\n",
      "step: 14677, loss: 0.05129515007138252\n",
      "step: 14678, loss: 1.9788720351243683e-07\n",
      "step: 14679, loss: 2.0027131597544212e-07\n",
      "step: 14680, loss: 1.2177276403235737e-05\n",
      "step: 14681, loss: 9.441303063795203e-07\n",
      "step: 14682, loss: 2.868034016501042e-06\n",
      "step: 14683, loss: 5.459747853819863e-07\n",
      "step: 14684, loss: 1.9502106169966282e-06\n",
      "step: 14685, loss: 3.457061836797948e-07\n",
      "step: 14686, loss: 0.00011503641144372523\n",
      "step: 14687, loss: 2.861016810129513e-07\n",
      "step: 14688, loss: 1.8572143289929954e-06\n",
      "step: 14689, loss: 1.740453399179387e-07\n",
      "step: 14690, loss: 1.2069583590346156e-06\n",
      "step: 14691, loss: 8.010740657482529e-07\n",
      "step: 14692, loss: 6.842570883236476e-07\n",
      "step: 14693, loss: 2.4484925233991817e-06\n",
      "step: 14694, loss: 3.2186341059059487e-07\n",
      "step: 14695, loss: 1.0728827959383125e-07\n",
      "step: 14696, loss: 2.646439440923132e-07\n",
      "step: 14697, loss: 1.9549929675122257e-06\n",
      "step: 14698, loss: 1.1202588211745024e-05\n",
      "step: 14699, loss: 1.0895632840401959e-06\n",
      "step: 14700, loss: 2.884856939999736e-07\n",
      "step: 14701, loss: 9.608094160284963e-07\n",
      "step: 14702, loss: 1.955029631517391e-07\n",
      "step: 14703, loss: 7.057166726553987e-07\n",
      "step: 14704, loss: 1.8119791889148473e-07\n",
      "step: 14705, loss: 7.963130315147282e-07\n",
      "step: 14706, loss: 4.18882109443075e-06\n",
      "step: 14707, loss: 4.529951525000797e-08\n",
      "step: 14708, loss: 2.6725474526756443e-06\n",
      "step: 14709, loss: 2.851326826203149e-06\n",
      "step: 14710, loss: 1.7857123566500377e-06\n",
      "step: 14711, loss: 3.552427472186537e-07\n",
      "step: 14712, loss: 4.1246173054787505e-07\n",
      "step: 14713, loss: 7.176344638537557e-07\n",
      "step: 14714, loss: 1.2945946537001873e-06\n",
      "step: 14715, loss: 1.5020363264284242e-07\n",
      "step: 14716, loss: 1.133000750996871e-05\n",
      "step: 14717, loss: 1.0418735882922192e-06\n",
      "step: 14718, loss: 2.047995621978771e-06\n",
      "step: 14719, loss: 4.768354813222686e-07\n",
      "step: 14720, loss: 4.148476477894292e-07\n",
      "step: 14721, loss: 7.533956249972107e-07\n",
      "step: 14722, loss: 1.2159337359207711e-07\n",
      "step: 14723, loss: 9.07544199435506e-06\n",
      "step: 14724, loss: 0.12416260689496994\n",
      "step: 14725, loss: 2.717969493915007e-07\n",
      "step: 14726, loss: 7.653134730389866e-07\n",
      "step: 14727, loss: 1.2302302820899058e-06\n",
      "step: 14728, loss: 2.7179655148756865e-07\n",
      "step: 14729, loss: 0.0003656158223748207\n",
      "step: 14730, loss: 4.62528902289705e-07\n",
      "step: 14731, loss: 2.717965230658592e-07\n",
      "step: 14732, loss: 1.6426600950580905e-06\n",
      "step: 14733, loss: 4.792199774783512e-07\n",
      "step: 14734, loss: 1.6927685919654323e-07\n",
      "step: 14735, loss: 6.222696811164496e-07\n",
      "step: 14736, loss: 3.0517514915118227e-07\n",
      "step: 14737, loss: 8.559127309126779e-07\n",
      "step: 14738, loss: 6.508802243843093e-07\n",
      "step: 14739, loss: 2.503391840491531e-07\n",
      "step: 14740, loss: 8.821329515740217e-07\n",
      "step: 14741, loss: 1.5806784858796163e-06\n",
      "step: 14742, loss: 9.298319270101274e-08\n",
      "step: 14743, loss: 2.1219238988123834e-07\n",
      "step: 14744, loss: 5.149825597072777e-07\n",
      "step: 14745, loss: 6.05581021773105e-07\n",
      "step: 14746, loss: 1.1920909059881524e-07\n",
      "step: 14747, loss: 4.2199943095511117e-07\n",
      "step: 14748, loss: 7.986910190993513e-07\n",
      "step: 14749, loss: 1.239775713202107e-07\n",
      "step: 14750, loss: 4.1723109234226285e-07\n",
      "step: 14751, loss: 6.675717401094516e-08\n",
      "step: 14752, loss: 1.621243939098349e-07\n",
      "step: 14753, loss: 9.584341569279786e-07\n",
      "step: 14754, loss: 6.365741000990965e-07\n",
      "step: 14755, loss: 8.106228222004574e-08\n",
      "step: 14756, loss: 2.479548868450365e-07\n",
      "step: 14757, loss: 1.0657163329597097e-06\n",
      "step: 14758, loss: 2.50339240892572e-07\n",
      "step: 14759, loss: 1.2159333095951297e-07\n",
      "step: 14760, loss: 6.246542625376605e-07\n",
      "step: 14761, loss: 0.00039758175262250006\n",
      "step: 14762, loss: 2.577218310761964e-06\n",
      "step: 14763, loss: 4.696818223237642e-07\n",
      "step: 14764, loss: 3.0279102247732226e-07\n",
      "step: 14765, loss: 1.2159208608863992e-06\n",
      "step: 14766, loss: 4.434576510448096e-07\n",
      "step: 14767, loss: 0.03245346620678902\n",
      "step: 14768, loss: 0.006851790938526392\n",
      "step: 14769, loss: 0.07030905038118362\n",
      "step: 14770, loss: 8.344582056452055e-07\n",
      "step: 14771, loss: 5.936592515354278e-07\n",
      "step: 14772, loss: 2.2515669115819037e-05\n",
      "step: 14773, loss: 3.933902235075948e-07\n",
      "step: 14774, loss: 3.409377029583993e-07\n",
      "step: 14775, loss: 2.3817283363314345e-06\n",
      "step: 14776, loss: 6.198879987096007e-08\n",
      "step: 14777, loss: 1.549719286231266e-07\n",
      "step: 14778, loss: 1.549719286231266e-07\n",
      "step: 14779, loss: 8.106229643090046e-08\n",
      "step: 14780, loss: 4.980071480531478e-06\n",
      "step: 14781, loss: 8.249220400102786e-07\n",
      "step: 14782, loss: 5.292882860885584e-07\n",
      "step: 14783, loss: 2.098079505685746e-07\n",
      "step: 14784, loss: 0.052580833435058594\n",
      "step: 14785, loss: 4.0530994738219306e-07\n",
      "step: 14786, loss: 2.7179663675269694e-07\n",
      "step: 14787, loss: 3.218644906155532e-07\n",
      "step: 14788, loss: 5.53128927549551e-07\n",
      "step: 14789, loss: 1.0657169013938983e-06\n",
      "step: 14790, loss: 6.771073230993352e-07\n",
      "step: 14791, loss: 4.14847306728916e-07\n",
      "step: 14792, loss: 1.6021555211409577e-06\n",
      "step: 14793, loss: 2.694126521873841e-07\n",
      "step: 14794, loss: 6.515059340017615e-06\n",
      "step: 14795, loss: 2.0503968300999986e-07\n",
      "step: 14796, loss: 1.072883222263954e-07\n",
      "step: 14797, loss: 2.1552707494265633e-06\n",
      "step: 14798, loss: 9.870442454484873e-07\n",
      "step: 14799, loss: 1.1157752624058048e-06\n",
      "step: 14800, loss: 6.198877144925063e-08\n",
      "step: 14801, loss: 0.13696493208408356\n",
      "step: 14802, loss: 6.604132636311988e-07\n",
      "step: 14803, loss: 3.623955819875846e-07\n",
      "step: 14804, loss: 4.95908977882209e-07\n",
      "step: 14805, loss: 9.20288300676475e-07\n",
      "step: 14806, loss: 6.675675194856012e-07\n",
      "step: 14807, loss: 0.07757552713155746\n",
      "step: 14808, loss: 1.859659306546746e-07\n",
      "step: 14809, loss: 5.936555567132018e-07\n",
      "step: 14810, loss: 1.5830780739634065e-06\n",
      "step: 14811, loss: 2.2411292377455538e-07\n",
      "step: 14812, loss: 2.193447130593995e-07\n",
      "step: 14813, loss: 4.2438352920726174e-07\n",
      "step: 14814, loss: 7.605505629726395e-07\n",
      "step: 14815, loss: 8.941774467530195e-06\n",
      "step: 14816, loss: 2.0932443476340268e-06\n",
      "step: 14817, loss: 1.573561405621149e-07\n",
      "step: 14818, loss: 3.2663265869814495e-07\n",
      "step: 14819, loss: 2.527231117710471e-07\n",
      "step: 14820, loss: 2.0262783436919563e-05\n",
      "step: 14821, loss: 4.100794228634186e-07\n",
      "step: 14822, loss: 3.170960667375766e-07\n",
      "step: 14823, loss: 3.1709620884612377e-07\n",
      "step: 14824, loss: 0.031889740377664566\n",
      "step: 14825, loss: 9.632004775994574e-07\n",
      "step: 14826, loss: 1.5282389540516306e-06\n",
      "step: 14827, loss: 3.5524186614566133e-07\n",
      "step: 14828, loss: 7.980863301781937e-06\n",
      "step: 14829, loss: 4.172316039330326e-07\n",
      "step: 14830, loss: 1.573561405621149e-07\n",
      "step: 14831, loss: 1.692769160399621e-07\n",
      "step: 14832, loss: 5.698190079783672e-07\n",
      "step: 14833, loss: 1.6045112261053873e-06\n",
      "step: 14834, loss: 5.483626352997817e-08\n",
      "step: 14835, loss: 3.2663189131199033e-07\n",
      "step: 14836, loss: 9.298309322502973e-08\n",
      "step: 14837, loss: 1.0566830496827606e-05\n",
      "step: 14838, loss: 7.963110419950681e-07\n",
      "step: 14839, loss: 1.3289993148646317e-05\n",
      "step: 14840, loss: 2.956381592866819e-07\n",
      "step: 14841, loss: 2.646442567311169e-07\n",
      "step: 14842, loss: 9.775155973557048e-08\n",
      "step: 14843, loss: 1.382826582130292e-07\n",
      "step: 14844, loss: 5.650477987728664e-07\n",
      "step: 14845, loss: 8.77369075169554e-07\n",
      "step: 14846, loss: 5.364398134588555e-07\n",
      "step: 14847, loss: 4.982925929652993e-07\n",
      "step: 14848, loss: 5.030620400248154e-07\n",
      "step: 14849, loss: 2.360340829454799e-07\n",
      "step: 14850, loss: 5.483625642455081e-08\n",
      "step: 14851, loss: 9.960494207916781e-05\n",
      "step: 14852, loss: 0.08265157043933868\n",
      "step: 14853, loss: 5.745112048316514e-06\n",
      "step: 14854, loss: 1.0251932280880283e-06\n",
      "step: 14855, loss: 1.4066689857372694e-07\n",
      "step: 14856, loss: 1.978870756147444e-07\n",
      "step: 14857, loss: 2.837169290614838e-07\n",
      "step: 14858, loss: 1.0228089877273305e-06\n",
      "step: 14859, loss: 2.620100303829531e-06\n",
      "step: 14860, loss: 8.773706667852821e-07\n",
      "step: 14861, loss: 0.09750036150217056\n",
      "step: 14862, loss: 7.55780035888165e-07\n",
      "step: 14863, loss: 3.1947993761605176e-07\n",
      "step: 14864, loss: 4.839862413064111e-07\n",
      "step: 14865, loss: 1.7428152432330535e-06\n",
      "step: 14866, loss: 3.890665084327338e-06\n",
      "step: 14867, loss: 1.544909764561453e-06\n",
      "step: 14868, loss: 0.0650513768196106\n",
      "step: 14869, loss: 3.027901414043299e-07\n",
      "step: 14870, loss: 0.0026786434464156628\n",
      "step: 14871, loss: 4.363046173239127e-07\n",
      "step: 14872, loss: 5.101713668409502e-06\n",
      "step: 14873, loss: 9.775100124898017e-07\n",
      "step: 14874, loss: 2.729726247707731e-06\n",
      "step: 14875, loss: 7.867809159733952e-08\n",
      "step: 14876, loss: 2.861017662780796e-07\n",
      "step: 14877, loss: 5.578968966801767e-07\n",
      "step: 14878, loss: 1.4471696658802102e-06\n",
      "step: 14879, loss: 1.6450863427053264e-07\n",
      "step: 14880, loss: 7.775748599669896e-06\n",
      "step: 14881, loss: 4.291511856990837e-07\n",
      "step: 14882, loss: 8.106182463052392e-07\n",
      "step: 14883, loss: 6.198855544425896e-07\n",
      "step: 14884, loss: 4.386889429497387e-07\n",
      "step: 14885, loss: 5.26900635122729e-07\n",
      "step: 14886, loss: 6.03196497195313e-07\n",
      "step: 14887, loss: 3.528586489665031e-07\n",
      "step: 14888, loss: 2.0980810688797646e-07\n",
      "step: 14889, loss: 1.4495605000774958e-06\n",
      "step: 14890, loss: 2.3626696474821074e-06\n",
      "step: 14891, loss: 5.960405360383447e-07\n",
      "step: 14892, loss: 2.656132164702285e-05\n",
      "step: 14893, loss: 3.6023816392116714e-06\n",
      "step: 14894, loss: 1.8596628592604247e-07\n",
      "step: 14895, loss: 1.0871722224692348e-06\n",
      "step: 14896, loss: 1.406667564651798e-07\n",
      "step: 14897, loss: 4.6536692934751045e-06\n",
      "step: 14898, loss: 3.528586489665031e-07\n",
      "step: 14899, loss: 1.2874592414391373e-07\n",
      "step: 14900, loss: 2.93954644803307e-06\n",
      "step: 14901, loss: 3.7193248658695666e-07\n",
      "step: 14902, loss: 7.390973166820913e-08\n",
      "step: 14903, loss: 0.0005416795029304922\n",
      "step: 14904, loss: 0.168989360332489\n",
      "step: 14905, loss: 2.7607834454101976e-06\n",
      "step: 14906, loss: 1.0550276783760637e-05\n",
      "step: 14907, loss: 2.8657318580371793e-06\n",
      "step: 14908, loss: 5.960451403552725e-07\n",
      "step: 14909, loss: 4.76837058727142e-08\n",
      "step: 14910, loss: 6.842556672381761e-07\n",
      "step: 14911, loss: 3.4570643947517965e-07\n",
      "step: 14912, loss: 5.555117468247772e-07\n",
      "step: 14913, loss: 3.075595600421366e-07\n",
      "step: 14914, loss: 1.668928035769568e-07\n",
      "step: 14915, loss: 1.8358203135449003e-07\n",
      "step: 14916, loss: 2.849840257113101e-07\n",
      "step: 14917, loss: 2.6415245883981697e-06\n",
      "step: 14918, loss: 1.4781937807128998e-07\n",
      "step: 14919, loss: 1.1444083014566786e-07\n",
      "step: 14920, loss: 2.6702832656155806e-07\n",
      "step: 14921, loss: 4.529951169729429e-08\n",
      "step: 14922, loss: 2.713087269512471e-06\n",
      "step: 14923, loss: 2.217288113115501e-07\n",
      "step: 14924, loss: 2.8610182312149846e-07\n",
      "step: 14925, loss: 0.0036185907665640116\n",
      "step: 14926, loss: 3.0229368803702528e-06\n",
      "step: 14927, loss: 0.00723726861178875\n",
      "step: 14928, loss: 0.0975518599152565\n",
      "step: 14929, loss: 2.1457643128997006e-07\n",
      "step: 14930, loss: 4.7683624870842323e-07\n",
      "step: 14931, loss: 1.0371153393862187e-06\n",
      "step: 14932, loss: 1.6689246251644363e-07\n",
      "step: 14933, loss: 8.654433827359753e-07\n",
      "step: 14934, loss: 7.128676315915072e-07\n",
      "step: 14935, loss: 4.720669721791637e-07\n",
      "step: 14936, loss: 2.3364991363905574e-07\n",
      "step: 14937, loss: 2.1766543341072975e-06\n",
      "step: 14938, loss: 1.0967243468940069e-07\n",
      "step: 14939, loss: 2.1790879145555664e-06\n",
      "step: 14940, loss: 3.075588494994008e-07\n",
      "step: 14941, loss: 0.007570781745016575\n",
      "step: 14942, loss: 1.053793880601006e-06\n",
      "step: 14943, loss: 2.4080253524516593e-07\n",
      "step: 14944, loss: 2.6702812760959205e-07\n",
      "step: 14945, loss: 2.5510681211926567e-07\n",
      "step: 14946, loss: 1.1992208328592824e-06\n",
      "step: 14947, loss: 1.740416223583452e-06\n",
      "step: 14948, loss: 7.629303127032472e-07\n",
      "step: 14949, loss: 3.981570557698433e-07\n",
      "step: 14950, loss: 1.6097339539555833e-05\n",
      "step: 14951, loss: 4.5299407247512136e-07\n",
      "step: 14952, loss: 3.123276144378906e-07\n",
      "step: 14953, loss: 0.0867827907204628\n",
      "step: 14954, loss: 1.2731414926747675e-06\n",
      "step: 14955, loss: 4.959071020493866e-07\n",
      "step: 14956, loss: 8.106110271910438e-07\n",
      "step: 14957, loss: 9.155189104603778e-07\n",
      "step: 14958, loss: 2.264972494003814e-07\n",
      "step: 14959, loss: 5.93660786307737e-07\n",
      "step: 14960, loss: 1.4924621609679889e-06\n",
      "step: 14961, loss: 1.327960376329429e-06\n",
      "step: 14962, loss: 8.559171078559302e-07\n",
      "step: 14963, loss: 1.270754296456289e-06\n",
      "step: 14964, loss: 1.7166117061151454e-07\n",
      "step: 14965, loss: 2.789906648104079e-05\n",
      "step: 14966, loss: 1.7427985312679084e-06\n",
      "step: 14967, loss: 2.5128081233560806e-06\n",
      "step: 14968, loss: 4.5060900788485014e-07\n",
      "step: 14969, loss: 1.8119346805178793e-06\n",
      "step: 14970, loss: 9.083589702640893e-07\n",
      "step: 14971, loss: 1.6052648788900115e-05\n",
      "step: 14972, loss: 2.036010073425132e-06\n",
      "step: 14973, loss: 3.6213710700394586e-05\n",
      "step: 14974, loss: 1.2159338780293183e-07\n",
      "step: 14975, loss: 1.234980913977779e-06\n",
      "step: 14976, loss: 1.4614603287554928e-06\n",
      "step: 14977, loss: 9.918149999066372e-07\n",
      "step: 14978, loss: 1.1920920428565296e-07\n",
      "step: 14979, loss: 5.340555730981578e-07\n",
      "step: 14980, loss: 2.1933897187409457e-06\n",
      "step: 14981, loss: 1.5687692211940885e-06\n",
      "step: 14982, loss: 1.3396623216976877e-05\n",
      "step: 14983, loss: 9.961749128706288e-06\n",
      "step: 14984, loss: 2.026484480666113e-06\n",
      "step: 14985, loss: 7.128651304810774e-07\n",
      "step: 14986, loss: 5.633213277178584e-06\n",
      "step: 14987, loss: 1.3490041965269484e-05\n",
      "step: 14988, loss: 4.696826749750471e-07\n",
      "step: 14989, loss: 4.5676069930777885e-06\n",
      "step: 14990, loss: 5.557141776080243e-06\n",
      "step: 14991, loss: 6.604145141864137e-07\n",
      "step: 14992, loss: 7.295529940165579e-07\n",
      "step: 14993, loss: 8.678347285240307e-07\n",
      "step: 14994, loss: 2.424615104246186e-06\n",
      "step: 14995, loss: 9.27431301533943e-07\n",
      "step: 14996, loss: 0.004051079973578453\n",
      "step: 14997, loss: 3.6713379358843667e-06\n",
      "step: 14998, loss: 0.003502091160044074\n",
      "step: 14999, loss: 1.072882866992586e-07\n",
      "step: 15000, loss: 4.601463956532825e-07\n",
      "step: 15001, loss: 3.480901398233982e-07\n",
      "step: 15002, loss: 3.5498335364536615e-06\n",
      "step: 15003, loss: 1.5401299151562853e-06\n",
      "step: 15004, loss: 2.851321369234938e-06\n",
      "step: 15005, loss: 4.7445104200960486e-07\n",
      "step: 15006, loss: 4.42474492956535e-06\n",
      "step: 15007, loss: 1.1634633665380534e-06\n",
      "step: 15008, loss: 1.4209612118065706e-06\n",
      "step: 15009, loss: 0.015070638619363308\n",
      "step: 15010, loss: 1.4543245470122201e-06\n",
      "step: 15011, loss: 0.047660160809755325\n",
      "step: 15012, loss: 8.749920539230516e-07\n",
      "step: 15013, loss: 4.816006367036607e-07\n",
      "step: 15014, loss: 7.128671768441563e-07\n",
      "step: 15015, loss: 0.12656459212303162\n",
      "step: 15016, loss: 6.10348877216893e-07\n",
      "step: 15017, loss: 8.278605491796043e-06\n",
      "step: 15018, loss: 2.6225987426187203e-07\n",
      "step: 15019, loss: 3.836046289507067e-06\n",
      "step: 15020, loss: 9.512852443549491e-07\n",
      "step: 15021, loss: 3.4093736189788615e-07\n",
      "step: 15022, loss: 7.510153068324144e-07\n",
      "step: 15023, loss: 1.0419556929264218e-05\n",
      "step: 15024, loss: 1.1816882761195302e-05\n",
      "step: 15025, loss: 9.131338174483972e-07\n",
      "step: 15026, loss: 3.1947945444699144e-07\n",
      "step: 15027, loss: 8.106225379833631e-08\n",
      "step: 15028, loss: 2.670284686701052e-07\n",
      "step: 15029, loss: 1.8762941635941388e-06\n",
      "step: 15030, loss: 1.7404526886366511e-07\n",
      "step: 15031, loss: 1.6569783838349395e-06\n",
      "step: 15032, loss: 7.533348252763972e-06\n",
      "step: 15033, loss: 3.919409209629521e-06\n",
      "step: 15034, loss: 4.3630447521536553e-07\n",
      "step: 15035, loss: 0.04449489712715149\n",
      "step: 15036, loss: 0.09418921172618866\n",
      "step: 15037, loss: 2.0956033040420152e-05\n",
      "step: 15038, loss: 4.6729914515708515e-07\n",
      "step: 15039, loss: 0.06825349479913712\n",
      "step: 15040, loss: 1.9073446821948892e-07\n",
      "step: 15041, loss: 1.115785380534362e-06\n",
      "step: 15042, loss: 3.623944451192074e-07\n",
      "step: 15043, loss: 0.04072561115026474\n",
      "step: 15044, loss: 2.3005697585176677e-05\n",
      "step: 15045, loss: 1.144400698649406e-06\n",
      "step: 15046, loss: 1.2391740710882004e-05\n",
      "step: 15047, loss: 5.0254511734237894e-06\n",
      "step: 15048, loss: 1.3534831850847695e-05\n",
      "step: 15049, loss: 0.0697668045759201\n",
      "step: 15050, loss: 6.461117436629138e-07\n",
      "step: 15051, loss: 7.094697593856836e-06\n",
      "step: 15052, loss: 8.416121204390947e-07\n",
      "step: 15053, loss: 8.106108566607872e-07\n",
      "step: 15054, loss: 3.7847799831070006e-05\n",
      "step: 15055, loss: 0.00012528372462838888\n",
      "step: 15056, loss: 7.724727311142487e-07\n",
      "step: 15057, loss: 3.7048739613965154e-06\n",
      "step: 15058, loss: 9.369767894895631e-07\n",
      "step: 15059, loss: 5.865062462362403e-07\n",
      "step: 15060, loss: 0.058539390563964844\n",
      "step: 15061, loss: 1.2636172641578014e-07\n",
      "step: 15062, loss: 8.034647862587008e-07\n",
      "step: 15063, loss: 7.194661975518102e-06\n",
      "step: 15064, loss: 9.88992633210728e-06\n",
      "step: 15065, loss: 8.344645863189726e-08\n",
      "step: 15066, loss: 2.408025068234565e-07\n",
      "step: 15067, loss: 1.559221459501714e-06\n",
      "step: 15068, loss: 1.5735608371869603e-07\n",
      "step: 15069, loss: 8.964439075498376e-07\n",
      "step: 15070, loss: 1.2254583907633787e-06\n",
      "step: 15071, loss: 2.1933869902568404e-06\n",
      "step: 15072, loss: 1.9144374618917936e-06\n",
      "step: 15073, loss: 6.394091087713605e-06\n",
      "step: 15074, loss: 1.5973527069945703e-06\n",
      "step: 15075, loss: 7.886423554737121e-05\n",
      "step: 15076, loss: 5.841229722136632e-07\n",
      "step: 15077, loss: 5.412080099631567e-07\n",
      "step: 15078, loss: 5.555131110668299e-07\n",
      "step: 15079, loss: 2.0884601781290257e-06\n",
      "step: 15080, loss: 1.246917918251711e-06\n",
      "step: 15081, loss: 2.612953721836675e-06\n",
      "step: 15082, loss: 1.0955306606774684e-05\n",
      "step: 15083, loss: 9.427499026060104e-05\n",
      "step: 15084, loss: 2.9347695090109482e-05\n",
      "step: 15085, loss: 2.2887802515469957e-06\n",
      "step: 15086, loss: 1.8596632855860662e-07\n",
      "step: 15087, loss: 4.408163931657327e-06\n",
      "step: 15088, loss: 3.955239662900567e-06\n",
      "step: 15089, loss: 3.2186432008529664e-07\n",
      "step: 15090, loss: 1.7404532570708398e-07\n",
      "step: 15091, loss: 4.458414935015753e-07\n",
      "step: 15092, loss: 2.379374564043246e-06\n",
      "step: 15093, loss: 0.08086104691028595\n",
      "step: 15094, loss: 3.127931222479674e-06\n",
      "step: 15095, loss: 1.904939608721179e-06\n",
      "step: 15096, loss: 4.053100894907402e-07\n",
      "step: 15097, loss: 3.31401309949797e-07\n",
      "step: 15098, loss: 4.816039904653735e-07\n",
      "step: 15099, loss: 0.00012083814362995327\n",
      "step: 15100, loss: 5.030615284340456e-07\n",
      "step: 15101, loss: 6.777667294954881e-06\n",
      "step: 15102, loss: 6.008114041833323e-07\n",
      "step: 15103, loss: 1.5258768826242886e-07\n",
      "step: 15104, loss: 1.695124637990375e-06\n",
      "step: 15105, loss: 1.029272607411258e-05\n",
      "step: 15106, loss: 2.694125669222558e-07\n",
      "step: 15107, loss: 6.914137173907875e-08\n",
      "step: 15108, loss: 8.940636462284601e-07\n",
      "step: 15109, loss: 9.791921002033632e-06\n",
      "step: 15110, loss: 8.058495382101682e-07\n",
      "step: 15111, loss: 3.278041276644217e-06\n",
      "step: 15112, loss: 1.1205552254978102e-06\n",
      "step: 15113, loss: 6.11246196058346e-06\n",
      "step: 15114, loss: 1.6565083569730632e-05\n",
      "step: 15115, loss: 3.290071390438243e-06\n",
      "step: 15116, loss: 2.5749136511876713e-07\n",
      "step: 15117, loss: 7.033264637357206e-07\n",
      "step: 15118, loss: 2.074239233706976e-07\n",
      "step: 15119, loss: 1.6689267567926436e-07\n",
      "step: 15120, loss: 7.057170137159119e-07\n",
      "step: 15121, loss: 2.4674945962033235e-06\n",
      "step: 15122, loss: 2.2649746256320213e-07\n",
      "step: 15123, loss: 4.308095867600059e-06\n",
      "step: 15124, loss: 9.155197631116607e-07\n",
      "step: 15125, loss: 1.2039956800435903e-06\n",
      "step: 15126, loss: 2.1456867216329556e-06\n",
      "step: 15127, loss: 1.914463382490794e-06\n",
      "step: 15128, loss: 1.1730044207070023e-06\n",
      "step: 15129, loss: 1.3328151908353902e-05\n",
      "step: 15130, loss: 1.4662556395705906e-06\n",
      "step: 15131, loss: 5.340562552191841e-07\n",
      "step: 15132, loss: 1.1300933238089783e-06\n",
      "step: 15133, loss: 2.808457338687731e-06\n",
      "step: 15134, loss: 8.106226090376367e-08\n",
      "step: 15135, loss: 4.7445166728721233e-07\n",
      "step: 15136, loss: 4.243811417836696e-07\n",
      "step: 15137, loss: 8.821314736451313e-07\n",
      "step: 15138, loss: 8.654470775582013e-07\n",
      "step: 15139, loss: 1.9073468138230965e-07\n",
      "step: 15140, loss: 2.9086987751725246e-07\n",
      "step: 15141, loss: 4.0769526776784915e-07\n",
      "step: 15142, loss: 2.125193077517906e-06\n",
      "step: 15143, loss: 3.4808971349775675e-07\n",
      "step: 15144, loss: 1.5877298210398294e-05\n",
      "step: 15145, loss: 4.4107346752753074e-07\n",
      "step: 15146, loss: 4.222015377308708e-06\n",
      "step: 15147, loss: 3.576263338800345e-07\n",
      "step: 15148, loss: 3.62395013553396e-07\n",
      "step: 15149, loss: 6.269533059821697e-06\n",
      "step: 15150, loss: 3.547497726685833e-06\n",
      "step: 15151, loss: 4.672971272157156e-07\n",
      "step: 15152, loss: 8.351150609087199e-05\n",
      "step: 15153, loss: 9.89426098385593e-07\n",
      "step: 15154, loss: 1.5137014997890219e-05\n",
      "step: 15155, loss: 1.4185555983203813e-06\n",
      "step: 15156, loss: 1.2397755710935598e-07\n",
      "step: 15157, loss: 7.6097221608506516e-06\n",
      "step: 15158, loss: 2.12422537515522e-06\n",
      "step: 15159, loss: 8.249181178143772e-07\n",
      "step: 15160, loss: 2.973021992147551e-06\n",
      "step: 15161, loss: 4.684460691350978e-06\n",
      "step: 15162, loss: 3.81469078547525e-07\n",
      "step: 15163, loss: 0.07771361619234085\n",
      "step: 15164, loss: 5.221356786933029e-07\n",
      "step: 15165, loss: 1.2874592414391373e-07\n",
      "step: 15166, loss: 5.531287570192944e-07\n",
      "step: 15167, loss: 1.1396329000490368e-06\n",
      "step: 15168, loss: 4.94438882014947e-06\n",
      "step: 15169, loss: 4.6968347078291117e-07\n",
      "step: 15170, loss: 1.5854526509428979e-06\n",
      "step: 15171, loss: 1.287444888475875e-06\n",
      "step: 15172, loss: 3.0158646495692665e-06\n",
      "step: 15173, loss: 6.270362291616038e-07\n",
      "step: 15174, loss: 2.713149342525867e-06\n",
      "step: 15175, loss: 3.099440704090739e-08\n",
      "step: 15176, loss: 5.13530831085518e-05\n",
      "step: 15177, loss: 3.805145024671219e-05\n",
      "step: 15178, loss: 1.2338502529019024e-05\n",
      "step: 15179, loss: 4.5299398720999307e-07\n",
      "step: 15180, loss: 9.274398280467722e-07\n",
      "step: 15181, loss: 4.255585281498497e-06\n",
      "step: 15182, loss: 0.0038884254172444344\n",
      "step: 15183, loss: 3.315860885777511e-05\n",
      "step: 15184, loss: 0.007773956749588251\n",
      "step: 15185, loss: 1.9764609078265494e-06\n",
      "step: 15186, loss: 4.195743713353295e-06\n",
      "step: 15187, loss: 9.870481108009699e-07\n",
      "step: 15188, loss: 8.630681804788765e-07\n",
      "step: 15189, loss: 2.7309262804919854e-05\n",
      "step: 15190, loss: 4.792202048520267e-07\n",
      "step: 15191, loss: 1.325583980360534e-06\n",
      "step: 15192, loss: 4.053114821545023e-08\n",
      "step: 15193, loss: 1.2063890153513057e-06\n",
      "step: 15194, loss: 9.08064903342165e-05\n",
      "step: 15195, loss: 6.365752938108926e-07\n",
      "step: 15196, loss: 0.0006079938611947\n",
      "step: 15197, loss: 4.038106635562144e-05\n",
      "step: 15198, loss: 9.417425985702721e-07\n",
      "step: 15199, loss: 1.224092466145521e-05\n",
      "step: 15200, loss: 4.5935401431052014e-05\n",
      "step: 15201, loss: 2.5081112653424498e-06\n",
      "step: 15202, loss: 5.269020562082005e-07\n",
      "step: 15203, loss: 0.12949426472187042\n",
      "step: 15204, loss: 2.7942078304477036e-06\n",
      "step: 15205, loss: 1.4829442989139352e-06\n",
      "step: 15206, loss: 7.424466457450762e-05\n",
      "step: 15207, loss: 4.951475602865685e-06\n",
      "step: 15208, loss: 3.621497398853535e-06\n",
      "step: 15209, loss: 8.774731395533308e-05\n",
      "step: 15210, loss: 0.08352525532245636\n",
      "step: 15211, loss: 4.742001237900695e-06\n",
      "step: 15212, loss: 3.0130104278214276e-05\n",
      "step: 15213, loss: 0.038022641092538834\n",
      "step: 15214, loss: 0.002223373856395483\n",
      "step: 15215, loss: 1.584365418239031e-05\n",
      "step: 15216, loss: 1.2540738225652603e-06\n",
      "step: 15217, loss: 2.4775661586318165e-05\n",
      "step: 15218, loss: 8.655691090098117e-06\n",
      "step: 15219, loss: 0.004721479490399361\n",
      "step: 15220, loss: 5.025728114560479e-06\n",
      "step: 15221, loss: 0.040823761373758316\n",
      "step: 15222, loss: 0.0001748107315506786\n",
      "step: 15223, loss: 1.33273556457425e-06\n",
      "step: 15224, loss: 0.34774330258369446\n",
      "step: 15225, loss: 0.00014286824443843216\n",
      "step: 15226, loss: 0.0002594464167486876\n",
      "step: 15227, loss: 4.379279289423721e-06\n",
      "step: 15228, loss: 6.413429218810052e-07\n",
      "step: 15229, loss: 3.785881517615053e-06\n",
      "step: 15230, loss: 3.795510338022723e-06\n",
      "step: 15231, loss: 7.049230589473154e-06\n",
      "step: 15232, loss: 4.05310998985442e-07\n",
      "step: 15233, loss: 4.887543241238745e-07\n",
      "step: 15234, loss: 0.00043754602666012943\n",
      "step: 15235, loss: 4.629346585716121e-05\n",
      "step: 15236, loss: 0.007231804542243481\n",
      "step: 15237, loss: 4.588260344462469e-05\n",
      "step: 15238, loss: 0.00016071621212176979\n",
      "step: 15239, loss: 5.3542778914561495e-05\n",
      "step: 15240, loss: 0.0005383942625485361\n",
      "step: 15241, loss: 0.003293826477602124\n",
      "step: 15242, loss: 0.014526293613016605\n",
      "step: 15243, loss: 0.000651492562610656\n",
      "step: 15244, loss: 9.015479008667171e-05\n",
      "step: 15245, loss: 1.711826598693733e-06\n",
      "step: 15246, loss: 1.7952784219232854e-06\n",
      "step: 15247, loss: 0.013100343756377697\n",
      "step: 15248, loss: 1.020252784655895e-05\n",
      "step: 15249, loss: 0.0001484677050029859\n",
      "step: 15250, loss: 1.1979904229519889e-05\n",
      "step: 15251, loss: 2.139536263712216e-05\n",
      "step: 15252, loss: 0.00019150674052070826\n",
      "step: 15253, loss: 0.0031623775139451027\n",
      "step: 15254, loss: 3.421132305447827e-06\n",
      "step: 15255, loss: 1.1012840332114138e-05\n",
      "step: 15256, loss: 3.6475744309427682e-06\n",
      "step: 15257, loss: 2.6085479476023465e-05\n",
      "step: 15258, loss: 0.0014194812392815948\n",
      "step: 15259, loss: 2.9194769012974575e-05\n",
      "step: 15260, loss: 0.06390155106782913\n",
      "step: 15261, loss: 0.00017743150237947702\n",
      "step: 15262, loss: 9.991161277866922e-06\n",
      "step: 15263, loss: 0.00012689571303781122\n",
      "step: 15264, loss: 0.05673696845769882\n",
      "step: 15265, loss: 1.4648416254203767e-05\n",
      "step: 15266, loss: 8.393246389459819e-05\n",
      "step: 15267, loss: 0.0009170954581350088\n",
      "step: 15268, loss: 0.07593017816543579\n",
      "step: 15269, loss: 0.16050855815410614\n",
      "step: 15270, loss: 0.14977166056632996\n",
      "step: 15271, loss: 0.005608169361948967\n",
      "step: 15272, loss: 0.03429155424237251\n",
      "step: 15273, loss: 5.346351827029139e-05\n",
      "step: 15274, loss: 0.03197682276368141\n",
      "step: 15275, loss: 0.006315331906080246\n",
      "step: 15276, loss: 0.000823447946459055\n",
      "step: 15277, loss: 0.00018536756397224963\n",
      "step: 15278, loss: 0.012137651443481445\n",
      "step: 15279, loss: 0.00014761139755137265\n",
      "step: 15280, loss: 2.4081164156086743e-05\n",
      "step: 15281, loss: 0.028693588450551033\n",
      "step: 15282, loss: 0.0052814423106610775\n",
      "step: 15283, loss: 0.00041984725976362824\n",
      "step: 15284, loss: 0.00020254269475117326\n",
      "step: 15285, loss: 0.00023299836902879179\n",
      "step: 15286, loss: 0.003369072685018182\n",
      "step: 15287, loss: 0.000616591889411211\n",
      "step: 15288, loss: 5.053760469309054e-05\n",
      "step: 15289, loss: 0.007351408712565899\n",
      "step: 15290, loss: 0.18542587757110596\n",
      "step: 15291, loss: 0.3337477743625641\n",
      "step: 15292, loss: 0.18159914016723633\n",
      "step: 15293, loss: 0.13674640655517578\n",
      "step: 15294, loss: 0.003228108398616314\n",
      "step: 15295, loss: 0.04756241664290428\n",
      "step: 15296, loss: 0.034271664917469025\n",
      "step: 15297, loss: 7.567412831122056e-05\n",
      "step: 15298, loss: 0.25271183252334595\n",
      "step: 15299, loss: 0.006467264145612717\n",
      "step: 15300, loss: 0.1511298567056656\n",
      "step: 15301, loss: 0.06488320231437683\n",
      "step: 15302, loss: 5.926942321821116e-05\n",
      "step: 15303, loss: 6.64403796690749e-06\n",
      "step: 15304, loss: 0.06596864014863968\n",
      "step: 15305, loss: 0.07811149954795837\n",
      "step: 15306, loss: 7.189421921793837e-06\n",
      "step: 15307, loss: 6.945033965166658e-05\n",
      "step: 15308, loss: 0.024143490940332413\n",
      "step: 15309, loss: 0.45596861839294434\n",
      "step: 15310, loss: 9.854487871052697e-05\n",
      "step: 15311, loss: 0.0006385630113072693\n",
      "step: 15312, loss: 0.2574244439601898\n",
      "step: 15313, loss: 0.14043058454990387\n",
      "step: 15314, loss: 1.1478009582788218e-05\n",
      "step: 15315, loss: 0.0001903401134768501\n",
      "step: 15316, loss: 0.0004574463819153607\n",
      "step: 15317, loss: 0.006975224707275629\n",
      "step: 15318, loss: 2.282438799738884e-05\n",
      "step: 15319, loss: 0.051833562552928925\n",
      "step: 15320, loss: 0.3515731692314148\n",
      "step: 15321, loss: 0.0005961227579973638\n",
      "step: 15322, loss: 0.16375060379505157\n",
      "step: 15323, loss: 0.003976596985012293\n",
      "step: 15324, loss: 0.0018049216596409678\n",
      "step: 15325, loss: 0.23166193068027496\n",
      "step: 15326, loss: 5.698184395441785e-07\n",
      "step: 15327, loss: 0.06073607876896858\n",
      "step: 15328, loss: 0.0006820823182351887\n",
      "step: 15329, loss: 0.13470253348350525\n",
      "step: 15330, loss: 0.014156920835375786\n",
      "step: 15331, loss: 0.07582630962133408\n",
      "step: 15332, loss: 0.00011686857760651037\n",
      "step: 15333, loss: 0.0186619870364666\n",
      "step: 15334, loss: 1.5700166841270402e-05\n",
      "step: 15335, loss: 0.24757516384124756\n",
      "step: 15336, loss: 1.0538022934269975e-06\n",
      "step: 15337, loss: 0.00047865055967122316\n",
      "step: 15338, loss: 0.00816437229514122\n",
      "step: 15339, loss: 0.11686345189809799\n",
      "step: 15340, loss: 0.015620543621480465\n",
      "step: 15341, loss: 0.007678652182221413\n",
      "step: 15342, loss: 0.025989815592765808\n",
      "step: 15343, loss: 0.0001398899039486423\n",
      "step: 15344, loss: 0.0009437962435185909\n",
      "step: 15345, loss: 0.14839951694011688\n",
      "step: 15346, loss: 0.0064927274361252785\n",
      "step: 15347, loss: 0.0010024075163528323\n",
      "step: 15348, loss: 9.312480688095093e-05\n",
      "step: 15349, loss: 0.00023532810155302286\n",
      "step: 15350, loss: 0.005434679798781872\n",
      "step: 15351, loss: 0.10997840762138367\n",
      "step: 15352, loss: 1.3962034245196264e-05\n",
      "step: 15353, loss: 0.06442651152610779\n",
      "step: 15354, loss: 0.07255688309669495\n",
      "step: 15355, loss: 0.0015868593472987413\n",
      "step: 15356, loss: 0.003512871917337179\n",
      "step: 15357, loss: 0.0002748916740529239\n",
      "step: 15358, loss: 0.00012321962276473641\n",
      "step: 15359, loss: 0.2724178433418274\n",
      "step: 15360, loss: 0.20235905051231384\n",
      "step: 15361, loss: 0.014761271886527538\n",
      "step: 15362, loss: 0.027406655251979828\n",
      "step: 15363, loss: 0.09048666805028915\n",
      "step: 15364, loss: 0.00021680057398043573\n",
      "step: 15365, loss: 0.14286260306835175\n",
      "step: 15366, loss: 0.006124296225607395\n",
      "step: 15367, loss: 0.01654144562780857\n",
      "step: 15368, loss: 0.02946498617529869\n",
      "step: 15369, loss: 0.011014682240784168\n",
      "step: 15370, loss: 0.19594413042068481\n",
      "step: 15371, loss: 0.5628421902656555\n",
      "step: 15372, loss: 0.044566601514816284\n",
      "step: 15373, loss: 0.2860676646232605\n",
      "step: 15374, loss: 0.05798523500561714\n",
      "step: 15375, loss: 0.05670034512877464\n",
      "step: 15376, loss: 0.07372716069221497\n",
      "step: 15377, loss: 0.001554579590447247\n",
      "step: 15378, loss: 0.00025845292839221656\n",
      "step: 15379, loss: 3.5953598853666335e-05\n",
      "step: 15380, loss: 4.5775857415719656e-07\n",
      "step: 15381, loss: 0.3614726662635803\n",
      "step: 15382, loss: 0.15155667066574097\n",
      "step: 15383, loss: 0.03826586529612541\n",
      "step: 15384, loss: 0.19373780488967896\n",
      "step: 15385, loss: 0.681935727596283\n",
      "step: 15386, loss: 0.22936151921749115\n",
      "step: 15387, loss: 0.007903169840574265\n",
      "step: 15388, loss: 0.10734328627586365\n",
      "step: 15389, loss: 0.00703310826793313\n",
      "step: 15390, loss: 0.006223892793059349\n",
      "step: 15391, loss: 0.2653719186782837\n",
      "step: 15392, loss: 5.9506222896743566e-05\n",
      "step: 15393, loss: 0.07152312248945236\n",
      "step: 15394, loss: 0.14111922681331635\n",
      "step: 15395, loss: 0.0700802132487297\n",
      "step: 15396, loss: 0.47522658109664917\n",
      "step: 15397, loss: 0.6103562712669373\n",
      "step: 15398, loss: 0.9001789093017578\n",
      "step: 15399, loss: 0.5245793461799622\n",
      "step: 15400, loss: 0.011483395472168922\n",
      "step: 15401, loss: 0.09156826138496399\n",
      "step: 15402, loss: 2.670283834049769e-07\n",
      "step: 15403, loss: 0.2966739535331726\n",
      "step: 15404, loss: 0.011900687590241432\n",
      "step: 15405, loss: 0.03465955704450607\n",
      "step: 15406, loss: 0.0012957033468410373\n",
      "step: 15407, loss: 0.0007049664272926748\n",
      "step: 15408, loss: 0.0031768199987709522\n",
      "step: 15409, loss: 4.732161687570624e-05\n",
      "step: 15410, loss: 0.18381454050540924\n",
      "step: 15411, loss: 0.12075765430927277\n",
      "step: 15412, loss: 0.06831593811511993\n",
      "step: 15413, loss: 0.15081144869327545\n",
      "step: 15414, loss: 0.007412056904286146\n",
      "step: 15415, loss: 0.5402633547782898\n",
      "step: 15416, loss: 0.06111251935362816\n",
      "step: 15417, loss: 0.09057117253541946\n",
      "step: 15418, loss: 0.02364620380103588\n",
      "step: 15419, loss: 0.23038166761398315\n",
      "step: 15420, loss: 0.021488158032298088\n",
      "step: 15421, loss: 0.01446770504117012\n",
      "step: 15422, loss: 0.038863059133291245\n",
      "step: 15423, loss: 0.05716710537672043\n",
      "step: 15424, loss: 0.24718500673770905\n",
      "step: 15425, loss: 0.014740166254341602\n",
      "step: 15426, loss: 0.059635456651449203\n",
      "step: 15427, loss: 0.10794582962989807\n",
      "step: 15428, loss: 0.011692630127072334\n",
      "step: 15429, loss: 0.253170371055603\n",
      "step: 15430, loss: 0.24834361672401428\n",
      "step: 15431, loss: 0.0006814649095758796\n",
      "step: 15432, loss: 0.000521076493896544\n",
      "step: 15433, loss: 0.04621366411447525\n",
      "step: 15434, loss: 0.013500042259693146\n",
      "step: 15435, loss: 0.14071373641490936\n",
      "step: 15436, loss: 0.30263474583625793\n",
      "step: 15437, loss: 0.78267502784729\n",
      "step: 15438, loss: 0.31498512625694275\n",
      "step: 15439, loss: 0.10065633058547974\n",
      "step: 15440, loss: 0.177250474691391\n",
      "step: 15441, loss: 2.269543256261386e-05\n",
      "step: 15442, loss: 3.118217136943713e-05\n",
      "step: 15443, loss: 9.912990208249539e-05\n",
      "step: 15444, loss: 0.22521136701107025\n",
      "step: 15445, loss: 0.002439639065414667\n",
      "step: 15446, loss: 0.026499517261981964\n",
      "step: 15447, loss: 0.0001243974984390661\n",
      "step: 15448, loss: 0.062063537538051605\n",
      "step: 15449, loss: 1.1981960597040597e-05\n",
      "step: 15450, loss: 0.16912220418453217\n",
      "step: 15451, loss: 0.0006765819853171706\n",
      "step: 15452, loss: 0.035035163164138794\n",
      "step: 15453, loss: 0.1692679226398468\n",
      "step: 15454, loss: 7.729621574981138e-05\n",
      "step: 15455, loss: 0.0012589495163410902\n",
      "step: 15456, loss: 0.02172364667057991\n",
      "step: 15457, loss: 0.3387380540370941\n",
      "step: 15458, loss: 7.032083522062749e-05\n",
      "step: 15459, loss: 0.016624456271529198\n",
      "step: 15460, loss: 0.005200034473091364\n",
      "step: 15461, loss: 0.07258226722478867\n",
      "step: 15462, loss: 0.011152668856084347\n",
      "step: 15463, loss: 0.00011597484262892976\n",
      "step: 15464, loss: 0.00046159251360222697\n",
      "step: 15465, loss: 0.4286520779132843\n",
      "step: 15466, loss: 0.0015552109107375145\n",
      "step: 15467, loss: 1.3118321476213168e-05\n",
      "step: 15468, loss: 1.9364755644346587e-05\n",
      "step: 15469, loss: 0.17072713375091553\n",
      "step: 15470, loss: 0.0033613708801567554\n",
      "step: 15471, loss: 1.4876891327730846e-06\n",
      "step: 15472, loss: 0.00024348421720787883\n",
      "step: 15473, loss: 0.023484129458665848\n",
      "step: 15474, loss: 0.0939868614077568\n",
      "step: 15475, loss: 0.07273536920547485\n",
      "step: 15476, loss: 0.037717897444963455\n",
      "step: 15477, loss: 0.015231835655868053\n",
      "step: 15478, loss: 0.0026595976669341326\n",
      "step: 15479, loss: 0.13650041818618774\n",
      "step: 15480, loss: 0.001521982136182487\n",
      "step: 15481, loss: 0.23895667493343353\n",
      "step: 15482, loss: 0.03690283000469208\n",
      "step: 15483, loss: 0.04248352348804474\n",
      "step: 15484, loss: 0.0035642480943351984\n",
      "step: 15485, loss: 0.0027174686547368765\n",
      "step: 15486, loss: 0.12169120460748672\n",
      "step: 15487, loss: 0.0020862731616944075\n",
      "step: 15488, loss: 0.47520533204078674\n",
      "step: 15489, loss: 0.3672465980052948\n",
      "step: 15490, loss: 0.00041324232006445527\n",
      "step: 15491, loss: 0.532151997089386\n",
      "step: 15492, loss: 0.3560410737991333\n",
      "step: 15493, loss: 2.3368877009488642e-05\n",
      "step: 15494, loss: 0.03352178633213043\n",
      "step: 15495, loss: 0.18405799567699432\n",
      "step: 15496, loss: 0.12985976040363312\n",
      "step: 15497, loss: 0.25514233112335205\n",
      "step: 15498, loss: 0.20600031316280365\n",
      "step: 15499, loss: 0.4020954966545105\n",
      "step: 15500, loss: 0.37391650676727295\n",
      "step: 15501, loss: 0.023899545893073082\n",
      "step: 15502, loss: 0.12240832298994064\n",
      "step: 15503, loss: 0.0006861971924081445\n",
      "step: 15504, loss: 0.2781103253364563\n",
      "step: 15505, loss: 3.4878885344369337e-06\n",
      "step: 15506, loss: 0.19466105103492737\n",
      "step: 15507, loss: 0.039033010601997375\n",
      "step: 15508, loss: 0.00012173650611657649\n",
      "step: 15509, loss: 0.268885999917984\n",
      "step: 15510, loss: 0.3703383505344391\n",
      "step: 15511, loss: 0.5356118083000183\n",
      "step: 15512, loss: 0.00012370078184176236\n",
      "step: 15513, loss: 0.024674585089087486\n",
      "step: 15514, loss: 7.390969614107235e-08\n",
      "step: 15515, loss: 0.0675993263721466\n",
      "step: 15516, loss: 5.531283591153624e-07\n",
      "step: 15517, loss: 6.610829132114304e-06\n",
      "step: 15518, loss: 0.4669608771800995\n",
      "step: 15519, loss: 0.00023353248252533376\n",
      "step: 15520, loss: 0.3774983584880829\n",
      "step: 15521, loss: 0.03580425679683685\n",
      "step: 15522, loss: 0.018421635031700134\n",
      "step: 15523, loss: 0.08943700790405273\n",
      "step: 15524, loss: 0.053003229200839996\n",
      "step: 15525, loss: 0.15573053061962128\n",
      "step: 15526, loss: 0.008770459331572056\n",
      "step: 15527, loss: 0.1047254353761673\n",
      "step: 15528, loss: 0.10545923560857773\n",
      "step: 15529, loss: 0.0024707824923098087\n",
      "step: 15530, loss: 0.0004008288378827274\n",
      "step: 15531, loss: 0.05090825632214546\n",
      "step: 15532, loss: 0.0476541705429554\n",
      "step: 15533, loss: 0.05410457029938698\n",
      "step: 15534, loss: 0.48593980073928833\n",
      "step: 15535, loss: 2.074174062727252e-06\n",
      "step: 15536, loss: 0.2646089494228363\n",
      "step: 15537, loss: 0.0162051934748888\n",
      "step: 15538, loss: 0.03679327294230461\n",
      "step: 15539, loss: 0.5296074748039246\n",
      "step: 15540, loss: 0.6057383418083191\n",
      "step: 15541, loss: 0.004337688907980919\n",
      "step: 15542, loss: 0.0001550758461235091\n",
      "step: 15543, loss: 0.0299404077231884\n",
      "step: 15544, loss: 0.18258827924728394\n",
      "step: 15545, loss: 0.058096207678318024\n",
      "step: 15546, loss: 0.04593595862388611\n",
      "step: 15547, loss: 0.05465462803840637\n",
      "step: 15548, loss: 0.5858583450317383\n",
      "step: 15549, loss: 0.005117162596434355\n",
      "step: 15550, loss: 0.0005473613273352385\n",
      "step: 15551, loss: 0.7548815011978149\n",
      "step: 15552, loss: 7.727518823230639e-05\n",
      "step: 15553, loss: 0.8690741062164307\n",
      "step: 15554, loss: 0.0339173786342144\n",
      "step: 15555, loss: 0.1241130605340004\n",
      "step: 15556, loss: 0.34922054409980774\n",
      "step: 15557, loss: 0.01354765985161066\n",
      "step: 15558, loss: 0.008947298862040043\n",
      "step: 15559, loss: 0.05344114825129509\n",
      "step: 15560, loss: 0.3194614052772522\n",
      "step: 15561, loss: 0.0007968670106492937\n",
      "step: 15562, loss: 0.13481004536151886\n",
      "step: 15563, loss: 4.415041530592134e-06\n",
      "step: 15564, loss: 0.24267661571502686\n",
      "step: 15565, loss: 0.09779433906078339\n",
      "step: 15566, loss: 6.222689989954233e-07\n",
      "step: 15567, loss: 0.34582775831222534\n",
      "step: 15568, loss: 6.866236799396574e-05\n",
      "step: 15569, loss: 0.28813380002975464\n",
      "step: 15570, loss: 0.03253421187400818\n",
      "step: 15571, loss: 0.12782251834869385\n",
      "step: 15572, loss: 0.3231903910636902\n",
      "step: 15573, loss: 0.21351590752601624\n",
      "step: 15574, loss: 0.33559074997901917\n",
      "step: 15575, loss: 0.2381514608860016\n",
      "step: 15576, loss: 0.21156901121139526\n",
      "step: 15577, loss: 7.335494592553005e-05\n",
      "step: 15578, loss: 0.12050878256559372\n",
      "step: 15579, loss: 0.0011441626120358706\n",
      "step: 15580, loss: 0.05068613588809967\n",
      "step: 15581, loss: 0.17609001696109772\n",
      "step: 15582, loss: 0.5039740800857544\n",
      "step: 15583, loss: 0.00016304253949783742\n",
      "step: 15584, loss: 0.03844035044312477\n",
      "step: 15585, loss: 0.004731465131044388\n",
      "step: 15586, loss: 0.5596430897712708\n",
      "step: 15587, loss: 0.13133089244365692\n",
      "step: 15588, loss: 0.9146841168403625\n",
      "step: 15589, loss: 0.39062610268592834\n",
      "step: 15590, loss: 0.00021810982434544712\n",
      "step: 15591, loss: 1.1636173725128174\n",
      "step: 15592, loss: 0.08522623032331467\n",
      "step: 15593, loss: 0.16206341981887817\n",
      "step: 15594, loss: 0.0027386252768337727\n",
      "step: 15595, loss: 0.00013332437083590776\n",
      "step: 15596, loss: 7.520982035202906e-05\n",
      "step: 15597, loss: 0.06259114295244217\n",
      "step: 15598, loss: 0.0059159910306334496\n",
      "step: 15599, loss: 0.0037142871879041195\n",
      "step: 15600, loss: 0.20571570098400116\n",
      "step: 15601, loss: 0.21599945425987244\n",
      "step: 15602, loss: 0.004200463183224201\n",
      "step: 15603, loss: 0.007514987140893936\n",
      "step: 15604, loss: 0.013727101497352123\n",
      "step: 15605, loss: 5.981481990602333e-06\n",
      "step: 15606, loss: 0.4006683826446533\n",
      "step: 15607, loss: 0.24681828916072845\n",
      "step: 15608, loss: 0.018230078741908073\n",
      "step: 15609, loss: 2.363988278375473e-05\n",
      "step: 15610, loss: 0.03691074624657631\n",
      "step: 15611, loss: 0.009931471198797226\n",
      "step: 15612, loss: 0.2569009065628052\n",
      "step: 15613, loss: 0.00017394717724528164\n",
      "step: 15614, loss: 0.004871925804764032\n",
      "step: 15615, loss: 3.215352990082465e-05\n",
      "step: 15616, loss: 0.0038428360130637884\n",
      "step: 15617, loss: 2.199312439188361e-05\n",
      "step: 15618, loss: 5.376350600272417e-05\n",
      "step: 15619, loss: 0.15083469450473785\n",
      "step: 15620, loss: 0.004036549478769302\n",
      "step: 15621, loss: 0.300848126411438\n",
      "step: 15622, loss: 0.002381825353950262\n",
      "step: 15623, loss: 0.0006715855561196804\n",
      "step: 15624, loss: 8.055521902861074e-06\n",
      "step: 15625, loss: 0.0033486313186585903\n",
      "step: 15626, loss: 0.017622407525777817\n",
      "step: 15627, loss: 0.1037568598985672\n",
      "step: 15628, loss: 5.401845555752516e-06\n",
      "step: 15629, loss: 2.4293640308314934e-06\n",
      "step: 15630, loss: 0.05152979865670204\n",
      "step: 15631, loss: 0.0839511901140213\n",
      "step: 15632, loss: 0.000844046997372061\n",
      "step: 15633, loss: 0.0002114909002557397\n",
      "step: 15634, loss: 0.14971670508384705\n",
      "step: 15635, loss: 0.00017743802163749933\n",
      "step: 15636, loss: 0.0008368600974790752\n",
      "step: 15637, loss: 0.1555076390504837\n",
      "step: 15638, loss: 1.540174889669288e-05\n",
      "step: 15639, loss: 0.0002483850112184882\n",
      "step: 15640, loss: 0.05106888338923454\n",
      "step: 15641, loss: 1.1796991202572826e-05\n",
      "step: 15642, loss: 1.2206322026031557e-05\n",
      "step: 15643, loss: 1.2856336979893968e-05\n",
      "step: 15644, loss: 0.0001498274941695854\n",
      "step: 15645, loss: 0.09818187355995178\n",
      "step: 15646, loss: 0.0021679929923266172\n",
      "step: 15647, loss: 0.001750362804159522\n",
      "step: 15648, loss: 1.430511264999268e-08\n",
      "step: 15649, loss: 2.0027113123433082e-07\n",
      "step: 15650, loss: 0.10268671810626984\n",
      "step: 15651, loss: 0.0007427071104757488\n",
      "step: 15652, loss: 6.961705594221712e-07\n",
      "step: 15653, loss: 0.01111710537225008\n",
      "step: 15654, loss: 0.062125835567712784\n",
      "step: 15655, loss: 0.02269907295703888\n",
      "step: 15656, loss: 0.09196016192436218\n",
      "step: 15657, loss: 0.137065127491951\n",
      "step: 15658, loss: 0.11448360234498978\n",
      "step: 15659, loss: 0.004652578849345446\n",
      "step: 15660, loss: 4.673785952036269e-05\n",
      "step: 15661, loss: 4.074209755344782e-06\n",
      "step: 15662, loss: 0.003782599465921521\n",
      "step: 15663, loss: 0.30105435848236084\n",
      "step: 15664, loss: 7.173636549850926e-05\n",
      "step: 15665, loss: 3.154119440296199e-06\n",
      "step: 15666, loss: 8.362347216461785e-06\n",
      "step: 15667, loss: 0.20921340584754944\n",
      "step: 15668, loss: 0.07016237080097198\n",
      "step: 15669, loss: 0.016311928629875183\n",
      "step: 15670, loss: 1.1205656846868806e-07\n",
      "step: 15671, loss: 0.050219904631376266\n",
      "step: 15672, loss: 0.007742160465568304\n",
      "step: 15673, loss: 0.06998497247695923\n",
      "step: 15674, loss: 9.84305443125777e-05\n",
      "step: 15675, loss: 0.0009509160299785435\n",
      "step: 15676, loss: 0.5357444286346436\n",
      "step: 15677, loss: 3.0755782631786133e-07\n",
      "step: 15678, loss: 5.483622800284138e-08\n",
      "step: 15679, loss: 0.025661109015345573\n",
      "step: 15680, loss: 0.0002546557516325265\n",
      "step: 15681, loss: 0.5917237997055054\n",
      "step: 15682, loss: 0.05851541832089424\n",
      "step: 15683, loss: 0.5570781230926514\n",
      "step: 15684, loss: 0.02581343799829483\n",
      "step: 15685, loss: 0.002698377938941121\n",
      "step: 15686, loss: 0.08889008313417435\n",
      "step: 15687, loss: 0.10612300038337708\n",
      "step: 15688, loss: 5.5050288210622966e-05\n",
      "step: 15689, loss: 0.1212019994854927\n",
      "step: 15690, loss: 0.03198881447315216\n",
      "step: 15691, loss: 0.07366356253623962\n",
      "step: 15692, loss: 0.13795527815818787\n",
      "step: 15693, loss: 1.1682490708153637e-07\n",
      "step: 15694, loss: 0.08355483412742615\n",
      "step: 15695, loss: 0.29290148615837097\n",
      "step: 15696, loss: 0.00811776239424944\n",
      "step: 15697, loss: 0.1927339732646942\n",
      "step: 15698, loss: 0.037478379905223846\n",
      "step: 15699, loss: 0.01025234442204237\n",
      "step: 15700, loss: 0.38825470209121704\n",
      "step: 15701, loss: 0.011891135945916176\n",
      "step: 15702, loss: 0.0012894667452201247\n",
      "step: 15703, loss: 0.2736290991306305\n",
      "step: 15704, loss: 0.001015053829178214\n",
      "step: 15705, loss: 0.0261822659522295\n",
      "step: 15706, loss: 8.03146103862673e-05\n",
      "step: 15707, loss: 0.05275256186723709\n",
      "step: 15708, loss: 6.867761840112507e-05\n",
      "step: 15709, loss: 0.015026602894067764\n",
      "step: 15710, loss: 1.1491446230138536e-06\n",
      "step: 15711, loss: 0.00015601646737195551\n",
      "step: 15712, loss: 2.5265935619245283e-05\n",
      "step: 15713, loss: 0.006276208907365799\n",
      "step: 15714, loss: 0.0012872277293354273\n",
      "step: 15715, loss: 0.2472258359193802\n",
      "step: 15716, loss: 0.3136900067329407\n",
      "step: 15717, loss: 0.016371406614780426\n",
      "step: 15718, loss: 3.211349621778936e-06\n",
      "step: 15719, loss: 0.26207953691482544\n",
      "step: 15720, loss: 0.1560223549604416\n",
      "step: 15721, loss: 0.17139065265655518\n",
      "step: 15722, loss: 0.9381539225578308\n",
      "step: 15723, loss: 0.09313660860061646\n",
      "step: 15724, loss: 0.6061850786209106\n",
      "step: 15725, loss: 7.62176641728729e-05\n",
      "step: 15726, loss: 0.00019200393580831587\n",
      "step: 15727, loss: 0.12445991486310959\n",
      "step: 15728, loss: 0.15763099491596222\n",
      "step: 15729, loss: 0.001437832834199071\n",
      "step: 15730, loss: 0.09661125391721725\n",
      "step: 15731, loss: 0.008417058736085892\n",
      "step: 15732, loss: 1.9002231056219898e-05\n",
      "step: 15733, loss: 0.11047742515802383\n",
      "step: 15734, loss: 1.1939265277760569e-05\n",
      "step: 15735, loss: 0.03539872169494629\n",
      "step: 15736, loss: 3.196807665517554e-05\n",
      "step: 15737, loss: 0.05435917526483536\n",
      "step: 15738, loss: 0.11270435154438019\n",
      "step: 15739, loss: 0.003311519045382738\n",
      "step: 15740, loss: 0.17659491300582886\n",
      "step: 15741, loss: 3.893186203640653e-06\n",
      "step: 15742, loss: 0.002506576245650649\n",
      "step: 15743, loss: 6.031918360349664e-07\n",
      "step: 15744, loss: 7.400256436085328e-05\n",
      "step: 15745, loss: 2.5771930722839897e-06\n",
      "step: 15746, loss: 1.9073449664119835e-07\n",
      "step: 15747, loss: 0.010264518670737743\n",
      "step: 15748, loss: 0.07912193238735199\n",
      "step: 15749, loss: 0.1864461451768875\n",
      "step: 15750, loss: 1.451924504181079e-06\n",
      "step: 15751, loss: 0.08641240000724792\n",
      "step: 15752, loss: 1.3681607924809214e-05\n",
      "step: 15753, loss: 0.3491346836090088\n",
      "step: 15754, loss: 1.6116770211738185e-06\n",
      "step: 15755, loss: 0.0027561387978494167\n",
      "step: 15756, loss: 0.00014056370127946138\n",
      "step: 15757, loss: 0.006434711627662182\n",
      "step: 15758, loss: 0.33078423142433167\n",
      "step: 15759, loss: 0.35984882712364197\n",
      "step: 15760, loss: 0.10168275237083435\n",
      "step: 15761, loss: 0.11937576532363892\n",
      "step: 15762, loss: 0.0025123695377260447\n",
      "step: 15763, loss: 0.03142985701560974\n",
      "step: 15764, loss: 0.06967759877443314\n",
      "step: 15765, loss: 0.31261980533599854\n",
      "step: 15766, loss: 0.44300395250320435\n",
      "step: 15767, loss: 3.688131300805253e-06\n",
      "step: 15768, loss: 0.3339509665966034\n",
      "step: 15769, loss: 0.32084447145462036\n",
      "step: 15770, loss: 0.13324397802352905\n",
      "step: 15771, loss: 0.09725512564182281\n",
      "step: 15772, loss: 4.680968777392991e-05\n",
      "step: 15773, loss: 0.009546838700771332\n",
      "step: 15774, loss: 0.00014906756405252963\n",
      "step: 15775, loss: 0.0007750990334898233\n",
      "step: 15776, loss: 0.0002721112687140703\n",
      "step: 15777, loss: 0.0582536943256855\n",
      "step: 15778, loss: 0.06964123994112015\n",
      "step: 15779, loss: 0.11799474060535431\n",
      "step: 15780, loss: 4.586251816363074e-05\n",
      "step: 15781, loss: 8.749850053391128e-07\n",
      "step: 15782, loss: 0.07116848230361938\n",
      "step: 15783, loss: 0.018872685730457306\n",
      "step: 15784, loss: 2.2882835764903575e-05\n",
      "step: 15785, loss: 0.05051831901073456\n",
      "step: 15786, loss: 0.2274441123008728\n",
      "step: 15787, loss: 0.16089344024658203\n",
      "step: 15788, loss: 0.033207863569259644\n",
      "step: 15789, loss: 0.31846266984939575\n",
      "step: 15790, loss: 0.02609582617878914\n",
      "step: 15791, loss: 0.07129676640033722\n",
      "step: 15792, loss: 0.002376120537519455\n",
      "step: 15793, loss: 0.12594696879386902\n",
      "step: 15794, loss: 6.961737426536274e-07\n",
      "step: 15795, loss: 0.0793183222413063\n",
      "step: 15796, loss: 0.0025477325543761253\n",
      "step: 15797, loss: 0.1611425280570984\n",
      "step: 15798, loss: 0.6830520033836365\n",
      "step: 15799, loss: 9.008017514133826e-06\n",
      "step: 15800, loss: 0.3389033377170563\n",
      "step: 15801, loss: 5.492636319104349e-06\n",
      "step: 15802, loss: 2.615337507450022e-06\n",
      "step: 15803, loss: 0.11855734139680862\n",
      "step: 15804, loss: 0.003196254139766097\n",
      "step: 15805, loss: 0.0006631291471421719\n",
      "step: 15806, loss: 0.4695384204387665\n",
      "step: 15807, loss: 0.06580012291669846\n",
      "step: 15808, loss: 0.0001570151944179088\n",
      "step: 15809, loss: 0.001971604535356164\n",
      "step: 15810, loss: 0.0013625561259686947\n",
      "step: 15811, loss: 0.2207859754562378\n",
      "step: 15812, loss: 0.0002475588698871434\n",
      "step: 15813, loss: 0.0029504785779863596\n",
      "step: 15814, loss: 0.00033748266287148\n",
      "step: 15815, loss: 4.5299501039153256e-08\n",
      "step: 15816, loss: 0.089730404317379\n",
      "step: 15817, loss: 0.00020585568563546985\n",
      "step: 15818, loss: 7.51206825952977e-05\n",
      "step: 15819, loss: 0.0007297187112271786\n",
      "step: 15820, loss: 0.06953182071447372\n",
      "step: 15821, loss: 3.040419687749818e-05\n",
      "step: 15822, loss: 3.883549652528018e-06\n",
      "step: 15823, loss: 0.0018856963142752647\n",
      "step: 15824, loss: 0.0008139134733937681\n",
      "step: 15825, loss: 1.4543508086717338e-07\n",
      "step: 15826, loss: 5.722000651076087e-07\n",
      "step: 15827, loss: 1.6259537005680613e-06\n",
      "step: 15828, loss: 2.8610218194558e-08\n",
      "step: 15829, loss: 3.0994396382766354e-08\n",
      "step: 15830, loss: 0.0006004313472658396\n",
      "step: 15831, loss: 0.2375830113887787\n",
      "step: 15832, loss: 1.1038541742891539e-06\n",
      "step: 15833, loss: 0.05250682681798935\n",
      "step: 15834, loss: 3.7431371424645477e-07\n",
      "step: 15835, loss: 1.3959310308564454e-05\n",
      "step: 15836, loss: 2.630177914397791e-05\n",
      "step: 15837, loss: 2.5033838824128907e-07\n",
      "step: 15838, loss: 2.59624812315451e-06\n",
      "step: 15839, loss: 1.094312665372854e-06\n",
      "step: 15840, loss: 0.00039023670251481235\n",
      "step: 15841, loss: 0.0016883722273632884\n",
      "step: 15842, loss: 8.271436854556669e-06\n",
      "step: 15843, loss: 4.193856511847116e-05\n",
      "step: 15844, loss: 0.002516033360734582\n",
      "step: 15845, loss: 2.265606053697411e-05\n",
      "step: 15846, loss: 0.048338379710912704\n",
      "step: 15847, loss: 0.0002635607961565256\n",
      "step: 15848, loss: 0.0006239823997020721\n",
      "step: 15849, loss: 0.00013112678425386548\n",
      "step: 15850, loss: 0.0012554751010611653\n",
      "step: 15851, loss: 4.959064767717791e-07\n",
      "step: 15852, loss: 5.873309783055447e-05\n",
      "step: 15853, loss: 0.0002559139975346625\n",
      "step: 15854, loss: 3.201767867722083e-06\n",
      "step: 15855, loss: 2.6724935651145643e-06\n",
      "step: 15856, loss: 4.698804332292639e-06\n",
      "step: 15857, loss: 1.2111304386053234e-06\n",
      "step: 15858, loss: 1.0681022786229732e-06\n",
      "step: 15859, loss: 0.0970459058880806\n",
      "step: 15860, loss: 0.025717008858919144\n",
      "step: 15861, loss: 2.8275555905565852e-06\n",
      "step: 15862, loss: 1.079995035979664e-05\n",
      "step: 15863, loss: 0.00018591941625345498\n",
      "step: 15864, loss: 0.05659914016723633\n",
      "step: 15865, loss: 0.13917176425457\n",
      "step: 15866, loss: 9.584239251125837e-07\n",
      "step: 15867, loss: 1.6516059986315668e-05\n",
      "step: 15868, loss: 0.0030236660968512297\n",
      "step: 15869, loss: 0.007286282256245613\n",
      "step: 15870, loss: 5.197463792683266e-07\n",
      "step: 15871, loss: 0.33210888504981995\n",
      "step: 15872, loss: 0.1407373994588852\n",
      "step: 15873, loss: 8.405902190133929e-05\n",
      "step: 15874, loss: 0.003921812400221825\n",
      "step: 15875, loss: 2.968223952848348e-06\n",
      "step: 15876, loss: 0.09073232859373093\n",
      "step: 15877, loss: 0.0010296901455149055\n",
      "step: 15878, loss: 1.907348234908568e-08\n",
      "step: 15879, loss: 3.1543186196358874e-05\n",
      "step: 15880, loss: 6.584988295799121e-05\n",
      "step: 15881, loss: 0.17715592682361603\n",
      "step: 15882, loss: 4.610811447491869e-05\n",
      "step: 15883, loss: 0.003544083796441555\n",
      "step: 15884, loss: 9.53674117454284e-09\n",
      "step: 15885, loss: 3.113009370281361e-05\n",
      "step: 15886, loss: 1.942840390256606e-05\n",
      "step: 15887, loss: 0.03927423059940338\n",
      "step: 15888, loss: 2.38418573772492e-09\n",
      "step: 15889, loss: 3.01636664516991e-05\n",
      "step: 15890, loss: 1.0037333595391829e-06\n",
      "step: 15891, loss: 0.059795722365379333\n",
      "step: 15892, loss: 0.015542544424533844\n",
      "step: 15893, loss: 4.794092455995269e-05\n",
      "step: 15894, loss: 5.340542088561051e-07\n",
      "step: 15895, loss: 8.517007699992973e-06\n",
      "step: 15896, loss: 0.0035415603779256344\n",
      "step: 15897, loss: 0.0023813736625015736\n",
      "step: 15898, loss: 2.694113447887503e-07\n",
      "step: 15899, loss: 0.14067497849464417\n",
      "step: 15900, loss: 0.2358119785785675\n",
      "step: 15901, loss: 0.017783772200345993\n",
      "step: 15902, loss: 0.07520058751106262\n",
      "step: 15903, loss: 4.266476389602758e-05\n",
      "step: 15904, loss: 1.187303155347763e-06\n",
      "step: 15905, loss: 1.5088601685420144e-05\n",
      "step: 15906, loss: 1.2302234608796425e-06\n",
      "step: 15907, loss: 0.006390925496816635\n",
      "step: 15908, loss: 4.291530331101967e-08\n",
      "step: 15909, loss: 1.668929527909313e-08\n",
      "step: 15910, loss: 4.872682438872289e-06\n",
      "step: 15911, loss: 3.3378579900045224e-08\n",
      "step: 15912, loss: 3.9100450521800667e-07\n",
      "step: 15913, loss: 2.169604016444282e-07\n",
      "step: 15914, loss: 0.018256187438964844\n",
      "step: 15915, loss: 4.017073933937354e-06\n",
      "step: 15916, loss: 0.0010937770130112767\n",
      "step: 15917, loss: 0.002115366980433464\n",
      "step: 15918, loss: 9.815990779316053e-06\n",
      "step: 15919, loss: 0.01377510093152523\n",
      "step: 15920, loss: 0.00785047933459282\n",
      "step: 15921, loss: 7.843898970349983e-07\n",
      "step: 15922, loss: 8.026637260627467e-06\n",
      "step: 15923, loss: 1.4543495296948095e-07\n",
      "step: 15924, loss: 0.17180135846138\n",
      "step: 15925, loss: 0.000664222810883075\n",
      "step: 15926, loss: 0.015669535845518112\n",
      "step: 15927, loss: 2.92041954708111e-06\n",
      "step: 15928, loss: 3.528577110500919e-07\n",
      "step: 15929, loss: 1.16824800500126e-07\n",
      "step: 15930, loss: 2.1049118004157208e-05\n",
      "step: 15931, loss: 0.0017147113103419542\n",
      "step: 15932, loss: 1.2628283911908511e-05\n",
      "step: 15933, loss: 0.010431167669594288\n",
      "step: 15934, loss: 1.7977516108658165e-05\n",
      "step: 15935, loss: 6.140800451248651e-06\n",
      "step: 15936, loss: 3.5521213703759713e-06\n",
      "step: 15937, loss: 2.574905977326125e-07\n",
      "step: 15938, loss: 0.0809357613325119\n",
      "step: 15939, loss: 0.0017813447630032897\n",
      "step: 15940, loss: 0.0015133862616494298\n",
      "step: 15941, loss: 1.4543253428200842e-06\n",
      "step: 15942, loss: 0.07557696849107742\n",
      "step: 15943, loss: 2.5459601602051407e-05\n",
      "step: 15944, loss: 3.832731454167515e-05\n",
      "step: 15945, loss: 6.365738158820022e-07\n",
      "step: 15946, loss: 3.4092245186911896e-06\n",
      "step: 15947, loss: 1.4066678488688922e-07\n",
      "step: 15948, loss: 8.773720310273347e-07\n",
      "step: 15949, loss: 0.05751289427280426\n",
      "step: 15950, loss: 6.209982075233711e-06\n",
      "step: 15951, loss: 6.607885552512016e-06\n",
      "step: 15952, loss: 0.0012476047268137336\n",
      "step: 15953, loss: 1.6879556596904877e-06\n",
      "step: 15954, loss: 0.0024387496523559093\n",
      "step: 15955, loss: 3.8146957592744e-08\n",
      "step: 15956, loss: 3.3616811379033606e-07\n",
      "step: 15957, loss: 0.0010051364079117775\n",
      "step: 15958, loss: 2.1839156033820473e-05\n",
      "step: 15959, loss: 2.426281753287185e-05\n",
      "step: 15960, loss: 4.0727063606027514e-05\n",
      "step: 15961, loss: 7.80002028477611e-06\n",
      "step: 15962, loss: 1.1444075198596693e-07\n",
      "step: 15963, loss: 0.3809664249420166\n",
      "step: 15964, loss: 0.03439614549279213\n",
      "step: 15965, loss: 5.058815986558329e-06\n",
      "step: 15966, loss: 0.1416286677122116\n",
      "step: 15967, loss: 4.3899523006984964e-05\n",
      "step: 15968, loss: 0.07033304870128632\n",
      "step: 15969, loss: 0.02156740240752697\n",
      "step: 15970, loss: 6.507635407615453e-05\n",
      "step: 15971, loss: 0.15855582058429718\n",
      "step: 15972, loss: 0.004268437158316374\n",
      "step: 15973, loss: 0.20595255494117737\n",
      "step: 15974, loss: 0.001157374819740653\n",
      "step: 15975, loss: 0.0001054480962920934\n",
      "step: 15976, loss: 0.0018913421081379056\n",
      "step: 15977, loss: 0.0003083392512053251\n",
      "step: 15978, loss: 1.3375029084272683e-06\n",
      "step: 15979, loss: 9.628089173929766e-06\n",
      "step: 15980, loss: 7.939268584777892e-07\n",
      "step: 15981, loss: 1.7165803001262248e-06\n",
      "step: 15982, loss: 1.78883765329374e-05\n",
      "step: 15983, loss: 0.23115520179271698\n",
      "step: 15984, loss: 0.10872507840394974\n",
      "step: 15985, loss: 0.0026130853220820427\n",
      "step: 15986, loss: 1.8095337281920365e-06\n",
      "step: 15987, loss: 3.337859055818626e-08\n",
      "step: 15988, loss: 0.12624435126781464\n",
      "step: 15989, loss: 5.058460374129936e-05\n",
      "step: 15990, loss: 0.13813433051109314\n",
      "step: 15991, loss: 1.5258503935910994e-06\n",
      "step: 15992, loss: 1.2893549865111709e-05\n",
      "step: 15993, loss: 1.2142128980485722e-05\n",
      "step: 15994, loss: 5.006730248169333e-07\n",
      "step: 15995, loss: 0.03447248414158821\n",
      "step: 15996, loss: 0.07828415185213089\n",
      "step: 15997, loss: 0.00020305346697568893\n",
      "step: 15998, loss: 0.03426792845129967\n",
      "step: 15999, loss: 1.2046420124534052e-05\n",
      "step: 16000, loss: 0.020161280408501625\n",
      "step: 16001, loss: 3.259058075855137e-06\n",
      "step: 16002, loss: 7.0371302172134165e-06\n",
      "step: 16003, loss: 7.963093366925023e-07\n",
      "step: 16004, loss: 0.0051118237897753716\n",
      "step: 16005, loss: 0.001550258370116353\n",
      "step: 16006, loss: 0.00019654730567708611\n",
      "step: 16007, loss: 0.01712724380195141\n",
      "step: 16008, loss: 9.465119319429505e-07\n",
      "step: 16009, loss: 0.00012765424617100507\n",
      "step: 16010, loss: 0.06004476547241211\n",
      "step: 16011, loss: 4.76837147544984e-09\n",
      "step: 16012, loss: 0.001187888323329389\n",
      "step: 16013, loss: 4.3659849325194955e-05\n",
      "step: 16014, loss: 0.00019265591981820762\n",
      "step: 16015, loss: 2.1099426703585777e-06\n",
      "step: 16016, loss: 0.0499635748565197\n",
      "step: 16017, loss: 0.00879841297864914\n",
      "step: 16018, loss: 0.00015118304872885346\n",
      "step: 16019, loss: 5.483592531163595e-07\n",
      "step: 16020, loss: 0.005177260842174292\n",
      "step: 16021, loss: 7.5377865869086236e-06\n",
      "step: 16022, loss: 0.19459758698940277\n",
      "step: 16023, loss: 0.0001246655301656574\n",
      "step: 16024, loss: 0.0007647557649761438\n",
      "step: 16025, loss: 0.023818375542759895\n",
      "step: 16026, loss: 1.889011764433235e-05\n",
      "step: 16027, loss: 3.4807912925316487e-06\n",
      "step: 16028, loss: 3.978999302489683e-06\n",
      "step: 16029, loss: 0.02220727875828743\n",
      "step: 16030, loss: 0.024798355996608734\n",
      "step: 16031, loss: 0.0008544836309738457\n",
      "step: 16032, loss: 2.6463631002116017e-06\n",
      "step: 16033, loss: 2.1119529264979064e-05\n",
      "step: 16034, loss: 0.01947700046002865\n",
      "step: 16035, loss: 1.7700434909784235e-05\n",
      "step: 16036, loss: 6.198878566010535e-08\n",
      "step: 16037, loss: 0.0003366185410413891\n",
      "step: 16038, loss: 0.007839005440473557\n",
      "step: 16039, loss: 0.009524336084723473\n",
      "step: 16040, loss: 2.264969651832871e-07\n",
      "step: 16041, loss: 1.1155007996421773e-05\n",
      "step: 16042, loss: 9.446863259654492e-06\n",
      "step: 16043, loss: 4.792184995494608e-07\n",
      "step: 16044, loss: 6.232585292309523e-05\n",
      "step: 16045, loss: 7.271638651218382e-07\n",
      "step: 16046, loss: 3.3527348364259524e-07\n",
      "step: 16047, loss: 1.1801388382082223e-06\n",
      "step: 16048, loss: 0.0\n",
      "step: 16049, loss: 4.1246050841436954e-07\n",
      "step: 16050, loss: 0.024475326761603355\n",
      "step: 16051, loss: 4.76837147544984e-09\n",
      "step: 16052, loss: 0.00010465617378940806\n",
      "step: 16053, loss: 0.00028959187329746783\n",
      "step: 16054, loss: 2.3506174329668283e-05\n",
      "step: 16055, loss: 0.01053560245782137\n",
      "step: 16056, loss: 1.0323410606360994e-06\n",
      "step: 16057, loss: 1.1920903375539638e-07\n",
      "step: 16058, loss: 4.792183858626231e-07\n",
      "step: 16059, loss: 1.2993600648769643e-06\n",
      "step: 16060, loss: 4.291530331101967e-08\n",
      "step: 16061, loss: 1.974009137484245e-06\n",
      "step: 16062, loss: 1.1774732229241636e-05\n",
      "step: 16063, loss: 1.510091624368215e-05\n",
      "step: 16064, loss: 6.789119652239606e-05\n",
      "step: 16065, loss: 0.00011836234625661746\n",
      "step: 16066, loss: 0.00016149593284353614\n",
      "step: 16067, loss: 5.2478550060186535e-05\n",
      "step: 16068, loss: 5.3701420256402344e-05\n",
      "step: 16069, loss: 1.0490155091247289e-06\n",
      "step: 16070, loss: 3.637945383161423e-06\n",
      "step: 16071, loss: 6.712760750815505e-06\n",
      "step: 16072, loss: 0.0011504035210236907\n",
      "step: 16073, loss: 0.0030071723740547895\n",
      "step: 16074, loss: 8.582985628891038e-07\n",
      "step: 16075, loss: 4.839873213313695e-07\n",
      "step: 16076, loss: 5.626616825793462e-07\n",
      "step: 16077, loss: 6.198878566010535e-08\n",
      "step: 16078, loss: 7.963120651766076e-07\n",
      "step: 16079, loss: 4.696383257396519e-06\n",
      "step: 16080, loss: 0.06150693818926811\n",
      "step: 16081, loss: 0.050058960914611816\n",
      "step: 16082, loss: 3.766984377762128e-07\n",
      "step: 16083, loss: 6.021576155035291e-06\n",
      "step: 16084, loss: 3.232823246435146e-06\n",
      "step: 16085, loss: 5.793530135633773e-07\n",
      "step: 16086, loss: 2.3409311324940063e-05\n",
      "step: 16087, loss: 3.7928853089397307e-06\n",
      "step: 16088, loss: 0.00041105554555542767\n",
      "step: 16089, loss: 8.129948128043907e-07\n",
      "step: 16090, loss: 2.0575046164594823e-06\n",
      "step: 16091, loss: 3.6355665997689357e-06\n",
      "step: 16092, loss: 4.074396656505996e-06\n",
      "step: 16093, loss: 0.00014639952860306948\n",
      "step: 16094, loss: 5.173650947654096e-07\n",
      "step: 16095, loss: 0.0\n",
      "step: 16096, loss: 5.960460924825384e-08\n",
      "step: 16097, loss: 5.74584873902495e-07\n",
      "step: 16098, loss: 0.04908764362335205\n",
      "step: 16099, loss: 7.15255676908555e-09\n",
      "step: 16100, loss: 6.699465870951826e-07\n",
      "step: 16101, loss: 0.0001152326658484526\n",
      "step: 16102, loss: 0.1285998523235321\n",
      "step: 16103, loss: 2.8848583610852074e-07\n",
      "step: 16104, loss: 2.7179535777577257e-07\n",
      "step: 16105, loss: 1.2874588151134958e-07\n",
      "step: 16106, loss: 2.8371678695293667e-07\n",
      "step: 16107, loss: 3.645284778031055e-06\n",
      "step: 16108, loss: 7.915446644801705e-07\n",
      "step: 16109, loss: 2.1773983462480828e-05\n",
      "step: 16110, loss: 5.6235367082990706e-05\n",
      "step: 16111, loss: 3.0158691515680403e-05\n",
      "step: 16112, loss: 0.0009656235342845321\n",
      "step: 16113, loss: 1.3470261137626949e-06\n",
      "step: 16114, loss: 2.9635864848387428e-05\n",
      "step: 16115, loss: 2.908696217218676e-07\n",
      "step: 16116, loss: 6.5983513195533305e-06\n",
      "step: 16117, loss: 7.41108124202583e-06\n",
      "step: 16118, loss: 0.006159323267638683\n",
      "step: 16119, loss: 0.041368335485458374\n",
      "step: 16120, loss: 7.141753940231865e-06\n",
      "step: 16121, loss: 2.3734030037303455e-05\n",
      "step: 16122, loss: 0.068476103246212\n",
      "step: 16123, loss: 1.907348234908568e-08\n",
      "step: 16124, loss: 9.608416803530417e-06\n",
      "step: 16125, loss: 0.18637733161449432\n",
      "step: 16126, loss: 9.512700671621133e-07\n",
      "step: 16127, loss: 3.3378579900045224e-08\n",
      "step: 16128, loss: 3.0659807634947356e-06\n",
      "step: 16129, loss: 2.8610209312773804e-08\n",
      "step: 16130, loss: 9.989488489736686e-07\n",
      "step: 16131, loss: 5.292864670991548e-07\n",
      "step: 16132, loss: 7.301599907805212e-06\n",
      "step: 16133, loss: 3.9338868873528554e-07\n",
      "step: 16134, loss: 0.00010065465176012367\n",
      "step: 16135, loss: 1.8286285694557591e-06\n",
      "step: 16136, loss: 6.839063644292764e-06\n",
      "step: 16137, loss: 0.0114204166457057\n",
      "step: 16138, loss: 1.430510998545742e-08\n",
      "step: 16139, loss: 8.106222537662688e-08\n",
      "step: 16140, loss: 2.3841805329993804e-07\n",
      "step: 16141, loss: 0.1233631893992424\n",
      "step: 16142, loss: 1.192092646817855e-08\n",
      "step: 16143, loss: 9.72724933490099e-07\n",
      "step: 16144, loss: 0.00011153155355714262\n",
      "step: 16145, loss: 0.11087652295827866\n",
      "step: 16146, loss: 9.512725682725431e-07\n",
      "step: 16147, loss: 7.580708188470453e-05\n",
      "step: 16148, loss: 3.4806137136911275e-06\n",
      "step: 16149, loss: 2.538996568546281e-06\n",
      "step: 16150, loss: 0.0013170422753319144\n",
      "step: 16151, loss: 0.0\n",
      "step: 16152, loss: 4.17835071857553e-05\n",
      "step: 16153, loss: 7.303853180928854e-06\n",
      "step: 16154, loss: 3.5762759864610416e-08\n",
      "step: 16155, loss: 1.1682495681952787e-07\n",
      "step: 16156, loss: 6.008112904964946e-07\n",
      "step: 16157, loss: 2.9563736347881786e-07\n",
      "step: 16158, loss: 7.057143989186443e-07\n",
      "step: 16159, loss: 1.5830609072509105e-06\n",
      "step: 16160, loss: 4.76837147544984e-09\n",
      "step: 16161, loss: 1.4877068679197691e-06\n",
      "step: 16162, loss: 4.714085298473947e-05\n",
      "step: 16163, loss: 0.0006532766856253147\n",
      "step: 16164, loss: 2.857676736311987e-05\n",
      "step: 16165, loss: 4.76837147544984e-09\n",
      "step: 16166, loss: 2.38418573772492e-09\n",
      "step: 16167, loss: 0.00027252835570834577\n",
      "step: 16168, loss: 7.141808055166621e-06\n",
      "step: 16169, loss: 8.664708002470434e-05\n",
      "step: 16170, loss: 0.005119327921420336\n",
      "step: 16171, loss: 8.344476896127162e-07\n",
      "step: 16172, loss: 2.38418573772492e-09\n",
      "step: 16173, loss: 0.0013310062931850553\n",
      "step: 16174, loss: 0.0010285625467076898\n",
      "step: 16175, loss: 4.234622247167863e-05\n",
      "step: 16176, loss: 1.7261114635402919e-06\n",
      "step: 16177, loss: 0.009725991636514664\n",
      "step: 16178, loss: 0.005163175985217094\n",
      "step: 16179, loss: 2.3294267521123402e-05\n",
      "step: 16180, loss: 4.019438165414613e-06\n",
      "step: 16181, loss: 9.77514886812969e-08\n",
      "step: 16182, loss: 2.0503904352153768e-07\n",
      "step: 16183, loss: 7.589878350700019e-06\n",
      "step: 16184, loss: 1.1564728083612863e-05\n",
      "step: 16185, loss: 4.069428996444913e-06\n",
      "step: 16186, loss: 1.430511176181426e-08\n",
      "step: 16187, loss: 1.430511264999268e-08\n",
      "step: 16188, loss: 1.959715746124857e-06\n",
      "step: 16189, loss: 0.0\n",
      "step: 16190, loss: 3.337859055818626e-08\n",
      "step: 16191, loss: 0.4780866503715515\n",
      "step: 16192, loss: 3.5593011489254422e-06\n",
      "step: 16193, loss: 5.483622800284138e-08\n",
      "step: 16194, loss: 6.484928007921553e-07\n",
      "step: 16195, loss: 4.6392942749662325e-06\n",
      "step: 16196, loss: 1.5110050298972055e-05\n",
      "step: 16197, loss: 0.00032228147028945386\n",
      "step: 16198, loss: 1.2365550901449751e-05\n",
      "step: 16199, loss: 9.77514886812969e-08\n",
      "step: 16200, loss: 0.00010031728743342683\n",
      "step: 16201, loss: 7.128641072995379e-07\n",
      "step: 16202, loss: 7.915356263765716e-07\n",
      "step: 16203, loss: 3.2424665619146253e-07\n",
      "step: 16204, loss: 1.5544585494353669e-06\n",
      "step: 16205, loss: 3.5762763417324095e-08\n",
      "step: 16206, loss: 0.00036743900272995234\n",
      "step: 16207, loss: 0.002068037400022149\n",
      "step: 16208, loss: 1.5735604108613188e-07\n",
      "step: 16209, loss: 4.4345665628497954e-07\n",
      "step: 16210, loss: 2.5938707040040754e-06\n",
      "step: 16211, loss: 1.5735584213416587e-07\n",
      "step: 16212, loss: 0.13347448408603668\n",
      "step: 16213, loss: 2.622603467727913e-08\n",
      "step: 16214, loss: 4.053112334645448e-08\n",
      "step: 16215, loss: 8.056266233325005e-05\n",
      "step: 16216, loss: 5.5234511819435284e-05\n",
      "step: 16217, loss: 4.0384766180068254e-06\n",
      "step: 16218, loss: 7.6064902714279015e-06\n",
      "step: 16219, loss: 0.0\n",
      "step: 16220, loss: 4.768369876728684e-08\n",
      "step: 16221, loss: 0.00029302205075509846\n",
      "step: 16222, loss: 3.0384335332200862e-05\n",
      "step: 16223, loss: 1.3590009984909557e-05\n",
      "step: 16224, loss: 0.11599919199943542\n",
      "step: 16225, loss: 2.503246150808991e-06\n",
      "step: 16226, loss: 1.2874578203536657e-07\n",
      "step: 16227, loss: 0.00015349703608080745\n",
      "step: 16228, loss: 1.26361683783216e-07\n",
      "step: 16229, loss: 5.53126881186472e-07\n",
      "step: 16230, loss: 9.53674206272126e-09\n",
      "step: 16231, loss: 1.5497190020141716e-07\n",
      "step: 16232, loss: 7.390962508679877e-08\n",
      "step: 16233, loss: 0.00041026584221981466\n",
      "step: 16234, loss: 1.7718795788823627e-05\n",
      "step: 16235, loss: 0.0022138752974569798\n",
      "step: 16236, loss: 6.914134331736932e-08\n",
      "step: 16237, loss: 5.960462701182223e-08\n",
      "step: 16238, loss: 3.3855184256026405e-07\n",
      "step: 16239, loss: 4.76837147544984e-09\n",
      "step: 16240, loss: 2.355439391976688e-06\n",
      "step: 16241, loss: 5.555086204367399e-07\n",
      "step: 16242, loss: 5.722037599298346e-08\n",
      "step: 16243, loss: 1.0156442158404388e-06\n",
      "step: 16244, loss: 4.329220246290788e-06\n",
      "step: 16245, loss: 9.56047415456851e-07\n",
      "step: 16246, loss: 1.218283387061092e-06\n",
      "step: 16247, loss: 1.192092646817855e-08\n",
      "step: 16248, loss: 3.8146957592744e-08\n",
      "step: 16249, loss: 1.3708605592910317e-06\n",
      "step: 16250, loss: 2.622603467727913e-08\n",
      "step: 16251, loss: 0.0699591189622879\n",
      "step: 16252, loss: 2.0236344425939023e-05\n",
      "step: 16253, loss: 1.8358176134825044e-07\n",
      "step: 16254, loss: 0.1814989298582077\n",
      "step: 16255, loss: 0.06628790497779846\n",
      "step: 16256, loss: 4.0053964767139405e-07\n",
      "step: 16257, loss: 1.0490393975715051e-07\n",
      "step: 16258, loss: 8.364502718904987e-06\n",
      "step: 16259, loss: 7.470656782970764e-06\n",
      "step: 16260, loss: 6.675709585124423e-08\n",
      "step: 16261, loss: 3.89309752790723e-06\n",
      "step: 16262, loss: 2.6724935651145643e-06\n",
      "step: 16263, loss: 3.3185528991452884e-06\n",
      "step: 16264, loss: 0.01306139212101698\n",
      "step: 16265, loss: 0.00010367461800342426\n",
      "step: 16266, loss: 1.1864987754961476e-05\n",
      "step: 16267, loss: 5.4247750085778534e-05\n",
      "step: 16268, loss: 0.01170526072382927\n",
      "step: 16269, loss: 1.668929705544997e-08\n",
      "step: 16270, loss: 9.53674117454284e-09\n",
      "step: 16271, loss: 1.0823962384165497e-06\n",
      "step: 16272, loss: 9.76484898274066e-06\n",
      "step: 16273, loss: 0.41056880354881287\n",
      "step: 16274, loss: 6.198879276553271e-08\n",
      "step: 16275, loss: 3.2263040338875726e-05\n",
      "step: 16276, loss: 3.3378579900045224e-08\n",
      "step: 16277, loss: 2.5987503704527626e-07\n",
      "step: 16278, loss: 0.14396287500858307\n",
      "step: 16279, loss: 2.6226025795494934e-08\n",
      "step: 16280, loss: 0.08298392593860626\n",
      "step: 16281, loss: 6.437292654482007e-08\n",
      "step: 16282, loss: 5.960460214282648e-08\n",
      "step: 16283, loss: 1.2636162693979713e-07\n",
      "step: 16284, loss: 1.192092824453539e-08\n",
      "step: 16285, loss: 1.192092646817855e-08\n",
      "step: 16286, loss: 8.654491239212803e-07\n",
      "step: 16287, loss: 1.2564431699502165e-06\n",
      "step: 16288, loss: 1.9073478796372e-08\n",
      "step: 16289, loss: 1.25406336337619e-06\n",
      "step: 16290, loss: 3.719312360317417e-07\n",
      "step: 16291, loss: 3.0994396382766354e-08\n",
      "step: 16292, loss: 5.05440993947559e-07\n",
      "step: 16293, loss: 7.15255676908555e-09\n",
      "step: 16294, loss: 1.668929350273629e-08\n",
      "step: 16295, loss: 2.5510681211926567e-07\n",
      "step: 16296, loss: 0.0006533882115036249\n",
      "step: 16297, loss: 2.908687122271658e-07\n",
      "step: 16298, loss: 1.907348234908568e-08\n",
      "step: 16299, loss: 0.014727882109582424\n",
      "step: 16300, loss: 1.4066672804347036e-07\n",
      "step: 16301, loss: 4.7683684556432127e-08\n",
      "step: 16302, loss: 7.29207295080414e-06\n",
      "step: 16303, loss: 3.6927633573213825e-06\n",
      "step: 16304, loss: 8.582975965509831e-07\n",
      "step: 16305, loss: 2.5987500862356683e-07\n",
      "step: 16306, loss: 0.0\n",
      "step: 16307, loss: 4.76837147544984e-09\n",
      "step: 16308, loss: 8.249217557931843e-07\n",
      "step: 16309, loss: 1.9906970010197256e-06\n",
      "step: 16310, loss: 0.0722988098859787\n",
      "step: 16311, loss: 5.034815785620594e-06\n",
      "step: 16312, loss: 4.76837103136063e-09\n",
      "step: 16313, loss: 7.15255676908555e-09\n",
      "step: 16314, loss: 9.73944133875193e-06\n",
      "step: 16315, loss: 1.430511264999268e-08\n",
      "step: 16316, loss: 4.76837147544984e-09\n",
      "step: 16317, loss: 3.4925792533613276e-06\n",
      "step: 16318, loss: 0.0017928786110132933\n",
      "step: 16319, loss: 3.9815719787839043e-07\n",
      "step: 16320, loss: 4.2915321074588064e-08\n",
      "step: 16321, loss: 1.430510998545742e-08\n",
      "step: 16322, loss: 1.907348412544252e-08\n",
      "step: 16323, loss: 4.76837103136063e-09\n",
      "step: 16324, loss: 1.958731263584923e-05\n",
      "step: 16325, loss: 3.123264207260945e-07\n",
      "step: 16326, loss: 2.574915356490237e-07\n",
      "step: 16327, loss: 0.06425308436155319\n",
      "step: 16328, loss: 1.7213139926752774e-06\n",
      "step: 16329, loss: 3.8623485920652456e-07\n",
      "step: 16330, loss: 5.042224529461237e-06\n",
      "step: 16331, loss: 1.254060066457896e-06\n",
      "step: 16332, loss: 2.38418573772492e-09\n",
      "step: 16333, loss: 0.0004647943424060941\n",
      "step: 16334, loss: 3.362299321452156e-05\n",
      "step: 16335, loss: 1.192092646817855e-08\n",
      "step: 16336, loss: 1.3828248768277263e-07\n",
      "step: 16337, loss: 8.344643021018783e-08\n",
      "step: 16338, loss: 0.00014353399456012994\n",
      "step: 16339, loss: 1.5973762401699787e-06\n",
      "step: 16340, loss: 3.7193109392319457e-07\n",
      "step: 16341, loss: 2.861022529998536e-08\n",
      "step: 16342, loss: 8.984005035017617e-06\n",
      "step: 16343, loss: 7.15255632499634e-09\n",
      "step: 16344, loss: 6.103451823946671e-07\n",
      "step: 16345, loss: 5.0419848776073195e-06\n",
      "step: 16346, loss: 9.53674206272126e-09\n",
      "step: 16347, loss: 1.933525481945253e-06\n",
      "step: 16348, loss: 1.2540658644866198e-06\n",
      "step: 16349, loss: 1.4538369214278646e-05\n",
      "step: 16350, loss: 2.8846623081335565e-06\n",
      "step: 16351, loss: 0.0\n",
      "step: 16352, loss: 8.583059951661198e-08\n",
      "step: 16353, loss: 1.816667918319581e-06\n",
      "step: 16354, loss: 5.841196752953692e-07\n",
      "step: 16355, loss: 4.053112689916816e-08\n",
      "step: 16356, loss: 3.60009437372355e-07\n",
      "step: 16357, loss: 0.11958364397287369\n",
      "step: 16358, loss: 8.02356044005137e-06\n",
      "step: 16359, loss: 4.45797149950522e-06\n",
      "step: 16360, loss: 2.1194318833295256e-06\n",
      "step: 16361, loss: 4.179042207397288e-06\n",
      "step: 16362, loss: 1.2166970009275246e-05\n",
      "step: 16363, loss: 6.723345791215252e-07\n",
      "step: 16364, loss: 0.00046186737017706037\n",
      "step: 16365, loss: 2.1457660537294032e-08\n",
      "step: 16366, loss: 2.38418573772492e-09\n",
      "step: 16367, loss: 2.283966523464187e-06\n",
      "step: 16368, loss: 8.583058530575727e-08\n",
      "step: 16369, loss: 1.8119766309609986e-07\n",
      "step: 16370, loss: 2.8610209312773804e-08\n",
      "step: 16371, loss: 5.237711775407661e-06\n",
      "step: 16372, loss: 1.261222678294871e-06\n",
      "step: 16373, loss: 2.026550447453701e-07\n",
      "step: 16374, loss: 0.00014402180386241525\n",
      "step: 16375, loss: 1.2087597269783146e-06\n",
      "step: 16376, loss: 1.192092646817855e-08\n",
      "step: 16377, loss: 0.046137772500514984\n",
      "step: 16378, loss: 9.53674117454284e-09\n",
      "step: 16379, loss: 0.004967861343175173\n",
      "step: 16380, loss: 0.050508011132478714\n",
      "step: 16381, loss: 2.789482209664129e-07\n",
      "step: 16382, loss: 3.1471105899072427e-07\n",
      "step: 16383, loss: 3.1709561199022573e-07\n",
      "step: 16384, loss: 1.2159310358583753e-07\n",
      "step: 16385, loss: 1.907347702001516e-08\n",
      "step: 16386, loss: 4.601453440500336e-07\n",
      "step: 16387, loss: 9.059895234031501e-08\n",
      "step: 16388, loss: 8.958989928942174e-05\n",
      "step: 16389, loss: 1.668929527909313e-08\n",
      "step: 16390, loss: 4.1007817230820365e-07\n",
      "step: 16391, loss: 7.009431897131435e-07\n",
      "step: 16392, loss: 0.0\n",
      "step: 16393, loss: 4.76837103136063e-09\n",
      "step: 16394, loss: 7.892867870396003e-06\n",
      "step: 16395, loss: 1.907348412544252e-08\n",
      "step: 16396, loss: 0.00016340355796273798\n",
      "step: 16397, loss: 1.7093962014769204e-06\n",
      "step: 16398, loss: 4.5299501039153256e-08\n",
      "step: 16399, loss: 5.29285159700521e-07\n",
      "step: 16400, loss: 4.76837147544984e-09\n",
      "step: 16401, loss: 0.05652918294072151\n",
      "step: 16402, loss: 4.935212132295419e-07\n",
      "step: 16403, loss: 2.38418573772492e-09\n",
      "step: 16404, loss: 4.053114466273655e-08\n",
      "step: 16405, loss: 1.430511264999268e-08\n",
      "step: 16406, loss: 7.15255632499634e-09\n",
      "step: 16407, loss: 1.9073478796372e-08\n",
      "step: 16408, loss: 6.3195375332725234e-06\n",
      "step: 16409, loss: 2.38418573772492e-09\n",
      "step: 16410, loss: 5.793533546238905e-07\n",
      "step: 16411, loss: 9.53674117454284e-09\n",
      "step: 16412, loss: 2.3603318766163284e-07\n",
      "step: 16413, loss: 9.53674295089968e-09\n",
      "step: 16414, loss: 0.052128490060567856\n",
      "step: 16415, loss: 1.1872932645928813e-06\n",
      "step: 16416, loss: 1.3112979502238886e-07\n",
      "step: 16417, loss: 7.15255632499634e-09\n",
      "step: 16418, loss: 5.054451435171359e-07\n",
      "step: 16419, loss: 7.152545578037461e-08\n",
      "step: 16420, loss: 5.00678503101426e-08\n",
      "step: 16421, loss: 0.0\n",
      "step: 16422, loss: 1.4805244745730306e-06\n",
      "step: 16423, loss: 0.0040066661313176155\n",
      "step: 16424, loss: 2.622604000634965e-08\n",
      "step: 16425, loss: 3.0994396382766354e-08\n",
      "step: 16426, loss: 2.74179512871342e-07\n",
      "step: 16427, loss: 1.0728825117212182e-07\n",
      "step: 16428, loss: 2.408022226063622e-07\n",
      "step: 16429, loss: 6.508723231490876e-07\n",
      "step: 16430, loss: 0.041675858199596405\n",
      "step: 16431, loss: 6.198873592211385e-08\n",
      "step: 16432, loss: 1.9740546122193336e-06\n",
      "step: 16433, loss: 3.6477806020229764e-07\n",
      "step: 16434, loss: 3.95770996419742e-07\n",
      "step: 16435, loss: 5.722005198549596e-07\n",
      "step: 16436, loss: 9.0598852864332e-08\n",
      "step: 16437, loss: 4.6729522296118375e-07\n",
      "step: 16438, loss: 1.9790963051491417e-05\n",
      "step: 16439, loss: 1.4042384464119095e-06\n",
      "step: 16440, loss: 4.5299508144580614e-08\n",
      "step: 16441, loss: 0.18572509288787842\n",
      "step: 16442, loss: 2.6226025795494934e-08\n",
      "step: 16443, loss: 1.668929883180681e-08\n",
      "step: 16444, loss: 3.647771791293053e-07\n",
      "step: 16445, loss: 1.2924908332934137e-05\n",
      "step: 16446, loss: 5.9604552404834976e-08\n",
      "step: 16447, loss: 2.38418573772492e-09\n",
      "step: 16448, loss: 4.76837147544984e-09\n",
      "step: 16449, loss: 4.959051125297265e-07\n",
      "step: 16450, loss: 1.6450853479454963e-07\n",
      "step: 16451, loss: 9.432039405510295e-06\n",
      "step: 16452, loss: 4.529948327558486e-08\n",
      "step: 16453, loss: 2.8133209184488805e-07\n",
      "step: 16454, loss: 9.059886707518672e-08\n",
      "step: 16455, loss: 5.137359403306618e-05\n",
      "step: 16456, loss: 1.1205654004697863e-07\n",
      "step: 16457, loss: 1.0251979176700843e-07\n",
      "step: 16458, loss: 7.76147408032557e-06\n",
      "step: 16459, loss: 1.3606518223241437e-05\n",
      "step: 16460, loss: 2.8624283004319295e-05\n",
      "step: 16461, loss: 4.145669663557783e-06\n",
      "step: 16462, loss: 4.029234332847409e-07\n",
      "step: 16463, loss: 2.861022529998536e-08\n",
      "step: 16464, loss: 9.012019859255815e-07\n",
      "step: 16465, loss: 5.044379122409737e-06\n",
      "step: 16466, loss: 2.6226025795494934e-08\n",
      "step: 16467, loss: 0.21143968403339386\n",
      "step: 16468, loss: 8.712300768820569e-06\n",
      "step: 16469, loss: 5.737915671488736e-06\n",
      "step: 16470, loss: 1.3351406380479602e-07\n",
      "step: 16471, loss: 2.622588226586231e-07\n",
      "step: 16472, loss: 1.1205660399582484e-07\n",
      "step: 16473, loss: 1.0967241337311862e-07\n",
      "step: 16474, loss: 7.247837743307173e-07\n",
      "step: 16475, loss: 8.106221827119953e-08\n",
      "step: 16476, loss: 2.38418573772492e-09\n",
      "step: 16477, loss: 2.9563724979198014e-07\n",
      "step: 16478, loss: 1.4305095419331337e-07\n",
      "step: 16479, loss: 5.984247195556236e-07\n",
      "step: 16480, loss: 2.38418529363571e-08\n",
      "step: 16481, loss: 5.483619602841827e-08\n",
      "step: 16482, loss: 1.192092646817855e-08\n",
      "step: 16483, loss: 2.861022352362852e-08\n",
      "step: 16484, loss: 1.907348234908568e-08\n",
      "step: 16485, loss: 7.152459602366434e-07\n",
      "step: 16486, loss: 1.907348412544252e-08\n",
      "step: 16487, loss: 5.245204093284883e-08\n",
      "step: 16488, loss: 9.53674206272126e-09\n",
      "step: 16489, loss: 2.8108195238019107e-06\n",
      "step: 16490, loss: 4.053114111002287e-08\n",
      "step: 16491, loss: 6.648564522038214e-06\n",
      "step: 16492, loss: 6.5554504544707015e-06\n",
      "step: 16493, loss: 9.202815931530495e-07\n",
      "step: 16494, loss: 5.245182705948537e-07\n",
      "step: 16495, loss: 2.38418573772492e-09\n",
      "step: 16496, loss: 1.621241949578689e-07\n",
      "step: 16497, loss: 8.511366331731551e-07\n",
      "step: 16498, loss: 5.960457372111705e-08\n",
      "step: 16499, loss: 4.291530686373335e-08\n",
      "step: 16500, loss: 9.53674206272126e-09\n",
      "step: 16501, loss: 2.38418573772492e-09\n",
      "step: 16502, loss: 1.1682478628927129e-07\n",
      "step: 16503, loss: 1.430511264999268e-08\n",
      "step: 16504, loss: 4.7683684556432127e-08\n",
      "step: 16505, loss: 3.838519830878795e-07\n",
      "step: 16506, loss: 3.814696114545768e-08\n",
      "step: 16507, loss: 6.198879987096007e-08\n",
      "step: 16508, loss: 4.768369876728684e-08\n",
      "step: 16509, loss: 3.8862017959218065e-07\n",
      "step: 16510, loss: 5.0067846757428924e-08\n",
      "step: 16511, loss: 7.811492650944274e-06\n",
      "step: 16512, loss: 1.907348234908568e-08\n",
      "step: 16513, loss: 4.3630242885228654e-07\n",
      "step: 16514, loss: 1.6617109395156149e-06\n",
      "step: 16515, loss: 2.3602087821927853e-06\n",
      "step: 16516, loss: 2.384185116000026e-08\n",
      "step: 16517, loss: 3.6001031844534737e-07\n",
      "step: 16518, loss: 5.72196654502477e-07\n",
      "step: 16519, loss: 8.106225379833631e-08\n",
      "step: 16520, loss: 1.7642935290496098e-07\n",
      "step: 16521, loss: 1.096724489002554e-07\n",
      "step: 16522, loss: 3.926376848539803e-06\n",
      "step: 16523, loss: 8.344633073420482e-08\n",
      "step: 16524, loss: 1.0185043720412068e-05\n",
      "step: 16525, loss: 3.5762763417324095e-08\n",
      "step: 16526, loss: 7.700808737354237e-07\n",
      "step: 16527, loss: 0.04118775948882103\n",
      "step: 16528, loss: 8.781458745943382e-06\n",
      "step: 16529, loss: 1.5186769815045409e-06\n",
      "step: 16530, loss: 7.15255632499634e-09\n",
      "step: 16531, loss: 1.668929705544997e-08\n",
      "step: 16532, loss: 7.12861606189108e-07\n",
      "step: 16533, loss: 5.459735348267714e-07\n",
      "step: 16534, loss: 1.430511264999268e-08\n",
      "step: 16535, loss: 1.2707328096439596e-06\n",
      "step: 16536, loss: 6.857086555100977e-05\n",
      "step: 16537, loss: 0.1363546997308731\n",
      "step: 16538, loss: 1.5735608371869603e-07\n",
      "step: 16539, loss: 2.38418573772492e-09\n",
      "step: 16540, loss: 7.15255676908555e-09\n",
      "step: 16541, loss: 2.3364897572264454e-07\n",
      "step: 16542, loss: 1.4543508086717338e-07\n",
      "step: 16543, loss: 1.3589833258720319e-07\n",
      "step: 16544, loss: 5.245201961656676e-08\n",
      "step: 16545, loss: 1.668929883180681e-08\n",
      "step: 16546, loss: 1.0466420690136147e-06\n",
      "step: 16547, loss: 1.192092558000013e-08\n",
      "step: 16548, loss: 1.1205663241753427e-07\n",
      "step: 16549, loss: 4.76837147544984e-09\n",
      "step: 16550, loss: 1.0418701776870876e-06\n",
      "step: 16551, loss: 0.0\n",
      "step: 16552, loss: 4.792157710653555e-07\n",
      "step: 16553, loss: 5.984219910715183e-07\n",
      "step: 16554, loss: 1.621243796989802e-07\n",
      "step: 16555, loss: 2.455703622672445e-07\n",
      "step: 16556, loss: 0.12219125032424927\n",
      "step: 16557, loss: 4.76837103136063e-09\n",
      "step: 16558, loss: 3.0396386136999354e-06\n",
      "step: 16559, loss: 7.390962508679877e-08\n",
      "step: 16560, loss: 2.38418573772492e-09\n",
      "step: 16561, loss: 1.7166102850296738e-07\n",
      "step: 16562, loss: 0.004992072936147451\n",
      "step: 16563, loss: 7.15255676908555e-09\n",
      "step: 16564, loss: 9.53674206272126e-09\n",
      "step: 16565, loss: 7.15255632499634e-09\n",
      "step: 16566, loss: 1.907348234908568e-08\n",
      "step: 16567, loss: 2.145766764272139e-08\n",
      "step: 16568, loss: 3.5762766970037774e-08\n",
      "step: 16569, loss: 0.11838670074939728\n",
      "step: 16570, loss: 1.7881328062685498e-07\n",
      "step: 16571, loss: 3.099439993548003e-08\n",
      "step: 16572, loss: 1.9740093648579204e-06\n",
      "step: 16573, loss: 7.15255632499634e-09\n",
      "step: 16574, loss: 1.192092646817855e-08\n",
      "step: 16575, loss: 1.0728827248840389e-07\n",
      "step: 16576, loss: 1.8596591644381988e-07\n",
      "step: 16577, loss: 1.907347702001516e-08\n",
      "step: 16578, loss: 0.00030424733995459974\n",
      "step: 16579, loss: 6.437296207195686e-08\n",
      "step: 16580, loss: 3.576277052275145e-08\n",
      "step: 16581, loss: 8.821468355790785e-08\n",
      "step: 16582, loss: 2.8610209312773804e-08\n",
      "step: 16583, loss: 0.013243610970675945\n",
      "step: 16584, loss: 3.042422940779943e-05\n",
      "step: 16585, loss: 2.145766764272139e-08\n",
      "step: 16586, loss: 3.8146957592744e-08\n",
      "step: 16587, loss: 8.106221827119953e-08\n",
      "step: 16588, loss: 3.814679701008572e-07\n",
      "step: 16589, loss: 6.008078230479441e-07\n",
      "step: 16590, loss: 5.960459148468544e-08\n",
      "step: 16591, loss: 9.53674117454284e-09\n",
      "step: 16592, loss: 5.890724878554465e-06\n",
      "step: 16593, loss: 1.764293671158157e-07\n",
      "step: 16594, loss: 4.2915317521874385e-08\n",
      "step: 16595, loss: 1.668929527909313e-08\n",
      "step: 16596, loss: 1.668929883180681e-08\n",
      "step: 16597, loss: 0.0\n",
      "step: 16598, loss: 0.004398650489747524\n",
      "step: 16599, loss: 9.53674295089968e-09\n",
      "step: 16600, loss: 4.053114111002287e-08\n",
      "step: 16601, loss: 4.76837147544984e-09\n",
      "step: 16602, loss: 4.076916866324609e-07\n",
      "step: 16603, loss: 1.0013572904199464e-07\n",
      "step: 16604, loss: 2.38418573772492e-09\n",
      "step: 16605, loss: 4.743967565445928e-06\n",
      "step: 16606, loss: 8.726007081349962e-07\n",
      "step: 16607, loss: 0.0019568479619920254\n",
      "step: 16608, loss: 0.06481187045574188\n",
      "step: 16609, loss: 7.796167551532562e-07\n",
      "step: 16610, loss: 1.5020317789549154e-07\n",
      "step: 16611, loss: 5.435906587081263e-07\n",
      "step: 16612, loss: 3.8146946934602965e-08\n",
      "step: 16613, loss: 1.6689277515524736e-07\n",
      "step: 16614, loss: 1.430511176181426e-08\n",
      "step: 16615, loss: 7.15255632499634e-09\n",
      "step: 16616, loss: 9.53674295089968e-09\n",
      "step: 16617, loss: 1.4066664277834207e-07\n",
      "step: 16618, loss: 3.218625010958931e-07\n",
      "step: 16619, loss: 1.668929527909313e-08\n",
      "step: 16620, loss: 5.9604616353681195e-08\n",
      "step: 16621, loss: 1.4305065576536435e-07\n",
      "step: 16622, loss: 5.483625642455081e-08\n",
      "step: 16623, loss: 1.668929883180681e-08\n",
      "step: 16624, loss: 8.344639468305104e-08\n",
      "step: 16625, loss: 1.3017232731726835e-06\n",
      "step: 16626, loss: 2.002708185955271e-07\n",
      "step: 16627, loss: 4.053112334645448e-08\n",
      "step: 16628, loss: 5.269014877740119e-07\n",
      "step: 16629, loss: 2.145766764272139e-08\n",
      "step: 16630, loss: 3.1326135285780765e-06\n",
      "step: 16631, loss: 0.08137805759906769\n",
      "step: 16632, loss: 4.053114111002287e-08\n",
      "step: 16633, loss: 2.09807481610369e-07\n",
      "step: 16634, loss: 4.76837147544984e-09\n",
      "step: 16635, loss: 8.630566981082666e-07\n",
      "step: 16636, loss: 1.2993410791750648e-06\n",
      "step: 16637, loss: 1.9550245156096935e-07\n",
      "step: 16638, loss: 0.04668708145618439\n",
      "step: 16639, loss: 1.8596600170894817e-07\n",
      "step: 16640, loss: 0.00017898742225952446\n",
      "step: 16641, loss: 1.907348234908568e-08\n",
      "step: 16642, loss: 1.192092646817855e-08\n",
      "step: 16643, loss: 1.7404504148998967e-07\n",
      "step: 16644, loss: 6.217287136678351e-06\n",
      "step: 16645, loss: 4.503221134655178e-06\n",
      "step: 16646, loss: 0.1006164476275444\n",
      "step: 16647, loss: 1.8119757783097157e-07\n",
      "step: 16648, loss: 0.0013092750450596213\n",
      "step: 16649, loss: 7.15255676908555e-09\n",
      "step: 16650, loss: 1.955023947175505e-07\n",
      "step: 16651, loss: 2.3126553116981086e-07\n",
      "step: 16652, loss: 9.298315717387595e-08\n",
      "step: 16653, loss: 1.2636172641578014e-07\n",
      "step: 16654, loss: 7.15255632499634e-09\n",
      "step: 16655, loss: 1.168221615444054e-06\n",
      "step: 16656, loss: 4.768368100371845e-08\n",
      "step: 16657, loss: 2.384184938364342e-08\n",
      "step: 16658, loss: 4.0769350562186446e-07\n",
      "step: 16659, loss: 1.5145405086514074e-05\n",
      "step: 16660, loss: 4.76837103136063e-09\n",
      "step: 16661, loss: 4.601425587225094e-07\n",
      "step: 16662, loss: 2.6941179953610117e-07\n",
      "step: 16663, loss: 1.668929527909313e-08\n",
      "step: 16664, loss: 0.13405166566371918\n",
      "step: 16665, loss: 2.861021997091484e-08\n",
      "step: 16666, loss: 4.76837103136063e-09\n",
      "step: 16667, loss: 7.629380149865028e-08\n",
      "step: 16668, loss: 8.344640889390575e-08\n",
      "step: 16669, loss: 4.76837103136063e-09\n",
      "step: 16670, loss: 0.0\n",
      "step: 16671, loss: 2.956369371531764e-07\n",
      "step: 16672, loss: 2.622603290092229e-08\n",
      "step: 16673, loss: 4.768368100371845e-08\n",
      "step: 16674, loss: 2.1219199197730632e-07\n",
      "step: 16675, loss: 1.263615985180877e-07\n",
      "step: 16676, loss: 1.2874575361365714e-07\n",
      "step: 16677, loss: 1.9073478796372e-08\n",
      "step: 16678, loss: 6.890188046781986e-07\n",
      "step: 16679, loss: 3.4782365219143685e-06\n",
      "step: 16680, loss: 4.76837103136063e-09\n",
      "step: 16681, loss: 1.029943177854875e-06\n",
      "step: 16682, loss: 5.769654762843857e-07\n",
      "step: 16683, loss: 2.2172820024479734e-07\n",
      "step: 16684, loss: 2.338774265808752e-06\n",
      "step: 16685, loss: 6.675709585124423e-08\n",
      "step: 16686, loss: 4.558043656288646e-06\n",
      "step: 16687, loss: 3.475931862340076e-06\n",
      "step: 16688, loss: 4.872803401667625e-06\n",
      "step: 16689, loss: 7.152547709665669e-08\n",
      "step: 16690, loss: 0.0\n",
      "step: 16691, loss: 2.8155266136309365e-06\n",
      "step: 16692, loss: 7.343163588302559e-07\n",
      "step: 16693, loss: 9.059902339458858e-08\n",
      "step: 16694, loss: 1.0013572904199464e-07\n",
      "step: 16695, loss: 1.7642196326050907e-06\n",
      "step: 16696, loss: 1.1682495681952787e-07\n",
      "step: 16697, loss: 5.00678503101426e-08\n",
      "step: 16698, loss: 7.15255676908555e-09\n",
      "step: 16699, loss: 4.720659489976242e-07\n",
      "step: 16700, loss: 0.05945030227303505\n",
      "step: 16701, loss: 0.17282144725322723\n",
      "step: 16702, loss: 1.0132572469956358e-06\n",
      "step: 16703, loss: 1.8834998627426103e-07\n",
      "step: 16704, loss: 1.9073478796372e-08\n",
      "step: 16705, loss: 7.152554815093026e-08\n",
      "step: 16706, loss: 1.192092646817855e-08\n",
      "step: 16707, loss: 1.0490395396800523e-07\n",
      "step: 16708, loss: 2.1457660537294032e-08\n",
      "step: 16709, loss: 2.5987566232288373e-07\n",
      "step: 16710, loss: 2.884851539874944e-07\n",
      "step: 16711, loss: 0.07715311646461487\n",
      "step: 16712, loss: 3.337858700547258e-08\n",
      "step: 16713, loss: 0.00037277466617524624\n",
      "step: 16714, loss: 0.11163342744112015\n",
      "step: 16715, loss: 2.861022352362852e-08\n",
      "step: 16716, loss: 3.5285765420667303e-07\n",
      "step: 16717, loss: 5.722041862554761e-08\n",
      "step: 16718, loss: 1.668929883180681e-08\n",
      "step: 16719, loss: 2.019306066358695e-06\n",
      "step: 16720, loss: 7.629382992035971e-08\n",
      "step: 16721, loss: 2.8848509714407555e-07\n",
      "step: 16722, loss: 2.38418573772492e-09\n",
      "step: 16723, loss: 4.5299501039153256e-08\n",
      "step: 16724, loss: 3.8556319736926525e-07\n",
      "step: 16725, loss: 2.145766764272139e-08\n",
      "step: 16726, loss: 4.172292449311499e-07\n",
      "step: 16727, loss: 9.059896655116972e-08\n",
      "step: 16728, loss: 5.245207290727194e-08\n",
      "step: 16729, loss: 2.38418573772492e-09\n",
      "step: 16730, loss: 1.0013350220106076e-06\n",
      "step: 16731, loss: 1.1920899112283223e-07\n",
      "step: 16732, loss: 3.123264207260945e-07\n",
      "step: 16733, loss: 1.4972155213399674e-06\n",
      "step: 16734, loss: 1.2397727289226168e-07\n",
      "step: 16735, loss: 1.668929350273629e-08\n",
      "step: 16736, loss: 1.6450827899916476e-07\n",
      "step: 16737, loss: 5.960456661568969e-08\n",
      "step: 16738, loss: 3.5762763417324095e-08\n",
      "step: 16739, loss: 0.18550890684127808\n",
      "step: 16740, loss: 8.821475461218142e-08\n",
      "step: 16741, loss: 0.20044049620628357\n",
      "step: 16742, loss: 1.475759290769929e-06\n",
      "step: 16743, loss: 4.6014520194148645e-07\n",
      "step: 16744, loss: 7.867808449191216e-08\n",
      "step: 16745, loss: 0.006932971533387899\n",
      "step: 16746, loss: 4.3630242885228654e-07\n",
      "step: 16747, loss: 4.713049747806508e-06\n",
      "step: 16748, loss: 8.749796620577399e-07\n",
      "step: 16749, loss: 2.38418573772492e-09\n",
      "step: 16750, loss: 1.1444075198596693e-07\n",
      "step: 16751, loss: 2.4627597667858936e-06\n",
      "step: 16752, loss: 1.1444066672083864e-07\n",
      "step: 16753, loss: 8.583057109490255e-08\n",
      "step: 16754, loss: 3.404352810321143e-06\n",
      "step: 16755, loss: 3.099440348819371e-08\n",
      "step: 16756, loss: 5.4836245766409775e-08\n",
      "step: 16757, loss: 1.108619926526444e-06\n",
      "step: 16758, loss: 2.622604000634965e-08\n",
      "step: 16759, loss: 1.8358150555286556e-07\n",
      "step: 16760, loss: 3.337857279461787e-08\n",
      "step: 16761, loss: 2.8370600375637878e-06\n",
      "step: 16762, loss: 9.53674206272126e-09\n",
      "step: 16763, loss: 1.192092558000013e-08\n",
      "step: 16764, loss: 7.15255676908555e-09\n",
      "step: 16765, loss: 9.53674117454284e-09\n",
      "step: 16766, loss: 6.556406333402265e-07\n",
      "step: 16767, loss: 2.145766764272139e-08\n",
      "step: 16768, loss: 8.344643021018783e-08\n",
      "step: 16769, loss: 1.058563839251292e-06\n",
      "step: 16770, loss: 2.622603290092229e-08\n",
      "step: 16771, loss: 4.0531130451881836e-08\n",
      "step: 16772, loss: 2.5987549179262714e-07\n",
      "step: 16773, loss: 1.9550274998891837e-07\n",
      "step: 16774, loss: 1.6712447177269496e-06\n",
      "step: 16775, loss: 5.483570362230239e-07\n",
      "step: 16776, loss: 6.580245894838299e-07\n",
      "step: 16777, loss: 2.6389887352706864e-05\n",
      "step: 16778, loss: 2.38418573772492e-09\n",
      "step: 16779, loss: 4.1484418034087867e-07\n",
      "step: 16780, loss: 0.053912822157144547\n",
      "step: 16781, loss: 3.552407008555747e-07\n",
      "step: 16782, loss: 5.7220436389116e-08\n",
      "step: 16783, loss: 1.478190227999221e-07\n",
      "step: 16784, loss: 1.525875461538817e-07\n",
      "step: 16785, loss: 1.7881322378343611e-07\n",
      "step: 16786, loss: 1.907348234908568e-08\n",
      "step: 16787, loss: 1.6927647550346592e-07\n",
      "step: 16788, loss: 1.430511264999268e-08\n",
      "step: 16789, loss: 1.9311822541112633e-07\n",
      "step: 16790, loss: 1.8596561801587086e-07\n",
      "step: 16791, loss: 4.76837103136063e-09\n",
      "step: 16792, loss: 1.668929527909313e-08\n",
      "step: 16793, loss: 3.576277762817881e-08\n",
      "step: 16794, loss: 2.9802183121319104e-07\n",
      "step: 16795, loss: 4.124601673538564e-07\n",
      "step: 16796, loss: 1.6689267567926436e-07\n",
      "step: 16797, loss: 3.3208964396180818e-06\n",
      "step: 16798, loss: 6.60412752040429e-07\n",
      "step: 16799, loss: 4.3391935378167545e-07\n",
      "step: 16800, loss: 3.125459670627606e-06\n",
      "step: 16801, loss: 2.786919367281371e-06\n",
      "step: 16802, loss: 1.5735570002561872e-07\n",
      "step: 16803, loss: 2.174345127059496e-06\n",
      "step: 16804, loss: 1.001356508822937e-07\n",
      "step: 16805, loss: 3.33785834527589e-08\n",
      "step: 16806, loss: 1.192092558000013e-08\n",
      "step: 16807, loss: 1.0490408186569766e-07\n",
      "step: 16808, loss: 8.583054267319312e-08\n",
      "step: 16809, loss: 3.8146950487316644e-08\n",
      "step: 16810, loss: 3.147100642308942e-07\n",
      "step: 16811, loss: 9.727243650559103e-07\n",
      "step: 16812, loss: 1.3828257294790092e-07\n",
      "step: 16813, loss: 9.298115060119017e-07\n",
      "step: 16814, loss: 0.0\n",
      "step: 16815, loss: 0.0005137930274941027\n",
      "step: 16816, loss: 8.106226800919103e-08\n",
      "step: 16817, loss: 1.5806857618372305e-06\n",
      "step: 16818, loss: 2.124196726072114e-06\n",
      "step: 16819, loss: 5.006786096828364e-08\n",
      "step: 16820, loss: 1.5258251551131252e-06\n",
      "step: 16821, loss: 0.0\n",
      "step: 16822, loss: 1.7213293403983698e-06\n",
      "step: 16823, loss: 0.026628341525793076\n",
      "step: 16824, loss: 8.821483987730971e-08\n",
      "step: 16825, loss: 1.430511264999268e-08\n",
      "step: 16826, loss: 8.559080697523314e-07\n",
      "step: 16827, loss: 4.6729522296118375e-07\n",
      "step: 16828, loss: 8.821468355790785e-08\n",
      "step: 16829, loss: 0.01588856428861618\n",
      "step: 16830, loss: 1.3589833258720319e-07\n",
      "step: 16831, loss: 0.10311120003461838\n",
      "step: 16832, loss: 1.5735581371245644e-07\n",
      "step: 16833, loss: 3.814696469817136e-08\n",
      "step: 16834, loss: 7.343163588302559e-07\n",
      "step: 16835, loss: 7.15255632499634e-09\n",
      "step: 16836, loss: 6.675715980009045e-08\n",
      "step: 16837, loss: 4.76837103136063e-09\n",
      "step: 16838, loss: 5.0067868073711e-08\n",
      "step: 16839, loss: 1.3351200323086232e-06\n",
      "step: 16840, loss: 4.291529975830599e-08\n",
      "step: 16841, loss: 9.512678502687777e-07\n",
      "step: 16842, loss: 5.483620668655931e-08\n",
      "step: 16843, loss: 1.1682492129239108e-07\n",
      "step: 16844, loss: 0.00016630075697321445\n",
      "step: 16845, loss: 1.1190856639586855e-05\n",
      "step: 16846, loss: 3.099439993548003e-08\n",
      "step: 16847, loss: 2.622603467727913e-08\n",
      "step: 16848, loss: 3.5762763417324095e-08\n",
      "step: 16849, loss: 9.53674206272126e-09\n",
      "step: 16850, loss: 3.933889445306704e-07\n",
      "step: 16851, loss: 0.049574922770261765\n",
      "step: 16852, loss: 0.04557771608233452\n",
      "step: 16853, loss: 7.629384413121443e-08\n",
      "step: 16854, loss: 1.7761408344085794e-06\n",
      "step: 16855, loss: 1.406666854109062e-07\n",
      "step: 16856, loss: 9.536721279346239e-08\n",
      "step: 16857, loss: 0.20778010785579681\n",
      "step: 16858, loss: 1.2612031241587829e-06\n",
      "step: 16859, loss: 4.76837103136063e-09\n",
      "step: 16860, loss: 4.839843654735887e-07\n",
      "step: 16861, loss: 6.914129357937782e-08\n",
      "step: 16862, loss: 9.536731937487275e-08\n",
      "step: 16863, loss: 1.0490395396800523e-07\n",
      "step: 16864, loss: 2.8610211089130644e-08\n",
      "step: 16865, loss: 2.3841845830929742e-08\n",
      "step: 16866, loss: 7.629390097463329e-08\n",
      "step: 16867, loss: 1.764295660677817e-07\n",
      "step: 16868, loss: 7.15255676908555e-09\n",
      "step: 16869, loss: 0.0\n",
      "step: 16870, loss: 1.7881316694001725e-07\n",
      "step: 16871, loss: 0.06638748943805695\n",
      "step: 16872, loss: 1.8572026192487101e-06\n",
      "step: 16873, loss: 1.430511264999268e-08\n",
      "step: 16874, loss: 1.645084211077119e-07\n",
      "step: 16875, loss: 8.583058530575727e-08\n",
      "step: 16876, loss: 6.914128647395046e-08\n",
      "step: 16877, loss: 4.76837103136063e-09\n",
      "step: 16878, loss: 4.994246864953311e-06\n",
      "step: 16879, loss: 4.076917718975892e-07\n",
      "step: 16880, loss: 0.0\n",
      "step: 16881, loss: 5.507401397153444e-07\n",
      "step: 16882, loss: 1.5497178651457943e-07\n",
      "step: 16883, loss: 0.008584876544773579\n",
      "step: 16884, loss: 2.38418573772492e-09\n",
      "step: 16885, loss: 5.2452058696417225e-08\n",
      "step: 16886, loss: 0.15071773529052734\n",
      "step: 16887, loss: 0.08423911035060883\n",
      "step: 16888, loss: 5.9604584379258085e-08\n",
      "step: 16889, loss: 3.480887187379267e-07\n",
      "step: 16890, loss: 3.0755839475204994e-07\n",
      "step: 16891, loss: 1.335099682364671e-06\n",
      "step: 16892, loss: 1.9073404189384746e-07\n",
      "step: 16893, loss: 1.0180344816035358e-06\n",
      "step: 16894, loss: 3.604597623052541e-06\n",
      "step: 16895, loss: 2.9823044314980507e-05\n",
      "step: 16896, loss: 1.859661296066406e-07\n",
      "step: 16897, loss: 0.0\n",
      "step: 16898, loss: 4.291532462730174e-08\n",
      "step: 16899, loss: 2.86101197843891e-07\n",
      "step: 16900, loss: 9.536728384773596e-08\n",
      "step: 16901, loss: 7.629388676377857e-08\n",
      "step: 16902, loss: 0.0\n",
      "step: 16903, loss: 6.437292654482007e-08\n",
      "step: 16904, loss: 9.298314296302124e-08\n",
      "step: 16905, loss: 1.0013563667143899e-07\n",
      "step: 16906, loss: 4.76837147544984e-09\n",
      "step: 16907, loss: 2.3603303134223097e-07\n",
      "step: 16908, loss: 4.76837147544984e-09\n",
      "step: 16909, loss: 3.814695404003032e-08\n",
      "step: 16910, loss: 5.9604552404834976e-08\n",
      "step: 16911, loss: 5.0067875179138355e-08\n",
      "step: 16912, loss: 1.9311859489334893e-07\n",
      "step: 16913, loss: 1.006104753287218e-06\n",
      "step: 16914, loss: 1.668929527909313e-08\n",
      "step: 16915, loss: 0.00017003683024086058\n",
      "step: 16916, loss: 7.033258384581131e-07\n",
      "step: 16917, loss: 0.0\n",
      "step: 16918, loss: 0.05718514323234558\n",
      "step: 16919, loss: 1.668929350273629e-08\n",
      "step: 16920, loss: 1.668929350273629e-08\n",
      "step: 16921, loss: 2.8369906885927776e-06\n",
      "step: 16922, loss: 5.4836217344700344e-08\n",
      "step: 16923, loss: 3.814693982917561e-08\n",
      "step: 16924, loss: 8.583063504374877e-08\n",
      "step: 16925, loss: 2.4318563873748644e-07\n",
      "step: 16926, loss: 5.006737637813785e-07\n",
      "step: 16927, loss: 0.1683182567358017\n",
      "step: 16928, loss: 1.192092558000013e-08\n",
      "step: 16929, loss: 1.9549465832824353e-06\n",
      "step: 16930, loss: 1.192092646817855e-08\n",
      "step: 16931, loss: 1.907347702001516e-08\n",
      "step: 16932, loss: 4.2438071545802813e-07\n",
      "step: 16933, loss: 1.192092646817855e-08\n",
      "step: 16934, loss: 3.8861898588038457e-07\n",
      "step: 16935, loss: 1.2874573940280243e-07\n",
      "step: 16936, loss: 6.770974323444534e-07\n",
      "step: 16937, loss: 0.0\n",
      "step: 16938, loss: 8.582891837249917e-07\n",
      "step: 16939, loss: 2.145766764272139e-08\n",
      "step: 16940, loss: 2.1934472727025423e-07\n",
      "step: 16941, loss: 0.00014280792674981058\n",
      "step: 16942, loss: 2.264969651832871e-07\n",
      "step: 16943, loss: 6.198875013296856e-08\n",
      "step: 16944, loss: 5.340513666851621e-07\n",
      "step: 16945, loss: 1.8835035575648362e-07\n",
      "step: 16946, loss: 3.3378579900045224e-08\n",
      "step: 16947, loss: 0.00022256739612203091\n",
      "step: 16948, loss: 1.723715854495822e-06\n",
      "step: 16949, loss: 1.5497158756261342e-07\n",
      "step: 16950, loss: 1.0020888794315397e-06\n",
      "step: 16951, loss: 0.010425235144793987\n",
      "step: 16952, loss: 4.76837103136063e-09\n",
      "step: 16953, loss: 2.9205250484665157e-06\n",
      "step: 16954, loss: 9.0598852864332e-08\n",
      "step: 16955, loss: 4.291530686373335e-08\n",
      "step: 16956, loss: 2.861021997091484e-08\n",
      "step: 16957, loss: 1.8619639376993291e-06\n",
      "step: 16958, loss: 1.430511176181426e-08\n",
      "step: 16959, loss: 1.4066679909774393e-07\n",
      "step: 16960, loss: 9.53674295089968e-09\n",
      "step: 16961, loss: 1.6689277515524736e-07\n",
      "step: 16962, loss: 2.3841805329993804e-07\n",
      "step: 16963, loss: 1.4185409327183152e-06\n",
      "step: 16964, loss: 1.2874575361365714e-07\n",
      "step: 16965, loss: 5.2452055143703546e-08\n",
      "step: 16966, loss: 3.480900261365605e-07\n",
      "step: 16967, loss: 7.867805607020273e-08\n",
      "step: 16968, loss: 4.053114466273655e-08\n",
      "step: 16969, loss: 2.0742349704505614e-07\n",
      "step: 16970, loss: 5.4836245766409775e-08\n",
      "step: 16971, loss: 2.622604000634965e-08\n",
      "step: 16972, loss: 2.38418573772492e-09\n",
      "step: 16973, loss: 1.192092646817855e-08\n",
      "step: 16974, loss: 7.15255676908555e-09\n",
      "step: 16975, loss: 6.198875723839592e-08\n",
      "step: 16976, loss: 2.1265827854222152e-06\n",
      "step: 16977, loss: 1.3828244505020848e-07\n",
      "step: 16978, loss: 2.145766586636455e-08\n",
      "step: 16979, loss: 7.15255676908555e-09\n",
      "step: 16980, loss: 2.3841806751079275e-07\n",
      "step: 16981, loss: 5.674335739058733e-07\n",
      "step: 16982, loss: 3.0994396382766354e-08\n",
      "step: 16983, loss: 9.775158105185255e-06\n",
      "step: 16984, loss: 2.33649501524269e-07\n",
      "step: 16985, loss: 5.245204803827619e-08\n",
      "step: 16986, loss: 1.4471511349256616e-06\n",
      "step: 16987, loss: 2.9086984909554303e-07\n",
      "step: 16988, loss: 2.0265478894998523e-07\n",
      "step: 16989, loss: 4.86368776364543e-07\n",
      "step: 16990, loss: 3.5762763417324095e-08\n",
      "step: 16991, loss: 6.055741437194229e-07\n",
      "step: 16992, loss: 2.38418573772492e-09\n",
      "step: 16993, loss: 6.580251579180185e-07\n",
      "step: 16994, loss: 1.0871598306039232e-06\n",
      "step: 16995, loss: 8.773688477958785e-07\n",
      "step: 16996, loss: 3.0994280564300425e-07\n",
      "step: 16997, loss: 2.861021997091484e-08\n",
      "step: 16998, loss: 5.006786096828364e-08\n",
      "step: 16999, loss: 1.192092824453539e-08\n",
      "step: 17000, loss: 1.668929527909313e-08\n",
      "step: 17001, loss: 2.438873707433231e-06\n",
      "step: 17002, loss: 3.6954713777959114e-07\n",
      "step: 17003, loss: 0.00035146650043316185\n",
      "step: 17004, loss: 1.430511264999268e-08\n",
      "step: 17005, loss: 5.245205159098987e-08\n",
      "step: 17006, loss: 8.821478303389085e-08\n",
      "step: 17007, loss: 6.19887430275412e-08\n",
      "step: 17008, loss: 4.053114821545023e-08\n",
      "step: 17009, loss: 1.0967224284286203e-07\n",
      "step: 17010, loss: 2.3364964363281615e-07\n",
      "step: 17011, loss: 7.15255632499634e-09\n",
      "step: 17012, loss: 6.437296207195686e-08\n",
      "step: 17013, loss: 2.38418529363571e-08\n",
      "step: 17014, loss: 1.430511264999268e-08\n",
      "step: 17015, loss: 3.4570439311210066e-07\n",
      "step: 17016, loss: 0.06942076981067657\n",
      "step: 17017, loss: 2.622603645363597e-08\n",
      "step: 17018, loss: 7.390966061393556e-08\n",
      "step: 17019, loss: 8.201494097193063e-07\n",
      "step: 17020, loss: 1.7618768879401614e-06\n",
      "step: 17021, loss: 2.5749073984115967e-07\n",
      "step: 17022, loss: 3.004059863087605e-07\n",
      "step: 17023, loss: 1.9311850962822064e-07\n",
      "step: 17024, loss: 4.506094342104916e-07\n",
      "step: 17025, loss: 3.3140034361167636e-07\n",
      "step: 17026, loss: 0.0\n",
      "step: 17027, loss: 2.145766764272139e-08\n",
      "step: 17028, loss: 3.337858700547258e-08\n",
      "step: 17029, loss: 1.668929705544997e-08\n",
      "step: 17030, loss: 0.000329491711454466\n",
      "step: 17031, loss: 1.3053933798801154e-05\n",
      "step: 17032, loss: 1.955027926214825e-07\n",
      "step: 17033, loss: 3.9100271465031256e-07\n",
      "step: 17034, loss: 1.1682487865982694e-07\n",
      "step: 17035, loss: 7.390966771936291e-08\n",
      "step: 17036, loss: 2.38418529363571e-08\n",
      "step: 17037, loss: 1.668929705544997e-08\n",
      "step: 17038, loss: 2.0073839550605044e-06\n",
      "step: 17039, loss: 0.001340665272437036\n",
      "step: 17040, loss: 1.6689232040789648e-07\n",
      "step: 17041, loss: 1.668929705544997e-08\n",
      "step: 17042, loss: 1.971651499843574e-06\n",
      "step: 17043, loss: 4.76837103136063e-09\n",
      "step: 17044, loss: 0.05951874703168869\n",
      "step: 17045, loss: 3.8146950487316644e-08\n",
      "step: 17046, loss: 2.0503892983469996e-07\n",
      "step: 17047, loss: 4.76837147544984e-09\n",
      "step: 17048, loss: 9.298305769789295e-08\n",
      "step: 17049, loss: 6.914131489565989e-08\n",
      "step: 17050, loss: 0.03514494001865387\n",
      "step: 17051, loss: 4.2915317521874385e-08\n",
      "step: 17052, loss: 9.53674206272126e-09\n",
      "step: 17053, loss: 4.76837103136063e-09\n",
      "step: 17054, loss: 5.9604612800967516e-08\n",
      "step: 17055, loss: 2.2888103501372825e-07\n",
      "step: 17056, loss: 6.771006724193285e-07\n",
      "step: 17057, loss: 1.3613245073429425e-06\n",
      "step: 17058, loss: 4.76837103136063e-09\n",
      "step: 17059, loss: 6.437291943939272e-08\n",
      "step: 17060, loss: 2.6226029348208613e-08\n",
      "step: 17061, loss: 2.861022352362852e-08\n",
      "step: 17062, loss: 0.00019776473345700651\n",
      "step: 17063, loss: 3.099440348819371e-08\n",
      "step: 17064, loss: 1.430510998545742e-08\n",
      "step: 17065, loss: 0.14048989117145538\n",
      "step: 17066, loss: 5.7220425730974966e-08\n",
      "step: 17067, loss: 0.0\n",
      "step: 17068, loss: 3.5762673178396653e-07\n",
      "step: 17069, loss: 2.622603645363597e-08\n",
      "step: 17070, loss: 2.384185116000026e-08\n",
      "step: 17071, loss: 1.101463453778706e-06\n",
      "step: 17072, loss: 7.653118245798396e-07\n",
      "step: 17073, loss: 4.76837147544984e-09\n",
      "step: 17074, loss: 4.291532462730174e-08\n",
      "step: 17075, loss: 4.76837103136063e-09\n",
      "step: 17076, loss: 9.536726963688125e-08\n",
      "step: 17077, loss: 0.0617729090154171\n",
      "step: 17078, loss: 7.176297458499903e-07\n",
      "step: 17079, loss: 0.09929575026035309\n",
      "step: 17080, loss: 9.53674295089968e-09\n",
      "step: 17081, loss: 1.668929527909313e-08\n",
      "step: 17082, loss: 7.15255676908555e-09\n",
      "step: 17083, loss: 1.192092646817855e-08\n",
      "step: 17084, loss: 6.675711006209895e-08\n",
      "step: 17085, loss: 9.53674206272126e-09\n",
      "step: 17086, loss: 3.075575989441859e-07\n",
      "step: 17087, loss: 2.38418573772492e-09\n",
      "step: 17088, loss: 2.2649680886388524e-07\n",
      "step: 17089, loss: 2.016959797401796e-06\n",
      "step: 17090, loss: 1.192092824453539e-08\n",
      "step: 17091, loss: 1.5497187177970773e-07\n",
      "step: 17092, loss: 1.597398835428976e-07\n",
      "step: 17093, loss: 1.4209293794920086e-06\n",
      "step: 17094, loss: 4.76837103136063e-09\n",
      "step: 17095, loss: 0.012625842355191708\n",
      "step: 17096, loss: 1.4543493875862623e-07\n",
      "step: 17097, loss: 2.1696010321647918e-07\n",
      "step: 17098, loss: 0.0\n",
      "step: 17099, loss: 7.15255676908555e-09\n",
      "step: 17100, loss: 7.152553394007555e-08\n",
      "step: 17101, loss: 2.884844150230492e-07\n",
      "step: 17102, loss: 7.15255676908555e-09\n",
      "step: 17103, loss: 8.106222537662688e-08\n",
      "step: 17104, loss: 6.437296917738422e-08\n",
      "step: 17105, loss: 2.5008548618643545e-06\n",
      "step: 17106, loss: 0.0016940748319029808\n",
      "step: 17107, loss: 5.957551820756635e-06\n",
      "step: 17108, loss: 4.5299508144580614e-08\n",
      "step: 17109, loss: 1.0967245600568276e-07\n",
      "step: 17110, loss: 2.8610218194558e-08\n",
      "step: 17111, loss: 6.914132200108725e-08\n",
      "step: 17112, loss: 0.009549843147397041\n",
      "step: 17113, loss: 9.512757515039993e-07\n",
      "step: 17114, loss: 1.0895610103034414e-06\n",
      "step: 17115, loss: 1.0490391133544108e-07\n",
      "step: 17116, loss: 8.821480435017293e-08\n",
      "step: 17117, loss: 1.02519763345299e-07\n",
      "step: 17118, loss: 0.029801035299897194\n",
      "step: 17119, loss: 8.153776889230357e-07\n",
      "step: 17120, loss: 6.723324759150273e-07\n",
      "step: 17121, loss: 7.15255632499634e-09\n",
      "step: 17122, loss: 3.194784881088708e-07\n",
      "step: 17123, loss: 0.0\n",
      "step: 17124, loss: 2.0265498790195124e-07\n",
      "step: 17125, loss: 2.38418573772492e-09\n",
      "step: 17126, loss: 2.145766764272139e-08\n",
      "step: 17127, loss: 1.7166087218356552e-07\n",
      "step: 17128, loss: 2.384185116000026e-08\n",
      "step: 17129, loss: 7.15255676908555e-09\n",
      "step: 17130, loss: 1.5053009519760963e-05\n",
      "step: 17131, loss: 4.08614096158999e-06\n",
      "step: 17132, loss: 0.0967167392373085\n",
      "step: 17133, loss: 3.2424759410787374e-07\n",
      "step: 17134, loss: 3.263711960244109e-06\n",
      "step: 17135, loss: 8.857635111780837e-05\n",
      "step: 17136, loss: 1.668929350273629e-08\n",
      "step: 17137, loss: 8.821471908504463e-08\n",
      "step: 17138, loss: 2.622604000634965e-08\n",
      "step: 17139, loss: 1.075239651981974e-06\n",
      "step: 17140, loss: 4.0531134004595515e-08\n",
      "step: 17141, loss: 2.3126470694023737e-07\n",
      "step: 17142, loss: 1.1920899822825959e-07\n",
      "step: 17143, loss: 1.7945119907381013e-05\n",
      "step: 17144, loss: 2.38418573772492e-09\n",
      "step: 17145, loss: 7.15255632499634e-09\n",
      "step: 17146, loss: 4.33919126408e-07\n",
      "step: 17147, loss: 0.00016896228771656752\n",
      "step: 17148, loss: 5.0157100304204505e-06\n",
      "step: 17149, loss: 6.19887430275412e-08\n",
      "step: 17150, loss: 1.668929883180681e-08\n",
      "step: 17151, loss: 7.987811841303483e-06\n",
      "step: 17152, loss: 9.29831998064401e-08\n",
      "step: 17153, loss: 1.430511264999268e-08\n",
      "step: 17154, loss: 3.1947843126545195e-07\n",
      "step: 17155, loss: 3.5762759864610416e-08\n",
      "step: 17156, loss: 7.724687520749285e-07\n",
      "step: 17157, loss: 2.145766764272139e-08\n",
      "step: 17158, loss: 3.576277762817881e-08\n",
      "step: 17159, loss: 4.0531134004595515e-08\n",
      "step: 17160, loss: 0.0694127306342125\n",
      "step: 17161, loss: 0.07965926080942154\n",
      "step: 17162, loss: 1.218285433424171e-06\n",
      "step: 17163, loss: 6.675713848380838e-08\n",
      "step: 17164, loss: 0.0\n",
      "step: 17165, loss: 0.06347303837537766\n",
      "step: 17166, loss: 1.6212445075325377e-07\n",
      "step: 17167, loss: 6.890189183650364e-07\n",
      "step: 17168, loss: 2.2411222744267434e-07\n",
      "step: 17169, loss: 7.15255632499634e-09\n",
      "step: 17170, loss: 8.34464515264699e-08\n",
      "step: 17171, loss: 9.059895234031501e-08\n",
      "step: 17172, loss: 2.8133237606198236e-07\n",
      "step: 17173, loss: 4.76837103136063e-09\n",
      "step: 17174, loss: 1.1300772939648596e-06\n",
      "step: 17175, loss: 7.15255676908555e-09\n",
      "step: 17176, loss: 1.862644793959589e-08\n",
      "step: 17177, loss: 9.298318559558538e-08\n",
      "step: 17178, loss: 4.291529975830599e-08\n",
      "step: 17179, loss: 0.0006026829942129552\n",
      "step: 17180, loss: 1.5711271998952725e-06\n",
      "step: 17181, loss: 3.624491364462301e-05\n",
      "step: 17182, loss: 2.3841845830929742e-08\n",
      "step: 17183, loss: 9.53674206272126e-09\n",
      "step: 17184, loss: 2.38418573772492e-09\n",
      "step: 17185, loss: 4.291532462730174e-08\n",
      "step: 17186, loss: 1.0251989834841879e-07\n",
      "step: 17187, loss: 9.53674206272126e-09\n",
      "step: 17188, loss: 4.0292468383995583e-07\n",
      "step: 17189, loss: 3.838514430754003e-07\n",
      "step: 17190, loss: 1.192092646817855e-08\n",
      "step: 17191, loss: 2.2280135453911498e-05\n",
      "step: 17192, loss: 2.617720383568667e-06\n",
      "step: 17193, loss: 1.430510998545742e-08\n",
      "step: 17194, loss: 2.2863034701003926e-06\n",
      "step: 17195, loss: 3.576277762817881e-08\n",
      "step: 17196, loss: 1.6689260462499078e-07\n",
      "step: 17197, loss: 2.0265478894998523e-07\n",
      "step: 17198, loss: 1.4781934964958054e-07\n",
      "step: 17199, loss: 2.384185116000026e-08\n",
      "step: 17200, loss: 3.042130629182793e-06\n",
      "step: 17201, loss: 1.3774755643680692e-05\n",
      "step: 17202, loss: 7.319372343772557e-07\n",
      "step: 17203, loss: 7.15255676908555e-09\n",
      "step: 17204, loss: 1.0967238495140919e-07\n",
      "step: 17205, loss: 5.865013577022182e-07\n",
      "step: 17206, loss: 3.57625509650461e-07\n",
      "step: 17207, loss: 3.8146946934602965e-08\n",
      "step: 17208, loss: 3.9338908663921757e-07\n",
      "step: 17209, loss: 7.152549841293876e-08\n",
      "step: 17210, loss: 1.4311961422208697e-05\n",
      "step: 17211, loss: 8.58306137274667e-08\n",
      "step: 17212, loss: 2.5510652790217136e-07\n",
      "step: 17213, loss: 8.559809430153109e-06\n",
      "step: 17214, loss: 1.907348412544252e-08\n",
      "step: 17215, loss: 8.106222537662688e-08\n",
      "step: 17216, loss: 4.4107272856308555e-07\n",
      "step: 17217, loss: 0.0003411899378988892\n",
      "step: 17218, loss: 5.888871896786441e-07\n",
      "step: 17219, loss: 0.00024368036247324198\n",
      "step: 17220, loss: 8.58289070038154e-07\n",
      "step: 17221, loss: 0.08205689489841461\n",
      "step: 17222, loss: 1.2874568255938357e-07\n",
      "step: 17223, loss: 0.008514528162777424\n",
      "step: 17224, loss: 2.121918782904686e-07\n",
      "step: 17225, loss: 1.4066678488688922e-07\n",
      "step: 17226, loss: 2.384185116000026e-08\n",
      "step: 17227, loss: 3.576277407546513e-08\n",
      "step: 17228, loss: 1.2159334517036768e-07\n",
      "step: 17229, loss: 9.536731937487275e-08\n",
      "step: 17230, loss: 2.38418573772492e-09\n",
      "step: 17231, loss: 2.384184938364342e-08\n",
      "step: 17232, loss: 0.0\n",
      "step: 17233, loss: 1.7166115640065982e-07\n",
      "step: 17234, loss: 1.907348234908568e-08\n",
      "step: 17235, loss: 3.3378576347331546e-08\n",
      "step: 17236, loss: 4.2915321074588064e-08\n",
      "step: 17237, loss: 2.622603290092229e-08\n",
      "step: 17238, loss: 4.45335990661988e-06\n",
      "step: 17239, loss: 7.629388676377857e-08\n",
      "step: 17240, loss: 1.7404482832716894e-07\n",
      "step: 17241, loss: 5.006789294270675e-08\n",
      "step: 17242, loss: 1.8953558083012467e-06\n",
      "step: 17243, loss: 1.939213507284876e-05\n",
      "step: 17244, loss: 4.386881187201652e-07\n",
      "step: 17245, loss: 2.145766764272139e-08\n",
      "step: 17246, loss: 2.145766764272139e-08\n",
      "step: 17247, loss: 1.192092646817855e-08\n",
      "step: 17248, loss: 2.384185116000026e-08\n",
      "step: 17249, loss: 0.03687230870127678\n",
      "step: 17250, loss: 2.694111742584937e-07\n",
      "step: 17251, loss: 1.192092558000013e-08\n",
      "step: 17252, loss: 8.511367468599929e-07\n",
      "step: 17253, loss: 1.430510998545742e-08\n",
      "step: 17254, loss: 1.668929883180681e-08\n",
      "step: 17255, loss: 0.13147518038749695\n",
      "step: 17256, loss: 4.7683684556432127e-08\n",
      "step: 17257, loss: 1.668925193598625e-07\n",
      "step: 17258, loss: 2.38418573772492e-09\n",
      "step: 17259, loss: 0.0\n",
      "step: 17260, loss: 1.2636164115065185e-07\n",
      "step: 17261, loss: 3.158935214742087e-05\n",
      "step: 17262, loss: 0.11939989030361176\n",
      "step: 17263, loss: 6.675715980009045e-08\n",
      "step: 17264, loss: 1.8596567485928972e-07\n",
      "step: 17265, loss: 0.0\n",
      "step: 17266, loss: 1.2636154167466884e-07\n",
      "step: 17267, loss: 9.53674117454284e-09\n",
      "step: 17268, loss: 1.7618679066799814e-06\n",
      "step: 17269, loss: 3.8623588238806406e-07\n",
      "step: 17270, loss: 5.960459148468544e-08\n",
      "step: 17271, loss: 4.386856460314448e-07\n",
      "step: 17272, loss: 7.15255632499634e-09\n",
      "step: 17273, loss: 9.05989878674518e-08\n",
      "step: 17274, loss: 6.294168883869133e-07\n",
      "step: 17275, loss: 1.907348234908568e-08\n",
      "step: 17276, loss: 4.76837147544984e-09\n",
      "step: 17277, loss: 2.0503900088897353e-07\n",
      "step: 17278, loss: 8.583064214917613e-08\n",
      "step: 17279, loss: 1.4900621181368479e-06\n",
      "step: 17280, loss: 1.7881355063309456e-07\n",
      "step: 17281, loss: 1.3208070868131472e-06\n",
      "step: 17282, loss: 7.152553394007555e-08\n",
      "step: 17283, loss: 7.15255676908555e-09\n",
      "step: 17284, loss: 3.671626132017991e-07\n",
      "step: 17285, loss: 1.1038515594918863e-06\n",
      "step: 17286, loss: 1.3064911854598904e-06\n",
      "step: 17287, loss: 3.0994396382766354e-08\n",
      "step: 17288, loss: 7.390967482479027e-08\n",
      "step: 17289, loss: 3.0517389859596733e-07\n",
      "step: 17290, loss: 2.098079505685746e-07\n",
      "step: 17291, loss: 8.344642310476047e-08\n",
      "step: 17292, loss: 0.0\n",
      "step: 17293, loss: 1.7642896921188367e-07\n",
      "step: 17294, loss: 9.775154552471577e-08\n",
      "step: 17295, loss: 0.16341239213943481\n",
      "step: 17296, loss: 7.796147087901772e-07\n",
      "step: 17297, loss: 0.24149027466773987\n",
      "step: 17298, loss: 3.6716161844196904e-07\n",
      "step: 17299, loss: 2.8610043045773637e-07\n",
      "step: 17300, loss: 0.00013415075954981148\n",
      "step: 17301, loss: 2.38418573772492e-09\n",
      "step: 17302, loss: 1.8358183240252401e-07\n",
      "step: 17303, loss: 2.145766764272139e-08\n",
      "step: 17304, loss: 9.345945954919443e-07\n",
      "step: 17305, loss: 2.38418573772492e-09\n",
      "step: 17306, loss: 2.1219180723619502e-07\n",
      "step: 17307, loss: 2.2172855551616522e-07\n",
      "step: 17308, loss: 0.0\n",
      "step: 17309, loss: 2.7082651286036707e-06\n",
      "step: 17310, loss: 1.096723991622639e-07\n",
      "step: 17311, loss: 7.152551262379347e-08\n",
      "step: 17312, loss: 2.145766764272139e-08\n",
      "step: 17313, loss: 9.53674206272126e-09\n",
      "step: 17314, loss: 3.123265912563511e-07\n",
      "step: 17315, loss: 6.508737442345591e-07\n",
      "step: 17316, loss: 0.0016069408738985658\n",
      "step: 17317, loss: 4.188579168840079e-06\n",
      "step: 17318, loss: 1.3732537809119094e-06\n",
      "step: 17319, loss: 3.290163306246541e-07\n",
      "step: 17320, loss: 1.2754986755680875e-06\n",
      "step: 17321, loss: 2.5272314019275655e-07\n",
      "step: 17322, loss: 4.76837147544984e-09\n",
      "step: 17323, loss: 5.006786452099732e-08\n",
      "step: 17324, loss: 1.430508973498945e-07\n",
      "step: 17325, loss: 3.3378576347331546e-08\n",
      "step: 17326, loss: 2.622603645363597e-08\n",
      "step: 17327, loss: 1.955023236632769e-07\n",
      "step: 17328, loss: 3.45677631230501e-06\n",
      "step: 17329, loss: 0.07505305856466293\n",
      "step: 17330, loss: 2.4460491658828687e-06\n",
      "step: 17331, loss: 0.0\n",
      "step: 17332, loss: 4.0530937894800445e-07\n",
      "step: 17333, loss: 2.861021997091484e-08\n",
      "step: 17334, loss: 4.76837103136063e-09\n",
      "step: 17335, loss: 0.0\n",
      "step: 17336, loss: 3.576277052275145e-08\n",
      "step: 17337, loss: 2.670284686701052e-07\n",
      "step: 17338, loss: 4.0531130451881836e-08\n",
      "step: 17339, loss: 0.0\n",
      "step: 17340, loss: 2.5390022528881673e-06\n",
      "step: 17341, loss: 8.22528761545982e-07\n",
      "step: 17342, loss: 1.6927668866628665e-07\n",
      "step: 17343, loss: 3.099440704090739e-08\n",
      "step: 17344, loss: 1.192092824453539e-08\n",
      "step: 17345, loss: 1.7642949501350813e-07\n",
      "step: 17346, loss: 2.145766764272139e-08\n",
      "step: 17347, loss: 1.28745710981093e-07\n",
      "step: 17348, loss: 0.0\n",
      "step: 17349, loss: 3.1947965339895745e-07\n",
      "step: 17350, loss: 3.5762766970037774e-08\n",
      "step: 17351, loss: 9.53674206272126e-09\n",
      "step: 17352, loss: 9.083543659471616e-07\n",
      "step: 17353, loss: 8.106222537662688e-08\n",
      "step: 17354, loss: 3.547499090927886e-06\n",
      "step: 17355, loss: 9.53674206272126e-09\n",
      "step: 17356, loss: 3.099440348819371e-08\n",
      "step: 17357, loss: 1.4781898016735795e-07\n",
      "step: 17358, loss: 0.1818968951702118\n",
      "step: 17359, loss: 6.437296207195686e-08\n",
      "step: 17360, loss: 4.76837147544984e-09\n",
      "step: 17361, loss: 4.053111624102712e-08\n",
      "step: 17362, loss: 2.6226029348208613e-08\n",
      "step: 17363, loss: 1.5258748931046284e-07\n",
      "step: 17364, loss: 1.3351419170248846e-07\n",
      "step: 17365, loss: 7.533932375736185e-07\n",
      "step: 17366, loss: 7.39096535085082e-08\n",
      "step: 17367, loss: 1.3589841785233148e-07\n",
      "step: 17368, loss: 4.315333228532836e-07\n",
      "step: 17369, loss: 1.668929883180681e-08\n",
      "step: 17370, loss: 1.764293813266704e-07\n",
      "step: 17371, loss: 4.76837147544984e-09\n",
      "step: 17372, loss: 0.06256283074617386\n",
      "step: 17373, loss: 0.06366987526416779\n",
      "step: 17374, loss: 7.390963929765348e-08\n",
      "step: 17375, loss: 4.76837147544984e-09\n",
      "step: 17376, loss: 8.964341873252124e-07\n",
      "step: 17377, loss: 2.4782557375147007e-05\n",
      "step: 17378, loss: 3.947825916839065e-06\n",
      "step: 17379, loss: 1.2882594091934152e-05\n",
      "step: 17380, loss: 6.914133621194196e-08\n",
      "step: 17381, loss: 9.775138209988654e-08\n",
      "step: 17382, loss: 1.907348234908568e-08\n",
      "step: 17383, loss: 4.70824397780234e-06\n",
      "step: 17384, loss: 9.467745258007199e-06\n",
      "step: 17385, loss: 0.12731963396072388\n",
      "step: 17386, loss: 6.914133621194196e-08\n",
      "step: 17387, loss: 3.225671207474079e-06\n",
      "step: 17388, loss: 4.803826413990464e-06\n",
      "step: 17389, loss: 9.155089628620772e-07\n",
      "step: 17390, loss: 5.245206935455826e-08\n",
      "step: 17391, loss: 3.504725896164018e-07\n",
      "step: 17392, loss: 1.192092646817855e-08\n",
      "step: 17393, loss: 5.960459503739912e-08\n",
      "step: 17394, loss: 4.053114111002287e-08\n",
      "step: 17395, loss: 2.145766764272139e-08\n",
      "step: 17396, loss: 1.192092646817855e-08\n",
      "step: 17397, loss: 2.551074089751637e-07\n",
      "step: 17398, loss: 0.07670661807060242\n",
      "step: 17399, loss: 1.2874565413767414e-07\n",
      "step: 17400, loss: 3.8146957592744e-08\n",
      "step: 17401, loss: 1.4996253412391525e-06\n",
      "step: 17402, loss: 2.6077021431092362e-08\n",
      "step: 17403, loss: 9.53674206272126e-09\n",
      "step: 17404, loss: 6.675709585124423e-08\n",
      "step: 17405, loss: 3.8146957592744e-08\n",
      "step: 17406, loss: 4.458393334516586e-07\n",
      "step: 17407, loss: 0.0\n",
      "step: 17408, loss: 2.145766764272139e-08\n",
      "step: 17409, loss: 6.675711006209895e-08\n",
      "step: 17410, loss: 4.5299501039153256e-08\n",
      "step: 17411, loss: 7.033226552266569e-07\n",
      "step: 17412, loss: 4.5299501039153256e-08\n",
      "step: 17413, loss: 0.06984147429466248\n",
      "step: 17414, loss: 1.478192217518881e-07\n",
      "step: 17415, loss: 6.174998929964204e-07\n",
      "step: 17416, loss: 0.08148457854986191\n",
      "step: 17417, loss: 6.05574086876004e-07\n",
      "step: 17418, loss: 5.00678503101426e-08\n",
      "step: 17419, loss: 3.7193024127191165e-07\n",
      "step: 17420, loss: 4.148449477270333e-07\n",
      "step: 17421, loss: 6.198878566010535e-08\n",
      "step: 17422, loss: 2.861021997091484e-08\n",
      "step: 17423, loss: 4.0531134004595515e-08\n",
      "step: 17424, loss: 1.382824734719179e-07\n",
      "step: 17425, loss: 2.956369371531764e-07\n",
      "step: 17426, loss: 2.040776280409773e-06\n",
      "step: 17427, loss: 3.103968992945738e-06\n",
      "step: 17428, loss: 3.433212327763613e-07\n",
      "step: 17429, loss: 3.576268170490948e-07\n",
      "step: 17430, loss: 1.2182853197373333e-06\n",
      "step: 17431, loss: 0.0015599612379446626\n",
      "step: 17432, loss: 7.486248136956419e-07\n",
      "step: 17433, loss: 1.3231931461632485e-06\n",
      "step: 17434, loss: 1.3113005081777374e-07\n",
      "step: 17435, loss: 2.38418573772492e-09\n",
      "step: 17436, loss: 4.76837103136063e-09\n",
      "step: 17437, loss: 2.38418573772492e-09\n",
      "step: 17438, loss: 5.841211532242596e-07\n",
      "step: 17439, loss: 8.583059951661198e-08\n",
      "step: 17440, loss: 1.5258730456935155e-07\n",
      "step: 17441, loss: 7.867804185934801e-08\n",
      "step: 17442, loss: 9.263174433726817e-06\n",
      "step: 17443, loss: 0.00010289789497619495\n",
      "step: 17444, loss: 0.07307075709104538\n",
      "step: 17445, loss: 0.0\n",
      "step: 17446, loss: 2.2649699360499653e-07\n",
      "step: 17447, loss: 6.43729549665295e-08\n",
      "step: 17448, loss: 0.1408216804265976\n",
      "step: 17449, loss: 1.645086058488232e-07\n",
      "step: 17450, loss: 2.38418573772492e-09\n",
      "step: 17451, loss: 4.6252716856542975e-07\n",
      "step: 17452, loss: 4.529949748643958e-08\n",
      "step: 17453, loss: 4.7683688109145805e-08\n",
      "step: 17454, loss: 3.0994223720881564e-07\n",
      "step: 17455, loss: 7.15255632499634e-09\n",
      "step: 17456, loss: 0.05240936204791069\n",
      "step: 17457, loss: 1.907348234908568e-08\n",
      "step: 17458, loss: 2.3841844054572903e-08\n",
      "step: 17459, loss: 7.15255676908555e-09\n",
      "step: 17460, loss: 2.38418573772492e-09\n",
      "step: 17461, loss: 9.53674206272126e-09\n",
      "step: 17462, loss: 4.76837103136063e-09\n",
      "step: 17463, loss: 3.600095794809022e-07\n",
      "step: 17464, loss: 1.454350950780281e-07\n",
      "step: 17465, loss: 5.2452058696417225e-08\n",
      "step: 17466, loss: 3.337859055818626e-08\n",
      "step: 17467, loss: 0.10451072454452515\n",
      "step: 17468, loss: 9.298303638161087e-08\n",
      "step: 17469, loss: 4.1246090631830157e-07\n",
      "step: 17470, loss: 4.529947972287118e-08\n",
      "step: 17471, loss: 4.76837103136063e-09\n",
      "step: 17472, loss: 3.576277762817881e-08\n",
      "step: 17473, loss: 9.53674117454284e-09\n",
      "step: 17474, loss: 2.8823765205743257e-06\n",
      "step: 17475, loss: 0.0008021061075851321\n",
      "step: 17476, loss: 0.0002997165429405868\n",
      "step: 17477, loss: 2.3603369925240258e-07\n",
      "step: 17478, loss: 1.192092646817855e-08\n",
      "step: 17479, loss: 3.6239507039681484e-07\n",
      "step: 17480, loss: 7.263296993187396e-06\n",
      "step: 17481, loss: 2.145766231365087e-08\n",
      "step: 17482, loss: 2.9086919539622613e-07\n",
      "step: 17483, loss: 1.5974015354913718e-07\n",
      "step: 17484, loss: 1.2397741500080883e-07\n",
      "step: 17485, loss: 2.4079067770799156e-06\n",
      "step: 17486, loss: 0.0\n",
      "step: 17487, loss: 4.171899945504265e-06\n",
      "step: 17488, loss: 1.192092824453539e-08\n",
      "step: 17489, loss: 4.5060863840262755e-07\n",
      "step: 17490, loss: 5.068164227850502e-06\n",
      "step: 17491, loss: 4.792172489942459e-07\n",
      "step: 17492, loss: 1.990742930502165e-06\n",
      "step: 17493, loss: 4.2915317521874385e-08\n",
      "step: 17494, loss: 0.0044203815050423145\n",
      "step: 17495, loss: 1.1205647609813241e-07\n",
      "step: 17496, loss: 3.359121546964161e-05\n",
      "step: 17497, loss: 9.53674117454284e-09\n",
      "step: 17498, loss: 6.246478960747481e-07\n",
      "step: 17499, loss: 1.6593244254181627e-06\n",
      "step: 17500, loss: 4.76837147544984e-09\n",
      "step: 17501, loss: 3.8146950487316644e-08\n",
      "step: 17502, loss: 4.76837147544984e-09\n",
      "step: 17503, loss: 9.774975069376524e-07\n",
      "step: 17504, loss: 3.623943598540791e-07\n",
      "step: 17505, loss: 1.5258756036473642e-07\n",
      "step: 17506, loss: 9.083614145311003e-07\n",
      "step: 17507, loss: 1.2189029803266749e-05\n",
      "step: 17508, loss: 3.8146957592744e-08\n",
      "step: 17509, loss: 3.6954668303224025e-07\n",
      "step: 17510, loss: 8.10621898494901e-08\n",
      "step: 17511, loss: 0.046645794063806534\n",
      "step: 17512, loss: 2.4002318241400644e-05\n",
      "step: 17513, loss: 2.4080151206362643e-07\n",
      "step: 17514, loss: 6.437299049366629e-08\n",
      "step: 17515, loss: 1.2469160992623074e-06\n",
      "step: 17516, loss: 7.629385123664179e-08\n",
      "step: 17517, loss: 4.768369876728684e-08\n",
      "step: 17518, loss: 1.192092558000013e-08\n",
      "step: 17519, loss: 2.171876076317858e-06\n",
      "step: 17520, loss: 1.73565076693194e-05\n",
      "step: 17521, loss: 7.15255632499634e-09\n",
      "step: 17522, loss: 2.622604000634965e-08\n",
      "step: 17523, loss: 2.2172820024479734e-07\n",
      "step: 17524, loss: 2.1980034944135696e-05\n",
      "step: 17525, loss: 1.4305093998245866e-07\n",
      "step: 17526, loss: 2.145766764272139e-08\n",
      "step: 17527, loss: 0.20010852813720703\n",
      "step: 17528, loss: 4.76837103136063e-09\n",
      "step: 17529, loss: 7.15255632499634e-09\n",
      "step: 17530, loss: 2.38418573772492e-09\n",
      "step: 17531, loss: 5.483623510826874e-08\n",
      "step: 17532, loss: 7.390966771936291e-08\n",
      "step: 17533, loss: 1.1444074488053957e-07\n",
      "step: 17534, loss: 9.775154552471577e-08\n",
      "step: 17535, loss: 1.5986741345841438e-05\n",
      "step: 17536, loss: 8.702188551978907e-07\n",
      "step: 17537, loss: 8.344636626134161e-08\n",
      "step: 17538, loss: 7.343164156736748e-07\n",
      "step: 17539, loss: 3.1232673336489825e-07\n",
      "step: 17540, loss: 4.506082404986955e-07\n",
      "step: 17541, loss: 5.722043994182968e-08\n",
      "step: 17542, loss: 3.36168966441619e-07\n",
      "step: 17543, loss: 0.1535206437110901\n",
      "step: 17544, loss: 1.1181700756424107e-06\n",
      "step: 17545, loss: 4.841696409130236e-06\n",
      "step: 17546, loss: 0.09057188779115677\n",
      "step: 17547, loss: 1.3828233136337076e-07\n",
      "step: 17548, loss: 5.006785741556996e-08\n",
      "step: 17549, loss: 3.337857279461787e-08\n",
      "step: 17550, loss: 1.3112989449837187e-07\n",
      "step: 17551, loss: 2.38418573772492e-09\n",
      "step: 17552, loss: 1.239773865790994e-07\n",
      "step: 17553, loss: 3.433199253777275e-07\n",
      "step: 17554, loss: 2.3983518531167647e-06\n",
      "step: 17555, loss: 9.298316427930331e-08\n",
      "step: 17556, loss: 9.059886707518672e-08\n",
      "step: 17557, loss: 2.38418529363571e-08\n",
      "step: 17558, loss: 1.192092824453539e-08\n",
      "step: 17559, loss: 0.07420290261507034\n",
      "step: 17560, loss: 1.1910252396774013e-05\n",
      "step: 17561, loss: 6.437294075567479e-08\n",
      "step: 17562, loss: 2.0980752424293314e-07\n",
      "step: 17563, loss: 4.76837103136063e-09\n",
      "step: 17564, loss: 6.580258400390449e-07\n",
      "step: 17565, loss: 1.0728825117212182e-07\n",
      "step: 17566, loss: 0.0717117041349411\n",
      "step: 17567, loss: 2.0402989321155474e-05\n",
      "step: 17568, loss: 5.531271654035663e-07\n",
      "step: 17569, loss: 9.53674117454284e-09\n",
      "step: 17570, loss: 1.6736299812691868e-06\n",
      "step: 17571, loss: 0.00011060617543989792\n",
      "step: 17572, loss: 8.344640889390575e-08\n",
      "step: 17573, loss: 4.76837147544984e-09\n",
      "step: 17574, loss: 5.054427560935437e-07\n",
      "step: 17575, loss: 4.76837147544984e-09\n",
      "step: 17576, loss: 8.821481856102764e-08\n",
      "step: 17577, loss: 1.430507836630568e-07\n",
      "step: 17578, loss: 8.106216853320802e-08\n",
      "step: 17579, loss: 1.2469090506783687e-06\n",
      "step: 17580, loss: 0.0\n",
      "step: 17581, loss: 5.62665036341059e-07\n",
      "step: 17582, loss: 2.38418573772492e-09\n",
      "step: 17583, loss: 1.645084211077119e-07\n",
      "step: 17584, loss: 1.430510998545742e-08\n",
      "step: 17585, loss: 3.170954130382597e-07\n",
      "step: 17586, loss: 1.454348250717885e-07\n",
      "step: 17587, loss: 1.9119181160931475e-05\n",
      "step: 17588, loss: 6.6041070567735e-07\n",
      "step: 17589, loss: 4.768368100371845e-08\n",
      "step: 17590, loss: 3.313998888643255e-07\n",
      "step: 17591, loss: 6.031905286363326e-07\n",
      "step: 17592, loss: 4.76837147544984e-09\n",
      "step: 17593, loss: 2.4913219931477215e-06\n",
      "step: 17594, loss: 3.337859055818626e-08\n",
      "step: 17595, loss: 0.04911023750901222\n",
      "step: 17596, loss: 1.4256927443057066e-06\n",
      "step: 17597, loss: 5.602781811830937e-07\n",
      "step: 17598, loss: 0.0088562723249197\n",
      "step: 17599, loss: 1.0537960406509228e-06\n",
      "step: 17600, loss: 1.883499720634063e-07\n",
      "step: 17601, loss: 1.907348234908568e-08\n",
      "step: 17602, loss: 5.006788228456571e-08\n",
      "step: 17603, loss: 9.059893102403294e-08\n",
      "step: 17604, loss: 2.2291126242635073e-06\n",
      "step: 17605, loss: 1.4996252275523148e-06\n",
      "step: 17606, loss: 1.2636165536150656e-07\n",
      "step: 17607, loss: 2.38418573772492e-09\n",
      "step: 17608, loss: 1.907348234908568e-08\n",
      "step: 17609, loss: 8.583052135691105e-08\n",
      "step: 17610, loss: 1.2874585308964015e-07\n",
      "step: 17611, loss: 2.455699075198936e-07\n",
      "step: 17612, loss: 6.675712427295366e-08\n",
      "step: 17613, loss: 3.4570456364235724e-07\n",
      "step: 17614, loss: 1.668929350273629e-08\n",
      "step: 17615, loss: 2.38418573772492e-09\n",
      "step: 17616, loss: 9.53674206272126e-09\n",
      "step: 17617, loss: 1.2135174074501265e-06\n",
      "step: 17618, loss: 9.53674295089968e-09\n",
      "step: 17619, loss: 2.0027104596920253e-07\n",
      "step: 17620, loss: 7.629380149865028e-08\n",
      "step: 17621, loss: 5.9604612800967516e-08\n",
      "step: 17622, loss: 3.981197551183868e-06\n",
      "step: 17623, loss: 2.121920772424346e-07\n",
      "step: 17624, loss: 1.43051135381711e-08\n",
      "step: 17625, loss: 0.00010429667599964887\n",
      "step: 17626, loss: 3.981558336363378e-07\n",
      "step: 17627, loss: 5.960460924825384e-08\n",
      "step: 17628, loss: 1.6763802790364934e-08\n",
      "step: 17629, loss: 1.1205641214928619e-07\n",
      "step: 17630, loss: 7.724657962171477e-07\n",
      "step: 17631, loss: 1.430511264999268e-08\n",
      "step: 17632, loss: 3.9338684132417256e-07\n",
      "step: 17633, loss: 5.9604580826544407e-08\n",
      "step: 17634, loss: 0.0\n",
      "step: 17635, loss: 8.344639468305104e-08\n",
      "step: 17636, loss: 8.106219695491745e-08\n",
      "step: 17637, loss: 2.732104348979192e-06\n",
      "step: 17638, loss: 2.2172820024479734e-07\n",
      "step: 17639, loss: 1.9073478796372e-08\n",
      "step: 17640, loss: 4.053114111002287e-08\n",
      "step: 17641, loss: 2.4557030542382563e-07\n",
      "step: 17642, loss: 5.93653851410636e-07\n",
      "step: 17643, loss: 2.38418573772492e-09\n",
      "step: 17644, loss: 4.1961240526688925e-07\n",
      "step: 17645, loss: 4.76837147544984e-09\n",
      "step: 17646, loss: 6.03189960202144e-07\n",
      "step: 17647, loss: 1.7856760905488045e-06\n",
      "step: 17648, loss: 7.15255632499634e-09\n",
      "step: 17649, loss: 1.5615819393133279e-06\n",
      "step: 17650, loss: 1.8834984416571388e-07\n",
      "step: 17651, loss: 1.430511264999268e-08\n",
      "step: 17652, loss: 1.621241949578689e-07\n",
      "step: 17653, loss: 2.0265525790819083e-07\n",
      "step: 17654, loss: 0.0027177506126463413\n",
      "step: 17655, loss: 1.907348412544252e-08\n",
      "step: 17656, loss: 8.344640889390575e-08\n",
      "step: 17657, loss: 7.390971745735442e-08\n",
      "step: 17658, loss: 4.76837103136063e-09\n",
      "step: 17659, loss: 1.907347702001516e-08\n",
      "step: 17660, loss: 2.622603467727913e-08\n",
      "step: 17661, loss: 2.2649655306850036e-07\n",
      "step: 17662, loss: 9.53674206272126e-09\n",
      "step: 17663, loss: 3.576277407546513e-08\n",
      "step: 17664, loss: 9.536724121517182e-08\n",
      "step: 17665, loss: 3.099441059362107e-08\n",
      "step: 17666, loss: 3.623946440711734e-07\n",
      "step: 17667, loss: 4.053114111002287e-08\n",
      "step: 17668, loss: 5.006788228456571e-08\n",
      "step: 17669, loss: 0.0005058561800979078\n",
      "step: 17670, loss: 8.106222537662688e-08\n",
      "step: 17671, loss: 1.192092558000013e-08\n",
      "step: 17672, loss: 1.859658311786916e-07\n",
      "step: 17673, loss: 9.298310033045709e-08\n",
      "step: 17674, loss: 3.576277407546513e-08\n",
      "step: 17675, loss: 1.192091971802256e-07\n",
      "step: 17676, loss: 7.15255632499634e-09\n",
      "step: 17677, loss: 0.0\n",
      "step: 17678, loss: 6.914135042279668e-08\n",
      "step: 17679, loss: 4.76837103136063e-09\n",
      "step: 17680, loss: 1.4305082629562094e-07\n",
      "step: 17681, loss: 0.055980972945690155\n",
      "step: 17682, loss: 7.08693823980866e-06\n",
      "step: 17683, loss: 2.861022529998536e-08\n",
      "step: 17684, loss: 2.38418529363571e-08\n",
      "step: 17685, loss: 7.963041639413859e-07\n",
      "step: 17686, loss: 9.53674295089968e-09\n",
      "step: 17687, loss: 3.409362818729278e-07\n",
      "step: 17688, loss: 0.0\n",
      "step: 17689, loss: 3.5762766970037774e-08\n",
      "step: 17690, loss: 1.5902227232800215e-06\n",
      "step: 17691, loss: 1.325566813648038e-06\n",
      "step: 17692, loss: 7.15255632499634e-09\n",
      "step: 17693, loss: 2.38418573772492e-09\n",
      "step: 17694, loss: 2.217284418293275e-07\n",
      "step: 17695, loss: 3.814696469817136e-08\n",
      "step: 17696, loss: 2.145766586636455e-08\n",
      "step: 17697, loss: 2.9086913855280727e-07\n",
      "step: 17698, loss: 3.0994389277338996e-08\n",
      "step: 17699, loss: 2.0980728265840298e-07\n",
      "step: 17700, loss: 8.106222537662688e-08\n",
      "step: 17701, loss: 0.07619708776473999\n",
      "step: 17702, loss: 1.931187512127508e-07\n",
      "step: 17703, loss: 3.344742935951217e-06\n",
      "step: 17704, loss: 2.145766231365087e-08\n",
      "step: 17705, loss: 5.268982476991368e-07\n",
      "step: 17706, loss: 2.384185116000026e-08\n",
      "step: 17707, loss: 5.245205159098987e-08\n",
      "step: 17708, loss: 9.53674295089968e-09\n",
      "step: 17709, loss: 0.0\n",
      "step: 17710, loss: 1.6212442233154434e-07\n",
      "step: 17711, loss: 3.33785834527589e-08\n",
      "step: 17712, loss: 1.430511264999268e-08\n",
      "step: 17713, loss: 1.001355514063107e-07\n",
      "step: 17714, loss: 0.00016910875274334103\n",
      "step: 17715, loss: 3.266308397087414e-07\n",
      "step: 17716, loss: 2.38418573772492e-09\n",
      "step: 17717, loss: 0.000166439451277256\n",
      "step: 17718, loss: 0.0009426041506230831\n",
      "step: 17719, loss: 7.152552683464819e-08\n",
      "step: 17720, loss: 2.8371647431413294e-07\n",
      "step: 17721, loss: 3.576277407546513e-08\n",
      "step: 17722, loss: 0.09730031341314316\n",
      "step: 17723, loss: 1.0013558693344748e-07\n",
      "step: 17724, loss: 1.192092646817855e-08\n",
      "step: 17725, loss: 4.76837147544984e-09\n",
      "step: 17726, loss: 2.5272277071053395e-07\n",
      "step: 17727, loss: 2.38418573772492e-09\n",
      "step: 17728, loss: 4.2199735617032275e-07\n",
      "step: 17729, loss: 2.384184938364342e-08\n",
      "step: 17730, loss: 0.028582975268363953\n",
      "step: 17731, loss: 5.960456306297601e-08\n",
      "step: 17732, loss: 1.8834980153314973e-07\n",
      "step: 17733, loss: 0.0\n",
      "step: 17734, loss: 6.604097393392294e-07\n",
      "step: 17735, loss: 0.0010088167618960142\n",
      "step: 17736, loss: 2.1219143775397242e-07\n",
      "step: 17737, loss: 0.000185844735824503\n",
      "step: 17738, loss: 4.529947972287118e-08\n",
      "step: 17739, loss: 1.430511176181426e-08\n",
      "step: 17740, loss: 0.00012401335698086768\n",
      "step: 17741, loss: 5.721973366235034e-07\n",
      "step: 17742, loss: 4.76837103136063e-09\n",
      "step: 17743, loss: 4.7683691661859484e-08\n",
      "step: 17744, loss: 6.580244189535733e-07\n",
      "step: 17745, loss: 2.622603290092229e-08\n",
      "step: 17746, loss: 9.53674117454284e-09\n",
      "step: 17747, loss: 3.5762759864610416e-08\n",
      "step: 17748, loss: 2.38418573772492e-09\n",
      "step: 17749, loss: 1.668929350273629e-08\n",
      "step: 17750, loss: 3.05175063886054e-07\n",
      "step: 17751, loss: 3.576277407546513e-08\n",
      "step: 17752, loss: 2.2888110606800183e-07\n",
      "step: 17753, loss: 0.0013763995375484228\n",
      "step: 17754, loss: 5.483619602841827e-08\n",
      "step: 17755, loss: 1.1920899112283223e-07\n",
      "step: 17756, loss: 2.813324897488201e-07\n",
      "step: 17757, loss: 0.05791912600398064\n",
      "step: 17758, loss: 4.76837147544984e-09\n",
      "step: 17759, loss: 4.076931929830607e-07\n",
      "step: 17760, loss: 6.736790965078399e-06\n",
      "step: 17761, loss: 4.1007828599504137e-07\n",
      "step: 17762, loss: 1.8358151976372028e-07\n",
      "step: 17763, loss: 5.373262865759898e-06\n",
      "step: 17764, loss: 4.4822377276432235e-07\n",
      "step: 17765, loss: 9.653912456997205e-06\n",
      "step: 17766, loss: 3.8146782799231005e-07\n",
      "step: 17767, loss: 4.768369876728684e-08\n",
      "step: 17768, loss: 1.192092646817855e-08\n",
      "step: 17769, loss: 2.6226029348208613e-08\n",
      "step: 17770, loss: 8.344640889390575e-08\n",
      "step: 17771, loss: 5.960456306297601e-08\n",
      "step: 17772, loss: 1.3113016450461146e-07\n",
      "step: 17773, loss: 1.0967232810799032e-07\n",
      "step: 17774, loss: 4.8143527237698436e-05\n",
      "step: 17775, loss: 3.6716267004521796e-07\n",
      "step: 17776, loss: 1.4948303714845679e-06\n",
      "step: 17777, loss: 7.390968193021763e-08\n",
      "step: 17778, loss: 1.8596567485928972e-07\n",
      "step: 17779, loss: 7.390963929765348e-08\n",
      "step: 17780, loss: 3.814696469817136e-08\n",
      "step: 17781, loss: 1.668929883180681e-08\n",
      "step: 17782, loss: 1.8358177555910515e-07\n",
      "step: 17783, loss: 2.932335291916388e-06\n",
      "step: 17784, loss: 4.7444908091165416e-07\n",
      "step: 17785, loss: 7.796169825269317e-07\n",
      "step: 17786, loss: 0.11669556796550751\n",
      "step: 17787, loss: 0.003083369694650173\n",
      "step: 17788, loss: 7.15255632499634e-09\n",
      "step: 17789, loss: 5.483623155555506e-08\n",
      "step: 17790, loss: 6.357657184707932e-06\n",
      "step: 17791, loss: 2.5033799033735704e-07\n",
      "step: 17792, loss: 2.38418573772492e-09\n",
      "step: 17793, loss: 2.38418573772492e-09\n",
      "step: 17794, loss: 1.6689267567926436e-07\n",
      "step: 17795, loss: 2.1457570653637958e-07\n",
      "step: 17796, loss: 2.145766231365087e-08\n",
      "step: 17797, loss: 2.884849550355284e-07\n",
      "step: 17798, loss: 4.053114111002287e-08\n",
      "step: 17799, loss: 9.437952940061223e-06\n",
      "step: 17800, loss: 4.338757207733579e-06\n",
      "step: 17801, loss: 4.76837147544984e-09\n",
      "step: 17802, loss: 1.9073478796372e-08\n",
      "step: 17803, loss: 9.0598852864332e-08\n",
      "step: 17804, loss: 1.430511176181426e-08\n",
      "step: 17805, loss: 1.192092646817855e-08\n",
      "step: 17806, loss: 9.208478877553716e-05\n",
      "step: 17807, loss: 5.483620668655931e-08\n",
      "step: 17808, loss: 2.38418529363571e-08\n",
      "step: 17809, loss: 1.0490404633856087e-07\n",
      "step: 17810, loss: 1.1444070935340278e-07\n",
      "step: 17811, loss: 6.675711006209895e-08\n",
      "step: 17812, loss: 2.861022352362852e-08\n",
      "step: 17813, loss: 1.2159314621840167e-07\n",
      "step: 17814, loss: 4.0531134004595515e-08\n",
      "step: 17815, loss: 1.14440773302249e-07\n",
      "step: 17816, loss: 1.187307248073921e-06\n",
      "step: 17817, loss: 1.4781900858906738e-07\n",
      "step: 17818, loss: 1.5020329158232926e-07\n",
      "step: 17819, loss: 4.76837103136063e-09\n",
      "step: 17820, loss: 2.38418573772492e-09\n",
      "step: 17821, loss: 3.576277052275145e-08\n",
      "step: 17822, loss: 0.09557529538869858\n",
      "step: 17823, loss: 0.08332399278879166\n",
      "step: 17824, loss: 4.1246022419727524e-07\n",
      "step: 17825, loss: 3.0517514915118227e-07\n",
      "step: 17826, loss: 4.243824491823034e-07\n",
      "step: 17827, loss: 3.385527520549658e-07\n",
      "step: 17828, loss: 7.157060463214293e-05\n",
      "step: 17829, loss: 6.914127226309574e-08\n",
      "step: 17830, loss: 6.723363412675099e-07\n",
      "step: 17831, loss: 6.914133621194196e-08\n",
      "step: 17832, loss: 1.358981620569466e-07\n",
      "step: 17833, loss: 1.0967242047854597e-07\n",
      "step: 17834, loss: 9.53674206272126e-09\n",
      "step: 17835, loss: 0.01785585843026638\n",
      "step: 17836, loss: 0.0\n",
      "step: 17837, loss: 3.8060705264797434e-05\n",
      "step: 17838, loss: 7.22396066521469e-07\n",
      "step: 17839, loss: 1.668929527909313e-08\n",
      "step: 17840, loss: 5.865012440153805e-07\n",
      "step: 17841, loss: 2.9563696557488583e-07\n",
      "step: 17842, loss: 1.0251982729414522e-07\n",
      "step: 17843, loss: 5.888869623049686e-07\n",
      "step: 17844, loss: 3.0040672527320567e-07\n",
      "step: 17845, loss: 0.06980141997337341\n",
      "step: 17846, loss: 1.0728825117212182e-07\n",
      "step: 17847, loss: 3.600100910716719e-07\n",
      "step: 17848, loss: 0.2950657904148102\n",
      "step: 17849, loss: 4.252971393725602e-06\n",
      "step: 17850, loss: 2.4817834400892025e-06\n",
      "step: 17851, loss: 2.431865482321882e-07\n",
      "step: 17852, loss: 4.5775968260386435e-07\n",
      "step: 17853, loss: 1.192092646817855e-08\n",
      "step: 17854, loss: 1.8626374753694108e-07\n",
      "step: 17855, loss: 3.313990930564614e-07\n",
      "step: 17856, loss: 7.731742516625673e-05\n",
      "step: 17857, loss: 2.622604000634965e-08\n",
      "step: 17858, loss: 0.0\n",
      "step: 17859, loss: 4.768366679286373e-08\n",
      "step: 17860, loss: 2.38418529363571e-08\n",
      "step: 17861, loss: 4.053114111002287e-08\n",
      "step: 17862, loss: 1.3828254452619149e-07\n",
      "step: 17863, loss: 1.0394787750556134e-06\n",
      "step: 17864, loss: 3.528570857724844e-07\n",
      "step: 17865, loss: 2.6226029348208613e-08\n",
      "step: 17866, loss: 0.07621121406555176\n",
      "step: 17867, loss: 7.104749215613992e-07\n",
      "step: 17868, loss: 0.11612418293952942\n",
      "step: 17869, loss: 6.588820269826101e-06\n",
      "step: 17870, loss: 1.8596581696783687e-07\n",
      "step: 17871, loss: 1.668923488296059e-07\n",
      "step: 17872, loss: 8.964357220975216e-07\n",
      "step: 17873, loss: 7.986869832166121e-07\n",
      "step: 17874, loss: 0.0\n",
      "step: 17875, loss: 1.430511176181426e-08\n",
      "step: 17876, loss: 7.152547709665669e-08\n",
      "step: 17877, loss: 1.0728818011784824e-07\n",
      "step: 17878, loss: 3.33785834527589e-08\n",
      "step: 17879, loss: 2.145766231365087e-08\n",
      "step: 17880, loss: 2.9537902719312115e-06\n",
      "step: 17881, loss: 1.192092558000013e-08\n",
      "step: 17882, loss: 1.430511264999268e-08\n",
      "step: 17883, loss: 2.1934428673375805e-07\n",
      "step: 17884, loss: 1.358982615329296e-07\n",
      "step: 17885, loss: 7.15255676908555e-09\n",
      "step: 17886, loss: 4.053112689916816e-08\n",
      "step: 17887, loss: 1.3828244505020848e-07\n",
      "step: 17888, loss: 1.2826511692765052e-06\n",
      "step: 17889, loss: 7.15255632499634e-09\n",
      "step: 17890, loss: 3.051735859571636e-07\n",
      "step: 17891, loss: 3.862344328808831e-07\n",
      "step: 17892, loss: 1.1920912612595203e-07\n",
      "step: 17893, loss: 5.722042217826129e-08\n",
      "step: 17894, loss: 5.14978353294282e-07\n",
      "step: 17895, loss: 9.059895234031501e-08\n",
      "step: 17896, loss: 6.675715980009045e-08\n",
      "step: 17897, loss: 0.0\n",
      "step: 17898, loss: 1.335140495939413e-07\n",
      "step: 17899, loss: 1.907348234908568e-08\n",
      "step: 17900, loss: 2.3603369925240258e-07\n",
      "step: 17901, loss: 1.797597747099644e-06\n",
      "step: 17902, loss: 1.668923346187512e-07\n",
      "step: 17903, loss: 0.0020034760236740112\n",
      "step: 17904, loss: 2.38418573772492e-09\n",
      "step: 17905, loss: 9.536731937487275e-08\n",
      "step: 17906, loss: 2.38418573772492e-09\n",
      "step: 17907, loss: 1.192092646817855e-08\n",
      "step: 17908, loss: 0.00014867707795929164\n",
      "step: 17909, loss: 3.8146957592744e-08\n",
      "step: 17910, loss: 2.0360007511044387e-06\n",
      "step: 17911, loss: 1.668929527909313e-08\n",
      "step: 17912, loss: 2.0980790793601045e-07\n",
      "step: 17913, loss: 3.337836744776723e-07\n",
      "step: 17914, loss: 0.0\n",
      "step: 17915, loss: 4.76837147544984e-09\n",
      "step: 17916, loss: 1.430510998545742e-08\n",
      "step: 17917, loss: 3.337858700547258e-08\n",
      "step: 17918, loss: 2.622603467727913e-08\n",
      "step: 17919, loss: 2.3841845830929742e-08\n",
      "step: 17920, loss: 0.0\n",
      "step: 17921, loss: 1.2863337360613514e-05\n",
      "step: 17922, loss: 0.0\n",
      "step: 17923, loss: 2.38418573772492e-09\n",
      "step: 17924, loss: 2.2649739150892856e-07\n",
      "step: 17925, loss: 3.099441059362107e-08\n",
      "step: 17926, loss: 6.198878566010535e-08\n",
      "step: 17927, loss: 2.3841845830929742e-08\n",
      "step: 17928, loss: 1.978864361262822e-07\n",
      "step: 17929, loss: 1.1205656846868806e-07\n",
      "step: 17930, loss: 2.1457660537294032e-08\n",
      "step: 17931, loss: 3.480898556063039e-07\n",
      "step: 17932, loss: 0.126026913523674\n",
      "step: 17933, loss: 8.227493526646867e-05\n",
      "step: 17934, loss: 7.15255676908555e-09\n",
      "step: 17935, loss: 2.8610216418201162e-08\n",
      "step: 17936, loss: 6.770982849957363e-07\n",
      "step: 17937, loss: 1.525874608887534e-07\n",
      "step: 17938, loss: 8.106221116577217e-08\n",
      "step: 17939, loss: 2.0027114544518554e-07\n",
      "step: 17940, loss: 0.0886264517903328\n",
      "step: 17941, loss: 2.145766764272139e-08\n",
      "step: 17942, loss: 1.1444083725109522e-07\n",
      "step: 17943, loss: 7.15255676908555e-09\n",
      "step: 17944, loss: 5.00678503101426e-08\n",
      "step: 17945, loss: 7.15255632499634e-09\n",
      "step: 17946, loss: 3.480896566543379e-07\n",
      "step: 17947, loss: 2.0689054508693516e-05\n",
      "step: 17948, loss: 3.8146946934602965e-08\n",
      "step: 17949, loss: 7.390967482479027e-08\n",
      "step: 17950, loss: 1.2945762364324764e-06\n",
      "step: 17951, loss: 2.4795386366349703e-07\n",
      "step: 17952, loss: 1.2874588151134958e-07\n",
      "step: 17953, loss: 3.914594799425686e-06\n",
      "step: 17954, loss: 5.24518156908016e-07\n",
      "step: 17955, loss: 1.192092646817855e-08\n",
      "step: 17956, loss: 1.8835025628050062e-07\n",
      "step: 17957, loss: 9.822615538723767e-07\n",
      "step: 17958, loss: 3.433205222336255e-07\n",
      "step: 17959, loss: 1.215932599052394e-07\n",
      "step: 17960, loss: 8.344634494505954e-08\n",
      "step: 17961, loss: 3.2186264320444025e-07\n",
      "step: 17962, loss: 1.3112985186580772e-07\n",
      "step: 17963, loss: 2.861021997091484e-08\n",
      "step: 17964, loss: 1.2421432984410785e-06\n",
      "step: 17965, loss: 6.699515324726235e-07\n",
      "step: 17966, loss: 5.722039020383818e-08\n",
      "step: 17967, loss: 1.6450844952942134e-07\n",
      "step: 17968, loss: 5.459750695990806e-07\n",
      "step: 17969, loss: 4.7683659687436375e-08\n",
      "step: 17970, loss: 8.235732821049169e-06\n",
      "step: 17971, loss: 5.292853302307776e-07\n",
      "step: 17972, loss: 3.194799944594706e-07\n",
      "step: 17973, loss: 4.053077589105669e-07\n",
      "step: 17974, loss: 1.478192217518881e-07\n",
      "step: 17975, loss: 2.38418573772492e-09\n",
      "step: 17976, loss: 0.007012339774519205\n",
      "step: 17977, loss: 5.292853302307776e-07\n",
      "step: 17978, loss: 6.127265805844218e-07\n",
      "step: 17979, loss: 5.722038309841082e-08\n",
      "step: 17980, loss: 7.748456027911743e-07\n",
      "step: 17981, loss: 1.788134937896757e-07\n",
      "step: 17982, loss: 2.145766764272139e-08\n",
      "step: 17983, loss: 1.647405269977753e-06\n",
      "step: 17984, loss: 2.622603467727913e-08\n",
      "step: 17985, loss: 5.483623510826874e-08\n",
      "step: 17986, loss: 6.675714558923573e-08\n",
      "step: 17987, loss: 8.177604513548431e-07\n",
      "step: 17988, loss: 1.9248875105404295e-05\n",
      "step: 17989, loss: 0.0005754197482019663\n",
      "step: 17990, loss: 3.099440704090739e-08\n",
      "step: 17991, loss: 4.76837147544984e-09\n",
      "step: 17992, loss: 0.1122874990105629\n",
      "step: 17993, loss: 2.38418573772492e-09\n",
      "step: 17994, loss: 4.672975819630665e-07\n",
      "step: 17995, loss: 1.2636164115065185e-07\n",
      "step: 17996, loss: 2.581907665444305e-06\n",
      "step: 17997, loss: 1.1444076619682164e-07\n",
      "step: 17998, loss: 5.19749050909013e-07\n",
      "step: 17999, loss: 6.198879276553271e-08\n",
      "step: 18000, loss: 5.602772716883919e-07\n",
      "step: 18001, loss: 1.5497160177346814e-07\n",
      "step: 18002, loss: 7.65309209782572e-07\n",
      "step: 18003, loss: 6.67571669055178e-08\n",
      "step: 18004, loss: 1.7166070165330893e-07\n",
      "step: 18005, loss: 7.867804896477537e-08\n",
      "step: 18006, loss: 8.106221827119953e-08\n",
      "step: 18007, loss: 1.1444060277199242e-07\n",
      "step: 18008, loss: 2.2411276745515352e-07\n",
      "step: 18009, loss: 5.483622800284138e-08\n",
      "step: 18010, loss: 5.984222184451937e-07\n",
      "step: 18011, loss: 3.5762766970037774e-08\n",
      "step: 18012, loss: 0.06892058253288269\n",
      "step: 18013, loss: 0.0\n",
      "step: 18014, loss: 3.1709467407381453e-07\n",
      "step: 18015, loss: 6.96178005910042e-07\n",
      "step: 18016, loss: 1.1205646188727769e-07\n",
      "step: 18017, loss: 8.106222537662688e-08\n",
      "step: 18018, loss: 5.245205159098987e-08\n",
      "step: 18019, loss: 9.536731937487275e-08\n",
      "step: 18020, loss: 3.0994396382766354e-08\n",
      "step: 18021, loss: 1.192092646817855e-08\n",
      "step: 18022, loss: 0.002108857501298189\n",
      "step: 18023, loss: 1.1444078751310371e-07\n",
      "step: 18024, loss: 2.8610216418201162e-08\n",
      "step: 18025, loss: 3.099440704090739e-08\n",
      "step: 18026, loss: 1.6640957483105012e-06\n",
      "step: 18027, loss: 5.459752401293372e-07\n",
      "step: 18028, loss: 1.430511264999268e-08\n",
      "step: 18029, loss: 6.675713848380838e-08\n",
      "step: 18030, loss: 2.861022352362852e-08\n",
      "step: 18031, loss: 6.389542477336363e-07\n",
      "step: 18032, loss: 0.05235520005226135\n",
      "step: 18033, loss: 1.907348234908568e-08\n",
      "step: 18034, loss: 1.668929883180681e-08\n",
      "step: 18035, loss: 4.76837147544984e-09\n",
      "step: 18036, loss: 8.583059951661198e-08\n",
      "step: 18037, loss: 2.8060035219823476e-06\n",
      "step: 18038, loss: 2.38418573772492e-09\n",
      "step: 18039, loss: 2.38418573772492e-09\n",
      "step: 18040, loss: 7.15255632499634e-09\n",
      "step: 18041, loss: 7.867802054306594e-08\n",
      "step: 18042, loss: 0.07740314304828644\n",
      "step: 18043, loss: 2.38418573772492e-09\n",
      "step: 18044, loss: 2.6724947019829415e-06\n",
      "step: 18045, loss: 0.0\n",
      "step: 18046, loss: 0.0\n",
      "step: 18047, loss: 2.7916889848711435e-06\n",
      "step: 18048, loss: 4.291530331101967e-08\n",
      "step: 18049, loss: 1.1205654715240598e-07\n",
      "step: 18050, loss: 1.668929350273629e-08\n",
      "step: 18051, loss: 1.907348234908568e-08\n",
      "step: 18052, loss: 9.53674295089968e-09\n",
      "step: 18053, loss: 0.043518032878637314\n",
      "step: 18054, loss: 3.671630963708594e-07\n",
      "step: 18055, loss: 3.5285731314615987e-07\n",
      "step: 18056, loss: 2.813327739659144e-07\n",
      "step: 18057, loss: 4.1007760387401504e-07\n",
      "step: 18058, loss: 3.8146950487316644e-08\n",
      "step: 18059, loss: 1.1205644057099562e-07\n",
      "step: 18060, loss: 1.072881872232756e-07\n",
      "step: 18061, loss: 4.768367034557741e-08\n",
      "step: 18062, loss: 7.15255632499634e-09\n",
      "step: 18063, loss: 6.48489674404118e-07\n",
      "step: 18064, loss: 3.218638369162363e-07\n",
      "step: 18065, loss: 5.555113489208452e-07\n",
      "step: 18066, loss: 5.4836217344700344e-08\n",
      "step: 18067, loss: 7.629382992035971e-08\n",
      "step: 18068, loss: 0.0\n",
      "step: 18069, loss: 1.5735572844732815e-07\n",
      "step: 18070, loss: 0.09298484027385712\n",
      "step: 18071, loss: 8.385822184209246e-06\n",
      "step: 18072, loss: 2.0980755266464257e-07\n",
      "step: 18073, loss: 0.15561015903949738\n",
      "step: 18074, loss: 2.622603467727913e-08\n",
      "step: 18075, loss: 0.06793377548456192\n",
      "step: 18076, loss: 0.0\n",
      "step: 18077, loss: 6.67571669055178e-08\n",
      "step: 18078, loss: 0.0\n",
      "step: 18079, loss: 1.4900774658599403e-06\n",
      "step: 18080, loss: 5.7741949888168165e-08\n",
      "step: 18081, loss: 1.192092824453539e-08\n",
      "step: 18082, loss: 0.0376090407371521\n",
      "step: 18083, loss: 7.15255676908555e-09\n",
      "step: 18084, loss: 7.390847258648137e-07\n",
      "step: 18085, loss: 9.536730516401803e-08\n",
      "step: 18086, loss: 3.9338721080639516e-07\n",
      "step: 18087, loss: 1.430511264999268e-08\n",
      "step: 18088, loss: 6.174336158437654e-05\n",
      "step: 18089, loss: 7.867802054306594e-08\n",
      "step: 18090, loss: 4.768369876728684e-08\n",
      "step: 18091, loss: 4.76837103136063e-09\n",
      "step: 18092, loss: 1.3494235417965683e-06\n",
      "step: 18093, loss: 3.576275631189674e-08\n",
      "step: 18094, loss: 1.7166090060527495e-07\n",
      "step: 18095, loss: 8.106225379833631e-08\n",
      "step: 18096, loss: 8.177667609743366e-07\n",
      "step: 18097, loss: 5.0067846757428924e-08\n",
      "step: 18098, loss: 4.529949038101222e-08\n",
      "step: 18099, loss: 5.531246642931364e-07\n",
      "step: 18100, loss: 2.2649655306850036e-07\n",
      "step: 18101, loss: 1.3589811942438246e-07\n",
      "step: 18102, loss: 4.4345611627250037e-07\n",
      "step: 18103, loss: 1.668929883180681e-08\n",
      "step: 18104, loss: 2.057455731119262e-06\n",
      "step: 18105, loss: 1.430511264999268e-08\n",
      "step: 18106, loss: 4.0531130451881836e-08\n",
      "step: 18107, loss: 3.099440704090739e-08\n",
      "step: 18108, loss: 6.198877855467799e-08\n",
      "step: 18109, loss: 0.1057901680469513\n",
      "step: 18110, loss: 4.529951169729429e-08\n",
      "step: 18111, loss: 1.692770155159451e-07\n",
      "step: 18112, loss: 4.76837103136063e-09\n",
      "step: 18113, loss: 2.38418573772492e-09\n",
      "step: 18114, loss: 1.1205656846868806e-07\n",
      "step: 18115, loss: 0.1131751760840416\n",
      "step: 18116, loss: 4.2915317521874385e-08\n",
      "step: 18117, loss: 0.05187356099486351\n",
      "step: 18118, loss: 5.245205159098987e-08\n",
      "step: 18119, loss: 5.9604616353681195e-08\n",
      "step: 18120, loss: 4.76837147544984e-09\n",
      "step: 18121, loss: 2.0503917141923012e-07\n",
      "step: 18122, loss: 1.6689264725755493e-07\n",
      "step: 18123, loss: 4.053111624102712e-08\n",
      "step: 18124, loss: 1.3088964578855666e-06\n",
      "step: 18125, loss: 1.335140495939413e-07\n",
      "step: 18126, loss: 7.15255632499634e-09\n",
      "step: 18127, loss: 5.102093041386979e-07\n",
      "step: 18128, loss: 1.6450820794489118e-07\n",
      "step: 18129, loss: 0.0019960252102464437\n",
      "step: 18130, loss: 9.536734779658218e-08\n",
      "step: 18131, loss: 0.2967245280742645\n",
      "step: 18132, loss: 1.2159321727267525e-07\n",
      "step: 18133, loss: 1.192092558000013e-08\n",
      "step: 18134, loss: 1.1634657539616455e-06\n",
      "step: 18135, loss: 5.9604552404834976e-08\n",
      "step: 18136, loss: 1.9073478796372e-08\n",
      "step: 18137, loss: 1.1920899822825959e-07\n",
      "step: 18138, loss: 0.0\n",
      "step: 18139, loss: 1.5974022460341075e-07\n",
      "step: 18140, loss: 1.430511264999268e-08\n",
      "step: 18141, loss: 2.861021997091484e-08\n",
      "step: 18142, loss: 1.0967242758397333e-07\n",
      "step: 18143, loss: 6.437296207195686e-08\n",
      "step: 18144, loss: 1.1444076619682164e-07\n",
      "step: 18145, loss: 1.549715733517587e-07\n",
      "step: 18146, loss: 2.861022529998536e-08\n",
      "step: 18147, loss: 0.0\n",
      "step: 18148, loss: 5.412055656961456e-07\n",
      "step: 18149, loss: 3.8146957592744e-08\n",
      "step: 18150, loss: 8.821472619047199e-08\n",
      "step: 18151, loss: 9.29830648033203e-08\n",
      "step: 18152, loss: 4.291533528544278e-08\n",
      "step: 18153, loss: 1.9788649296970107e-07\n",
      "step: 18154, loss: 1.0251984150499993e-07\n",
      "step: 18155, loss: 7.15255676908555e-09\n",
      "step: 18156, loss: 6.079594072616601e-07\n",
      "step: 18157, loss: 6.675662120869674e-07\n",
      "step: 18158, loss: 1.811977909937923e-07\n",
      "step: 18159, loss: 4.768370232000052e-08\n",
      "step: 18160, loss: 2.717958409448329e-07\n",
      "step: 18161, loss: 1.0967241337311862e-07\n",
      "step: 18162, loss: 2.622603467727913e-08\n",
      "step: 18163, loss: 1.9073478796372e-08\n",
      "step: 18164, loss: 1.001355514063107e-07\n",
      "step: 18165, loss: 1.5974005407315417e-07\n",
      "step: 18166, loss: 0.06899810582399368\n",
      "step: 18167, loss: 3.9073020161595196e-06\n",
      "step: 18168, loss: 3.189884409948718e-06\n",
      "step: 18169, loss: 4.959076136401563e-07\n",
      "step: 18170, loss: 2.4412643142568413e-06\n",
      "step: 18171, loss: 2.1457660537294032e-08\n",
      "step: 18172, loss: 1.668929527909313e-08\n",
      "step: 18173, loss: 1.192092558000013e-08\n",
      "step: 18174, loss: 7.15255632499634e-09\n",
      "step: 18175, loss: 1.3589838943062205e-07\n",
      "step: 18176, loss: 2.622603467727913e-08\n",
      "step: 18177, loss: 4.291530331101967e-08\n",
      "step: 18178, loss: 1.2874561150511e-07\n",
      "step: 18179, loss: 2.8610211089130644e-08\n",
      "step: 18180, loss: 1.430510998545742e-08\n",
      "step: 18181, loss: 3.0035134841455147e-05\n",
      "step: 18182, loss: 1.192092646817855e-08\n",
      "step: 18183, loss: 1.0967235652969975e-07\n",
      "step: 18184, loss: 1.0490409607655238e-07\n",
      "step: 18185, loss: 1.3828258715875563e-07\n",
      "step: 18186, loss: 7.15255676908555e-09\n",
      "step: 18187, loss: 1.4066674225432507e-07\n",
      "step: 18188, loss: 1.1205649741441448e-07\n",
      "step: 18189, loss: 1.1920912612595203e-07\n",
      "step: 18190, loss: 7.390963929765348e-08\n",
      "step: 18191, loss: 3.337859411089994e-08\n",
      "step: 18192, loss: 1.263615132529594e-07\n",
      "step: 18193, loss: 6.198879987096007e-08\n",
      "step: 18194, loss: 1.668929883180681e-08\n",
      "step: 18195, loss: 1.668929705544997e-08\n",
      "step: 18196, loss: 7.462365942956239e-07\n",
      "step: 18197, loss: 1.5591970168316038e-06\n",
      "step: 18198, loss: 2.5272305492762825e-07\n",
      "step: 18199, loss: 1.907348412544252e-08\n",
      "step: 18200, loss: 7.15255676908555e-09\n",
      "step: 18201, loss: 0.20712217688560486\n",
      "step: 18202, loss: 4.4583805447473424e-07\n",
      "step: 18203, loss: 9.870357189356582e-07\n",
      "step: 18204, loss: 0.06678914278745651\n",
      "step: 18205, loss: 1.192092558000013e-08\n",
      "step: 18206, loss: 1.430511264999268e-08\n",
      "step: 18207, loss: 9.059888839146879e-08\n",
      "step: 18208, loss: 1.430511176181426e-08\n",
      "step: 18209, loss: 7.15255676908555e-09\n",
      "step: 18210, loss: 4.76837103136063e-09\n",
      "step: 18211, loss: 0.0\n",
      "step: 18212, loss: 9.727243650559103e-07\n",
      "step: 18213, loss: 3.3616751693443803e-07\n",
      "step: 18214, loss: 0.02886057458817959\n",
      "step: 18215, loss: 0.0\n",
      "step: 18216, loss: 1.2182823638795526e-06\n",
      "step: 18217, loss: 2.38418573772492e-09\n",
      "step: 18218, loss: 2.38418573772492e-09\n",
      "step: 18219, loss: 4.053114466273655e-08\n",
      "step: 18220, loss: 4.291530331101967e-08\n",
      "step: 18221, loss: 9.631913826524396e-07\n",
      "step: 18222, loss: 0.08700350672006607\n",
      "step: 18223, loss: 1.5974001144059002e-07\n",
      "step: 18224, loss: 8.106226090376367e-08\n",
      "step: 18225, loss: 1.907348234908568e-08\n",
      "step: 18226, loss: 3.337859055818626e-08\n",
      "step: 18227, loss: 0.06783562153577805\n",
      "step: 18228, loss: 4.2915317521874385e-08\n",
      "step: 18229, loss: 1.0490403923313352e-07\n",
      "step: 18230, loss: 4.7683688109145805e-08\n",
      "step: 18231, loss: 2.861022529998536e-08\n",
      "step: 18232, loss: 7.390963219222613e-08\n",
      "step: 18233, loss: 2.38418573772492e-09\n",
      "step: 18234, loss: 0.0\n",
      "step: 18235, loss: 4.506070752086089e-07\n",
      "step: 18236, loss: 2.527224296500208e-07\n",
      "step: 18237, loss: 5.2452055143703546e-08\n",
      "step: 18238, loss: 3.238787758164108e-05\n",
      "step: 18239, loss: 9.631886541683343e-07\n",
      "step: 18240, loss: 4.76837103136063e-09\n",
      "step: 18241, loss: 7.39097032464997e-08\n",
      "step: 18242, loss: 9.53672767423086e-08\n",
      "step: 18243, loss: 5.459715453071112e-07\n",
      "step: 18244, loss: 0.10992774367332458\n",
      "step: 18245, loss: 6.055745416233549e-07\n",
      "step: 18246, loss: 5.006788228456571e-08\n",
      "step: 18247, loss: 0.0\n",
      "step: 18248, loss: 3.4808951454579073e-07\n",
      "step: 18249, loss: 1.430511264999268e-08\n",
      "step: 18250, loss: 2.38418573772492e-09\n",
      "step: 18251, loss: 5.078258027424454e-07\n",
      "step: 18252, loss: 1.430511264999268e-08\n",
      "step: 18253, loss: 1.907348234908568e-08\n",
      "step: 18254, loss: 1.43051135381711e-08\n",
      "step: 18255, loss: 1.192092824453539e-08\n",
      "step: 18256, loss: 7.038146577542648e-05\n",
      "step: 18257, loss: 8.821468355790785e-08\n",
      "step: 18258, loss: 4.3630177515296964e-07\n",
      "step: 18259, loss: 2.38418573772492e-09\n",
      "step: 18260, loss: 1.192092646817855e-08\n",
      "step: 18261, loss: 1.0490410318197974e-07\n",
      "step: 18262, loss: 9.53674206272126e-09\n",
      "step: 18263, loss: 1.3112986607666244e-07\n",
      "step: 18264, loss: 4.76837103136063e-09\n",
      "step: 18265, loss: 2.38418573772492e-09\n",
      "step: 18266, loss: 9.53674206272126e-09\n",
      "step: 18267, loss: 9.059888128604143e-08\n",
      "step: 18268, loss: 9.77513892053139e-08\n",
      "step: 18269, loss: 4.2915321074588064e-08\n",
      "step: 18270, loss: 2.38418573772492e-09\n",
      "step: 18271, loss: 2.8610216418201162e-08\n",
      "step: 18272, loss: 1.192092824453539e-08\n",
      "step: 18273, loss: 1.7166068744245422e-07\n",
      "step: 18274, loss: 9.53674117454284e-09\n",
      "step: 18275, loss: 1.430511176181426e-08\n",
      "step: 18276, loss: 0.0006913011311553419\n",
      "step: 18277, loss: 2.38418529363571e-08\n",
      "step: 18278, loss: 7.15255676908555e-09\n",
      "step: 18279, loss: 9.53674206272126e-09\n",
      "step: 18280, loss: 3.5762516858994786e-07\n",
      "step: 18281, loss: 1.430511176181426e-08\n",
      "step: 18282, loss: 4.053114111002287e-08\n",
      "step: 18283, loss: 3.8146946934602965e-08\n",
      "step: 18284, loss: 3.8385064726753626e-07\n",
      "step: 18285, loss: 8.583051425148369e-08\n",
      "step: 18286, loss: 1.8596600170894817e-07\n",
      "step: 18287, loss: 4.29153317327291e-08\n",
      "step: 18288, loss: 7.15255632499634e-09\n",
      "step: 18289, loss: 0.058313772082328796\n",
      "step: 18290, loss: 2.9802174594806274e-07\n",
      "step: 18291, loss: 2.38418573772492e-09\n",
      "step: 18292, loss: 0.0\n",
      "step: 18293, loss: 1.668929883180681e-08\n",
      "step: 18294, loss: 2.38418573772492e-09\n",
      "step: 18295, loss: 0.010510701686143875\n",
      "step: 18296, loss: 0.07429073750972748\n",
      "step: 18297, loss: 9.27435394260101e-07\n",
      "step: 18298, loss: 1.5497160177346814e-07\n",
      "step: 18299, loss: 2.598758328531403e-07\n",
      "step: 18300, loss: 0.04212329909205437\n",
      "step: 18301, loss: 2.38418573772492e-09\n",
      "step: 18302, loss: 2.8610209312773804e-08\n",
      "step: 18303, loss: 4.76837103136063e-09\n",
      "step: 18304, loss: 2.455697938330559e-07\n",
      "step: 18305, loss: 2.38418573772492e-09\n",
      "step: 18306, loss: 0.003851932007819414\n",
      "step: 18307, loss: 2.622603467727913e-08\n",
      "step: 18308, loss: 9.53674117454284e-09\n",
      "step: 18309, loss: 1.668929883180681e-08\n",
      "step: 18310, loss: 6.103450118644105e-07\n",
      "step: 18311, loss: 2.38418573772492e-09\n",
      "step: 18312, loss: 3.0110040825093165e-06\n",
      "step: 18313, loss: 1.3231932598500862e-06\n",
      "step: 18314, loss: 1.9311814014599804e-07\n",
      "step: 18315, loss: 1.907348234908568e-08\n",
      "step: 18316, loss: 2.622604000634965e-08\n",
      "step: 18317, loss: 3.33785834527589e-08\n",
      "step: 18318, loss: 3.0278931717475643e-07\n",
      "step: 18319, loss: 5.722040796740657e-08\n",
      "step: 18320, loss: 6.914133621194196e-08\n",
      "step: 18321, loss: 2.6702730338001857e-07\n",
      "step: 18322, loss: 1.192092824453539e-08\n",
      "step: 18323, loss: 2.38418573772492e-09\n",
      "step: 18324, loss: 2.145766764272139e-08\n",
      "step: 18325, loss: 1.0490403923313352e-07\n",
      "step: 18326, loss: 0.0\n",
      "step: 18327, loss: 7.390967482479027e-08\n",
      "step: 18328, loss: 8.106222537662688e-08\n",
      "step: 18329, loss: 8.821479013931821e-08\n",
      "step: 18330, loss: 4.291530686373335e-08\n",
      "step: 18331, loss: 1.192092646817855e-08\n",
      "step: 18332, loss: 2.38418573772492e-09\n",
      "step: 18333, loss: 4.7683688109145805e-08\n",
      "step: 18334, loss: 3.576277407546513e-08\n",
      "step: 18335, loss: 4.529949748643958e-08\n",
      "step: 18336, loss: 1.928715619214927e-06\n",
      "step: 18337, loss: 0.07545148581266403\n",
      "step: 18338, loss: 2.384185116000026e-08\n",
      "step: 18339, loss: 4.768367034557741e-08\n",
      "step: 18340, loss: 3.099439993548003e-08\n",
      "step: 18341, loss: 6.818694373578182e-07\n",
      "step: 18342, loss: 0.0\n",
      "step: 18343, loss: 1.2636166957236128e-07\n",
      "step: 18344, loss: 0.0509437620639801\n",
      "step: 18345, loss: 4.29153317327291e-08\n",
      "step: 18346, loss: 6.198875013296856e-08\n",
      "step: 18347, loss: 1.907347702001516e-08\n",
      "step: 18348, loss: 5.0067875179138355e-08\n",
      "step: 18349, loss: 8.106221116577217e-08\n",
      "step: 18350, loss: 2.38418529363571e-08\n",
      "step: 18351, loss: 6.493034743471071e-05\n",
      "step: 18352, loss: 3.4808897453331156e-07\n",
      "step: 18353, loss: 1.430511264999268e-08\n",
      "step: 18354, loss: 7.146602001739666e-06\n",
      "step: 18355, loss: 2.38418529363571e-08\n",
      "step: 18356, loss: 1.8835027049135533e-07\n",
      "step: 18357, loss: 0.00015147325757425278\n",
      "step: 18358, loss: 2.7894878940060153e-07\n",
      "step: 18359, loss: 3.099439993548003e-08\n",
      "step: 18360, loss: 1.4781898016735795e-07\n",
      "step: 18361, loss: 4.76837103136063e-09\n",
      "step: 18362, loss: 4.410700853441085e-07\n",
      "step: 18363, loss: 4.76837147544984e-09\n",
      "step: 18364, loss: 5.7220425730974966e-08\n",
      "step: 18365, loss: 8.583057820032991e-08\n",
      "step: 18366, loss: 1.9310987227072474e-06\n",
      "step: 18367, loss: 0.03998420014977455\n",
      "step: 18368, loss: 1.4066671383261564e-07\n",
      "step: 18369, loss: 6.198879987096007e-08\n",
      "step: 18370, loss: 7.629387255292386e-08\n",
      "step: 18371, loss: 0.1156376451253891\n",
      "step: 18372, loss: 4.625286180726107e-07\n",
      "step: 18373, loss: 2.384185648907078e-08\n",
      "step: 18374, loss: 3.8623613818344893e-07\n",
      "step: 18375, loss: 3.576277407546513e-08\n",
      "step: 18376, loss: 1.001355514063107e-07\n",
      "step: 18377, loss: 1.5497177230372472e-07\n",
      "step: 18378, loss: 6.437291233396536e-08\n",
      "step: 18379, loss: 2.0980725423669355e-07\n",
      "step: 18380, loss: 3.075581958000839e-07\n",
      "step: 18381, loss: 7.86772091032617e-07\n",
      "step: 18382, loss: 1.192092824453539e-08\n",
      "step: 18383, loss: 1.430465772500611e-06\n",
      "step: 18384, loss: 9.536724121517182e-08\n",
      "step: 18385, loss: 3.576278118089249e-08\n",
      "step: 18386, loss: 1.9073478796372e-08\n",
      "step: 18387, loss: 3.337858700547258e-08\n",
      "step: 18388, loss: 5.006786096828364e-08\n",
      "step: 18389, loss: 2.861022529998536e-08\n",
      "step: 18390, loss: 5.006783965200157e-08\n",
      "step: 18391, loss: 4.458388502825983e-07\n",
      "step: 18392, loss: 7.15255676908555e-09\n",
      "step: 18393, loss: 1.28745710981093e-07\n",
      "step: 18394, loss: 3.576277407546513e-08\n",
      "step: 18395, loss: 2.861021997091484e-08\n",
      "step: 18396, loss: 3.099440704090739e-08\n",
      "step: 18397, loss: 4.25298594564083e-06\n",
      "step: 18398, loss: 2.8610097047021554e-07\n",
      "step: 18399, loss: 0.06700160354375839\n",
      "step: 18400, loss: 5.483622800284138e-08\n",
      "step: 18401, loss: 1.430511264999268e-08\n",
      "step: 18402, loss: 7.15255676908555e-09\n",
      "step: 18403, loss: 0.0\n",
      "step: 18404, loss: 8.344634494505954e-08\n",
      "step: 18405, loss: 2.38418573772492e-09\n",
      "step: 18406, loss: 2.2386277578334557e-06\n",
      "step: 18407, loss: 0.0\n",
      "step: 18408, loss: 0.00017620284052100033\n",
      "step: 18409, loss: 1.1682499234666466e-07\n",
      "step: 18410, loss: 1.931187512127508e-07\n",
      "step: 18411, loss: 5.483622800284138e-08\n",
      "step: 18412, loss: 4.196131726530439e-07\n",
      "step: 18413, loss: 1.907347702001516e-08\n",
      "step: 18414, loss: 2.622603467727913e-08\n",
      "step: 18415, loss: 2.38418529363571e-08\n",
      "step: 18416, loss: 2.2411268219002523e-07\n",
      "step: 18417, loss: 2.2172888236582367e-07\n",
      "step: 18418, loss: 1.430508973498945e-07\n",
      "step: 18419, loss: 0.11692482233047485\n",
      "step: 18420, loss: 4.7683688109145805e-08\n",
      "step: 18421, loss: 0.1851380318403244\n",
      "step: 18422, loss: 1.549717438820153e-07\n",
      "step: 18423, loss: 0.12477909028530121\n",
      "step: 18424, loss: 1.430511264999268e-08\n",
      "step: 18425, loss: 4.148470225118217e-07\n",
      "step: 18426, loss: 2.38418573772492e-09\n",
      "step: 18427, loss: 2.622603467727913e-08\n",
      "step: 18428, loss: 7.15255676908555e-09\n",
      "step: 18429, loss: 8.344639468305104e-08\n",
      "step: 18430, loss: 2.7797743769042427e-06\n",
      "step: 18431, loss: 1.0132536090168287e-06\n",
      "step: 18432, loss: 6.651829380643903e-07\n",
      "step: 18433, loss: 4.601452303631959e-07\n",
      "step: 18434, loss: 5.006785741556996e-08\n",
      "step: 18435, loss: 0.06894533336162567\n",
      "step: 18436, loss: 4.76837147544984e-09\n",
      "step: 18437, loss: 2.145766764272139e-08\n",
      "step: 18438, loss: 3.528564320731675e-07\n",
      "step: 18439, loss: 3.337859055818626e-08\n",
      "step: 18440, loss: 1.192092824453539e-08\n",
      "step: 18441, loss: 0.05194289609789848\n",
      "step: 18442, loss: 0.000567837618291378\n",
      "step: 18443, loss: 2.4557050437579164e-07\n",
      "step: 18444, loss: 3.623929956120264e-07\n",
      "step: 18445, loss: 4.100761827885435e-07\n",
      "step: 18446, loss: 6.174993245622318e-07\n",
      "step: 18447, loss: 1.5639698176528327e-06\n",
      "step: 18448, loss: 3.004062136824359e-07\n",
      "step: 18449, loss: 1.3351419170248846e-07\n",
      "step: 18450, loss: 2.527230833493377e-07\n",
      "step: 18451, loss: 4.76837147544984e-09\n",
      "step: 18452, loss: 2.38418573772492e-09\n",
      "step: 18453, loss: 1.430511264999268e-08\n",
      "step: 18454, loss: 3.1947860179570853e-07\n",
      "step: 18455, loss: 1.502034336908764e-07\n",
      "step: 18456, loss: 2.38418573772492e-09\n",
      "step: 18457, loss: 3.576278118089249e-08\n",
      "step: 18458, loss: 4.219981519781868e-07\n",
      "step: 18459, loss: 3.0994396382766354e-08\n",
      "step: 18460, loss: 2.114662265739753e-06\n",
      "step: 18461, loss: 9.53674206272126e-09\n",
      "step: 18462, loss: 7.15255632499634e-09\n",
      "step: 18463, loss: 1.192092646817855e-08\n",
      "step: 18464, loss: 4.7683688109145805e-08\n",
      "step: 18465, loss: 1.668929527909313e-08\n",
      "step: 18466, loss: 1.568735115142772e-06\n",
      "step: 18467, loss: 1.0251983439957257e-07\n",
      "step: 18468, loss: 1.430511264999268e-08\n",
      "step: 18469, loss: 9.059889549689615e-08\n",
      "step: 18470, loss: 0.0\n",
      "step: 18471, loss: 0.10966138541698456\n",
      "step: 18472, loss: 1.573559558210036e-07\n",
      "step: 18473, loss: 1.358983467980579e-07\n",
      "step: 18474, loss: 2.789481925447035e-07\n",
      "step: 18475, loss: 0.0\n",
      "step: 18476, loss: 0.0\n",
      "step: 18477, loss: 4.768366679286373e-08\n",
      "step: 18478, loss: 4.701079888036475e-06\n",
      "step: 18479, loss: 7.867804185934801e-08\n",
      "step: 18480, loss: 0.05413982272148132\n",
      "step: 18481, loss: 0.19132965803146362\n",
      "step: 18482, loss: 6.4124924392672256e-06\n",
      "step: 18483, loss: 2.145766586636455e-08\n",
      "step: 18484, loss: 5.006788938999307e-08\n",
      "step: 18485, loss: 2.5104036467382684e-06\n",
      "step: 18486, loss: 6.12727717452799e-07\n",
      "step: 18487, loss: 2.38418573772492e-09\n",
      "step: 18488, loss: 1.907348234908568e-08\n",
      "step: 18489, loss: 5.483623155555506e-08\n",
      "step: 18490, loss: 1.9788663507824822e-07\n",
      "step: 18491, loss: 4.005394202977186e-07\n",
      "step: 18492, loss: 6.91412793685231e-08\n",
      "step: 18493, loss: 1.5258730456935155e-07\n",
      "step: 18494, loss: 4.7683505499662715e-07\n",
      "step: 18495, loss: 2.145766586636455e-08\n",
      "step: 18496, loss: 3.5762759864610416e-08\n",
      "step: 18497, loss: 5.0067878731852034e-08\n",
      "step: 18498, loss: 4.76837147544984e-09\n",
      "step: 18499, loss: 2.38418573772492e-09\n",
      "step: 18500, loss: 0.0\n",
      "step: 18501, loss: 6.198879276553271e-08\n",
      "step: 18502, loss: 1.566350533721561e-06\n",
      "step: 18503, loss: 0.0\n",
      "step: 18504, loss: 5.960460924825384e-08\n",
      "step: 18505, loss: 0.0005905042635276914\n",
      "step: 18506, loss: 2.38418573772492e-09\n",
      "step: 18507, loss: 0.0\n",
      "step: 18508, loss: 2.38418573772492e-09\n",
      "step: 18509, loss: 9.53674295089968e-09\n",
      "step: 18510, loss: 7.486228241759818e-07\n",
      "step: 18511, loss: 2.980224280690891e-07\n",
      "step: 18512, loss: 4.053112689916816e-08\n",
      "step: 18513, loss: 0.0\n",
      "step: 18514, loss: 1.239773865790994e-07\n",
      "step: 18515, loss: 1.907343545326512e-07\n",
      "step: 18516, loss: 9.241357474820688e-06\n",
      "step: 18517, loss: 4.291530686373335e-08\n",
      "step: 18518, loss: 5.9815807617269456e-05\n",
      "step: 18519, loss: 2.0693664737336803e-06\n",
      "step: 18520, loss: 0.0\n",
      "step: 18521, loss: 9.53674206272126e-09\n",
      "step: 18522, loss: 1.192092824453539e-08\n",
      "step: 18523, loss: 0.23140503466129303\n",
      "step: 18524, loss: 1.8596568907014444e-07\n",
      "step: 18525, loss: 1.907347702001516e-08\n",
      "step: 18526, loss: 1.9073405610470218e-07\n",
      "step: 18527, loss: 1.239773723682447e-07\n",
      "step: 18528, loss: 1.7642923921812326e-07\n",
      "step: 18529, loss: 1.430511264999268e-08\n",
      "step: 18530, loss: 3.7118356885912362e-06\n",
      "step: 18531, loss: 0.0\n",
      "step: 18532, loss: 8.940678242197464e-08\n",
      "step: 18533, loss: 1.192092824453539e-08\n",
      "step: 18534, loss: 2.384185648907078e-08\n",
      "step: 18535, loss: 0.0797034278512001\n",
      "step: 18536, loss: 5.96045985901128e-08\n",
      "step: 18537, loss: 0.07040829211473465\n",
      "step: 18538, loss: 0.0\n",
      "step: 18539, loss: 1.4305091156074923e-07\n",
      "step: 18540, loss: 1.6927690182910737e-07\n",
      "step: 18541, loss: 0.001774814329110086\n",
      "step: 18542, loss: 3.0994389277338996e-08\n",
      "step: 18543, loss: 3.743151353319263e-07\n",
      "step: 18544, loss: 3.5762763417324095e-08\n",
      "step: 18545, loss: 7.15255676908555e-09\n",
      "step: 18546, loss: 2.38418529363571e-08\n",
      "step: 18547, loss: 4.76837147544984e-09\n",
      "step: 18548, loss: 0.02917204052209854\n",
      "step: 18549, loss: 2.861021997091484e-08\n",
      "step: 18550, loss: 0.0\n",
      "step: 18551, loss: 0.03886141628026962\n",
      "step: 18552, loss: 1.2397731552482583e-07\n",
      "step: 18553, loss: 1.430510998545742e-08\n",
      "step: 18554, loss: 1.7404482832716894e-07\n",
      "step: 18555, loss: 0.15452992916107178\n",
      "step: 18556, loss: 7.15255676908555e-09\n",
      "step: 18557, loss: 5.7220432836402324e-08\n",
      "step: 18558, loss: 9.059895234031501e-08\n",
      "step: 18559, loss: 4.529951169729429e-08\n",
      "step: 18560, loss: 1.192092646817855e-08\n",
      "step: 18561, loss: 4.053112334645448e-08\n",
      "step: 18562, loss: 4.76837103136063e-09\n",
      "step: 18563, loss: 3.8146946934602965e-08\n",
      "step: 18564, loss: 2.384185116000026e-08\n",
      "step: 18565, loss: 9.53674117454284e-09\n",
      "step: 18566, loss: 1.668929883180681e-08\n",
      "step: 18567, loss: 2.861021997091484e-08\n",
      "step: 18568, loss: 1.9311868015847722e-07\n",
      "step: 18569, loss: 1.192092646817855e-08\n",
      "step: 18570, loss: 7.15255632499634e-09\n",
      "step: 18571, loss: 4.76837147544984e-09\n",
      "step: 18572, loss: 4.5299501039153256e-08\n",
      "step: 18573, loss: 1.001333203021204e-06\n",
      "step: 18574, loss: 1.192092646817855e-08\n",
      "step: 18575, loss: 2.384184938364342e-08\n",
      "step: 18576, loss: 3.337859055818626e-08\n",
      "step: 18577, loss: 3.576275631189674e-08\n",
      "step: 18578, loss: 2.145766764272139e-08\n",
      "step: 18579, loss: 5.7220404414692894e-08\n",
      "step: 18580, loss: 4.5299501039153256e-08\n",
      "step: 18581, loss: 7.15255676908555e-09\n",
      "step: 18582, loss: 1.430511264999268e-08\n",
      "step: 18583, loss: 1.502034336908764e-07\n",
      "step: 18584, loss: 1.335142343350526e-07\n",
      "step: 18585, loss: 0.0\n",
      "step: 18586, loss: 1.8835000048511574e-07\n",
      "step: 18587, loss: 0.00028086986276321113\n",
      "step: 18588, loss: 0.0\n",
      "step: 18589, loss: 0.0\n",
      "step: 18590, loss: 9.53674206272126e-09\n",
      "step: 18591, loss: 5.245205159098987e-08\n",
      "step: 18592, loss: 1.335142201241979e-07\n",
      "step: 18593, loss: 1.320818228123244e-06\n",
      "step: 18594, loss: 2.7417982551014575e-07\n",
      "step: 18595, loss: 6.437297628281158e-08\n",
      "step: 18596, loss: 2.121920772424346e-07\n",
      "step: 18597, loss: 0.01226436160504818\n",
      "step: 18598, loss: 2.8610216418201162e-08\n",
      "step: 18599, loss: 0.03816759213805199\n",
      "step: 18600, loss: 9.775154552471577e-08\n",
      "step: 18601, loss: 1.907348412544252e-08\n",
      "step: 18602, loss: 5.006786096828364e-08\n",
      "step: 18603, loss: 3.099440704090739e-08\n",
      "step: 18604, loss: 1.9311812593514333e-07\n",
      "step: 18605, loss: 7.152551972922083e-08\n",
      "step: 18606, loss: 1.192092646817855e-08\n",
      "step: 18607, loss: 6.937872853995941e-07\n",
      "step: 18608, loss: 2.598751791538234e-07\n",
      "step: 18609, loss: 1.8358176134825044e-07\n",
      "step: 18610, loss: 1.192092646817855e-08\n",
      "step: 18611, loss: 0.0039024902507662773\n",
      "step: 18612, loss: 3.099440704090739e-08\n",
      "step: 18613, loss: 1.907348234908568e-08\n",
      "step: 18614, loss: 1.9311835330881877e-07\n",
      "step: 18615, loss: 6.91413291065146e-08\n",
      "step: 18616, loss: 2.384184938364342e-08\n",
      "step: 18617, loss: 0.06863235682249069\n",
      "step: 18618, loss: 9.53674206272126e-09\n",
      "step: 18619, loss: 2.38418573772492e-09\n",
      "step: 18620, loss: 1.668929350273629e-08\n",
      "step: 18621, loss: 2.479538352417876e-07\n",
      "step: 18622, loss: 0.0\n",
      "step: 18623, loss: 0.0010017160093411803\n",
      "step: 18624, loss: 6.914133621194196e-08\n",
      "step: 18625, loss: 1.2397745763337298e-07\n",
      "step: 18626, loss: 0.00029383928631432354\n",
      "step: 18627, loss: 3.576277407546513e-08\n",
      "step: 18628, loss: 0.07081717252731323\n",
      "step: 18629, loss: 3.099421235219779e-07\n",
      "step: 18630, loss: 5.722041507283393e-08\n",
      "step: 18631, loss: 3.647771791293053e-07\n",
      "step: 18632, loss: 1.430510998545742e-08\n",
      "step: 18633, loss: 2.38418529363571e-08\n",
      "step: 18634, loss: 5.984221616017749e-07\n",
      "step: 18635, loss: 9.059889549689615e-08\n",
      "step: 18636, loss: 9.53674206272126e-09\n",
      "step: 18637, loss: 2.38418573772492e-09\n",
      "step: 18638, loss: 2.861021997091484e-08\n",
      "step: 18639, loss: 1.1944442803724087e-06\n",
      "step: 18640, loss: 4.117310709261801e-06\n",
      "step: 18641, loss: 2.38418529363571e-08\n",
      "step: 18642, loss: 2.884845571315964e-07\n",
      "step: 18643, loss: 2.4795414788059134e-07\n",
      "step: 18644, loss: 0.0\n",
      "step: 18645, loss: 2.2147930849314434e-06\n",
      "step: 18646, loss: 5.4836245766409775e-08\n",
      "step: 18647, loss: 4.76837147544984e-09\n",
      "step: 18648, loss: 3.5762766970037774e-08\n",
      "step: 18649, loss: 2.384184938364342e-08\n",
      "step: 18650, loss: 1.907348234908568e-08\n",
      "step: 18651, loss: 1.7213188812092994e-06\n",
      "step: 18652, loss: 7.653125635442848e-07\n",
      "step: 18653, loss: 1.192092646817855e-08\n",
      "step: 18654, loss: 5.745810085500125e-07\n",
      "step: 18655, loss: 3.814695404003032e-08\n",
      "step: 18656, loss: 9.53674117454284e-09\n",
      "step: 18657, loss: 2.3841845830929742e-08\n",
      "step: 18658, loss: 0.02862049639225006\n",
      "step: 18659, loss: 0.0\n",
      "step: 18660, loss: 1.0251989834841879e-07\n",
      "step: 18661, loss: 4.76837147544984e-09\n",
      "step: 18662, loss: 1.2159334517036768e-07\n",
      "step: 18663, loss: 9.536721279346239e-08\n",
      "step: 18664, loss: 3.8146957592744e-08\n",
      "step: 18665, loss: 2.38418573772492e-09\n",
      "step: 18666, loss: 9.53674206272126e-09\n",
      "step: 18667, loss: 9.53674117454284e-09\n",
      "step: 18668, loss: 2.9325408945624076e-07\n",
      "step: 18669, loss: 1.192092646817855e-08\n",
      "step: 18670, loss: 9.53674117454284e-09\n",
      "step: 18671, loss: 6.402912731573451e-06\n",
      "step: 18672, loss: 2.5033884298863995e-07\n",
      "step: 18673, loss: 9.989657883124892e-07\n",
      "step: 18674, loss: 7.15255676908555e-09\n",
      "step: 18675, loss: 2.38418573772492e-09\n",
      "step: 18676, loss: 9.059886707518672e-08\n",
      "step: 18677, loss: 4.76837147544984e-09\n",
      "step: 18678, loss: 5.00678503101426e-08\n",
      "step: 18679, loss: 2.622604000634965e-08\n",
      "step: 18680, loss: 3.099440704090739e-08\n",
      "step: 18681, loss: 5.483623510826874e-08\n",
      "step: 18682, loss: 8.583060662203934e-08\n",
      "step: 18683, loss: 0.0\n",
      "step: 18684, loss: 4.76837147544984e-09\n",
      "step: 18685, loss: 9.53674206272126e-09\n",
      "step: 18686, loss: 2.145766764272139e-08\n",
      "step: 18687, loss: 8.153753014994436e-07\n",
      "step: 18688, loss: 5.841174015586148e-07\n",
      "step: 18689, loss: 0.0\n",
      "step: 18690, loss: 5.912694973631005e-07\n",
      "step: 18691, loss: 2.0027114544518554e-07\n",
      "step: 18692, loss: 0.0\n",
      "step: 18693, loss: 2.7475571187096648e-05\n",
      "step: 18694, loss: 4.935204742650967e-07\n",
      "step: 18695, loss: 7.15255632499634e-09\n",
      "step: 18696, loss: 1.0657134907887666e-06\n",
      "step: 18697, loss: 2.5510743739687314e-07\n",
      "step: 18698, loss: 3.099441059362107e-08\n",
      "step: 18699, loss: 3.0994389277338996e-08\n",
      "step: 18700, loss: 6.675714558923573e-08\n",
      "step: 18701, loss: 6.198875013296856e-08\n",
      "step: 18702, loss: 6.198878566010535e-08\n",
      "step: 18703, loss: 6.437219894905866e-07\n",
      "step: 18704, loss: 6.341841185530939e-07\n",
      "step: 18705, loss: 2.145766764272139e-08\n",
      "step: 18706, loss: 1.0537964953982737e-06\n",
      "step: 18707, loss: 3.814693627646193e-08\n",
      "step: 18708, loss: 5.722042217826129e-08\n",
      "step: 18709, loss: 1.3637077245221008e-06\n",
      "step: 18710, loss: 3.814693627646193e-08\n",
      "step: 18711, loss: 6.834305168013088e-06\n",
      "step: 18712, loss: 1.0013563667143899e-07\n",
      "step: 18713, loss: 3.099440704090739e-08\n",
      "step: 18714, loss: 1.478192075410334e-07\n",
      "step: 18715, loss: 2.38418573772492e-09\n",
      "step: 18716, loss: 1.1920918296937089e-07\n",
      "step: 18717, loss: 0.2262689620256424\n",
      "step: 18718, loss: 8.583063504374877e-08\n",
      "step: 18719, loss: 1.907348234908568e-08\n",
      "step: 18720, loss: 1.668929883180681e-08\n",
      "step: 18721, loss: 9.536721989888974e-08\n",
      "step: 18722, loss: 1.192092646817855e-08\n",
      "step: 18723, loss: 6.294208105828147e-07\n",
      "step: 18724, loss: 0.0\n",
      "step: 18725, loss: 7.15255676908555e-09\n",
      "step: 18726, loss: 6.437296207195686e-08\n",
      "step: 18727, loss: 4.2915313969160707e-08\n",
      "step: 18728, loss: 3.8146946934602965e-08\n",
      "step: 18729, loss: 1.549714738757757e-07\n",
      "step: 18730, loss: 1.192092646817855e-08\n",
      "step: 18731, loss: 9.77514389433054e-08\n",
      "step: 18732, loss: 1.5020346211258584e-07\n",
      "step: 18733, loss: 0.004834525752812624\n",
      "step: 18734, loss: 1.430511264999268e-08\n",
      "step: 18735, loss: 2.1457660537294032e-08\n",
      "step: 18736, loss: 0.09573116153478622\n",
      "step: 18737, loss: 5.960457727383073e-08\n",
      "step: 18738, loss: 0.0\n",
      "step: 18739, loss: 0.04461818188428879\n",
      "step: 18740, loss: 8.916688898352731e-07\n",
      "step: 18741, loss: 0.0012517785653471947\n",
      "step: 18742, loss: 3.67161220538037e-07\n",
      "step: 18743, loss: 9.53674206272126e-09\n",
      "step: 18744, loss: 1.907348412544252e-08\n",
      "step: 18745, loss: 1.0490391133544108e-07\n",
      "step: 18746, loss: 1.192092824453539e-08\n",
      "step: 18747, loss: 1.43051135381711e-08\n",
      "step: 18748, loss: 1.316829275310738e-05\n",
      "step: 18749, loss: 3.337859055818626e-08\n",
      "step: 18750, loss: 4.76837103136063e-09\n",
      "step: 18751, loss: 3.5762766970037774e-08\n",
      "step: 18752, loss: 1.430511264999268e-08\n",
      "step: 18753, loss: 4.053114466273655e-08\n",
      "step: 18754, loss: 0.08109109848737717\n",
      "step: 18755, loss: 7.152552683464819e-08\n",
      "step: 18756, loss: 2.38418573772492e-09\n",
      "step: 18757, loss: 6.198876434382328e-08\n",
      "step: 18758, loss: 8.288578214887821e-07\n",
      "step: 18759, loss: 2.384184938364342e-08\n",
      "step: 18760, loss: 3.814696114545768e-08\n",
      "step: 18761, loss: 2.384185116000026e-08\n",
      "step: 18762, loss: 9.560365015204297e-07\n",
      "step: 18763, loss: 1.206383558383095e-06\n",
      "step: 18764, loss: 1.2063633221259806e-06\n",
      "step: 18765, loss: 1.907348412544252e-08\n",
      "step: 18766, loss: 1.43051135381711e-08\n",
      "step: 18767, loss: 2.6464374514034716e-07\n",
      "step: 18768, loss: 5.32318063051207e-06\n",
      "step: 18769, loss: 1.8835036996733834e-07\n",
      "step: 18770, loss: 7.867804185934801e-08\n",
      "step: 18771, loss: 1.1682481471098072e-07\n",
      "step: 18772, loss: 7.152551262379347e-08\n",
      "step: 18773, loss: 3.576277407546513e-08\n",
      "step: 18774, loss: 2.38418573772492e-09\n",
      "step: 18775, loss: 4.291529975830599e-08\n",
      "step: 18776, loss: 2.38418529363571e-08\n",
      "step: 18777, loss: 0.001118162996135652\n",
      "step: 18778, loss: 7.748526513751131e-07\n",
      "step: 18779, loss: 1.8834984416571388e-07\n",
      "step: 18780, loss: 7.6293815709505e-08\n",
      "step: 18781, loss: 5.483619602841827e-08\n",
      "step: 18782, loss: 0.0\n",
      "step: 18783, loss: 0.14583450555801392\n",
      "step: 18784, loss: 2.670278149707883e-07\n",
      "step: 18785, loss: 1.192092824453539e-08\n",
      "step: 18786, loss: 1.0013561535515692e-07\n",
      "step: 18787, loss: 2.38418573772492e-09\n",
      "step: 18788, loss: 2.38418573772492e-09\n",
      "step: 18789, loss: 9.53674117454284e-09\n",
      "step: 18790, loss: 5.00678503101426e-08\n",
      "step: 18791, loss: 2.0265524369733612e-07\n",
      "step: 18792, loss: 2.4698933884792496e-06\n",
      "step: 18793, loss: 5.006785741556996e-08\n",
      "step: 18794, loss: 1.2429666639945935e-05\n",
      "step: 18795, loss: 2.1934408778179204e-07\n",
      "step: 18796, loss: 7.15255632499634e-09\n",
      "step: 18797, loss: 6.437296207195686e-08\n",
      "step: 18798, loss: 4.768367389829109e-08\n",
      "step: 18799, loss: 1.8835017101537233e-07\n",
      "step: 18800, loss: 2.217281576122332e-07\n",
      "step: 18801, loss: 3.433201243296935e-07\n",
      "step: 18802, loss: 0.00038409314583987\n",
      "step: 18803, loss: 1.4781932122787111e-07\n",
      "step: 18804, loss: 1.7523124142826418e-06\n",
      "step: 18805, loss: 7.867798501592915e-08\n",
      "step: 18806, loss: 1.907348234908568e-08\n",
      "step: 18807, loss: 5.006786452099732e-08\n",
      "step: 18808, loss: 0.0016492195427417755\n",
      "step: 18809, loss: 2.121919777664516e-07\n",
      "step: 18810, loss: 7.390971745735442e-08\n",
      "step: 18811, loss: 4.148460845954105e-07\n",
      "step: 18812, loss: 2.932530946964107e-07\n",
      "step: 18813, loss: 3.8146957592744e-08\n",
      "step: 18814, loss: 5.4836249319123453e-08\n",
      "step: 18815, loss: 5.960462701182223e-08\n",
      "step: 18816, loss: 1.2159333095951297e-07\n",
      "step: 18817, loss: 7.15255676908555e-09\n",
      "step: 18818, loss: 1.0251987703213672e-07\n",
      "step: 18819, loss: 1.907348234908568e-08\n",
      "step: 18820, loss: 6.67571171675263e-08\n",
      "step: 18821, loss: 2.4795380682007817e-07\n",
      "step: 18822, loss: 9.53674206272126e-09\n",
      "step: 18823, loss: 0.06541991978883743\n",
      "step: 18824, loss: 4.529951525000797e-08\n",
      "step: 18825, loss: 0.08150942623615265\n",
      "step: 18826, loss: 4.768369876728684e-08\n",
      "step: 18827, loss: 5.793489776806382e-07\n",
      "step: 18828, loss: 2.622604000634965e-08\n",
      "step: 18829, loss: 0.0\n",
      "step: 18830, loss: 3.448613279033452e-05\n",
      "step: 18831, loss: 1.668929883180681e-08\n",
      "step: 18832, loss: 7.15255676908555e-09\n",
      "step: 18833, loss: 3.337859055818626e-08\n",
      "step: 18834, loss: 0.0\n",
      "step: 18835, loss: 1.192092558000013e-08\n",
      "step: 18836, loss: 3.5762763417324095e-08\n",
      "step: 18837, loss: 1.430511264999268e-08\n",
      "step: 18838, loss: 4.76837103136063e-09\n",
      "step: 18839, loss: 1.4066654330235906e-07\n",
      "step: 18840, loss: 9.53674206272126e-09\n",
      "step: 18841, loss: 0.0\n",
      "step: 18842, loss: 2.38418573772492e-09\n",
      "step: 18843, loss: 2.8610218194558e-08\n",
      "step: 18844, loss: 3.5285756894154474e-07\n",
      "step: 18845, loss: 1.668929350273629e-08\n",
      "step: 18846, loss: 1.907348234908568e-08\n",
      "step: 18847, loss: 2.38418529363571e-08\n",
      "step: 18848, loss: 1.668929527909313e-08\n",
      "step: 18849, loss: 3.3378341868228745e-07\n",
      "step: 18850, loss: 4.76837147544984e-09\n",
      "step: 18851, loss: 0.045296426862478256\n",
      "step: 18852, loss: 4.7683468551440455e-07\n",
      "step: 18853, loss: 1.192092646817855e-08\n",
      "step: 18854, loss: 1.430511264999268e-08\n",
      "step: 18855, loss: 2.38418573772492e-09\n",
      "step: 18856, loss: 0.0\n",
      "step: 18857, loss: 0.06742087006568909\n",
      "step: 18858, loss: 9.53674117454284e-09\n",
      "step: 18859, loss: 3.814679701008572e-07\n",
      "step: 18860, loss: 9.53674117454284e-09\n",
      "step: 18861, loss: 2.38418529363571e-08\n",
      "step: 18862, loss: 4.529947972287118e-08\n",
      "step: 18863, loss: 1.6450857742711378e-07\n",
      "step: 18864, loss: 1.6331011920556193e-06\n",
      "step: 18865, loss: 2.38418573772492e-09\n",
      "step: 18866, loss: 4.76837147544984e-09\n",
      "step: 18867, loss: 4.053078441756952e-07\n",
      "step: 18868, loss: 5.483620668655931e-08\n",
      "step: 18869, loss: 2.717961251619272e-07\n",
      "step: 18870, loss: 3.004062705258548e-07\n",
      "step: 18871, loss: 2.8610216418201162e-08\n",
      "step: 18872, loss: 6.246510224627855e-07\n",
      "step: 18873, loss: 4.569978500512661e-06\n",
      "step: 18874, loss: 1.3351416328077903e-07\n",
      "step: 18875, loss: 6.675715980009045e-08\n",
      "step: 18876, loss: 1.668929527909313e-08\n",
      "step: 18877, loss: 6.675714558923573e-08\n",
      "step: 18878, loss: 9.53672767423086e-08\n",
      "step: 18879, loss: 5.4836242213696096e-08\n",
      "step: 18880, loss: 7.15255632499634e-09\n",
      "step: 18881, loss: 3.29014937960892e-07\n",
      "step: 18882, loss: 0.11236326396465302\n",
      "step: 18883, loss: 0.16879302263259888\n",
      "step: 18884, loss: 3.743137426681642e-07\n",
      "step: 18885, loss: 3.337859055818626e-08\n",
      "step: 18886, loss: 0.002014533383771777\n",
      "step: 18887, loss: 1.311300223960643e-07\n",
      "step: 18888, loss: 1.6212426601214247e-07\n",
      "step: 18889, loss: 1.0633262945702882e-06\n",
      "step: 18890, loss: 8.583060662203934e-08\n",
      "step: 18891, loss: 1.668929883180681e-08\n",
      "step: 18892, loss: 1.1920894138484073e-07\n",
      "step: 18893, loss: 5.722040796740657e-08\n",
      "step: 18894, loss: 1.1205646188727769e-07\n",
      "step: 18895, loss: 2.861022529998536e-08\n",
      "step: 18896, loss: 0.0\n",
      "step: 18897, loss: 1.263615985180877e-07\n",
      "step: 18898, loss: 2.38418573772492e-09\n",
      "step: 18899, loss: 2.38418573772492e-09\n",
      "step: 18900, loss: 2.38418573772492e-09\n",
      "step: 18901, loss: 1.0251980597786314e-07\n",
      "step: 18902, loss: 2.1027483398938784e-06\n",
      "step: 18903, loss: 1.4161569197312929e-06\n",
      "step: 18904, loss: 7.772312642373436e-07\n",
      "step: 18905, loss: 2.145766764272139e-08\n",
      "step: 18906, loss: 0.0\n",
      "step: 18907, loss: 7.6293815709505e-08\n",
      "step: 18908, loss: 5.722038309841082e-08\n",
      "step: 18909, loss: 6.437206820919528e-07\n",
      "step: 18910, loss: 0.0\n",
      "step: 18911, loss: 1.430510998545742e-08\n",
      "step: 18912, loss: 0.01567913219332695\n",
      "step: 18913, loss: 2.622603645363597e-08\n",
      "step: 18914, loss: 1.6927671708799608e-07\n",
      "step: 18915, loss: 1.9073478796372e-08\n",
      "step: 18916, loss: 5.006785741556996e-08\n",
      "step: 18917, loss: 4.76837103136063e-09\n",
      "step: 18918, loss: 8.34464017884784e-08\n",
      "step: 18919, loss: 1.907348412544252e-08\n",
      "step: 18920, loss: 1.2111449905205518e-06\n",
      "step: 18921, loss: 5.006786096828364e-08\n",
      "step: 18922, loss: 1.430510998545742e-08\n",
      "step: 18923, loss: 1.192092824453539e-08\n",
      "step: 18924, loss: 4.7683688109145805e-08\n",
      "step: 18925, loss: 1.072882866992586e-07\n",
      "step: 18926, loss: 6.198877855467799e-08\n",
      "step: 18927, loss: 2.38418573772492e-09\n",
      "step: 18928, loss: 1.192092558000013e-08\n",
      "step: 18929, loss: 2.384185116000026e-08\n",
      "step: 18930, loss: 1.0967232100256297e-07\n",
      "step: 18931, loss: 3.337858700547258e-08\n",
      "step: 18932, loss: 9.53674117454284e-09\n",
      "step: 18933, loss: 1.74753677129047e-06\n",
      "step: 18934, loss: 1.430511264999268e-08\n",
      "step: 18935, loss: 2.145766764272139e-08\n",
      "step: 18936, loss: 1.907348412544252e-08\n",
      "step: 18937, loss: 0.05703265592455864\n",
      "step: 18938, loss: 2.622604000634965e-08\n",
      "step: 18939, loss: 3.337859055818626e-08\n",
      "step: 18940, loss: 2.622603467727913e-08\n",
      "step: 18941, loss: 7.15255676908555e-09\n",
      "step: 18942, loss: 5.483623510826874e-08\n",
      "step: 18943, loss: 1.2159334517036768e-07\n",
      "step: 18944, loss: 1.430511264999268e-08\n",
      "step: 18945, loss: 0.0\n",
      "step: 18946, loss: 4.053112689916816e-08\n",
      "step: 18947, loss: 0.0\n",
      "step: 18948, loss: 4.053111624102712e-08\n",
      "step: 18949, loss: 7.629389386920593e-08\n",
      "step: 18950, loss: 1.192092646817855e-08\n",
      "step: 18951, loss: 3.576277407546513e-08\n",
      "step: 18952, loss: 1.1682496392495523e-07\n",
      "step: 18953, loss: 5.102098157294677e-07\n",
      "step: 18954, loss: 0.0\n",
      "step: 18955, loss: 3.814693627646193e-08\n",
      "step: 18956, loss: 0.00011268524394836277\n",
      "step: 18957, loss: 4.76837147544984e-09\n",
      "step: 18958, loss: 8.106218274406274e-08\n",
      "step: 18959, loss: 0.05864746496081352\n",
      "step: 18960, loss: 5.722039375655186e-08\n",
      "step: 18961, loss: 1.1682482892183543e-07\n",
      "step: 18962, loss: 0.0628829076886177\n",
      "step: 18963, loss: 0.06766300648450851\n",
      "step: 18964, loss: 1.192092824453539e-08\n",
      "step: 18965, loss: 2.38418573772492e-09\n",
      "step: 18966, loss: 2.38418573772492e-09\n",
      "step: 18967, loss: 2.38418573772492e-09\n",
      "step: 18968, loss: 1.907348234908568e-08\n",
      "step: 18969, loss: 3.5762759864610416e-08\n",
      "step: 18970, loss: 7.152551972922083e-08\n",
      "step: 18971, loss: 0.0\n",
      "step: 18972, loss: 2.145766764272139e-08\n",
      "step: 18973, loss: 3.528565173382958e-07\n",
      "step: 18974, loss: 1.668929350273629e-08\n",
      "step: 18975, loss: 0.09413016587495804\n",
      "step: 18976, loss: 3.8146950487316644e-08\n",
      "step: 18977, loss: 4.76837147544984e-09\n",
      "step: 18978, loss: 5.245203738013515e-08\n",
      "step: 18979, loss: 1.549714738757757e-07\n",
      "step: 18980, loss: 3.7669968833142775e-07\n",
      "step: 18981, loss: 4.76837103136063e-09\n",
      "step: 18982, loss: 2.6226025795494934e-08\n",
      "step: 18983, loss: 2.1457660537294032e-08\n",
      "step: 18984, loss: 7.431789867951011e-07\n",
      "step: 18985, loss: 7.152546288580197e-08\n",
      "step: 18986, loss: 2.38418573772492e-09\n",
      "step: 18987, loss: 1.192092558000013e-08\n",
      "step: 18988, loss: 6.532570182571362e-07\n",
      "step: 18989, loss: 0.0\n",
      "step: 18990, loss: 2.408024215583282e-07\n",
      "step: 18991, loss: 0.0016937900800257921\n",
      "step: 18992, loss: 4.76837147544984e-09\n",
      "step: 18993, loss: 2.3983491246326594e-06\n",
      "step: 18994, loss: 0.0\n",
      "step: 18995, loss: 1.5878115391387837e-06\n",
      "step: 18996, loss: 0.0\n",
      "step: 18997, loss: 1.096723991622639e-07\n",
      "step: 18998, loss: 0.0\n",
      "step: 18999, loss: 2.145766764272139e-08\n",
      "step: 19000, loss: 1.192092824453539e-08\n",
      "step: 19001, loss: 0.020266737788915634\n",
      "step: 19002, loss: 1.8834978732229501e-07\n",
      "step: 19003, loss: 4.506062509790354e-07\n",
      "step: 19004, loss: 1.7642904026615724e-07\n",
      "step: 19005, loss: 7.867799212135651e-08\n",
      "step: 19006, loss: 3.266323176376318e-07\n",
      "step: 19007, loss: 1.3828261558046506e-07\n",
      "step: 19008, loss: 5.9604552404834976e-08\n",
      "step: 19009, loss: 1.192092824453539e-08\n",
      "step: 19010, loss: 2.6226025795494934e-08\n",
      "step: 19011, loss: 4.649125457945047e-07\n",
      "step: 19012, loss: 1.0967242047854597e-07\n",
      "step: 19013, loss: 7.482618457288481e-06\n",
      "step: 19014, loss: 1.907348234908568e-08\n",
      "step: 19015, loss: 2.38418529363571e-08\n",
      "step: 19016, loss: 7.15255632499634e-09\n",
      "step: 19017, loss: 1.0013567930400313e-07\n",
      "step: 19018, loss: 5.030582315157517e-07\n",
      "step: 19019, loss: 2.722563294810243e-06\n",
      "step: 19020, loss: 1.668929350273629e-08\n",
      "step: 19021, loss: 0.06135402247309685\n",
      "step: 19022, loss: 1.192092824453539e-08\n",
      "step: 19023, loss: 1.7881372116335115e-07\n",
      "step: 19024, loss: 5.745814064539445e-07\n",
      "step: 19025, loss: 0.00018792087212204933\n",
      "step: 19026, loss: 9.775147447044219e-08\n",
      "step: 19027, loss: 3.5047335700255644e-07\n",
      "step: 19028, loss: 0.0\n",
      "step: 19029, loss: 2.4946906705736183e-05\n",
      "step: 19030, loss: 0.0\n",
      "step: 19031, loss: 1.907348234908568e-08\n",
      "step: 19032, loss: 1.461454530726769e-06\n",
      "step: 19033, loss: 7.15255632499634e-09\n",
      "step: 19034, loss: 4.24380573349481e-07\n",
      "step: 19035, loss: 0.0002349304995732382\n",
      "step: 19036, loss: 4.529949748643958e-08\n",
      "step: 19037, loss: 1.1444080172395843e-07\n",
      "step: 19038, loss: 1.8476586092219804e-06\n",
      "step: 19039, loss: 6.127268648015161e-07\n",
      "step: 19040, loss: 9.53674206272126e-09\n",
      "step: 19041, loss: 2.3126493431391282e-07\n",
      "step: 19042, loss: 9.53674117454284e-09\n",
      "step: 19043, loss: 1.0728751931310399e-06\n",
      "step: 19044, loss: 0.24701082706451416\n",
      "step: 19045, loss: 1.4305095419331337e-07\n",
      "step: 19046, loss: 3.830103378277272e-05\n",
      "step: 19047, loss: 0.0\n",
      "step: 19048, loss: 2.38418573772492e-09\n",
      "step: 19049, loss: 3.8146950487316644e-08\n",
      "step: 19050, loss: 0.0\n",
      "step: 19051, loss: 1.02519862821282e-07\n",
      "step: 19052, loss: 5.00678503101426e-08\n",
      "step: 19053, loss: 1.3828257294790092e-07\n",
      "step: 19054, loss: 5.245206935455826e-08\n",
      "step: 19055, loss: 1.716609574486938e-07\n",
      "step: 19056, loss: 6.993945135036483e-05\n",
      "step: 19057, loss: 5.17363275776006e-07\n",
      "step: 19058, loss: 0.0\n",
      "step: 19059, loss: 1.003723468784301e-06\n",
      "step: 19060, loss: 1.1682482892183543e-07\n",
      "step: 19061, loss: 3.814696114545768e-08\n",
      "step: 19062, loss: 2.360332445050517e-07\n",
      "step: 19063, loss: 4.529951169729429e-08\n",
      "step: 19064, loss: 1.3804462469124701e-05\n",
      "step: 19065, loss: 4.982890686733299e-07\n",
      "step: 19066, loss: 2.031223402809701e-06\n",
      "step: 19067, loss: 1.5974009670571832e-07\n",
      "step: 19068, loss: 9.53674206272126e-09\n",
      "step: 19069, loss: 3.099439993548003e-08\n",
      "step: 19070, loss: 0.00035327684599906206\n",
      "step: 19071, loss: 2.38418573772492e-09\n",
      "step: 19072, loss: 5.125937150296522e-07\n",
      "step: 19073, loss: 6.914127226309574e-08\n",
      "step: 19074, loss: 2.8610211089130644e-08\n",
      "step: 19075, loss: 0.0\n",
      "step: 19076, loss: 2.145766586636455e-08\n",
      "step: 19077, loss: 9.00781469681533e-06\n",
      "step: 19078, loss: 4.76837103136063e-09\n",
      "step: 19079, loss: 1.2397735815738997e-07\n",
      "step: 19080, loss: 4.76837103136063e-09\n",
      "step: 19081, loss: 2.4080134153336985e-07\n",
      "step: 19082, loss: 2.622603467727913e-08\n",
      "step: 19083, loss: 1.430511176181426e-08\n",
      "step: 19084, loss: 0.043576788157224655\n",
      "step: 19085, loss: 9.53674206272126e-09\n",
      "step: 19086, loss: 8.821476171760878e-08\n",
      "step: 19087, loss: 0.1118130087852478\n",
      "step: 19088, loss: 8.344635915591425e-08\n",
      "step: 19089, loss: 5.722041507283393e-08\n",
      "step: 19090, loss: 4.768369876728684e-08\n",
      "step: 19091, loss: 2.3603347187872714e-07\n",
      "step: 19092, loss: 4.76837103136063e-09\n",
      "step: 19093, loss: 1.2255804904270917e-05\n",
      "step: 19094, loss: 1.931185806824942e-07\n",
      "step: 19095, loss: 1.907347702001516e-08\n",
      "step: 19096, loss: 2.38418573772492e-09\n",
      "step: 19097, loss: 0.0\n",
      "step: 19098, loss: 3.4808942928066244e-07\n",
      "step: 19099, loss: 9.53674117454284e-09\n",
      "step: 19100, loss: 2.38418529363571e-08\n",
      "step: 19101, loss: 1.9073409873726632e-07\n",
      "step: 19102, loss: 0.00019806475029326975\n",
      "step: 19103, loss: 9.53674206272126e-09\n",
      "step: 19104, loss: 0.0\n",
      "step: 19105, loss: 1.1682477918384393e-07\n",
      "step: 19106, loss: 2.622603645363597e-08\n",
      "step: 19107, loss: 2.884851539874944e-07\n",
      "step: 19108, loss: 1.3899360737923416e-06\n",
      "step: 19109, loss: 1.907348412544252e-08\n",
      "step: 19110, loss: 5.722043994182968e-08\n",
      "step: 19111, loss: 3.552420082542085e-07\n",
      "step: 19112, loss: 7.08092102286173e-07\n",
      "step: 19113, loss: 7.15255676908555e-09\n",
      "step: 19114, loss: 0.0003207131812814623\n",
      "step: 19115, loss: 1.3589837521976733e-07\n",
      "step: 19116, loss: 7.390963219222613e-08\n",
      "step: 19117, loss: 0.0\n",
      "step: 19118, loss: 2.145766764272139e-08\n",
      "step: 19119, loss: 0.1718558669090271\n",
      "step: 19120, loss: 4.053114111002287e-08\n",
      "step: 19121, loss: 7.15255676908555e-09\n",
      "step: 19122, loss: 4.291530686373335e-08\n",
      "step: 19123, loss: 2.861021997091484e-08\n",
      "step: 19124, loss: 9.53674117454284e-09\n",
      "step: 19125, loss: 4.053112689916816e-08\n",
      "step: 19126, loss: 2.1457660537294032e-08\n",
      "step: 19127, loss: 4.100767228010227e-07\n",
      "step: 19128, loss: 2.38418573772492e-09\n",
      "step: 19129, loss: 9.059888128604143e-08\n",
      "step: 19130, loss: 1.645082221557459e-07\n",
      "step: 19131, loss: 9.298307190874766e-08\n",
      "step: 19132, loss: 4.2438296077307314e-07\n",
      "step: 19133, loss: 3.0994396382766354e-08\n",
      "step: 19134, loss: 4.529952235543533e-08\n",
      "step: 19135, loss: 9.53674117454284e-09\n",
      "step: 19136, loss: 0.0\n",
      "step: 19137, loss: 8.344633783963218e-08\n",
      "step: 19138, loss: 1.668929527909313e-08\n",
      "step: 19139, loss: 1.192092558000013e-08\n",
      "step: 19140, loss: 1.2874573940280243e-07\n",
      "step: 19141, loss: 8.106224669290896e-08\n",
      "step: 19142, loss: 1.5020330579318397e-07\n",
      "step: 19143, loss: 0.05662571266293526\n",
      "step: 19144, loss: 2.1457660537294032e-08\n",
      "step: 19145, loss: 6.461040698013676e-07\n",
      "step: 19146, loss: 3.337858700547258e-08\n",
      "step: 19147, loss: 7.15255676908555e-09\n",
      "step: 19148, loss: 1.668929350273629e-08\n",
      "step: 19149, loss: 3.051735859571636e-07\n",
      "step: 19150, loss: 3.2186272846956854e-07\n",
      "step: 19151, loss: 1.4066650066979491e-07\n",
      "step: 19152, loss: 7.15255632499634e-09\n",
      "step: 19153, loss: 1.9073395662871917e-07\n",
      "step: 19154, loss: 6.198879987096007e-08\n",
      "step: 19155, loss: 1.7881346536796627e-07\n",
      "step: 19156, loss: 3.0994396382766354e-08\n",
      "step: 19157, loss: 9.059886707518672e-08\n",
      "step: 19158, loss: 2.0265511579964368e-07\n",
      "step: 19159, loss: 4.76837103136063e-09\n",
      "step: 19160, loss: 1.2874561150511e-07\n",
      "step: 19161, loss: 1.0251737876387779e-06\n",
      "step: 19162, loss: 1.0728815880156617e-07\n",
      "step: 19163, loss: 2.6226029348208613e-08\n",
      "step: 19164, loss: 7.15255632499634e-09\n",
      "step: 19165, loss: 1.0084947462019045e-05\n",
      "step: 19166, loss: 3.051737564874202e-07\n",
      "step: 19167, loss: 2.145766231365087e-08\n",
      "step: 19168, loss: 5.268988161333255e-07\n",
      "step: 19169, loss: 6.226535333553329e-06\n",
      "step: 19170, loss: 1.3828250189362734e-07\n",
      "step: 19171, loss: 2.622603645363597e-08\n",
      "step: 19172, loss: 1.2397745763337298e-07\n",
      "step: 19173, loss: 8.106222537662688e-08\n",
      "step: 19174, loss: 1.668929883180681e-08\n",
      "step: 19175, loss: 2.2411231270780263e-07\n",
      "step: 19176, loss: 3.337859411089994e-08\n",
      "step: 19177, loss: 1.1920916165308881e-07\n",
      "step: 19178, loss: 2.38418529363571e-08\n",
      "step: 19179, loss: 1.668929705544997e-08\n",
      "step: 19180, loss: 2.38418573772492e-09\n",
      "step: 19181, loss: 2.38418573772492e-09\n",
      "step: 19182, loss: 0.003866659477353096\n",
      "step: 19183, loss: 4.529951169729429e-08\n",
      "step: 19184, loss: 5.388189947552746e-07\n",
      "step: 19185, loss: 8.582891837249917e-07\n",
      "step: 19186, loss: 7.867807028105744e-08\n",
      "step: 19187, loss: 1.6212430864470662e-07\n",
      "step: 19188, loss: 0.0\n",
      "step: 19189, loss: 1.5258760299730056e-07\n",
      "step: 19190, loss: 2.384185116000026e-08\n",
      "step: 19191, loss: 2.622603645363597e-08\n",
      "step: 19192, loss: 9.53674206272126e-09\n",
      "step: 19193, loss: 2.38418573772492e-09\n",
      "step: 19194, loss: 0.08763132244348526\n",
      "step: 19195, loss: 2.38418573772492e-09\n",
      "step: 19196, loss: 0.08754982054233551\n",
      "step: 19197, loss: 7.152551972922083e-08\n",
      "step: 19198, loss: 0.0\n",
      "step: 19199, loss: 4.768368100371845e-08\n",
      "step: 19200, loss: 1.192092824453539e-08\n",
      "step: 19201, loss: 9.53674206272126e-09\n",
      "step: 19202, loss: 9.53674206272126e-09\n",
      "step: 19203, loss: 1.430510998545742e-08\n",
      "step: 19204, loss: 0.07565664499998093\n",
      "step: 19205, loss: 0.023983318358659744\n",
      "step: 19206, loss: 2.622604000634965e-08\n",
      "step: 19207, loss: 2.145766586636455e-08\n",
      "step: 19208, loss: 7.15255676908555e-09\n",
      "step: 19209, loss: 2.38418573772492e-09\n",
      "step: 19210, loss: 2.9802311729554276e-08\n",
      "step: 19211, loss: 2.3841844054572903e-08\n",
      "step: 19212, loss: 7.390971745735442e-08\n",
      "step: 19213, loss: 5.030583452025894e-07\n",
      "step: 19214, loss: 9.53674117454284e-09\n",
      "step: 19215, loss: 0.0018881618743762374\n",
      "step: 19216, loss: 9.53674117454284e-09\n",
      "step: 19217, loss: 0.0\n",
      "step: 19218, loss: 8.179380529327318e-05\n",
      "step: 19219, loss: 3.8146950487316644e-08\n",
      "step: 19220, loss: 7.15255676908555e-09\n",
      "step: 19221, loss: 1.0790308806463145e-05\n",
      "step: 19222, loss: 8.559047728340374e-07\n",
      "step: 19223, loss: 2.622603467727913e-08\n",
      "step: 19224, loss: 1.668929350273629e-08\n",
      "step: 19225, loss: 4.76837147544984e-09\n",
      "step: 19226, loss: 4.291530331101967e-08\n",
      "step: 19227, loss: 5.483622800284138e-08\n",
      "step: 19228, loss: 2.735175621637609e-05\n",
      "step: 19229, loss: 2.8610209312773804e-08\n",
      "step: 19230, loss: 6.198877855467799e-08\n",
      "step: 19231, loss: 6.91413291065146e-08\n",
      "step: 19232, loss: 3.099440348819371e-08\n",
      "step: 19233, loss: 9.53674117454284e-09\n",
      "step: 19234, loss: 2.38418573772492e-09\n",
      "step: 19235, loss: 0.03519430384039879\n",
      "step: 19236, loss: 2.861021997091484e-08\n",
      "step: 19237, loss: 4.76834713936114e-07\n",
      "step: 19238, loss: 1.3828258715875563e-07\n",
      "step: 19239, loss: 3.337859055818626e-08\n",
      "step: 19240, loss: 1.907345392737625e-07\n",
      "step: 19241, loss: 1.382823739959349e-07\n",
      "step: 19242, loss: 2.38418573772492e-09\n",
      "step: 19243, loss: 0.04090017452836037\n",
      "step: 19244, loss: 2.861002599274798e-07\n",
      "step: 19245, loss: 0.00011694084241753444\n",
      "step: 19246, loss: 9.53674117454284e-09\n",
      "step: 19247, loss: 0.0\n",
      "step: 19248, loss: 4.7683688109145805e-08\n",
      "step: 19249, loss: 2.1457660537294032e-08\n",
      "step: 19250, loss: 4.6252941388047475e-07\n",
      "step: 19251, loss: 2.0980782267088216e-07\n",
      "step: 19252, loss: 1.3113005081777374e-07\n",
      "step: 19253, loss: 4.768370232000052e-08\n",
      "step: 19254, loss: 1.246891315531684e-06\n",
      "step: 19255, loss: 7.390962508679877e-08\n",
      "step: 19256, loss: 7.533930101999431e-07\n",
      "step: 19257, loss: 1.9668807453854242e-06\n",
      "step: 19258, loss: 8.106221116577217e-08\n",
      "step: 19259, loss: 2.145766764272139e-08\n",
      "step: 19260, loss: 2.8610209312773804e-08\n",
      "step: 19261, loss: 8.583057820032991e-08\n",
      "step: 19262, loss: 1.192092558000013e-08\n",
      "step: 19263, loss: 0.0\n",
      "step: 19264, loss: 4.219979814479302e-07\n",
      "step: 19265, loss: 2.3126467851852794e-07\n",
      "step: 19266, loss: 5.2452058696417225e-08\n",
      "step: 19267, loss: 1.1444078751310371e-07\n",
      "step: 19268, loss: 3.8146946934602965e-08\n",
      "step: 19269, loss: 1.788134937896757e-07\n",
      "step: 19270, loss: 1.192092824453539e-08\n",
      "step: 19271, loss: 4.982894665772619e-07\n",
      "step: 19272, loss: 1.43051135381711e-08\n",
      "step: 19273, loss: 1.430510998545742e-08\n",
      "step: 19274, loss: 3.3805472412495874e-06\n",
      "step: 19275, loss: 5.006786452099732e-08\n",
      "step: 19276, loss: 6.675717401094516e-08\n",
      "step: 19277, loss: 2.38418573772492e-09\n",
      "step: 19278, loss: 9.298317138473067e-08\n",
      "step: 19279, loss: 0.0\n",
      "step: 19280, loss: 1.192092824453539e-08\n",
      "step: 19281, loss: 2.74179512871342e-07\n",
      "step: 19282, loss: 2.5510652790217136e-07\n",
      "step: 19283, loss: 4.76837103136063e-09\n",
      "step: 19284, loss: 7.15255632499634e-09\n",
      "step: 19285, loss: 0.0011094514047726989\n",
      "step: 19286, loss: 8.986293687485158e-05\n",
      "step: 19287, loss: 9.53674295089968e-09\n",
      "step: 19288, loss: 7.15255676908555e-09\n",
      "step: 19289, loss: 0.0\n",
      "step: 19290, loss: 1.907348412544252e-08\n",
      "step: 19291, loss: 9.53674117454284e-09\n",
      "step: 19292, loss: 0.06865273416042328\n",
      "step: 19293, loss: 1.430509684041681e-07\n",
      "step: 19294, loss: 0.0\n",
      "step: 19295, loss: 5.245205159098987e-08\n",
      "step: 19296, loss: 1.668929527909313e-08\n",
      "step: 19297, loss: 2.861022529998536e-08\n",
      "step: 19298, loss: 0.09705016762018204\n",
      "step: 19299, loss: 0.0\n",
      "step: 19300, loss: 2.622604000634965e-08\n",
      "step: 19301, loss: 8.106221827119953e-08\n",
      "step: 19302, loss: 0.10565014183521271\n",
      "step: 19303, loss: 4.865576102019986e-06\n",
      "step: 19304, loss: 2.0980769477318972e-07\n",
      "step: 19305, loss: 3.5762759864610416e-08\n",
      "step: 19306, loss: 0.0001052121224347502\n",
      "step: 19307, loss: 7.15255632499634e-09\n",
      "step: 19308, loss: 3.3378579900045224e-08\n",
      "step: 19309, loss: 1.3112988028751715e-07\n",
      "step: 19310, loss: 8.821478303389085e-08\n",
      "step: 19311, loss: 7.15255632499634e-09\n",
      "step: 19312, loss: 3.027896582352696e-07\n",
      "step: 19313, loss: 6.914127226309574e-08\n",
      "step: 19314, loss: 3.316140009701485e-06\n",
      "step: 19315, loss: 7.152548420208404e-08\n",
      "step: 19316, loss: 0.0\n",
      "step: 19317, loss: 5.149793196324026e-07\n",
      "step: 19318, loss: 9.536730516401803e-08\n",
      "step: 19319, loss: 0.0\n",
      "step: 19320, loss: 4.76837103136063e-09\n",
      "step: 19321, loss: 4.52994939337259e-08\n",
      "step: 19322, loss: 2.38418573772492e-09\n",
      "step: 19323, loss: 3.576277762817881e-08\n",
      "step: 19324, loss: 3.123263070392568e-07\n",
      "step: 19325, loss: 1.5306244449675432e-06\n",
      "step: 19326, loss: 1.192092646817855e-08\n",
      "step: 19327, loss: 4.76837147544984e-09\n",
      "step: 19328, loss: 4.515140972216614e-06\n",
      "step: 19329, loss: 0.052888933569192886\n",
      "step: 19330, loss: 0.0\n",
      "step: 19331, loss: 7.152546288580197e-08\n",
      "step: 19332, loss: 7.15255676908555e-09\n",
      "step: 19333, loss: 3.40937617693271e-07\n",
      "step: 19334, loss: 7.629380860407764e-08\n",
      "step: 19335, loss: 0.0001514645991846919\n",
      "step: 19336, loss: 4.98292308748205e-07\n",
      "step: 19337, loss: 5.006786452099732e-08\n",
      "step: 19338, loss: 3.369367550476454e-05\n",
      "step: 19339, loss: 2.38418529363571e-08\n",
      "step: 19340, loss: 1.8929541738543776e-06\n",
      "step: 19341, loss: 1.907348234908568e-08\n",
      "step: 19342, loss: 4.5299508144580614e-08\n",
      "step: 19343, loss: 0.0944620817899704\n",
      "step: 19344, loss: 1.8834994364169688e-07\n",
      "step: 19345, loss: 2.622589363454608e-07\n",
      "step: 19346, loss: 5.9604616353681195e-08\n",
      "step: 19347, loss: 4.768369876728684e-08\n",
      "step: 19348, loss: 1.43051135381711e-08\n",
      "step: 19349, loss: 4.081310635228874e-06\n",
      "step: 19350, loss: 6.675717401094516e-08\n",
      "step: 19351, loss: 0.0009328936575911939\n",
      "step: 19352, loss: 1.668929527909313e-08\n",
      "step: 19353, loss: 4.5299302087187243e-07\n",
      "step: 19354, loss: 5.2452058696417225e-08\n",
      "step: 19355, loss: 0.0\n",
      "step: 19356, loss: 1.0490403212770616e-07\n",
      "step: 19357, loss: 2.3841845830929742e-08\n",
      "step: 19358, loss: 2.622604000634965e-08\n",
      "step: 19359, loss: 4.291533528544278e-08\n",
      "step: 19360, loss: 7.009392675172421e-07\n",
      "step: 19361, loss: 1.5258763141901e-07\n",
      "step: 19362, loss: 1.6212403863846703e-07\n",
      "step: 19363, loss: 0.08476898074150085\n",
      "step: 19364, loss: 1.0967239205683654e-07\n",
      "step: 19365, loss: 9.536734069115482e-08\n",
      "step: 19366, loss: 4.768366679286373e-08\n",
      "step: 19367, loss: 5.0067868073711e-08\n",
      "step: 19368, loss: 0.08472182601690292\n",
      "step: 19369, loss: 3.373339040990686e-06\n",
      "step: 19370, loss: 1.0204067848462728e-06\n",
      "step: 19371, loss: 4.291532462730174e-08\n",
      "step: 19372, loss: 5.483619602841827e-08\n",
      "step: 19373, loss: 1.192092824453539e-08\n",
      "step: 19374, loss: 1.430511264999268e-08\n",
      "step: 19375, loss: 0.0825890600681305\n",
      "step: 19376, loss: 3.099440704090739e-08\n",
      "step: 19377, loss: 3.6001122794004914e-07\n",
      "step: 19378, loss: 2.646436030318e-07\n",
      "step: 19379, loss: 2.38418573772492e-09\n",
      "step: 19380, loss: 1.740448709597331e-07\n",
      "step: 19381, loss: 7.15255676908555e-09\n",
      "step: 19382, loss: 1.907348234908568e-08\n",
      "step: 19383, loss: 9.298312164673916e-08\n",
      "step: 19384, loss: 3.337858700547258e-08\n",
      "step: 19385, loss: 1.2159310358583753e-07\n",
      "step: 19386, loss: 7.390971035192706e-08\n",
      "step: 19387, loss: 2.0574534573825076e-06\n",
      "step: 19388, loss: 9.298314296302124e-08\n",
      "step: 19389, loss: 3.337859055818626e-08\n",
      "step: 19390, loss: 6.675713848380838e-08\n",
      "step: 19391, loss: 0.0\n",
      "step: 19392, loss: 2.38418573772492e-09\n",
      "step: 19393, loss: 1.430511264999268e-08\n",
      "step: 19394, loss: 1.192092646817855e-08\n",
      "step: 19395, loss: 4.2438128389221674e-07\n",
      "step: 19396, loss: 7.37602895242162e-05\n",
      "step: 19397, loss: 4.053114466273655e-08\n",
      "step: 19398, loss: 2.694111742584937e-07\n",
      "step: 19399, loss: 6.62796082906425e-07\n",
      "step: 19400, loss: 0.0924774631857872\n",
      "step: 19401, loss: 3.242478214815492e-07\n",
      "step: 19402, loss: 1.0490410318197974e-07\n",
      "step: 19403, loss: 5.960456661568969e-08\n",
      "step: 19404, loss: 3.490149993012892e-06\n",
      "step: 19405, loss: 2.861021997091484e-08\n",
      "step: 19406, loss: 0.0\n",
      "step: 19407, loss: 1.7404526886366511e-07\n",
      "step: 19408, loss: 0.0\n",
      "step: 19409, loss: 1.239774718442277e-07\n",
      "step: 19410, loss: 1.2874575361365714e-07\n",
      "step: 19411, loss: 4.76837147544984e-09\n",
      "step: 19412, loss: 8.821478303389085e-08\n",
      "step: 19413, loss: 2.145766764272139e-08\n",
      "step: 19414, loss: 5.841218921887048e-07\n",
      "step: 19415, loss: 2.145766231365087e-08\n",
      "step: 19416, loss: 1.382823739959349e-07\n",
      "step: 19417, loss: 4.529948327558486e-08\n",
      "step: 19418, loss: 8.583051425148369e-08\n",
      "step: 19419, loss: 1.7236924350072513e-06\n",
      "step: 19420, loss: 8.82146906633352e-08\n",
      "step: 19421, loss: 1.907348234908568e-08\n",
      "step: 19422, loss: 3.1471077477362996e-07\n",
      "step: 19423, loss: 1.3828260136961035e-07\n",
      "step: 19424, loss: 1.192092558000013e-08\n",
      "step: 19425, loss: 3.0994389277338996e-08\n",
      "step: 19426, loss: 6.675676331724389e-07\n",
      "step: 19427, loss: 2.3841845830929742e-08\n",
      "step: 19428, loss: 1.0997627214237582e-05\n",
      "step: 19429, loss: 9.05989878674518e-08\n",
      "step: 19430, loss: 3.099440704090739e-08\n",
      "step: 19431, loss: 1.2636164115065185e-07\n",
      "step: 19432, loss: 2.38418573772492e-09\n",
      "step: 19433, loss: 1.1444080882938579e-07\n",
      "step: 19434, loss: 2.384185116000026e-08\n",
      "step: 19435, loss: 0.13910992443561554\n",
      "step: 19436, loss: 3.352760202801619e-08\n",
      "step: 19437, loss: 2.598749802018574e-07\n",
      "step: 19438, loss: 1.668929350273629e-08\n",
      "step: 19439, loss: 1.192092646817855e-08\n",
      "step: 19440, loss: 2.38418573772492e-09\n",
      "step: 19441, loss: 3.8385084621950227e-07\n",
      "step: 19442, loss: 0.0\n",
      "step: 19443, loss: 3.873997684422648e-06\n",
      "step: 19444, loss: 5.483619602841827e-08\n",
      "step: 19445, loss: 1.430511264999268e-08\n",
      "step: 19446, loss: 3.576277052275145e-08\n",
      "step: 19447, loss: 0.0026376552414149046\n",
      "step: 19448, loss: 7.223961233648879e-07\n",
      "step: 19449, loss: 2.908497208409244e-06\n",
      "step: 19450, loss: 1.668929883180681e-08\n",
      "step: 19451, loss: 1.192092824453539e-08\n",
      "step: 19452, loss: 2.503379334939382e-07\n",
      "step: 19453, loss: 2.0050135844940087e-06\n",
      "step: 19454, loss: 7.15255676908555e-09\n",
      "step: 19455, loss: 1.6569911167607643e-06\n",
      "step: 19456, loss: 2.503388998320588e-07\n",
      "step: 19457, loss: 1.0013569351485785e-07\n",
      "step: 19458, loss: 9.53674206272126e-09\n",
      "step: 19459, loss: 1.3589838943062205e-07\n",
      "step: 19460, loss: 6.961703320484958e-07\n",
      "step: 19461, loss: 6.031936550243699e-07\n",
      "step: 19462, loss: 7.533954544669541e-07\n",
      "step: 19463, loss: 7.15255676908555e-09\n",
      "step: 19464, loss: 1.668929527909313e-08\n",
      "step: 19465, loss: 1.642639858800976e-06\n",
      "step: 19466, loss: 1.0728809485271995e-07\n",
      "step: 19467, loss: 0.24403633177280426\n",
      "step: 19468, loss: 3.8146957592744e-08\n",
      "step: 19469, loss: 0.0\n",
      "step: 19470, loss: 5.506227898877114e-05\n",
      "step: 19471, loss: 7.152551262379347e-08\n",
      "step: 19472, loss: 8.344634494505954e-08\n",
      "step: 19473, loss: 1.43051135381711e-08\n",
      "step: 19474, loss: 6.556409744007397e-07\n",
      "step: 19475, loss: 3.0994389277338996e-08\n",
      "step: 19476, loss: 4.76837103136063e-09\n",
      "step: 19477, loss: 1.192092824453539e-08\n",
      "step: 19478, loss: 0.0025317806284874678\n",
      "step: 19479, loss: 2.145766764272139e-08\n",
      "step: 19480, loss: 2.3841788276968146e-07\n",
      "step: 19481, loss: 1.4305086892818508e-07\n",
      "step: 19482, loss: 1.1920893427941337e-07\n",
      "step: 19483, loss: 2.38418573772492e-09\n",
      "step: 19484, loss: 2.1457660537294032e-08\n",
      "step: 19485, loss: 2.6226029348208613e-08\n",
      "step: 19486, loss: 8.106226090376367e-08\n",
      "step: 19487, loss: 2.217284418293275e-07\n",
      "step: 19488, loss: 0.0\n",
      "step: 19489, loss: 1.907348234908568e-08\n",
      "step: 19490, loss: 2.360331592399234e-07\n",
      "step: 19491, loss: 1.668929527909313e-08\n",
      "step: 19492, loss: 1.6450849216198549e-07\n",
      "step: 19493, loss: 9.53674117454284e-09\n",
      "step: 19494, loss: 7.15255632499634e-09\n",
      "step: 19495, loss: 7.390963929765348e-08\n",
      "step: 19496, loss: 2.3364927415059356e-07\n",
      "step: 19497, loss: 1.192092646817855e-08\n",
      "step: 19498, loss: 2.861022352362852e-08\n",
      "step: 19499, loss: 3.790832749928086e-07\n",
      "step: 19500, loss: 0.0\n",
      "step: 19501, loss: 1.43051135381711e-08\n",
      "step: 19502, loss: 7.15255632499634e-09\n",
      "step: 19503, loss: 9.53674295089968e-09\n",
      "step: 19504, loss: 2.38418529363571e-08\n",
      "step: 19505, loss: 0.05192272737622261\n",
      "step: 19506, loss: 7.629387965835122e-08\n",
      "step: 19507, loss: 2.28392946155509e-06\n",
      "step: 19508, loss: 1.0251977755615371e-07\n",
      "step: 19509, loss: 6.91412793685231e-08\n",
      "step: 19510, loss: 3.6477939602264087e-07\n",
      "step: 19511, loss: 5.4836245766409775e-08\n",
      "step: 19512, loss: 0.0002544789167586714\n",
      "step: 19513, loss: 2.4795454578452336e-07\n",
      "step: 19514, loss: 4.76837147544984e-09\n",
      "step: 19515, loss: 0.24572741985321045\n",
      "step: 19516, loss: 1.907347702001516e-08\n",
      "step: 19517, loss: 9.775148157586955e-08\n",
      "step: 19518, loss: 2.38418573772492e-09\n",
      "step: 19519, loss: 7.390969614107235e-08\n",
      "step: 19520, loss: 7.853090937715024e-05\n",
      "step: 19521, loss: 1.525874466778987e-07\n",
      "step: 19522, loss: 7.15255676908555e-09\n",
      "step: 19523, loss: 7.152546288580197e-08\n",
      "step: 19524, loss: 2.1456523882079637e-06\n",
      "step: 19525, loss: 0.0009463693131692708\n",
      "step: 19526, loss: 0.10961893200874329\n",
      "step: 19527, loss: 9.77513892053139e-08\n",
      "step: 19528, loss: 9.53674206272126e-09\n",
      "step: 19529, loss: 3.33785834527589e-08\n",
      "step: 19530, loss: 1.764293813266704e-07\n",
      "step: 19531, loss: 2.839370608853642e-06\n",
      "step: 19532, loss: 1.430511264999268e-08\n",
      "step: 19533, loss: 1.907347702001516e-08\n",
      "step: 19534, loss: 1.668929883180681e-08\n",
      "step: 19535, loss: 4.7683659687436375e-08\n",
      "step: 19536, loss: 4.76837103136063e-09\n",
      "step: 19537, loss: 9.059900207830651e-08\n",
      "step: 19538, loss: 2.38418573772492e-09\n",
      "step: 19539, loss: 1.311300081852096e-07\n",
      "step: 19540, loss: 6.91413291065146e-08\n",
      "step: 19541, loss: 2.38418573772492e-09\n",
      "step: 19542, loss: 4.887522209173767e-07\n",
      "step: 19543, loss: 0.0\n",
      "step: 19544, loss: 9.53674206272126e-09\n",
      "step: 19545, loss: 9.53674295089968e-09\n",
      "step: 19546, loss: 5.960460924825384e-08\n",
      "step: 19547, loss: 1.7976053641177714e-06\n",
      "step: 19548, loss: 1.0013575035827671e-07\n",
      "step: 19549, loss: 2.38418573772492e-09\n",
      "step: 19550, loss: 1.1682494260867315e-07\n",
      "step: 19551, loss: 5.96045985901128e-08\n",
      "step: 19552, loss: 5.2452058696417225e-08\n",
      "step: 19553, loss: 4.053114466273655e-08\n",
      "step: 19554, loss: 2.38418573772492e-09\n",
      "step: 19555, loss: 1.1086165159213124e-06\n",
      "step: 19556, loss: 1.668929883180681e-08\n",
      "step: 19557, loss: 3.6954591564608563e-07\n",
      "step: 19558, loss: 2.145766764272139e-08\n",
      "step: 19559, loss: 1.0490401081142409e-07\n",
      "step: 19560, loss: 8.583050714605633e-08\n",
      "step: 19561, loss: 9.53674206272126e-09\n",
      "step: 19562, loss: 8.821481856102764e-08\n",
      "step: 19563, loss: 1.192092646817855e-08\n",
      "step: 19564, loss: 1.4485932297247928e-05\n",
      "step: 19565, loss: 1.0967232100256297e-07\n",
      "step: 19566, loss: 1.1444060277199242e-07\n",
      "step: 19567, loss: 1.382826440021745e-07\n",
      "step: 19568, loss: 7.867802054306594e-08\n",
      "step: 19569, loss: 3.099440704090739e-08\n",
      "step: 19570, loss: 5.507395144377369e-07\n",
      "step: 19571, loss: 2.38418573772492e-09\n",
      "step: 19572, loss: 4.76837103136063e-09\n",
      "step: 19573, loss: 7.15255632499634e-09\n",
      "step: 19574, loss: 1.192092646817855e-08\n",
      "step: 19575, loss: 1.668929527909313e-08\n",
      "step: 19576, loss: 0.0\n",
      "step: 19577, loss: 9.53674206272126e-09\n",
      "step: 19578, loss: 7.390966771936291e-08\n",
      "step: 19579, loss: 4.291529975830599e-08\n",
      "step: 19580, loss: 0.10915134102106094\n",
      "step: 19581, loss: 2.145766764272139e-08\n",
      "step: 19582, loss: 4.768369876728684e-08\n",
      "step: 19583, loss: 3.576275631189674e-08\n",
      "step: 19584, loss: 8.106223248205424e-08\n",
      "step: 19585, loss: 7.15255676908555e-09\n",
      "step: 19586, loss: 4.76837147544984e-09\n",
      "step: 19587, loss: 0.0\n",
      "step: 19588, loss: 2.38418573772492e-09\n",
      "step: 19589, loss: 4.76837147544984e-09\n",
      "step: 19590, loss: 2.1457660537294032e-08\n",
      "step: 19591, loss: 7.86780773864848e-08\n",
      "step: 19592, loss: 2.38418573772492e-09\n",
      "step: 19593, loss: 0.0\n",
      "step: 19594, loss: 4.959052262165642e-07\n",
      "step: 19595, loss: 2.38418573772492e-09\n",
      "step: 19596, loss: 3.8146950487316644e-08\n",
      "step: 19597, loss: 6.437299049366629e-08\n",
      "step: 19598, loss: 3.3378576347331546e-08\n",
      "step: 19599, loss: 4.529949038101222e-08\n",
      "step: 19600, loss: 4.76837147544984e-09\n",
      "step: 19601, loss: 4.7683691661859484e-08\n",
      "step: 19602, loss: 2.38418573772492e-09\n",
      "step: 19603, loss: 3.337858700547258e-08\n",
      "step: 19604, loss: 6.914074788255675e-07\n",
      "step: 19605, loss: 7.15255676908555e-09\n",
      "step: 19606, loss: 7.152544867494726e-08\n",
      "step: 19607, loss: 6.91412793685231e-08\n",
      "step: 19608, loss: 8.201507171179401e-07\n",
      "step: 19609, loss: 8.106076165859122e-07\n",
      "step: 19610, loss: 9.53674206272126e-09\n",
      "step: 19611, loss: 1.192092646817855e-08\n",
      "step: 19612, loss: 0.04623972997069359\n",
      "step: 19613, loss: 7.867805607020273e-08\n",
      "step: 19614, loss: 1.192092646817855e-08\n",
      "step: 19615, loss: 9.059893102403294e-08\n",
      "step: 19616, loss: 0.0730387419462204\n",
      "step: 19617, loss: 6.198873592211385e-08\n",
      "step: 19618, loss: 0.0\n",
      "step: 19619, loss: 7.15255676908555e-09\n",
      "step: 19620, loss: 0.00010679292608983815\n",
      "step: 19621, loss: 1.0490401081142409e-07\n",
      "step: 19622, loss: 1.668929350273629e-08\n",
      "step: 19623, loss: 0.1497858464717865\n",
      "step: 19624, loss: 2.6226025795494934e-08\n",
      "step: 19625, loss: 2.145766764272139e-08\n",
      "step: 19626, loss: 2.145766586636455e-08\n",
      "step: 19627, loss: 1.6927678814226965e-07\n",
      "step: 19628, loss: 0.0\n",
      "step: 19629, loss: 0.0\n",
      "step: 19630, loss: 3.623937061547622e-07\n",
      "step: 19631, loss: 3.075577126310236e-07\n",
      "step: 19632, loss: 6.914133621194196e-08\n",
      "step: 19633, loss: 5.722041507283393e-08\n",
      "step: 19634, loss: 0.0\n",
      "step: 19635, loss: 0.10642504692077637\n",
      "step: 19636, loss: 9.775149578672426e-08\n",
      "step: 19637, loss: 4.053114466273655e-08\n",
      "step: 19638, loss: 0.0011145484168082476\n",
      "step: 19639, loss: 0.07136262208223343\n",
      "step: 19640, loss: 0.07482432574033737\n",
      "step: 19641, loss: 2.38418529363571e-08\n",
      "step: 19642, loss: 7.15255676908555e-09\n",
      "step: 19643, loss: 3.027901129826205e-07\n",
      "step: 19644, loss: 2.145766764272139e-08\n",
      "step: 19645, loss: 1.668929350273629e-08\n",
      "step: 19646, loss: 4.291530331101967e-08\n",
      "step: 19647, loss: 1.3112986607666244e-07\n",
      "step: 19648, loss: 4.7683688109145805e-08\n",
      "step: 19649, loss: 3.576278118089249e-08\n",
      "step: 19650, loss: 7.39096535085082e-08\n",
      "step: 19651, loss: 9.53674206272126e-09\n",
      "step: 19652, loss: 7.15255632499634e-09\n",
      "step: 19653, loss: 1.0013567930400313e-07\n",
      "step: 19654, loss: 7.867800633221123e-08\n",
      "step: 19655, loss: 0.0\n",
      "step: 19656, loss: 2.6702736022343743e-07\n",
      "step: 19657, loss: 3.814696114545768e-08\n",
      "step: 19658, loss: 0.062492746859788895\n",
      "step: 19659, loss: 5.173633894628438e-07\n",
      "step: 19660, loss: 4.7683659687436375e-08\n",
      "step: 19661, loss: 4.5299508144580614e-08\n",
      "step: 19662, loss: 1.303851249190302e-08\n",
      "step: 19663, loss: 5.960462345910855e-08\n",
      "step: 19664, loss: 3.885916157742031e-06\n",
      "step: 19665, loss: 9.53674295089968e-09\n",
      "step: 19666, loss: 3.0040644105611136e-07\n",
      "step: 19667, loss: 4.76837103136063e-09\n",
      "step: 19668, loss: 3.5285663102513354e-07\n",
      "step: 19669, loss: 7.15254913075114e-08\n",
      "step: 19670, loss: 9.0598852864332e-08\n",
      "step: 19671, loss: 2.6226029348208613e-08\n",
      "step: 19672, loss: 1.668929883180681e-08\n",
      "step: 19673, loss: 8.940505153987033e-07\n",
      "step: 19674, loss: 2.861022529998536e-08\n",
      "step: 19675, loss: 3.8146950487316644e-08\n",
      "step: 19676, loss: 0.00037764673470519483\n",
      "step: 19677, loss: 1.430511264999268e-08\n",
      "step: 19678, loss: 1.668929527909313e-08\n",
      "step: 19679, loss: 4.7683684556432127e-08\n",
      "step: 19680, loss: 4.76837147544984e-09\n",
      "step: 19681, loss: 1.430510998545742e-08\n",
      "step: 19682, loss: 2.384185116000026e-08\n",
      "step: 19683, loss: 0.0\n",
      "step: 19684, loss: 4.76837147544984e-09\n",
      "step: 19685, loss: 4.76837103136063e-09\n",
      "step: 19686, loss: 1.3113005081777374e-07\n",
      "step: 19687, loss: 1.430510998545742e-08\n",
      "step: 19688, loss: 4.76837103136063e-09\n",
      "step: 19689, loss: 0.0019459564937278628\n",
      "step: 19690, loss: 9.53674206272126e-09\n",
      "step: 19691, loss: 1.907348234908568e-08\n",
      "step: 19692, loss: 5.9604580826544407e-08\n",
      "step: 19693, loss: 5.9604616353681195e-08\n",
      "step: 19694, loss: 0.0033067050389945507\n",
      "step: 19695, loss: 1.668929527909313e-08\n",
      "step: 19696, loss: 1.192092824453539e-08\n",
      "step: 19697, loss: 2.581598710094113e-05\n",
      "step: 19698, loss: 1.239772871031164e-07\n",
      "step: 19699, loss: 9.059888839146879e-08\n",
      "step: 19700, loss: 1.0251989834841879e-07\n",
      "step: 19701, loss: 2.861022529998536e-08\n",
      "step: 19702, loss: 0.0\n",
      "step: 19703, loss: 9.53674206272126e-09\n",
      "step: 19704, loss: 9.53674206272126e-09\n",
      "step: 19705, loss: 2.8610031677089864e-07\n",
      "step: 19706, loss: 1.668929527909313e-08\n",
      "step: 19707, loss: 9.059888128604143e-08\n",
      "step: 19708, loss: 3.576277762817881e-08\n",
      "step: 19709, loss: 2.622603290092229e-08\n",
      "step: 19710, loss: 4.76837147544984e-09\n",
      "step: 19711, loss: 7.15255632499634e-09\n",
      "step: 19712, loss: 1.0013562956601163e-07\n",
      "step: 19713, loss: 2.622604000634965e-08\n",
      "step: 19714, loss: 7.15255676908555e-09\n",
      "step: 19715, loss: 3.099440704090739e-08\n",
      "step: 19716, loss: 6.675711006209895e-08\n",
      "step: 19717, loss: 2.145766764272139e-08\n",
      "step: 19718, loss: 1.1444080172395843e-07\n",
      "step: 19719, loss: 0.05409764125943184\n",
      "step: 19720, loss: 2.38418573772492e-09\n",
      "step: 19721, loss: 9.536721279346239e-08\n",
      "step: 19722, loss: 1.192092558000013e-08\n",
      "step: 19723, loss: 4.863680942435167e-07\n",
      "step: 19724, loss: 0.1754162609577179\n",
      "step: 19725, loss: 4.76837147544984e-09\n",
      "step: 19726, loss: 3.337857279461787e-08\n",
      "step: 19727, loss: 1.8524281131249154e-06\n",
      "step: 19728, loss: 0.0032525244168937206\n",
      "step: 19729, loss: 0.050438664853572845\n",
      "step: 19730, loss: 0.006419728975743055\n",
      "step: 19731, loss: 1.6927688761825266e-07\n",
      "step: 19732, loss: 0.01587311550974846\n",
      "step: 19733, loss: 9.536734069115482e-08\n",
      "step: 19734, loss: 6.675713137838102e-08\n",
      "step: 19735, loss: 2.1457660537294032e-08\n",
      "step: 19736, loss: 2.38418573772492e-09\n",
      "step: 19737, loss: 2.38418573772492e-09\n",
      "step: 19738, loss: 4.76837147544984e-09\n",
      "step: 19739, loss: 2.145766231365087e-08\n",
      "step: 19740, loss: 3.328046659589745e-06\n",
      "step: 19741, loss: 4.291532462730174e-08\n",
      "step: 19742, loss: 1.192092824453539e-08\n",
      "step: 19743, loss: 1.907348412544252e-08\n",
      "step: 19744, loss: 3.337859055818626e-08\n",
      "step: 19745, loss: 2.384184938364342e-08\n",
      "step: 19746, loss: 3.8146946934602965e-08\n",
      "step: 19747, loss: 0.06368622928857803\n",
      "step: 19748, loss: 1.5497164440603228e-07\n",
      "step: 19749, loss: 0.0\n",
      "step: 19750, loss: 0.0\n",
      "step: 19751, loss: 2.384185116000026e-08\n",
      "step: 19752, loss: 3.099440704090739e-08\n",
      "step: 19753, loss: 6.21227445662953e-06\n",
      "step: 19754, loss: 1.192092824453539e-08\n",
      "step: 19755, loss: 1.430511176181426e-08\n",
      "step: 19756, loss: 0.0\n",
      "step: 19757, loss: 1.4305084050647565e-07\n",
      "step: 19758, loss: 1.2874573940280243e-07\n",
      "step: 19759, loss: 0.11344967782497406\n",
      "step: 19760, loss: 1.430511176181426e-08\n",
      "step: 19761, loss: 1.4305072681963793e-07\n",
      "step: 19762, loss: 2.3364886203580681e-07\n",
      "step: 19763, loss: 2.145766231365087e-08\n",
      "step: 19764, loss: 1.430508973498945e-07\n",
      "step: 19765, loss: 2.38418573772492e-09\n",
      "step: 19766, loss: 1.0013567930400313e-07\n",
      "step: 19767, loss: 0.004324556328356266\n",
      "step: 19768, loss: 4.2915321074588064e-08\n",
      "step: 19769, loss: 1.3589811942438246e-07\n",
      "step: 19770, loss: 2.38418573772492e-09\n",
      "step: 19771, loss: 4.005413529739599e-07\n",
      "step: 19772, loss: 6.198875013296856e-08\n",
      "step: 19773, loss: 1.0888070391956717e-05\n",
      "step: 19774, loss: 5.960456306297601e-08\n",
      "step: 19775, loss: 4.76837103136063e-09\n",
      "step: 19776, loss: 0.0\n",
      "step: 19777, loss: 4.2915317521874385e-08\n",
      "step: 19778, loss: 1.192092646817855e-08\n",
      "step: 19779, loss: 6.67571669055178e-08\n",
      "step: 19780, loss: 3.933876087103272e-07\n",
      "step: 19781, loss: 7.390892164949037e-07\n",
      "step: 19782, loss: 3.33785834527589e-08\n",
      "step: 19783, loss: 3.6954656934540253e-07\n",
      "step: 19784, loss: 4.053114821545023e-08\n",
      "step: 19785, loss: 7.724616466475709e-07\n",
      "step: 19786, loss: 6.675713137838102e-08\n",
      "step: 19787, loss: 2.622603290092229e-08\n",
      "step: 19788, loss: 0.0\n",
      "step: 19789, loss: 1.668929883180681e-08\n",
      "step: 19790, loss: 4.5299508144580614e-08\n",
      "step: 19791, loss: 0.05169432610273361\n",
      "step: 19792, loss: 1.3565565950557357e-06\n",
      "step: 19793, loss: 1.0490403212770616e-07\n",
      "step: 19794, loss: 2.38418573772492e-09\n",
      "step: 19795, loss: 2.8610218194558e-08\n",
      "step: 19796, loss: 2.551066700107185e-07\n",
      "step: 19797, loss: 4.291530331101967e-08\n",
      "step: 19798, loss: 4.76837103136063e-09\n",
      "step: 19799, loss: 4.76837147544984e-09\n",
      "step: 19800, loss: 1.2636144219868584e-07\n",
      "step: 19801, loss: 5.2452055143703546e-08\n",
      "step: 19802, loss: 2.479540910371725e-07\n",
      "step: 19803, loss: 1.4543506665631867e-07\n",
      "step: 19804, loss: 6.437296917738422e-08\n",
      "step: 19805, loss: 3.9883457247924525e-06\n",
      "step: 19806, loss: 0.0\n",
      "step: 19807, loss: 0.05433010682463646\n",
      "step: 19808, loss: 1.8119757783097157e-07\n",
      "step: 19809, loss: 6.914133621194196e-08\n",
      "step: 19810, loss: 2.8848566557826416e-07\n",
      "step: 19811, loss: 3.0994396382766354e-08\n",
      "step: 19812, loss: 0.0\n",
      "step: 19813, loss: 2.145766586636455e-08\n",
      "step: 19814, loss: 2.408013131116604e-07\n",
      "step: 19815, loss: 1.597401819708466e-07\n",
      "step: 19816, loss: 9.53674295089968e-09\n",
      "step: 19817, loss: 4.76837103136063e-09\n",
      "step: 19818, loss: 7.15255676908555e-09\n",
      "step: 19819, loss: 2.145766764272139e-08\n",
      "step: 19820, loss: 1.3351419170248846e-07\n",
      "step: 19821, loss: 2.38418573772492e-09\n",
      "step: 19822, loss: 9.53674206272126e-09\n",
      "step: 19823, loss: 9.059888839146879e-08\n",
      "step: 19824, loss: 1.430511264999268e-08\n",
      "step: 19825, loss: 6.198873592211385e-08\n",
      "step: 19826, loss: 4.768368100371845e-08\n",
      "step: 19827, loss: 7.390963219222613e-08\n",
      "step: 19828, loss: 1.668929883180681e-08\n",
      "step: 19829, loss: 7.963045050018991e-07\n",
      "step: 19830, loss: 0.07923876494169235\n",
      "step: 19831, loss: 6.437298338823894e-08\n",
      "step: 19832, loss: 4.76837147544984e-09\n",
      "step: 19833, loss: 1.4471711438091006e-06\n",
      "step: 19834, loss: 5.741686254623346e-05\n",
      "step: 19835, loss: 3.552424914232688e-07\n",
      "step: 19836, loss: 7.867804896477537e-08\n",
      "step: 19837, loss: 6.484920618277101e-07\n",
      "step: 19838, loss: 7.15255676908555e-09\n",
      "step: 19839, loss: 1.192092646817855e-08\n",
      "step: 19840, loss: 1.509133880972513e-06\n",
      "step: 19841, loss: 2.38418573772492e-09\n",
      "step: 19842, loss: 1.192092646817855e-08\n",
      "step: 19843, loss: 9.53674206272126e-09\n",
      "step: 19844, loss: 7.15255632499634e-09\n",
      "step: 19845, loss: 4.768370232000052e-08\n",
      "step: 19846, loss: 0.18227344751358032\n",
      "step: 19847, loss: 0.0\n",
      "step: 19848, loss: 1.096723991622639e-07\n",
      "step: 19849, loss: 5.103893727209652e-06\n",
      "step: 19850, loss: 1.4543483928264322e-07\n",
      "step: 19851, loss: 5.483623510826874e-08\n",
      "step: 19852, loss: 1.6927660340115835e-07\n",
      "step: 19853, loss: 7.6293815709505e-08\n",
      "step: 19854, loss: 4.76837147544984e-09\n",
      "step: 19855, loss: 4.76837147544984e-09\n",
      "step: 19856, loss: 1.096723991622639e-07\n",
      "step: 19857, loss: 4.76837147544984e-09\n",
      "step: 19858, loss: 2.145766586636455e-08\n",
      "step: 19859, loss: 1.401852046001295e-06\n",
      "step: 19860, loss: 0.1744021773338318\n",
      "step: 19861, loss: 4.100781438864942e-07\n",
      "step: 19862, loss: 3.5762766970037774e-08\n",
      "step: 19863, loss: 2.38418529363571e-08\n",
      "step: 19864, loss: 0.0\n",
      "step: 19865, loss: 5.483623155555506e-08\n",
      "step: 19866, loss: 4.4345387095745536e-07\n",
      "step: 19867, loss: 0.05783722549676895\n",
      "step: 19868, loss: 0.024449201300740242\n",
      "step: 19869, loss: 2.1457660537294032e-08\n",
      "step: 19870, loss: 7.629387255292386e-08\n",
      "step: 19871, loss: 0.0\n",
      "step: 19872, loss: 5.340532425179845e-07\n",
      "step: 19873, loss: 1.4066672804347036e-07\n",
      "step: 19874, loss: 3.3378576347331546e-08\n",
      "step: 19875, loss: 2.3126493431391282e-07\n",
      "step: 19876, loss: 2.408018531241396e-07\n",
      "step: 19877, loss: 4.76837147544984e-09\n",
      "step: 19878, loss: 2.145766586636455e-08\n",
      "step: 19879, loss: 7.86779779105018e-08\n",
      "step: 19880, loss: 2.2888077921834338e-07\n",
      "step: 19881, loss: 7.390971035192706e-08\n",
      "step: 19882, loss: 1.668929883180681e-08\n",
      "step: 19883, loss: 1.2397741500080883e-07\n",
      "step: 19884, loss: 4.76837147544984e-09\n",
      "step: 19885, loss: 5.006786452099732e-08\n",
      "step: 19886, loss: 0.0\n",
      "step: 19887, loss: 1.6927680235312437e-07\n",
      "step: 19888, loss: 2.6263074914822937e-07\n",
      "step: 19889, loss: 1.192092824453539e-08\n",
      "step: 19890, loss: 3.623946724928828e-07\n",
      "step: 19891, loss: 0.1445741206407547\n",
      "step: 19892, loss: 1.430511176181426e-08\n",
      "step: 19893, loss: 0.12321056425571442\n",
      "step: 19894, loss: 1.430511264999268e-08\n",
      "step: 19895, loss: 0.0\n",
      "step: 19896, loss: 4.76837103136063e-09\n",
      "step: 19897, loss: 2.6702767286224116e-07\n",
      "step: 19898, loss: 1.0728809485271995e-07\n",
      "step: 19899, loss: 1.430511264999268e-08\n",
      "step: 19900, loss: 2.38418529363571e-08\n",
      "step: 19901, loss: 7.15255632499634e-09\n",
      "step: 19902, loss: 0.03612527623772621\n",
      "step: 19903, loss: 2.38418573772492e-09\n",
      "step: 19904, loss: 2.38418529363571e-08\n",
      "step: 19905, loss: 5.817371970806562e-07\n",
      "step: 19906, loss: 0.13159528374671936\n",
      "step: 19907, loss: 1.668929883180681e-08\n",
      "step: 19908, loss: 2.145766941907823e-08\n",
      "step: 19909, loss: 1.1419933798606507e-06\n",
      "step: 19910, loss: 1.6855487956490833e-06\n",
      "step: 19911, loss: 2.4556985067647474e-07\n",
      "step: 19912, loss: 4.76837103136063e-09\n",
      "step: 19913, loss: 2.312650195790411e-07\n",
      "step: 19914, loss: 1.430511176181426e-08\n",
      "step: 19915, loss: 2.8798915536754066e-06\n",
      "step: 19916, loss: 2.38418573772492e-09\n",
      "step: 19917, loss: 1.2159334517036768e-07\n",
      "step: 19918, loss: 2.622603290092229e-08\n",
      "step: 19919, loss: 1.6450816531232704e-07\n",
      "step: 19920, loss: 2.38418573772492e-09\n",
      "step: 19921, loss: 7.15255632499634e-09\n",
      "step: 19922, loss: 8.344636626134161e-08\n",
      "step: 19923, loss: 0.06485605984926224\n",
      "step: 19924, loss: 9.53674206272126e-09\n",
      "step: 19925, loss: 9.53674206272126e-09\n",
      "step: 19926, loss: 0.06800592690706253\n",
      "step: 19927, loss: 6.437296207195686e-08\n",
      "step: 19928, loss: 2.145766764272139e-08\n",
      "step: 19929, loss: 5.4836249319123453e-08\n",
      "step: 19930, loss: 3.576277407546513e-08\n",
      "step: 19931, loss: 9.68422245932743e-05\n",
      "step: 19932, loss: 1.192092646817855e-08\n",
      "step: 19933, loss: 6.675713848380838e-08\n",
      "step: 19934, loss: 1.2397750026593712e-07\n",
      "step: 19935, loss: 4.76837103136063e-09\n",
      "step: 19936, loss: 9.53674206272126e-09\n",
      "step: 19937, loss: 1.5258730456935155e-07\n",
      "step: 19938, loss: 7.629389386920593e-08\n",
      "step: 19939, loss: 1.430511264999268e-08\n",
      "step: 19940, loss: 3.4570402362987807e-07\n",
      "step: 19941, loss: 7.152546288580197e-08\n",
      "step: 19942, loss: 7.15255676908555e-09\n",
      "step: 19943, loss: 4.76837147544984e-09\n",
      "step: 19944, loss: 1.5020353316685942e-07\n",
      "step: 19945, loss: 4.76837103136063e-09\n",
      "step: 19946, loss: 2.7418019499236834e-07\n",
      "step: 19947, loss: 9.53674295089968e-09\n",
      "step: 19948, loss: 2.38418573772492e-09\n",
      "step: 19949, loss: 2.145766764272139e-08\n",
      "step: 19950, loss: 9.53674206272126e-09\n",
      "step: 19951, loss: 4.76837147544984e-09\n",
      "step: 19952, loss: 3.1709430459159194e-07\n",
      "step: 19953, loss: 1.907348412544252e-08\n",
      "step: 19954, loss: 1.9073478796372e-08\n",
      "step: 19955, loss: 1.3351426275676204e-07\n",
      "step: 19956, loss: 3.576275631189674e-08\n",
      "step: 19957, loss: 1.43051135381711e-08\n",
      "step: 19958, loss: 1.43051135381711e-08\n",
      "step: 19959, loss: 1.645082221557459e-07\n",
      "step: 19960, loss: 4.76837103136063e-09\n",
      "step: 19961, loss: 3.099440704090739e-08\n",
      "step: 19962, loss: 1.0967224284286203e-07\n",
      "step: 19963, loss: 0.0\n",
      "step: 19964, loss: 1.740447999054595e-07\n",
      "step: 19965, loss: 2.38418573772492e-09\n",
      "step: 19966, loss: 3.2663095339557913e-07\n",
      "step: 19967, loss: 2.5033827455445135e-07\n",
      "step: 19968, loss: 0.06671548634767532\n",
      "step: 19969, loss: 3.099440704090739e-08\n",
      "step: 19970, loss: 0.005061739590018988\n",
      "step: 19971, loss: 4.76837103136063e-09\n",
      "step: 19972, loss: 4.76837147544984e-09\n",
      "step: 19973, loss: 0.0\n",
      "step: 19974, loss: 2.861022352362852e-08\n",
      "step: 19975, loss: 4.768369876728684e-08\n",
      "step: 19976, loss: 7.15255676908555e-09\n",
      "step: 19977, loss: 2.5272260018027737e-07\n",
      "step: 19978, loss: 1.406664864589402e-07\n",
      "step: 19979, loss: 2.2972735678195022e-05\n",
      "step: 19980, loss: 2.145766941907823e-08\n",
      "step: 19981, loss: 3.576277052275145e-08\n",
      "step: 19982, loss: 8.344640889390575e-08\n",
      "step: 19983, loss: 6.198873592211385e-08\n",
      "step: 19984, loss: 4.768368100371845e-08\n",
      "step: 19985, loss: 3.981564020705264e-07\n",
      "step: 19986, loss: 2.145766231365087e-08\n",
      "step: 19987, loss: 1.1205641214928619e-07\n",
      "step: 19988, loss: 4.76837103136063e-09\n",
      "step: 19989, loss: 4.76837147544984e-09\n",
      "step: 19990, loss: 2.38418573772492e-09\n",
      "step: 19991, loss: 3.099419245700119e-07\n",
      "step: 19992, loss: 4.76837103136063e-09\n",
      "step: 19993, loss: 3.337857279461787e-08\n",
      "step: 19994, loss: 7.15255676908555e-09\n",
      "step: 19995, loss: 1.1444070224797542e-07\n",
      "step: 19996, loss: 8.583059951661198e-08\n",
      "step: 19997, loss: 3.480897419194662e-07\n",
      "step: 19998, loss: 1.335141348590696e-07\n",
      "step: 19999, loss: 1.430511264999268e-08\n",
      "step: 20000, loss: 1.3351419170248846e-07\n",
      "step: 20001, loss: 0.0077251261100173\n",
      "step: 20002, loss: 4.76837103136063e-09\n",
      "step: 20003, loss: 2.8610218194558e-08\n",
      "step: 20004, loss: 6.437294075567479e-08\n",
      "step: 20005, loss: 6.198873592211385e-08\n",
      "step: 20006, loss: 2.861021997091484e-08\n",
      "step: 20007, loss: 2.2172812919052376e-07\n",
      "step: 20008, loss: 7.390962508679877e-08\n",
      "step: 20009, loss: 4.76837103136063e-09\n",
      "step: 20010, loss: 0.04913901910185814\n",
      "step: 20011, loss: 5.102093041386979e-07\n",
      "step: 20012, loss: 4.76837147544984e-09\n",
      "step: 20013, loss: 2.6226025795494934e-08\n",
      "step: 20014, loss: 4.291532462730174e-08\n",
      "step: 20015, loss: 4.76837103136063e-09\n",
      "step: 20016, loss: 3.814696469817136e-08\n",
      "step: 20017, loss: 4.76837103136063e-09\n",
      "step: 20018, loss: 4.76837103136063e-09\n",
      "step: 20019, loss: 7.15255632499634e-09\n",
      "step: 20020, loss: 2.6226029348208613e-08\n",
      "step: 20021, loss: 1.668929350273629e-08\n",
      "step: 20022, loss: 4.529952235543533e-08\n",
      "step: 20023, loss: 0.15651781857013702\n",
      "step: 20024, loss: 1.645082221557459e-07\n",
      "step: 20025, loss: 0.0\n",
      "step: 20026, loss: 2.38418573772492e-09\n",
      "step: 20027, loss: 4.76837103136063e-09\n",
      "step: 20028, loss: 2.38418573772492e-09\n",
      "step: 20029, loss: 6.914129357937782e-08\n",
      "step: 20030, loss: 5.364377102523576e-07\n",
      "step: 20031, loss: 0.0\n",
      "step: 20032, loss: 0.0\n",
      "step: 20033, loss: 4.053111624102712e-08\n",
      "step: 20034, loss: 0.0\n",
      "step: 20035, loss: 5.435895786831679e-07\n",
      "step: 20036, loss: 5.9604552404834976e-08\n",
      "step: 20037, loss: 3.337859055818626e-08\n",
      "step: 20038, loss: 4.291533528544278e-08\n",
      "step: 20039, loss: 3.3378576347331546e-08\n",
      "step: 20040, loss: 7.557752610409807e-07\n",
      "step: 20041, loss: 9.53674206272126e-09\n",
      "step: 20042, loss: 5.4836245766409775e-08\n",
      "step: 20043, loss: 4.2915321074588064e-08\n",
      "step: 20044, loss: 2.38418529363571e-08\n",
      "step: 20045, loss: 4.76837103136063e-09\n",
      "step: 20046, loss: 5.269011467134987e-07\n",
      "step: 20047, loss: 7.15255632499634e-09\n",
      "step: 20048, loss: 0.15636608004570007\n",
      "step: 20049, loss: 4.339171653100493e-07\n",
      "step: 20050, loss: 0.0691027119755745\n",
      "step: 20051, loss: 5.00678503101426e-08\n",
      "step: 20052, loss: 0.037999287247657776\n",
      "step: 20053, loss: 0.08822966367006302\n",
      "step: 20054, loss: 3.5762763417324095e-08\n",
      "step: 20055, loss: 3.0994396382766354e-08\n",
      "step: 20056, loss: 0.0\n",
      "step: 20057, loss: 8.583064214917613e-08\n",
      "step: 20058, loss: 2.38418573772492e-09\n",
      "step: 20059, loss: 9.53674206272126e-09\n",
      "step: 20060, loss: 0.0\n",
      "step: 20061, loss: 2.813319497363409e-07\n",
      "step: 20062, loss: 1.0251979887243579e-07\n",
      "step: 20063, loss: 2.145766764272139e-08\n",
      "step: 20064, loss: 0.0\n",
      "step: 20065, loss: 8.630601087133982e-07\n",
      "step: 20066, loss: 5.006786452099732e-08\n",
      "step: 20067, loss: 7.867798501592915e-08\n",
      "step: 20068, loss: 1.192092646817855e-08\n",
      "step: 20069, loss: 3.099440704090739e-08\n",
      "step: 20070, loss: 2.38418573772492e-09\n",
      "step: 20071, loss: 7.15255676908555e-09\n",
      "step: 20072, loss: 2.38418573772492e-09\n",
      "step: 20073, loss: 7.390962508679877e-08\n",
      "step: 20074, loss: 1.192092646817855e-08\n",
      "step: 20075, loss: 1.3351419170248846e-07\n",
      "step: 20076, loss: 7.15255676908555e-09\n",
      "step: 20077, loss: 8.821474040132671e-08\n",
      "step: 20078, loss: 2.38418573772492e-09\n",
      "step: 20079, loss: 4.76837147544984e-09\n",
      "step: 20080, loss: 0.0036602711770683527\n",
      "step: 20081, loss: 1.1205653294155127e-07\n",
      "step: 20082, loss: 8.106224669290896e-08\n",
      "step: 20083, loss: 4.76837103136063e-09\n",
      "step: 20084, loss: 2.1696035901186406e-07\n",
      "step: 20085, loss: 2.38418573772492e-09\n",
      "step: 20086, loss: 4.76837103136063e-09\n",
      "step: 20087, loss: 7.15255676908555e-09\n",
      "step: 20088, loss: 1.9788703298218024e-07\n",
      "step: 20089, loss: 4.7683688109145805e-08\n",
      "step: 20090, loss: 1.430511264999268e-08\n",
      "step: 20091, loss: 5.006788228456571e-08\n",
      "step: 20092, loss: 0.0\n",
      "step: 20093, loss: 1.907348234908568e-08\n",
      "step: 20094, loss: 6.198806659085676e-07\n",
      "step: 20095, loss: 2.2888112027885654e-07\n",
      "step: 20096, loss: 3.099441059362107e-08\n",
      "step: 20097, loss: 4.482229769564583e-07\n",
      "step: 20098, loss: 5.7259971981693525e-06\n",
      "step: 20099, loss: 1.6212398179504817e-07\n",
      "step: 20100, loss: 3.3139932043013687e-07\n",
      "step: 20101, loss: 7.15255676908555e-09\n",
      "step: 20102, loss: 7.15255676908555e-09\n",
      "step: 20103, loss: 1.668929527909313e-08\n",
      "step: 20104, loss: 0.06394071877002716\n",
      "step: 20105, loss: 1.0728809485271995e-07\n",
      "step: 20106, loss: 1.2397758553106542e-07\n",
      "step: 20107, loss: 6.651816306657565e-07\n",
      "step: 20108, loss: 2.3841845830929742e-08\n",
      "step: 20109, loss: 4.76837147544984e-09\n",
      "step: 20110, loss: 1.192092558000013e-08\n",
      "step: 20111, loss: 2.38418573772492e-09\n",
      "step: 20112, loss: 5.96045985901128e-08\n",
      "step: 20113, loss: 4.52994939337259e-08\n",
      "step: 20114, loss: 1.862644793959589e-08\n",
      "step: 20115, loss: 0.0\n",
      "step: 20116, loss: 4.76837147544984e-09\n",
      "step: 20117, loss: 1.5973982669947873e-07\n",
      "step: 20118, loss: 2.38418573772492e-09\n",
      "step: 20119, loss: 0.06240369379520416\n",
      "step: 20120, loss: 2.551063573719148e-07\n",
      "step: 20121, loss: 2.622603467727913e-08\n",
      "step: 20122, loss: 5.722042217826129e-08\n",
      "step: 20123, loss: 9.53674206272126e-09\n",
      "step: 20124, loss: 2.3841845830929742e-08\n",
      "step: 20125, loss: 2.38418573772492e-09\n",
      "step: 20126, loss: 1.2015942729703966e-06\n",
      "step: 20127, loss: 2.622604000634965e-08\n",
      "step: 20128, loss: 4.2915317521874385e-08\n",
      "step: 20129, loss: 4.76837103136063e-09\n",
      "step: 20130, loss: 0.0\n",
      "step: 20131, loss: 1.907348234908568e-08\n",
      "step: 20132, loss: 4.7683684556432127e-08\n",
      "step: 20133, loss: 2.38418529363571e-08\n",
      "step: 20134, loss: 4.5299501039153256e-08\n",
      "step: 20135, loss: 2.0298282834119163e-05\n",
      "step: 20136, loss: 2.38418573772492e-09\n",
      "step: 20137, loss: 0.0\n",
      "step: 20138, loss: 2.145766764272139e-08\n",
      "step: 20139, loss: 0.0\n",
      "step: 20140, loss: 2.9802239964737964e-07\n",
      "step: 20141, loss: 7.15255676908555e-09\n",
      "step: 20142, loss: 4.76837103136063e-09\n",
      "step: 20143, loss: 1.6927653234688478e-07\n",
      "step: 20144, loss: 0.04835505038499832\n",
      "step: 20145, loss: 7.15255632499634e-09\n",
      "step: 20146, loss: 6.221403600648046e-05\n",
      "step: 20147, loss: 7.629384413121443e-08\n",
      "step: 20148, loss: 4.76837147544984e-09\n",
      "step: 20149, loss: 0.0\n",
      "step: 20150, loss: 2.38418529363571e-08\n",
      "step: 20151, loss: 1.4066316680327873e-06\n",
      "step: 20152, loss: 5.483623155555506e-08\n",
      "step: 20153, loss: 1.907347702001516e-08\n",
      "step: 20154, loss: 5.0067875179138355e-08\n",
      "step: 20155, loss: 1.192092824453539e-08\n",
      "step: 20156, loss: 5.9604552404834976e-08\n",
      "step: 20157, loss: 4.76837103136063e-09\n",
      "step: 20158, loss: 2.907100315496791e-05\n",
      "step: 20159, loss: 0.0\n",
      "step: 20160, loss: 1.3351414906992431e-07\n",
      "step: 20161, loss: 2.38418573772492e-09\n",
      "step: 20162, loss: 1.907347702001516e-08\n",
      "step: 20163, loss: 0.0008040304528549314\n",
      "step: 20164, loss: 4.76837147544984e-09\n",
      "step: 20165, loss: 3.576278118089249e-08\n",
      "step: 20166, loss: 2.861022352362852e-08\n",
      "step: 20167, loss: 1.907348234908568e-08\n",
      "step: 20168, loss: 1.3077761650492903e-05\n",
      "step: 20169, loss: 2.479540341937536e-07\n",
      "step: 20170, loss: 2.861021997091484e-08\n",
      "step: 20171, loss: 2.38418529363571e-08\n",
      "step: 20172, loss: 2.145766941907823e-08\n",
      "step: 20173, loss: 4.76837147544984e-09\n",
      "step: 20174, loss: 9.298310743588445e-08\n",
      "step: 20175, loss: 7.15255632499634e-09\n",
      "step: 20176, loss: 0.0\n",
      "step: 20177, loss: 6.19887430275412e-08\n",
      "step: 20178, loss: 1.475758494962065e-06\n",
      "step: 20179, loss: 3.0994389277338996e-08\n",
      "step: 20180, loss: 6.914134331736932e-08\n",
      "step: 20181, loss: 1.5735572844732815e-07\n",
      "step: 20182, loss: 1.192092824453539e-08\n",
      "step: 20183, loss: 4.053114111002287e-08\n",
      "step: 20184, loss: 1.192089627011228e-07\n",
      "step: 20185, loss: 9.059895234031501e-08\n",
      "step: 20186, loss: 4.76837147544984e-09\n",
      "step: 20187, loss: 1.430511264999268e-08\n",
      "step: 20188, loss: 0.040085311979055405\n",
      "step: 20189, loss: 1.0013571483113992e-07\n",
      "step: 20190, loss: 1.502031352629274e-07\n",
      "step: 20191, loss: 7.15255676908555e-09\n",
      "step: 20192, loss: 2.38418573772492e-09\n",
      "step: 20193, loss: 1.9073478796372e-08\n",
      "step: 20194, loss: 5.006786096828364e-08\n",
      "step: 20195, loss: 2.38418573772492e-09\n",
      "step: 20196, loss: 7.15255676908555e-09\n",
      "step: 20197, loss: 2.813329729178804e-07\n",
      "step: 20198, loss: 0.0008615179103799164\n",
      "step: 20199, loss: 4.5299501039153256e-08\n",
      "step: 20200, loss: 0.0\n",
      "step: 20201, loss: 5.03057265177631e-07\n",
      "step: 20202, loss: 1.0013563667143899e-07\n",
      "step: 20203, loss: 9.298302927618352e-08\n",
      "step: 20204, loss: 7.15255632499634e-09\n",
      "step: 20205, loss: 7.295485033864679e-07\n",
      "step: 20206, loss: 1.764291965855591e-07\n",
      "step: 20207, loss: 1.192092646817855e-08\n",
      "step: 20208, loss: 5.483623510826874e-08\n",
      "step: 20209, loss: 2.3841845830929742e-08\n",
      "step: 20210, loss: 1.7404532570708398e-07\n",
      "step: 20211, loss: 0.05276835337281227\n",
      "step: 20212, loss: 3.79083502366484e-07\n",
      "step: 20213, loss: 1.859662148717689e-07\n",
      "step: 20214, loss: 1.907348234908568e-08\n",
      "step: 20215, loss: 1.0490406765484295e-07\n",
      "step: 20216, loss: 6.675715980009045e-08\n",
      "step: 20217, loss: 2.384184938364342e-08\n",
      "step: 20218, loss: 4.76837103136063e-09\n",
      "step: 20219, loss: 2.8610218194558e-08\n",
      "step: 20220, loss: 2.38418529363571e-08\n",
      "step: 20221, loss: 2.38418573772492e-09\n",
      "step: 20222, loss: 2.861022352362852e-08\n",
      "step: 20223, loss: 1.192092824453539e-08\n",
      "step: 20224, loss: 1.192092824453539e-08\n",
      "step: 20225, loss: 0.00023379029880743474\n",
      "step: 20226, loss: 2.622604000634965e-08\n",
      "step: 20227, loss: 0.10894861072301865\n",
      "step: 20228, loss: 8.344643021018783e-08\n",
      "step: 20229, loss: 0.06555257737636566\n",
      "step: 20230, loss: 4.625282485903881e-07\n",
      "step: 20231, loss: 0.09504205733537674\n",
      "step: 20232, loss: 2.026554142275927e-07\n",
      "step: 20233, loss: 2.002706480652705e-07\n",
      "step: 20234, loss: 3.814696469817136e-08\n",
      "step: 20235, loss: 6.67571171675263e-08\n",
      "step: 20236, loss: 0.09525959193706512\n",
      "step: 20237, loss: 2.6702775812736945e-07\n",
      "step: 20238, loss: 3.7193183288763976e-07\n",
      "step: 20239, loss: 9.298317138473067e-08\n",
      "step: 20240, loss: 1.3351397853966773e-07\n",
      "step: 20241, loss: 3.170941909047542e-07\n",
      "step: 20242, loss: 9.536730516401803e-08\n",
      "step: 20243, loss: 2.145766764272139e-08\n",
      "step: 20244, loss: 2.38418573772492e-09\n",
      "step: 20245, loss: 4.744473471873789e-07\n",
      "step: 20246, loss: 4.434561446942098e-07\n",
      "step: 20247, loss: 2.38418573772492e-09\n",
      "step: 20248, loss: 5.364353796721844e-07\n",
      "step: 20249, loss: 4.76837103136063e-09\n",
      "step: 20250, loss: 7.15255632499634e-09\n",
      "step: 20251, loss: 3.33785834527589e-08\n",
      "step: 20252, loss: 1.907348234908568e-08\n",
      "step: 20253, loss: 9.53674117454284e-09\n",
      "step: 20254, loss: 1.2159310358583753e-07\n",
      "step: 20255, loss: 1.859585950114706e-06\n",
      "step: 20256, loss: 3.29015335864824e-07\n",
      "step: 20257, loss: 0.0\n",
      "step: 20258, loss: 2.1457660537294032e-08\n",
      "step: 20259, loss: 0.0\n",
      "step: 20260, loss: 4.6729729774597217e-07\n",
      "step: 20261, loss: 3.4093653766831267e-07\n",
      "step: 20262, loss: 9.53674117454284e-09\n",
      "step: 20263, loss: 0.06166040897369385\n",
      "step: 20264, loss: 5.483623155555506e-08\n",
      "step: 20265, loss: 1.8835035575648362e-07\n",
      "step: 20266, loss: 2.38418573772492e-09\n",
      "step: 20267, loss: 5.364349249248335e-07\n",
      "step: 20268, loss: 1.0537837624724489e-06\n",
      "step: 20269, loss: 0.13147631287574768\n",
      "step: 20270, loss: 1.192092824453539e-08\n",
      "step: 20271, loss: 1.966862100744038e-06\n",
      "step: 20272, loss: 6.198879276553271e-08\n",
      "step: 20273, loss: 0.018397435545921326\n",
      "step: 20274, loss: 2.8989604743401287e-06\n",
      "step: 20275, loss: 2.00023578145192e-06\n",
      "step: 20276, loss: 1.668929883180681e-08\n",
      "step: 20277, loss: 3.218636379642703e-07\n",
      "step: 20278, loss: 1.5258757457559113e-07\n",
      "step: 20279, loss: 7.390967482479027e-08\n",
      "step: 20280, loss: 1.668929527909313e-08\n",
      "step: 20281, loss: 2.694118279578106e-07\n",
      "step: 20282, loss: 5.9604612800967516e-08\n",
      "step: 20283, loss: 2.2888123396569426e-07\n",
      "step: 20284, loss: 1.192092646817855e-08\n",
      "step: 20285, loss: 5.006783965200157e-08\n",
      "step: 20286, loss: 3.576277407546513e-08\n",
      "step: 20287, loss: 9.53674295089968e-09\n",
      "step: 20288, loss: 0.06782633811235428\n",
      "step: 20289, loss: 5.716470241168281e-06\n",
      "step: 20290, loss: 3.576277762817881e-08\n",
      "step: 20291, loss: 4.52994939337259e-08\n",
      "step: 20292, loss: 1.1444068803712071e-07\n",
      "step: 20293, loss: 3.6239347878108674e-07\n",
      "step: 20294, loss: 3.3378576347331546e-08\n",
      "step: 20295, loss: 1.192092646817855e-08\n",
      "step: 20296, loss: 2.3841844054572903e-08\n",
      "step: 20297, loss: 9.25051665490173e-07\n",
      "step: 20298, loss: 4.053112334645448e-08\n",
      "step: 20299, loss: 3.8385064726753626e-07\n",
      "step: 20300, loss: 3.8146957592744e-08\n",
      "step: 20301, loss: 2.9325397576940304e-07\n",
      "step: 20302, loss: 1.132456304731022e-06\n",
      "step: 20303, loss: 0.2028636485338211\n",
      "step: 20304, loss: 3.3378579900045224e-08\n",
      "step: 20305, loss: 6.866337685096369e-07\n",
      "step: 20306, loss: 0.18150436878204346\n",
      "step: 20307, loss: 1.0251986992670936e-07\n",
      "step: 20308, loss: 6.127328333604964e-07\n",
      "step: 20309, loss: 3.719312360317417e-07\n",
      "step: 20310, loss: 2.95637704539331e-07\n",
      "step: 20311, loss: 4.76837147544984e-09\n",
      "step: 20312, loss: 2.38418573772492e-09\n",
      "step: 20313, loss: 1.907347702001516e-08\n",
      "step: 20314, loss: 1.0967237074055447e-07\n",
      "step: 20315, loss: 1.192092646817855e-08\n",
      "step: 20316, loss: 0.0\n",
      "step: 20317, loss: 6.914135042279668e-08\n",
      "step: 20318, loss: 9.53674295089968e-09\n",
      "step: 20319, loss: 9.53674295089968e-09\n",
      "step: 20320, loss: 2.145766764272139e-08\n",
      "step: 20321, loss: 7.367029866145458e-07\n",
      "step: 20322, loss: 3.0994252142590994e-07\n",
      "step: 20323, loss: 1.8835031312391948e-07\n",
      "step: 20324, loss: 6.437296917738422e-08\n",
      "step: 20325, loss: 2.145766586636455e-08\n",
      "step: 20326, loss: 3.313992351650086e-07\n",
      "step: 20327, loss: 7.15255676908555e-09\n",
      "step: 20328, loss: 4.196126326405647e-07\n",
      "step: 20329, loss: 1.6689244830558891e-07\n",
      "step: 20330, loss: 7.390971035192706e-08\n",
      "step: 20331, loss: 4.76837147544984e-09\n",
      "step: 20332, loss: 0.0\n",
      "step: 20333, loss: 2.6941131636704085e-07\n",
      "step: 20334, loss: 9.059886707518672e-08\n",
      "step: 20335, loss: 1.192092558000013e-08\n",
      "step: 20336, loss: 7.86780773864848e-08\n",
      "step: 20337, loss: 8.106215432235331e-08\n",
      "step: 20338, loss: 2.0742378126215044e-07\n",
      "step: 20339, loss: 1.668929350273629e-08\n",
      "step: 20340, loss: 2.92433156801053e-07\n",
      "step: 20341, loss: 1.5329746929637622e-06\n",
      "step: 20342, loss: 0.43376004695892334\n",
      "step: 20343, loss: 7.15255632499634e-09\n",
      "step: 20344, loss: 1.5234446664180723e-06\n",
      "step: 20345, loss: 2.5033884298863995e-07\n",
      "step: 20346, loss: 9.226590691469028e-07\n",
      "step: 20347, loss: 0.00012440374121069908\n",
      "step: 20348, loss: 0.0\n",
      "step: 20349, loss: 0.0\n",
      "step: 20350, loss: 2.145761044403116e-07\n",
      "step: 20351, loss: 0.0\n",
      "step: 20352, loss: 3.33785834527589e-08\n",
      "step: 20353, loss: 5.2452058696417225e-08\n",
      "step: 20354, loss: 3.9100410731407464e-07\n",
      "step: 20355, loss: 0.0\n",
      "step: 20356, loss: 0.09903901070356369\n",
      "step: 20357, loss: 2.38418573772492e-09\n",
      "step: 20358, loss: 8.106221116577217e-08\n",
      "step: 20359, loss: 3.337857279461787e-08\n",
      "step: 20360, loss: 0.0\n",
      "step: 20361, loss: 1.430511176181426e-08\n",
      "step: 20362, loss: 1.192092824453539e-08\n",
      "step: 20363, loss: 2.5033818928932305e-07\n",
      "step: 20364, loss: 1.668929527909313e-08\n",
      "step: 20365, loss: 2.861012262656004e-07\n",
      "step: 20366, loss: 5.7220432836402324e-08\n",
      "step: 20367, loss: 0.0007804606575518847\n",
      "step: 20368, loss: 4.529949038101222e-08\n",
      "step: 20369, loss: 4.76837147544984e-09\n",
      "step: 20370, loss: 1.1515309097376303e-06\n",
      "step: 20371, loss: 1.668929350273629e-08\n",
      "step: 20372, loss: 2.38418573772492e-09\n",
      "step: 20373, loss: 4.76837147544984e-09\n",
      "step: 20374, loss: 5.7220404414692894e-08\n",
      "step: 20375, loss: 2.622604000634965e-08\n",
      "step: 20376, loss: 1.907348234908568e-08\n",
      "step: 20377, loss: 1.0084979749080958e-06\n",
      "step: 20378, loss: 0.03483400493860245\n",
      "step: 20379, loss: 2.38418573772492e-09\n",
      "step: 20380, loss: 1.5973992617546173e-07\n",
      "step: 20381, loss: 1.192092824453539e-08\n",
      "step: 20382, loss: 0.0005528883193619549\n",
      "step: 20383, loss: 1.668929705544997e-08\n",
      "step: 20384, loss: 4.76837103136063e-09\n",
      "step: 20385, loss: 1.192092646817855e-08\n",
      "step: 20386, loss: 3.337857279461787e-08\n",
      "step: 20387, loss: 4.76837147544984e-09\n",
      "step: 20388, loss: 1.0013558693344748e-07\n",
      "step: 20389, loss: 4.76837147544984e-09\n",
      "step: 20390, loss: 2.38418573772492e-09\n",
      "step: 20391, loss: 2.7417956971476087e-07\n",
      "step: 20392, loss: 2.8610216418201162e-08\n",
      "step: 20393, loss: 3.123259659787436e-07\n",
      "step: 20394, loss: 7.763061876175925e-05\n",
      "step: 20395, loss: 1.430511264999268e-08\n",
      "step: 20396, loss: 3.8146957592744e-08\n",
      "step: 20397, loss: 6.413379196601454e-07\n",
      "step: 20398, loss: 2.145766764272139e-08\n",
      "step: 20399, loss: 3.8146946934602965e-08\n",
      "step: 20400, loss: 4.76837147544984e-09\n",
      "step: 20401, loss: 3.0755785473957076e-07\n",
      "step: 20402, loss: 3.1232607966558135e-07\n",
      "step: 20403, loss: 1.6212399600590288e-07\n",
      "step: 20404, loss: 2.38418573772492e-09\n",
      "step: 20405, loss: 4.6028668293729424e-05\n",
      "step: 20406, loss: 1.0967225705371675e-07\n",
      "step: 20407, loss: 2.38418573772492e-09\n",
      "step: 20408, loss: 0.0768304094672203\n",
      "step: 20409, loss: 1.716610427138221e-07\n",
      "step: 20410, loss: 1.3351399275052245e-07\n",
      "step: 20411, loss: 5.125348252477124e-06\n",
      "step: 20412, loss: 4.76837103136063e-09\n",
      "step: 20413, loss: 1.907348234908568e-08\n",
      "step: 20414, loss: 0.00017284724162891507\n",
      "step: 20415, loss: 1.31129837654953e-07\n",
      "step: 20416, loss: 1.907348412544252e-08\n",
      "step: 20417, loss: 4.2199727090519445e-07\n",
      "step: 20418, loss: 0.0036407550796866417\n",
      "step: 20419, loss: 3.5762766970037774e-08\n",
      "step: 20420, loss: 4.76837147544984e-09\n",
      "step: 20421, loss: 1.192092558000013e-08\n",
      "step: 20422, loss: 1.3589820468951075e-07\n",
      "step: 20423, loss: 4.76837103136063e-09\n",
      "step: 20424, loss: 2.384185116000026e-08\n",
      "step: 20425, loss: 2.2172839919676335e-07\n",
      "step: 20426, loss: 4.529951169729429e-08\n",
      "step: 20427, loss: 0.0\n",
      "step: 20428, loss: 7.629388676377857e-08\n",
      "step: 20429, loss: 9.53674206272126e-09\n",
      "step: 20430, loss: 8.106216853320802e-08\n",
      "step: 20431, loss: 5.00678503101426e-08\n",
      "step: 20432, loss: 1.668923346187512e-07\n",
      "step: 20433, loss: 2.3006070932751754e-06\n",
      "step: 20434, loss: 5.7220432836402324e-08\n",
      "step: 20435, loss: 1.6497887145305867e-06\n",
      "step: 20436, loss: 4.76837147544984e-09\n",
      "step: 20437, loss: 1.4781929280616168e-07\n",
      "step: 20438, loss: 1.192092558000013e-08\n",
      "step: 20439, loss: 8.58305639894752e-08\n",
      "step: 20440, loss: 3.099440704090739e-08\n",
      "step: 20441, loss: 4.768369876728684e-08\n",
      "step: 20442, loss: 7.86780773864848e-08\n",
      "step: 20443, loss: 1.4066671383261564e-07\n",
      "step: 20444, loss: 2.2172804392539547e-07\n",
      "step: 20445, loss: 4.2915321074588064e-08\n",
      "step: 20446, loss: 6.009631306369556e-06\n",
      "step: 20447, loss: 1.668929883180681e-08\n",
      "step: 20448, loss: 4.529949748643958e-08\n",
      "step: 20449, loss: 3.3139920674329915e-07\n",
      "step: 20450, loss: 2.38418573772492e-09\n",
      "step: 20451, loss: 1.0323389005861827e-06\n",
      "step: 20452, loss: 2.861022529998536e-08\n",
      "step: 20453, loss: 9.298144618696824e-07\n",
      "step: 20454, loss: 2.1696033059015463e-07\n",
      "step: 20455, loss: 4.0769199927126465e-07\n",
      "step: 20456, loss: 9.842342478805222e-06\n",
      "step: 20457, loss: 1.668929883180681e-08\n",
      "step: 20458, loss: 3.3378579900045224e-08\n",
      "step: 20459, loss: 0.0\n",
      "step: 20460, loss: 5.006783965200157e-08\n",
      "step: 20461, loss: 2.38418573772492e-09\n",
      "step: 20462, loss: 1.668929350273629e-08\n",
      "step: 20463, loss: 2.38418573772492e-09\n",
      "step: 20464, loss: 6.170290726004168e-05\n",
      "step: 20465, loss: 1.4209363143891096e-06\n",
      "step: 20466, loss: 1.668929883180681e-08\n",
      "step: 20467, loss: 0.050536006689071655\n",
      "step: 20468, loss: 1.1253041520831175e-06\n",
      "step: 20469, loss: 0.0\n",
      "step: 20470, loss: 2.861021997091484e-08\n",
      "step: 20471, loss: 1.9073404189384746e-07\n",
      "step: 20472, loss: 4.7683688109145805e-08\n",
      "step: 20473, loss: 3.099439993548003e-08\n",
      "step: 20474, loss: 1.5782704849698348e-06\n",
      "step: 20475, loss: 2.861021997091484e-08\n",
      "step: 20476, loss: 1.1682182048389222e-06\n",
      "step: 20477, loss: 1.907348412544252e-08\n",
      "step: 20478, loss: 8.583051425148369e-08\n",
      "step: 20479, loss: 1.2636157009637827e-07\n",
      "step: 20480, loss: 1.1205654004697863e-07\n",
      "step: 20481, loss: 3.0994396382766354e-08\n",
      "step: 20482, loss: 3.099440704090739e-08\n",
      "step: 20483, loss: 7.080943191795086e-07\n",
      "step: 20484, loss: 3.504725896164018e-07\n",
      "step: 20485, loss: 0.13902445137500763\n",
      "step: 20486, loss: 7.15255676908555e-09\n",
      "step: 20487, loss: 9.53674117454284e-09\n",
      "step: 20488, loss: 1.5973982669947873e-07\n",
      "step: 20489, loss: 1.907347702001516e-08\n",
      "step: 20490, loss: 2.38418573772492e-09\n",
      "step: 20491, loss: 9.536724121517182e-08\n",
      "step: 20492, loss: 1.0490403212770616e-07\n",
      "step: 20493, loss: 2.1003577330702683e-06\n",
      "step: 20494, loss: 0.0\n",
      "step: 20495, loss: 7.390969614107235e-08\n",
      "step: 20496, loss: 8.82146906633352e-08\n",
      "step: 20497, loss: 2.384184938364342e-08\n",
      "step: 20498, loss: 3.123259375570342e-07\n",
      "step: 20499, loss: 6.318046530395804e-07\n",
      "step: 20500, loss: 3.3378576347331546e-08\n",
      "step: 20501, loss: 0.00013522364315576851\n",
      "step: 20502, loss: 1.192092558000013e-08\n",
      "step: 20503, loss: 4.768367034557741e-08\n",
      "step: 20504, loss: 4.2915321074588064e-08\n",
      "step: 20505, loss: 7.15255632499634e-09\n",
      "step: 20506, loss: 3.337859411089994e-08\n",
      "step: 20507, loss: 2.861022529998536e-08\n",
      "step: 20508, loss: 7.15255676908555e-09\n",
      "step: 20509, loss: 1.4305085471733037e-07\n",
      "step: 20510, loss: 1.192092558000013e-08\n",
      "step: 20511, loss: 0.006463056430220604\n",
      "step: 20512, loss: 2.6226025795494934e-08\n",
      "step: 20513, loss: 7.867798501592915e-08\n",
      "step: 20514, loss: 1.430511264999268e-08\n",
      "step: 20515, loss: 1.1682497813580994e-07\n",
      "step: 20516, loss: 2.622603290092229e-08\n",
      "step: 20517, loss: 4.649120910471538e-07\n",
      "step: 20518, loss: 0.0\n",
      "step: 20519, loss: 1.430510998545742e-08\n",
      "step: 20520, loss: 9.322029654867947e-07\n",
      "step: 20521, loss: 1.144405885611377e-07\n",
      "step: 20522, loss: 2.38418573772492e-09\n",
      "step: 20523, loss: 3.5762763417324095e-08\n",
      "step: 20524, loss: 0.03446793928742409\n",
      "step: 20525, loss: 2.145766764272139e-08\n",
      "step: 20526, loss: 2.503380471807759e-07\n",
      "step: 20527, loss: 1.0490391133544108e-07\n",
      "step: 20528, loss: 2.145766764272139e-08\n",
      "step: 20529, loss: 7.867807028105744e-08\n",
      "step: 20530, loss: 0.00011953802459174767\n",
      "step: 20531, loss: 0.030393194407224655\n",
      "step: 20532, loss: 1.7642948080265342e-07\n",
      "step: 20533, loss: 2.38418573772492e-09\n",
      "step: 20534, loss: 4.339200359027018e-07\n",
      "step: 20535, loss: 1.6715694073354825e-05\n",
      "step: 20536, loss: 2.9163749786675908e-05\n",
      "step: 20537, loss: 0.0008638965082354844\n",
      "step: 20538, loss: 3.9338868873528554e-07\n",
      "step: 20539, loss: 0.0645817220211029\n",
      "step: 20540, loss: 9.53674206272126e-09\n",
      "step: 20541, loss: 2.622603290092229e-08\n",
      "step: 20542, loss: 1.0561665249042562e-06\n",
      "step: 20543, loss: 4.506073025822843e-07\n",
      "step: 20544, loss: 5.673962732544169e-06\n",
      "step: 20545, loss: 6.437299049366629e-08\n",
      "step: 20546, loss: 2.622603467727913e-08\n",
      "step: 20547, loss: 2.38418573772492e-09\n",
      "step: 20548, loss: 3.099439993548003e-08\n",
      "step: 20549, loss: 7.504008863179479e-06\n",
      "step: 20550, loss: 2.38418573772492e-09\n",
      "step: 20551, loss: 3.337859055818626e-08\n",
      "step: 20552, loss: 3.8146957592744e-08\n",
      "step: 20553, loss: 7.15255632499634e-09\n",
      "step: 20554, loss: 3.0279073826022795e-07\n",
      "step: 20555, loss: 1.811399488360621e-05\n",
      "step: 20556, loss: 2.38418573772492e-09\n",
      "step: 20557, loss: 9.107415053222212e-07\n",
      "step: 20558, loss: 5.483620668655931e-08\n",
      "step: 20559, loss: 7.00938926456729e-07\n",
      "step: 20560, loss: 5.292841365189815e-07\n",
      "step: 20561, loss: 0.15188966691493988\n",
      "step: 20562, loss: 5.102099862597242e-07\n",
      "step: 20563, loss: 4.5299508144580614e-08\n",
      "step: 20564, loss: 0.00019377790158614516\n",
      "step: 20565, loss: 0.12971749901771545\n",
      "step: 20566, loss: 0.0\n",
      "step: 20567, loss: 9.059899497287915e-08\n",
      "step: 20568, loss: 5.722038309841082e-08\n",
      "step: 20569, loss: 0.000309706520056352\n",
      "step: 20570, loss: 4.76837147544984e-09\n",
      "step: 20571, loss: 0.0\n",
      "step: 20572, loss: 1.668929350273629e-08\n",
      "step: 20573, loss: 2.8610218194558e-08\n",
      "step: 20574, loss: 9.53674117454284e-09\n",
      "step: 20575, loss: 7.390963219222613e-08\n",
      "step: 20576, loss: 7.15255676908555e-09\n",
      "step: 20577, loss: 1.430508973498945e-07\n",
      "step: 20578, loss: 4.76837147544984e-09\n",
      "step: 20579, loss: 3.3757332857931033e-06\n",
      "step: 20580, loss: 9.53674206272126e-09\n",
      "step: 20581, loss: 2.6226025795494934e-08\n",
      "step: 20582, loss: 4.053114821545023e-08\n",
      "step: 20583, loss: 3.33785834527589e-08\n",
      "step: 20584, loss: 4.887538125331048e-07\n",
      "step: 20585, loss: 2.0980787951430102e-07\n",
      "step: 20586, loss: 7.15255676908555e-09\n",
      "step: 20587, loss: 3.29014937960892e-07\n",
      "step: 20588, loss: 0.0\n",
      "step: 20589, loss: 0.02290167659521103\n",
      "step: 20590, loss: 1.1682492839781844e-07\n",
      "step: 20591, loss: 3.6000901104671357e-07\n",
      "step: 20592, loss: 9.53674206272126e-09\n",
      "step: 20593, loss: 0.0\n",
      "step: 20594, loss: 2.38418573772492e-09\n",
      "step: 20595, loss: 1.430511264999268e-08\n",
      "step: 20596, loss: 1.192092646817855e-08\n",
      "step: 20597, loss: 8.344637336676897e-08\n",
      "step: 20598, loss: 1.6307438954754616e-06\n",
      "step: 20599, loss: 1.1664884368656203e-05\n",
      "step: 20600, loss: 4.053112334645448e-08\n",
      "step: 20601, loss: 1.430511176181426e-08\n",
      "step: 20602, loss: 1.192092824453539e-08\n",
      "step: 20603, loss: 2.86101197843891e-07\n",
      "step: 20604, loss: 3.361685116942681e-07\n",
      "step: 20605, loss: 2.813319497363409e-07\n",
      "step: 20606, loss: 5.245205159098987e-08\n",
      "step: 20607, loss: 1.668929527909313e-08\n",
      "step: 20608, loss: 2.384185116000026e-08\n",
      "step: 20609, loss: 3.385519846688112e-07\n",
      "step: 20610, loss: 0.12637601792812347\n",
      "step: 20611, loss: 1.192092646817855e-08\n",
      "step: 20612, loss: 1.192092646817855e-08\n",
      "step: 20613, loss: 2.38418573772492e-09\n",
      "step: 20614, loss: 2.38418573772492e-09\n",
      "step: 20615, loss: 0.0\n",
      "step: 20616, loss: 0.0\n",
      "step: 20617, loss: 2.3841845830929742e-08\n",
      "step: 20618, loss: 1.2159311779669224e-07\n",
      "step: 20619, loss: 4.76837147544984e-09\n",
      "step: 20620, loss: 9.53674295089968e-09\n",
      "step: 20621, loss: 6.914130779023253e-08\n",
      "step: 20622, loss: 7.15255676908555e-09\n",
      "step: 20623, loss: 4.529949038101222e-08\n",
      "step: 20624, loss: 9.536737621829161e-08\n",
      "step: 20625, loss: 2.38418529363571e-08\n",
      "step: 20626, loss: 2.0265530054075498e-07\n",
      "step: 20627, loss: 9.53674295089968e-09\n",
      "step: 20628, loss: 1.593433626112528e-05\n",
      "step: 20629, loss: 1.3828233136337076e-07\n",
      "step: 20630, loss: 9.53674206272126e-09\n",
      "step: 20631, loss: 7.15255676908555e-09\n",
      "step: 20632, loss: 7.15255676908555e-09\n",
      "step: 20633, loss: 1.8596625750433304e-07\n",
      "step: 20634, loss: 2.145766764272139e-08\n",
      "step: 20635, loss: 1.7166080112929194e-07\n",
      "step: 20636, loss: 3.40937219789339e-07\n",
      "step: 20637, loss: 1.907348412544252e-08\n",
      "step: 20638, loss: 0.0018356654327362776\n",
      "step: 20639, loss: 1.5258756036473642e-07\n",
      "step: 20640, loss: 6.198879276553271e-08\n",
      "step: 20641, loss: 3.0994396382766354e-08\n",
      "step: 20642, loss: 1.430511264999268e-08\n",
      "step: 20643, loss: 4.76837147544984e-09\n",
      "step: 20644, loss: 5.245204093284883e-08\n",
      "step: 20645, loss: 1.192092558000013e-08\n",
      "step: 20646, loss: 2.3873029931564815e-05\n",
      "step: 20647, loss: 2.38418573772492e-09\n",
      "step: 20648, loss: 1.668929883180681e-08\n",
      "step: 20649, loss: 1.668929527909313e-08\n",
      "step: 20650, loss: 2.38418573772492e-09\n",
      "step: 20651, loss: 2.145766764272139e-08\n",
      "step: 20652, loss: 2.8371619009703863e-07\n",
      "step: 20653, loss: 1.5258740404533455e-07\n",
      "step: 20654, loss: 2.408021373412339e-07\n",
      "step: 20655, loss: 2.861022352362852e-08\n",
      "step: 20656, loss: 0.00044253224041312933\n",
      "step: 20657, loss: 1.192092646817855e-08\n",
      "step: 20658, loss: 0.0\n",
      "step: 20659, loss: 1.430511264999268e-08\n",
      "step: 20660, loss: 0.0755983367562294\n",
      "step: 20661, loss: 9.059891681317822e-08\n",
      "step: 20662, loss: 6.365677336361841e-07\n",
      "step: 20663, loss: 1.692768307748338e-07\n",
      "step: 20664, loss: 0.0\n",
      "step: 20665, loss: 0.035828087478876114\n",
      "step: 20666, loss: 4.76837147544984e-09\n",
      "step: 20667, loss: 4.053114466273655e-08\n",
      "step: 20668, loss: 1.5973984091033344e-07\n",
      "step: 20669, loss: 2.38418529363571e-08\n",
      "step: 20670, loss: 7.65309607686504e-07\n",
      "step: 20671, loss: 4.76837147544984e-09\n",
      "step: 20672, loss: 7.867807028105744e-08\n",
      "step: 20673, loss: 4.839841949433321e-07\n",
      "step: 20674, loss: 9.536728384773596e-08\n",
      "step: 20675, loss: 1.4781898016735795e-07\n",
      "step: 20676, loss: 5.00678503101426e-08\n",
      "step: 20677, loss: 1.6689259041413607e-07\n",
      "step: 20678, loss: 2.38418573772492e-09\n",
      "step: 20679, loss: 4.053114111002287e-08\n",
      "step: 20680, loss: 9.53674206272126e-09\n",
      "step: 20681, loss: 2.38418573772492e-09\n",
      "step: 20682, loss: 0.057686179876327515\n",
      "step: 20683, loss: 0.1459880918264389\n",
      "step: 20684, loss: 4.053114111002287e-08\n",
      "step: 20685, loss: 2.145766231365087e-08\n",
      "step: 20686, loss: 0.0\n",
      "step: 20687, loss: 5.245204093284883e-08\n",
      "step: 20688, loss: 7.15255676908555e-09\n",
      "step: 20689, loss: 2.38418573772492e-09\n",
      "step: 20690, loss: 0.06688925623893738\n",
      "step: 20691, loss: 5.245203738013515e-08\n",
      "step: 20692, loss: 9.53674206272126e-09\n",
      "step: 20693, loss: 3.8146957592744e-08\n",
      "step: 20694, loss: 2.6702738864514686e-07\n",
      "step: 20695, loss: 0.03438493609428406\n",
      "step: 20696, loss: 6.437298338823894e-08\n",
      "step: 20697, loss: 7.15255676908555e-09\n",
      "step: 20698, loss: 3.5762766970037774e-08\n",
      "step: 20699, loss: 4.529952235543533e-08\n",
      "step: 20700, loss: 9.298319270101274e-08\n",
      "step: 20701, loss: 2.5033799033735704e-07\n",
      "step: 20702, loss: 0.08724378794431686\n",
      "step: 20703, loss: 2.1457660537294032e-08\n",
      "step: 20704, loss: 4.529951525000797e-08\n",
      "step: 20705, loss: 0.0\n",
      "step: 20706, loss: 6.675717401094516e-08\n",
      "step: 20707, loss: 1.4376350918610115e-06\n",
      "step: 20708, loss: 1.907348412544252e-08\n",
      "step: 20709, loss: 6.198877855467799e-08\n",
      "step: 20710, loss: 7.15255676908555e-09\n",
      "step: 20711, loss: 4.768367034557741e-08\n",
      "step: 20712, loss: 2.38418573772492e-09\n",
      "step: 20713, loss: 2.2172855551616522e-07\n",
      "step: 20714, loss: 3.8146957592744e-08\n",
      "step: 20715, loss: 7.15255632499634e-09\n",
      "step: 20716, loss: 2.202869836764876e-06\n",
      "step: 20717, loss: 9.298303638161087e-08\n",
      "step: 20718, loss: 5.245205159098987e-08\n",
      "step: 20719, loss: 2.38418573772492e-09\n",
      "step: 20720, loss: 7.152552683464819e-08\n",
      "step: 20721, loss: 1.5735007536932244e-06\n",
      "step: 20722, loss: 1.668929883180681e-08\n",
      "step: 20723, loss: 3.475871835689759e-06\n",
      "step: 20724, loss: 0.11445438116788864\n",
      "step: 20725, loss: 8.583058530575727e-08\n",
      "step: 20726, loss: 7.15255676908555e-09\n",
      "step: 20727, loss: 4.76837103136063e-09\n",
      "step: 20728, loss: 1.0490410318197974e-07\n",
      "step: 20729, loss: 2.38418573772492e-09\n",
      "step: 20730, loss: 1.668929705544997e-08\n",
      "step: 20731, loss: 4.7444794404327695e-07\n",
      "step: 20732, loss: 4.76837147544984e-09\n",
      "step: 20733, loss: 7.080910222612147e-07\n",
      "step: 20734, loss: 1.907348234908568e-08\n",
      "step: 20735, loss: 5.4836245766409775e-08\n",
      "step: 20736, loss: 9.059895944574237e-08\n",
      "step: 20737, loss: 2.479545742062328e-07\n",
      "step: 20738, loss: 1.192092824453539e-08\n",
      "step: 20739, loss: 1.430511264999268e-08\n",
      "step: 20740, loss: 2.38418573772492e-09\n",
      "step: 20741, loss: 8.421557140536606e-06\n",
      "step: 20742, loss: 0.0\n",
      "step: 20743, loss: 4.053114111002287e-08\n",
      "step: 20744, loss: 0.0\n",
      "step: 20745, loss: 7.15255676908555e-09\n",
      "step: 20746, loss: 2.5272277071053395e-07\n",
      "step: 20747, loss: 3.0994389277338996e-08\n",
      "step: 20748, loss: 8.988205877358268e-07\n",
      "step: 20749, loss: 6.91412793685231e-08\n",
      "step: 20750, loss: 0.12661761045455933\n",
      "step: 20751, loss: 1.573559700318583e-07\n",
      "step: 20752, loss: 7.152553394007555e-08\n",
      "step: 20753, loss: 1.3589811942438246e-07\n",
      "step: 20754, loss: 7.867805607020273e-08\n",
      "step: 20755, loss: 1.0251973492358957e-07\n",
      "step: 20756, loss: 7.390966061393556e-08\n",
      "step: 20757, loss: 5.9604616353681195e-08\n",
      "step: 20758, loss: 1.1610752608248731e-06\n",
      "step: 20759, loss: 0.16470523178577423\n",
      "step: 20760, loss: 7.152550551836612e-08\n",
      "step: 20761, loss: 4.2915321074588064e-08\n",
      "step: 20762, loss: 5.078287586002261e-07\n",
      "step: 20763, loss: 3.640337581600761e-06\n",
      "step: 20764, loss: 1.907348234908568e-08\n",
      "step: 20765, loss: 1.668929883180681e-08\n",
      "step: 20766, loss: 5.316663873600191e-07\n",
      "step: 20767, loss: 9.298309322502973e-08\n",
      "step: 20768, loss: 1.6212433706641605e-07\n",
      "step: 20769, loss: 0.0\n",
      "step: 20770, loss: 9.53674206272126e-09\n",
      "step: 20771, loss: 1.2945712342116167e-06\n",
      "step: 20772, loss: 7.629380860407764e-08\n",
      "step: 20773, loss: 0.18385334312915802\n",
      "step: 20774, loss: 4.529951169729429e-08\n",
      "step: 20775, loss: 9.822605306908372e-07\n",
      "step: 20776, loss: 1.5497177230372472e-07\n",
      "step: 20777, loss: 6.675714558923573e-08\n",
      "step: 20778, loss: 1.192092558000013e-08\n",
      "step: 20779, loss: 3.576277407546513e-08\n",
      "step: 20780, loss: 2.145766764272139e-08\n",
      "step: 20781, loss: 8.106221116577217e-08\n",
      "step: 20782, loss: 1.192092824453539e-08\n",
      "step: 20783, loss: 2.479538352417876e-07\n",
      "step: 20784, loss: 2.38418573772492e-09\n",
      "step: 20785, loss: 0.3112720549106598\n",
      "step: 20786, loss: 9.53674117454284e-09\n",
      "step: 20787, loss: 1.0967230679170825e-07\n",
      "step: 20788, loss: 7.629382992035971e-08\n",
      "step: 20789, loss: 1.907348412544252e-08\n",
      "step: 20790, loss: 1.0013562956601163e-07\n",
      "step: 20791, loss: 1.692768307748338e-07\n",
      "step: 20792, loss: 3.539021875553772e-08\n",
      "step: 20793, loss: 6.67571669055178e-08\n",
      "step: 20794, loss: 3.337858700547258e-08\n",
      "step: 20795, loss: 6.198876434382328e-08\n",
      "step: 20796, loss: 1.0490393975715051e-07\n",
      "step: 20797, loss: 4.053114821545023e-08\n",
      "step: 20798, loss: 3.0994396382766354e-08\n",
      "step: 20799, loss: 1.192092824453539e-08\n",
      "step: 20800, loss: 5.602759642897581e-07\n",
      "step: 20801, loss: 2.861021997091484e-08\n",
      "step: 20802, loss: 2.145766764272139e-08\n",
      "step: 20803, loss: 0.0\n",
      "step: 20804, loss: 9.798783366932184e-07\n",
      "step: 20805, loss: 0.0\n",
      "step: 20806, loss: 1.3589823311122018e-07\n",
      "step: 20807, loss: 5.0067875179138355e-08\n",
      "step: 20808, loss: 7.152474950089527e-07\n",
      "step: 20809, loss: 2.431860650631279e-07\n",
      "step: 20810, loss: 0.0\n",
      "step: 20811, loss: 1.192092646817855e-08\n",
      "step: 20812, loss: 0.0\n",
      "step: 20813, loss: 1.0728824406669446e-07\n",
      "step: 20814, loss: 2.38418573772492e-09\n",
      "step: 20815, loss: 1.0967242047854597e-07\n",
      "step: 20816, loss: 4.482222379920131e-07\n",
      "step: 20817, loss: 5.55199994778377e-06\n",
      "step: 20818, loss: 3.337858700547258e-08\n",
      "step: 20819, loss: 2.38418573772492e-09\n",
      "step: 20820, loss: 0.0\n",
      "step: 20821, loss: 7.152546288580197e-08\n",
      "step: 20822, loss: 2.1934407357093733e-07\n",
      "step: 20823, loss: 8.106221827119953e-08\n",
      "step: 20824, loss: 1.668928177878115e-07\n",
      "step: 20825, loss: 1.7073018170776777e-05\n",
      "step: 20826, loss: 1.192092824453539e-08\n",
      "step: 20827, loss: 0.0\n",
      "step: 20828, loss: 5.722041862554761e-08\n",
      "step: 20829, loss: 7.15255632499634e-09\n",
      "step: 20830, loss: 5.776276793767465e-06\n",
      "step: 20831, loss: 6.675709585124423e-08\n",
      "step: 20832, loss: 4.053114111002287e-08\n",
      "step: 20833, loss: 3.5762766970037774e-08\n",
      "step: 20834, loss: 2.6225887950204196e-07\n",
      "step: 20835, loss: 5.483619602841827e-08\n",
      "step: 20836, loss: 5.769658741883177e-07\n",
      "step: 20837, loss: 7.629385123664179e-08\n",
      "step: 20838, loss: 7.15255632499634e-09\n",
      "step: 20839, loss: 8.821475461218142e-08\n",
      "step: 20840, loss: 7.15255632499634e-09\n",
      "step: 20841, loss: 7.986868695297744e-07\n",
      "step: 20842, loss: 0.07821719348430634\n",
      "step: 20843, loss: 0.016415104269981384\n",
      "step: 20844, loss: 9.53674117454284e-09\n",
      "step: 20845, loss: 2.38418573772492e-09\n",
      "step: 20846, loss: 0.0008484607096761465\n",
      "step: 20847, loss: 4.768367034557741e-08\n",
      "step: 20848, loss: 1.3756296084466157e-06\n",
      "step: 20849, loss: 2.5056358481378993e-06\n",
      "step: 20850, loss: 3.099439993548003e-08\n",
      "step: 20851, loss: 3.600218042265624e-05\n",
      "step: 20852, loss: 3.099440348819371e-08\n",
      "step: 20853, loss: 2.9325323680495785e-07\n",
      "step: 20854, loss: 5.84040299145272e-06\n",
      "step: 20855, loss: 3.3378579900045224e-08\n",
      "step: 20856, loss: 1.7404521202024625e-07\n",
      "step: 20857, loss: 2.38418573772492e-09\n",
      "step: 20858, loss: 3.6239396195014706e-07\n",
      "step: 20859, loss: 1.192092558000013e-08\n",
      "step: 20860, loss: 0.05884188041090965\n",
      "step: 20861, loss: 2.38418529363571e-08\n",
      "step: 20862, loss: 1.2874583887878543e-07\n",
      "step: 20863, loss: 9.53674295089968e-09\n",
      "step: 20864, loss: 1.907347702001516e-08\n",
      "step: 20865, loss: 0.0647650733590126\n",
      "step: 20866, loss: 2.384184938364342e-08\n",
      "step: 20867, loss: 0.06973841786384583\n",
      "step: 20868, loss: 4.053114111002287e-08\n",
      "step: 20869, loss: 2.8610209312773804e-08\n",
      "step: 20870, loss: 4.053114466273655e-08\n",
      "step: 20871, loss: 2.9013483526796335e-06\n",
      "step: 20872, loss: 5.245201961656676e-08\n",
      "step: 20873, loss: 4.76837147544984e-09\n",
      "step: 20874, loss: 5.245206935455826e-08\n",
      "step: 20875, loss: 6.437294075567479e-08\n",
      "step: 20876, loss: 1.4066650066979491e-07\n",
      "step: 20877, loss: 7.390963219222613e-08\n",
      "step: 20878, loss: 1.740451693876821e-07\n",
      "step: 20879, loss: 6.437296917738422e-08\n",
      "step: 20880, loss: 0.00016126057016663253\n",
      "step: 20881, loss: 1.668929705544997e-08\n",
      "step: 20882, loss: 1.1444075198596693e-07\n",
      "step: 20883, loss: 1.0490390423001372e-07\n",
      "step: 20884, loss: 4.76837103136063e-09\n",
      "step: 20885, loss: 3.528567731336807e-07\n",
      "step: 20886, loss: 1.6211987485803547e-06\n",
      "step: 20887, loss: 0.0\n",
      "step: 20888, loss: 4.4345443939164397e-07\n",
      "step: 20889, loss: 1.668929883180681e-08\n",
      "step: 20890, loss: 3.361679716817889e-07\n",
      "step: 20891, loss: 2.8610211089130644e-08\n",
      "step: 20892, loss: 0.202499657869339\n",
      "step: 20893, loss: 3.8146957592744e-08\n",
      "step: 20894, loss: 6.437291943939272e-08\n",
      "step: 20895, loss: 5.721965408156393e-07\n",
      "step: 20896, loss: 5.960460924825384e-08\n",
      "step: 20897, loss: 0.05530508980154991\n",
      "step: 20898, loss: 4.76837103136063e-09\n",
      "step: 20899, loss: 2.0265511579964368e-07\n",
      "step: 20900, loss: 1.9073478796372e-08\n",
      "step: 20901, loss: 1.723759123706259e-05\n",
      "step: 20902, loss: 4.768369876728684e-08\n",
      "step: 20903, loss: 3.6716264162350853e-07\n",
      "step: 20904, loss: 2.431857808460336e-07\n",
      "step: 20905, loss: 0.0\n",
      "step: 20906, loss: 2.38418529363571e-08\n",
      "step: 20907, loss: 0.09061713516712189\n",
      "step: 20908, loss: 2.9086987751725246e-07\n",
      "step: 20909, loss: 4.768368100371845e-08\n",
      "step: 20910, loss: 7.15255676908555e-09\n",
      "step: 20911, loss: 3.5762759864610416e-08\n",
      "step: 20912, loss: 1.81197705728664e-07\n",
      "step: 20913, loss: 0.23889793455600739\n",
      "step: 20914, loss: 7.867798501592915e-08\n",
      "step: 20915, loss: 1.0013568640943049e-07\n",
      "step: 20916, loss: 4.291530331101967e-08\n",
      "step: 20917, loss: 2.3841844054572903e-08\n",
      "step: 20918, loss: 2.622604000634965e-08\n",
      "step: 20919, loss: 1.192092558000013e-08\n",
      "step: 20920, loss: 0.05173860117793083\n",
      "step: 20921, loss: 0.18294650316238403\n",
      "step: 20922, loss: 2.38418573772492e-09\n",
      "step: 20923, loss: 3.93386869745882e-07\n",
      "step: 20924, loss: 2.145766764272139e-08\n",
      "step: 20925, loss: 0.050356995314359665\n",
      "step: 20926, loss: 2.145766586636455e-08\n",
      "step: 20927, loss: 2.861022352362852e-08\n",
      "step: 20928, loss: 5.912694405196817e-07\n",
      "step: 20929, loss: 2.8610218194558e-08\n",
      "step: 20930, loss: 2.1457660537294032e-08\n",
      "step: 20931, loss: 9.53674295089968e-09\n",
      "step: 20932, loss: 9.53674295089968e-09\n",
      "step: 20933, loss: 0.0\n",
      "step: 20934, loss: 0.0\n",
      "step: 20935, loss: 1.43051135381711e-08\n",
      "step: 20936, loss: 1.0490403212770616e-07\n",
      "step: 20937, loss: 2.193342652390129e-06\n",
      "step: 20938, loss: 9.53674117454284e-09\n",
      "step: 20939, loss: 1.192092646817855e-08\n",
      "step: 20940, loss: 5.245206935455826e-08\n",
      "step: 20941, loss: 2.6226025795494934e-08\n",
      "step: 20942, loss: 1.4376134913618444e-06\n",
      "step: 20943, loss: 8.344479169863916e-07\n",
      "step: 20944, loss: 1.668929883180681e-08\n",
      "step: 20945, loss: 5.960460924825384e-08\n",
      "step: 20946, loss: 1.0490396817885994e-07\n",
      "step: 20947, loss: 1.668929527909313e-08\n",
      "step: 20948, loss: 3.5762759864610416e-08\n",
      "step: 20949, loss: 2.0265480316083995e-07\n",
      "step: 20950, loss: 9.53674117454284e-09\n",
      "step: 20951, loss: 4.76837103136063e-09\n",
      "step: 20952, loss: 0.0\n",
      "step: 20953, loss: 3.576275631189674e-08\n",
      "step: 20954, loss: 1.4305068418707378e-07\n",
      "step: 20955, loss: 4.76837103136063e-09\n",
      "step: 20956, loss: 8.344646573732462e-08\n",
      "step: 20957, loss: 6.198875013296856e-08\n",
      "step: 20958, loss: 2.622603467727913e-08\n",
      "step: 20959, loss: 9.298307901417502e-08\n",
      "step: 20960, loss: 5.245205159098987e-08\n",
      "step: 20961, loss: 3.576277407546513e-08\n",
      "step: 20962, loss: 8.344635915591425e-08\n",
      "step: 20963, loss: 1.430511176181426e-08\n",
      "step: 20964, loss: 4.76837147544984e-09\n",
      "step: 20965, loss: 3.9100447679629724e-07\n",
      "step: 20966, loss: 2.861021997091484e-08\n",
      "step: 20967, loss: 1.7881323799429083e-07\n",
      "step: 20968, loss: 3.3685771541058784e-06\n",
      "step: 20969, loss: 2.0027090386065538e-07\n",
      "step: 20970, loss: 2.861022352362852e-08\n",
      "step: 20971, loss: 4.76837147544984e-09\n",
      "step: 20972, loss: 4.053112689916816e-08\n",
      "step: 20973, loss: 2.145766941907823e-08\n",
      "step: 20974, loss: 4.768367389829109e-08\n",
      "step: 20975, loss: 2.38418573772492e-09\n",
      "step: 20976, loss: 1.668929705544997e-08\n",
      "step: 20977, loss: 0.0\n",
      "step: 20978, loss: 2.38418573772492e-09\n",
      "step: 20979, loss: 3.33785834527589e-08\n",
      "step: 20980, loss: 3.2663237448105065e-07\n",
      "step: 20981, loss: 0.21121257543563843\n",
      "step: 20982, loss: 4.83984251786751e-07\n",
      "step: 20983, loss: 1.668929527909313e-08\n",
      "step: 20984, loss: 3.7908409922238206e-07\n",
      "step: 20985, loss: 1.192092558000013e-08\n",
      "step: 20986, loss: 3.337859055818626e-08\n",
      "step: 20987, loss: 5.245205159098987e-08\n",
      "step: 20988, loss: 2.38418573772492e-09\n",
      "step: 20989, loss: 0.0\n",
      "step: 20990, loss: 7.152432317525381e-07\n",
      "step: 20991, loss: 1.0728554116212763e-06\n",
      "step: 20992, loss: 6.914129357937782e-08\n",
      "step: 20993, loss: 0.1049489751458168\n",
      "step: 20994, loss: 3.3378576347331546e-08\n",
      "step: 20995, loss: 2.3841844054572903e-08\n",
      "step: 20996, loss: 7.867808449191216e-08\n",
      "step: 20997, loss: 0.018359025940299034\n",
      "step: 20998, loss: 7.152544867494726e-08\n",
      "step: 20999, loss: 1.621240102167576e-07\n",
      "step: 21000, loss: 3.814696469817136e-08\n",
      "step: 21001, loss: 4.76837103136063e-09\n",
      "step: 21002, loss: 6.437296207195686e-08\n",
      "step: 21003, loss: 9.29830648033203e-08\n",
      "step: 21004, loss: 3.8146946934602965e-08\n",
      "step: 21005, loss: 1.883500431176799e-07\n",
      "step: 21006, loss: 7.152546288580197e-08\n",
      "step: 21007, loss: 1.668929705544997e-08\n",
      "step: 21008, loss: 1.668929705544997e-08\n",
      "step: 21009, loss: 1.0013561535515692e-07\n",
      "step: 21010, loss: 9.298230452259304e-07\n",
      "step: 21011, loss: 4.7683688109145805e-08\n",
      "step: 21012, loss: 0.0\n",
      "step: 21013, loss: 3.5762759864610416e-08\n",
      "step: 21014, loss: 1.1872912182298023e-06\n",
      "step: 21015, loss: 1.4662358807981946e-05\n",
      "step: 21016, loss: 1.668929883180681e-08\n",
      "step: 21017, loss: 2.145766231365087e-08\n",
      "step: 21018, loss: 5.587935003603661e-09\n",
      "step: 21019, loss: 4.935239417136472e-07\n",
      "step: 21020, loss: 1.0728822985583975e-07\n",
      "step: 21021, loss: 8.583059951661198e-08\n",
      "step: 21022, loss: 2.38418573772492e-09\n",
      "step: 21023, loss: 2.9563688030975754e-07\n",
      "step: 21024, loss: 1.430511264999268e-08\n",
      "step: 21025, loss: 6.675713137838102e-08\n",
      "step: 21026, loss: 2.1528053366637323e-06\n",
      "step: 21027, loss: 4.1961342844842875e-07\n",
      "step: 21028, loss: 8.344640889390575e-08\n",
      "step: 21029, loss: 0.0\n",
      "step: 21030, loss: 1.0728822985583975e-07\n",
      "step: 21031, loss: 4.76837103136063e-09\n",
      "step: 21032, loss: 8.344639468305104e-08\n",
      "step: 21033, loss: 1.430511264999268e-08\n",
      "step: 21034, loss: 5.316663873600191e-07\n",
      "step: 21035, loss: 7.15255676908555e-09\n",
      "step: 21036, loss: 2.384184938364342e-08\n",
      "step: 21037, loss: 2.38418573772492e-09\n",
      "step: 21038, loss: 6.675714558923573e-08\n",
      "step: 21039, loss: 5.269018856779439e-07\n",
      "step: 21040, loss: 1.668929705544997e-08\n",
      "step: 21041, loss: 1.1157744665979408e-06\n",
      "step: 21042, loss: 1.2636174062663486e-07\n",
      "step: 21043, loss: 0.031825292855501175\n",
      "step: 21044, loss: 1.6497887145305867e-06\n",
      "step: 21045, loss: 9.059894523488765e-08\n",
      "step: 21046, loss: 1.430511264999268e-08\n",
      "step: 21047, loss: 2.38418573772492e-09\n",
      "step: 21048, loss: 6.605705129913986e-05\n",
      "step: 21049, loss: 2.38418573772492e-09\n",
      "step: 21050, loss: 7.438543434545863e-07\n",
      "step: 21051, loss: 2.38418573772492e-09\n",
      "step: 21052, loss: 2.6319719381717732e-06\n",
      "step: 21053, loss: 4.7683688109145805e-08\n",
      "step: 21054, loss: 1.3112979502238886e-07\n",
      "step: 21055, loss: 3.290165579983295e-07\n",
      "step: 21056, loss: 1.668929527909313e-08\n",
      "step: 21057, loss: 1.430511176181426e-08\n",
      "step: 21058, loss: 8.344634494505954e-08\n",
      "step: 21059, loss: 4.76837103136063e-09\n",
      "step: 21060, loss: 1.907347702001516e-08\n",
      "step: 21061, loss: 0.006807721685618162\n",
      "step: 21062, loss: 1.270372922590468e-05\n",
      "step: 21063, loss: 1.1682494260867315e-07\n",
      "step: 21064, loss: 0.04411007836461067\n",
      "step: 21065, loss: 6.67571669055178e-08\n",
      "step: 21066, loss: 2.622604000634965e-08\n",
      "step: 21067, loss: 1.6927678814226965e-07\n",
      "step: 21068, loss: 1.2325868965490372e-06\n",
      "step: 21069, loss: 4.529948327558486e-08\n",
      "step: 21070, loss: 3.457056436673156e-07\n",
      "step: 21071, loss: 0.0539076030254364\n",
      "step: 21072, loss: 3.8623616660515836e-07\n",
      "step: 21073, loss: 2.8680008199444273e-06\n",
      "step: 21074, loss: 6.413392270587792e-07\n",
      "step: 21075, loss: 0.0\n",
      "step: 21076, loss: 2.145766231365087e-08\n",
      "step: 21077, loss: 1.430511264999268e-08\n",
      "step: 21078, loss: 7.15255676908555e-09\n",
      "step: 21079, loss: 1.668929883180681e-08\n",
      "step: 21080, loss: 4.76837103136063e-09\n",
      "step: 21081, loss: 2.6941205533148604e-07\n",
      "step: 21082, loss: 3.099441059362107e-08\n",
      "step: 21083, loss: 1.0490406765484295e-07\n",
      "step: 21084, loss: 4.744491945984919e-07\n",
      "step: 21085, loss: 7.629387255292386e-08\n",
      "step: 21086, loss: 2.38418573772492e-09\n",
      "step: 21087, loss: 2.861021997091484e-08\n",
      "step: 21088, loss: 4.2914950881822733e-07\n",
      "step: 21089, loss: 3.7175472243689e-05\n",
      "step: 21090, loss: 7.15255676908555e-09\n",
      "step: 21091, loss: 7.867807028105744e-08\n",
      "step: 21092, loss: 2.38418573772492e-09\n",
      "step: 21093, loss: 4.482234317038092e-07\n",
      "step: 21094, loss: 7.176336112024728e-07\n",
      "step: 21095, loss: 2.384185116000026e-08\n",
      "step: 21096, loss: 2.3841845830929742e-08\n",
      "step: 21097, loss: 3.337857279461787e-08\n",
      "step: 21098, loss: 7.15255676908555e-09\n",
      "step: 21099, loss: 1.6450816531232704e-07\n",
      "step: 21100, loss: 6.91412793685231e-08\n",
      "step: 21101, loss: 4.768366679286373e-08\n",
      "step: 21102, loss: 3.576277407546513e-08\n",
      "step: 21103, loss: 2.861022529998536e-08\n",
      "step: 21104, loss: 2.86101396795857e-07\n",
      "step: 21105, loss: 9.53674117454284e-09\n",
      "step: 21106, loss: 3.3378576347331546e-08\n",
      "step: 21107, loss: 5.006788228456571e-08\n",
      "step: 21108, loss: 2.1934420146862976e-07\n",
      "step: 21109, loss: 1.668929527909313e-08\n",
      "step: 21110, loss: 1.43051135381711e-08\n",
      "step: 21111, loss: 5.722038309841082e-08\n",
      "step: 21112, loss: 2.3364951573512371e-07\n",
      "step: 21113, loss: 0.0\n",
      "step: 21114, loss: 3.576277052275145e-08\n",
      "step: 21115, loss: 1.5735564318219986e-07\n",
      "step: 21116, loss: 8.344476327692973e-07\n",
      "step: 21117, loss: 0.0\n",
      "step: 21118, loss: 1.1444078751310371e-07\n",
      "step: 21119, loss: 1.5974005407315417e-07\n",
      "step: 21120, loss: 1.907348412544252e-08\n",
      "step: 21121, loss: 3.8146946934602965e-08\n",
      "step: 21122, loss: 2.38418573772492e-09\n",
      "step: 21123, loss: 9.53674206272126e-09\n",
      "step: 21124, loss: 1.907347702001516e-08\n",
      "step: 21125, loss: 0.017027905210852623\n",
      "step: 21126, loss: 3.0137240173644386e-05\n",
      "step: 21127, loss: 9.53674117454284e-09\n",
      "step: 21128, loss: 3.337858700547258e-08\n",
      "step: 21129, loss: 1.5974022460341075e-07\n",
      "step: 21130, loss: 3.1471012107431307e-07\n",
      "step: 21131, loss: 7.15255676908555e-09\n",
      "step: 21132, loss: 6.198877855467799e-08\n",
      "step: 21133, loss: 0.07131798565387726\n",
      "step: 21134, loss: 2.622603467727913e-08\n",
      "step: 21135, loss: 1.1444080882938579e-07\n",
      "step: 21136, loss: 2.1457613286202104e-07\n",
      "step: 21137, loss: 6.198878566010535e-08\n",
      "step: 21138, loss: 7.819976417522412e-07\n",
      "step: 21139, loss: 2.38418573772492e-09\n",
      "step: 21140, loss: 2.38418573772492e-09\n",
      "step: 21141, loss: 3.8146950487316644e-08\n",
      "step: 21142, loss: 3.337858700547258e-08\n",
      "step: 21143, loss: 1.43051135381711e-08\n",
      "step: 21144, loss: 2.38418573772492e-09\n",
      "step: 21145, loss: 4.0769191400613636e-07\n",
      "step: 21146, loss: 2.694111742584937e-07\n",
      "step: 21147, loss: 7.867807028105744e-08\n",
      "step: 21148, loss: 9.298307190874766e-08\n",
      "step: 21149, loss: 2.38418573772492e-09\n",
      "step: 21150, loss: 7.15255676908555e-09\n",
      "step: 21151, loss: 5.459717726807867e-07\n",
      "step: 21152, loss: 1.5353580238297582e-06\n",
      "step: 21153, loss: 4.76837103136063e-09\n",
      "step: 21154, loss: 7.867799212135651e-08\n",
      "step: 21155, loss: 6.914127226309574e-08\n",
      "step: 21156, loss: 0.0\n",
      "step: 21157, loss: 4.291530331101967e-08\n",
      "step: 21158, loss: 3.3139912147817085e-07\n",
      "step: 21159, loss: 0.0\n",
      "step: 21160, loss: 4.76837147544984e-09\n",
      "step: 21161, loss: 1.764293671158157e-07\n",
      "step: 21162, loss: 2.861007430965401e-07\n",
      "step: 21163, loss: 6.67571171675263e-08\n",
      "step: 21164, loss: 1.668929527909313e-08\n",
      "step: 21165, loss: 1.907348234908568e-08\n",
      "step: 21166, loss: 2.2649700781585125e-07\n",
      "step: 21167, loss: 7.15255676908555e-09\n",
      "step: 21168, loss: 7.295485033864679e-07\n",
      "step: 21169, loss: 7.152546288580197e-08\n",
      "step: 21170, loss: 4.76837103136063e-09\n",
      "step: 21171, loss: 3.7431519217534515e-07\n",
      "step: 21172, loss: 9.53674206272126e-09\n",
      "step: 21173, loss: 3.337858700547258e-08\n",
      "step: 21174, loss: 3.266321471073752e-07\n",
      "step: 21175, loss: 1.406665859349232e-07\n",
      "step: 21176, loss: 1.2159324569438468e-07\n",
      "step: 21177, loss: 3.123266765214794e-07\n",
      "step: 21178, loss: 2.6226025795494934e-08\n",
      "step: 21179, loss: 2.1457660537294032e-08\n",
      "step: 21180, loss: 4.76837103136063e-09\n",
      "step: 21181, loss: 0.08219906687736511\n",
      "step: 21182, loss: 1.668929527909313e-08\n",
      "step: 21183, loss: 6.675709585124423e-08\n",
      "step: 21184, loss: 2.2172869762471237e-07\n",
      "step: 21185, loss: 0.07731502503156662\n",
      "step: 21186, loss: 1.430511264999268e-08\n",
      "step: 21187, loss: 9.53674206272126e-09\n",
      "step: 21188, loss: 6.675679742329521e-07\n",
      "step: 21189, loss: 1.4781909385419567e-07\n",
      "step: 21190, loss: 1.7404508412255382e-07\n",
      "step: 21191, loss: 3.099439993548003e-08\n",
      "step: 21192, loss: 1.668929883180681e-08\n",
      "step: 21193, loss: 2.74179512871342e-07\n",
      "step: 21194, loss: 0.0\n",
      "step: 21195, loss: 3.337857279461787e-08\n",
      "step: 21196, loss: 2.38418529363571e-08\n",
      "step: 21197, loss: 2.09807481610369e-07\n",
      "step: 21198, loss: 0.0\n",
      "step: 21199, loss: 1.668929527909313e-08\n",
      "step: 21200, loss: 1.0833272426680196e-05\n",
      "step: 21201, loss: 4.76837103136063e-09\n",
      "step: 21202, loss: 5.245203738013515e-08\n",
      "step: 21203, loss: 0.08383016288280487\n",
      "step: 21204, loss: 2.5176639155688463e-06\n",
      "step: 21205, loss: 0.0\n",
      "step: 21206, loss: 5.626656047752476e-07\n",
      "step: 21207, loss: 8.344640889390575e-08\n",
      "step: 21208, loss: 9.53674206272126e-09\n",
      "step: 21209, loss: 1.192092646817855e-08\n",
      "step: 21210, loss: 8.821479013931821e-08\n",
      "step: 21211, loss: 7.629382992035971e-08\n",
      "step: 21212, loss: 2.3841845830929742e-08\n",
      "step: 21213, loss: 6.675715980009045e-08\n",
      "step: 21214, loss: 2.59875150732114e-07\n",
      "step: 21215, loss: 7.629380860407764e-08\n",
      "step: 21216, loss: 1.907348412544252e-08\n",
      "step: 21217, loss: 2.9563688030975754e-07\n",
      "step: 21218, loss: 3.6856415590591496e-06\n",
      "step: 21219, loss: 2.76563639545202e-07\n",
      "step: 21220, loss: 1.5758871541038388e-06\n",
      "step: 21221, loss: 1.0490403212770616e-07\n",
      "step: 21222, loss: 2.38418573772492e-09\n",
      "step: 21223, loss: 6.437296207195686e-08\n",
      "step: 21224, loss: 2.38418573772492e-09\n",
      "step: 21225, loss: 6.198877855467799e-08\n",
      "step: 21226, loss: 1.192092646817855e-08\n",
      "step: 21227, loss: 0.1469777226448059\n",
      "step: 21228, loss: 0.08252628147602081\n",
      "step: 21229, loss: 1.645083216317289e-07\n",
      "step: 21230, loss: 1.1444078751310371e-07\n",
      "step: 21231, loss: 0.0\n",
      "step: 21232, loss: 1.668929705544997e-08\n",
      "step: 21233, loss: 7.867800633221123e-08\n",
      "step: 21234, loss: 4.053114821545023e-08\n",
      "step: 21235, loss: 0.0\n",
      "step: 21236, loss: 0.02165006473660469\n",
      "step: 21237, loss: 0.01177382841706276\n",
      "step: 21238, loss: 1.358982473220749e-07\n",
      "step: 21239, loss: 0.0016451711999252439\n",
      "step: 21240, loss: 0.10097095370292664\n",
      "step: 21241, loss: 6.198877855467799e-08\n",
      "step: 21242, loss: 2.8610031677089864e-07\n",
      "step: 21243, loss: 0.04438021034002304\n",
      "step: 21244, loss: 0.004408444277942181\n",
      "step: 21245, loss: 0.005270275287330151\n",
      "step: 21246, loss: 4.24380772301447e-07\n",
      "step: 21247, loss: 1.430510998545742e-08\n",
      "step: 21248, loss: 5.960457727383073e-08\n",
      "step: 21249, loss: 0.0\n",
      "step: 21250, loss: 3.170946456521051e-07\n",
      "step: 21251, loss: 0.0\n",
      "step: 21252, loss: 4.291529975830599e-08\n",
      "step: 21253, loss: 1.192092558000013e-08\n",
      "step: 21254, loss: 1.192092558000013e-08\n",
      "step: 21255, loss: 1.0490409607655238e-07\n",
      "step: 21256, loss: 1.3351426275676204e-07\n",
      "step: 21257, loss: 1.43051135381711e-08\n",
      "step: 21258, loss: 1.907348234908568e-08\n",
      "step: 21259, loss: 0.304586797952652\n",
      "step: 21260, loss: 1.4543516613230167e-07\n",
      "step: 21261, loss: 5.793490345240571e-07\n",
      "step: 21262, loss: 1.1682478628927129e-07\n",
      "step: 21263, loss: 2.145766941907823e-08\n",
      "step: 21264, loss: 1.1920894138484073e-07\n",
      "step: 21265, loss: 3.766988925235637e-07\n",
      "step: 21266, loss: 2.145766586636455e-08\n",
      "step: 21267, loss: 0.0\n",
      "step: 21268, loss: 3.576277407546513e-08\n",
      "step: 21269, loss: 0.0\n",
      "step: 21270, loss: 1.2636174062663486e-07\n",
      "step: 21271, loss: 7.390968903564499e-08\n",
      "step: 21272, loss: 1.0115963959833607e-05\n",
      "step: 21273, loss: 0.0001137046201620251\n",
      "step: 21274, loss: 2.3841844054572903e-08\n",
      "step: 21275, loss: 0.0\n",
      "step: 21276, loss: 1.4996174968473497e-06\n",
      "step: 21277, loss: 1.192092824453539e-08\n",
      "step: 21278, loss: 1.7404499885742553e-07\n",
      "step: 21279, loss: 0.08963567763566971\n",
      "step: 21280, loss: 2.3126513326587883e-07\n",
      "step: 21281, loss: 3.409356565953203e-07\n",
      "step: 21282, loss: 5.865063030796591e-07\n",
      "step: 21283, loss: 8.583054267319312e-08\n",
      "step: 21284, loss: 8.583064214917613e-08\n",
      "step: 21285, loss: 2.38418529363571e-08\n",
      "step: 21286, loss: 2.38418573772492e-09\n",
      "step: 21287, loss: 2.861021997091484e-08\n",
      "step: 21288, loss: 1.1539128763615736e-06\n",
      "step: 21289, loss: 5.2452055143703546e-08\n",
      "step: 21290, loss: 1.955023236632769e-07\n",
      "step: 21291, loss: 1.9426446669967845e-05\n",
      "step: 21292, loss: 8.58296857586538e-07\n",
      "step: 21293, loss: 0.0\n",
      "step: 21294, loss: 1.5973995459717116e-07\n",
      "step: 21295, loss: 1.668929705544997e-08\n",
      "step: 21296, loss: 6.746109647792764e-06\n",
      "step: 21297, loss: 4.291529975830599e-08\n",
      "step: 21298, loss: 1.6927670287714136e-07\n",
      "step: 21299, loss: 2.2172852709445579e-07\n",
      "step: 21300, loss: 6.365719400491798e-07\n",
      "step: 21301, loss: 9.53674206272126e-09\n",
      "step: 21302, loss: 8.598528802394867e-05\n",
      "step: 21303, loss: 2.38418573772492e-09\n",
      "step: 21304, loss: 2.479547447364894e-07\n",
      "step: 21305, loss: 1.192092558000013e-08\n",
      "step: 21306, loss: 5.722037599298346e-08\n",
      "step: 21307, loss: 3.64777292816143e-07\n",
      "step: 21308, loss: 3.337858700547258e-08\n",
      "step: 21309, loss: 1.3142516763764434e-05\n",
      "step: 21310, loss: 1.907347702001516e-08\n",
      "step: 21311, loss: 2.9802231438225135e-07\n",
      "step: 21312, loss: 8.344643021018783e-08\n",
      "step: 21313, loss: 9.53674117454284e-09\n",
      "step: 21314, loss: 5.245204803827619e-08\n",
      "step: 21315, loss: 1.983551328521571e-06\n",
      "step: 21316, loss: 2.38418573772492e-09\n",
      "step: 21317, loss: 2.38418573772492e-09\n",
      "step: 21318, loss: 5.197472887630283e-07\n",
      "step: 21319, loss: 2.145766764272139e-08\n",
      "step: 21320, loss: 2.408022510280716e-07\n",
      "step: 21321, loss: 3.0755813895666506e-07\n",
      "step: 21322, loss: 4.76837147544984e-09\n",
      "step: 21323, loss: 3.1802635476196883e-06\n",
      "step: 21324, loss: 2.622604000634965e-08\n",
      "step: 21325, loss: 9.53674117454284e-09\n",
      "step: 21326, loss: 1.192092824453539e-08\n",
      "step: 21327, loss: 1.668929350273629e-08\n",
      "step: 21328, loss: 1.015646603264031e-06\n",
      "step: 21329, loss: 1.1062369367209612e-06\n",
      "step: 21330, loss: 1.430511264999268e-08\n",
      "step: 21331, loss: 8.821479013931821e-08\n",
      "step: 21332, loss: 1.2636157009637827e-07\n",
      "step: 21333, loss: 1.525874466778987e-07\n",
      "step: 21334, loss: 5.960457727383073e-08\n",
      "step: 21335, loss: 7.15255676908555e-09\n",
      "step: 21336, loss: 2.38418573772492e-09\n",
      "step: 21337, loss: 2.38418573772492e-09\n",
      "step: 21338, loss: 0.0\n",
      "step: 21339, loss: 0.09491316974163055\n",
      "step: 21340, loss: 8.559059097024146e-07\n",
      "step: 21341, loss: 0.0\n",
      "step: 21342, loss: 0.06496032327413559\n",
      "step: 21343, loss: 1.907348234908568e-08\n",
      "step: 21344, loss: 1.263615132529594e-07\n",
      "step: 21345, loss: 5.7220436389116e-08\n",
      "step: 21346, loss: 7.152547709665669e-08\n",
      "step: 21347, loss: 5.3950498113408685e-06\n",
      "step: 21348, loss: 1.2874575361365714e-07\n",
      "step: 21349, loss: 4.76837103136063e-09\n",
      "step: 21350, loss: 2.861022529998536e-08\n",
      "step: 21351, loss: 9.345878879685188e-07\n",
      "step: 21352, loss: 3.3378579900045224e-08\n",
      "step: 21353, loss: 6.67571669055178e-08\n",
      "step: 21354, loss: 0.05218467861413956\n",
      "step: 21355, loss: 2.932528957444447e-07\n",
      "step: 21356, loss: 2.622604000634965e-08\n",
      "step: 21357, loss: 8.797495638646069e-07\n",
      "step: 21358, loss: 4.76837147544984e-09\n",
      "step: 21359, loss: 4.672981503972551e-07\n",
      "step: 21360, loss: 5.245206580184458e-08\n",
      "step: 21361, loss: 0.0\n",
      "step: 21362, loss: 1.4137850712359068e-06\n",
      "step: 21363, loss: 1.406666712000515e-07\n",
      "step: 21364, loss: 1.8358187503508816e-07\n",
      "step: 21365, loss: 7.867804896477537e-08\n",
      "step: 21366, loss: 1.2445298125385307e-06\n",
      "step: 21367, loss: 0.03783924877643585\n",
      "step: 21368, loss: 9.53674206272126e-09\n",
      "step: 21369, loss: 1.9550257945866178e-07\n",
      "step: 21370, loss: 2.7273729301668936e-06\n",
      "step: 21371, loss: 0.0\n",
      "step: 21372, loss: 5.391958256950602e-05\n",
      "step: 21373, loss: 2.8610216418201162e-08\n",
      "step: 21374, loss: 0.15037833154201508\n",
      "step: 21375, loss: 1.4233359024728998e-06\n",
      "step: 21376, loss: 3.0994389277338996e-08\n",
      "step: 21377, loss: 7.867808449191216e-08\n",
      "step: 21378, loss: 6.198875723839592e-08\n",
      "step: 21379, loss: 1.1455241292424034e-05\n",
      "step: 21380, loss: 2.6226029348208613e-08\n",
      "step: 21381, loss: 1.8548519165051403e-06\n",
      "step: 21382, loss: 0.28258273005485535\n",
      "step: 21383, loss: 2.658237463037949e-06\n",
      "step: 21384, loss: 1.907348234908568e-08\n",
      "step: 21385, loss: 6.071769348636735e-06\n",
      "step: 21386, loss: 1.430507978739115e-07\n",
      "step: 21387, loss: 1.907348234908568e-08\n",
      "step: 21388, loss: 0.1205584704875946\n",
      "step: 21389, loss: 2.109904926328454e-06\n",
      "step: 21390, loss: 2.622603645363597e-08\n",
      "step: 21391, loss: 7.15255632499634e-09\n",
      "step: 21392, loss: 2.145766231365087e-08\n",
      "step: 21393, loss: 4.76837147544984e-09\n",
      "step: 21394, loss: 7.15255676908555e-09\n",
      "step: 21395, loss: 8.583050714605633e-08\n",
      "step: 21396, loss: 2.6226025795494934e-08\n",
      "step: 21397, loss: 7.15255676908555e-09\n",
      "step: 21398, loss: 7.152551972922083e-08\n",
      "step: 21399, loss: 8.344635915591425e-08\n",
      "step: 21400, loss: 1.8358170450483158e-07\n",
      "step: 21401, loss: 4.76837147544984e-09\n",
      "step: 21402, loss: 6.890236363688018e-07\n",
      "step: 21403, loss: 2.145766764272139e-08\n",
      "step: 21404, loss: 0.0\n",
      "step: 21405, loss: 7.15255676908555e-09\n",
      "step: 21406, loss: 5.412050541053759e-07\n",
      "step: 21407, loss: 5.2452058696417225e-08\n",
      "step: 21408, loss: 0.06554476916790009\n",
      "step: 21409, loss: 0.0\n",
      "step: 21410, loss: 1.668929527909313e-08\n",
      "step: 21411, loss: 0.0\n",
      "step: 21412, loss: 4.76837147544984e-09\n",
      "step: 21413, loss: 2.1457660537294032e-08\n",
      "step: 21414, loss: 8.583054267319312e-08\n",
      "step: 21415, loss: 7.15255632499634e-09\n",
      "step: 21416, loss: 1.8333557818550617e-06\n",
      "step: 21417, loss: 2.622603645363597e-08\n",
      "step: 21418, loss: 3.0994389277338996e-08\n",
      "step: 21419, loss: 7.819980964995921e-07\n",
      "step: 21420, loss: 1.192092558000013e-08\n",
      "step: 21421, loss: 3.099440704090739e-08\n",
      "step: 21422, loss: 9.775154552471577e-08\n",
      "step: 21423, loss: 2.622604000634965e-08\n",
      "step: 21424, loss: 2.384184938364342e-08\n",
      "step: 21425, loss: 2.6464354618838115e-07\n",
      "step: 21426, loss: 7.15255676908555e-09\n",
      "step: 21427, loss: 1.041862219608447e-06\n",
      "step: 21428, loss: 0.09057817608118057\n",
      "step: 21429, loss: 2.384185648907078e-08\n",
      "step: 21430, loss: 3.337859055818626e-08\n",
      "step: 21431, loss: 0.0001901923242257908\n",
      "step: 21432, loss: 7.223989086924121e-07\n",
      "step: 21433, loss: 0.38676947355270386\n",
      "step: 21434, loss: 5.7220404414692894e-08\n",
      "step: 21435, loss: 0.0\n",
      "step: 21436, loss: 9.536733358572747e-08\n",
      "step: 21437, loss: 7.390962508679877e-08\n",
      "step: 21438, loss: 1.382824734719179e-07\n",
      "step: 21439, loss: 1.668929527909313e-08\n",
      "step: 21440, loss: 1.883500573285346e-07\n",
      "step: 21441, loss: 8.106222537662688e-08\n",
      "step: 21442, loss: 5.245207290727194e-08\n",
      "step: 21443, loss: 1.907348234908568e-08\n",
      "step: 21444, loss: 0.08173468708992004\n",
      "step: 21445, loss: 3.1754918836668367e-06\n",
      "step: 21446, loss: 4.836927928408841e-06\n",
      "step: 21447, loss: 1.6351215890608728e-05\n",
      "step: 21448, loss: 1.879551746242214e-05\n",
      "step: 21449, loss: 0.07829327136278152\n",
      "step: 21450, loss: 8.845143497637764e-07\n",
      "step: 21451, loss: 1.3374852869674214e-06\n",
      "step: 21452, loss: 2.329702692804858e-05\n",
      "step: 21453, loss: 1.9914972654078156e-05\n",
      "step: 21454, loss: 0.0006578745087608695\n",
      "step: 21455, loss: 1.16824800500126e-07\n",
      "step: 21456, loss: 8.416071750616538e-07\n",
      "step: 21457, loss: 7.629384413121443e-08\n",
      "step: 21458, loss: 3.0994396382766354e-08\n",
      "step: 21459, loss: 3.4407585189910606e-05\n",
      "step: 21460, loss: 8.58305639894752e-08\n",
      "step: 21461, loss: 5.483564677888353e-07\n",
      "step: 21462, loss: 5.0067875179138355e-08\n",
      "step: 21463, loss: 2.38418573772492e-09\n",
      "step: 21464, loss: 2.38418573772492e-09\n",
      "step: 21465, loss: 3.576277407546513e-08\n",
      "step: 21466, loss: 1.430510998545742e-08\n",
      "step: 21467, loss: 1.6950890540101682e-06\n",
      "step: 21468, loss: 0.0\n",
      "step: 21469, loss: 2.217284418293275e-07\n",
      "step: 21470, loss: 6.146722597577536e-08\n",
      "step: 21471, loss: 5.0067878731852034e-08\n",
      "step: 21472, loss: 7.390967482479027e-08\n",
      "step: 21473, loss: 2.717962956921838e-07\n",
      "step: 21474, loss: 9.775150999757898e-08\n",
      "step: 21475, loss: 2.9427803383441642e-05\n",
      "step: 21476, loss: 5.483623510826874e-08\n",
      "step: 21477, loss: 1.0013563667143899e-07\n",
      "step: 21478, loss: 3.075586221257254e-07\n",
      "step: 21479, loss: 1.907348412544252e-08\n",
      "step: 21480, loss: 1.6450849216198549e-07\n",
      "step: 21481, loss: 2.288809923811641e-07\n",
      "step: 21482, loss: 9.298313585759388e-08\n",
      "step: 21483, loss: 4.7683688109145805e-08\n",
      "step: 21484, loss: 1.2349742064543534e-06\n",
      "step: 21485, loss: 6.437294075567479e-08\n",
      "step: 21486, loss: 0.0\n",
      "step: 21487, loss: 0.18640965223312378\n",
      "step: 21488, loss: 9.53674295089968e-09\n",
      "step: 21489, loss: 7.15255676908555e-09\n",
      "step: 21490, loss: 1.1905477549589705e-05\n",
      "step: 21491, loss: 0.0\n",
      "step: 21492, loss: 2.3841845830929742e-08\n",
      "step: 21493, loss: 1.2397754289850127e-07\n",
      "step: 21494, loss: 1.168248857652543e-07\n",
      "step: 21495, loss: 7.390971745735442e-08\n",
      "step: 21496, loss: 2.38418573772492e-09\n",
      "step: 21497, loss: 9.53674206272126e-09\n",
      "step: 21498, loss: 7.15255632499634e-09\n",
      "step: 21499, loss: 0.0010178928496316075\n",
      "step: 21500, loss: 2.861022352362852e-08\n",
      "step: 21501, loss: 2.0503932773863198e-07\n",
      "step: 21502, loss: 8.106223248205424e-08\n",
      "step: 21503, loss: 5.006783965200157e-08\n",
      "step: 21504, loss: 4.76837103136063e-09\n",
      "step: 21505, loss: 9.53674206272126e-09\n",
      "step: 21506, loss: 9.53674117454284e-09\n",
      "step: 21507, loss: 3.814696114545768e-08\n",
      "step: 21508, loss: 2.622603467727913e-08\n",
      "step: 21509, loss: 4.529948327558486e-08\n",
      "step: 21510, loss: 0.0012510194210335612\n",
      "step: 21511, loss: 4.529947972287118e-08\n",
      "step: 21512, loss: 9.298305769789295e-08\n",
      "step: 21513, loss: 0.0\n",
      "step: 21514, loss: 2.622603467727913e-08\n",
      "step: 21515, loss: 2.622603467727913e-08\n",
      "step: 21516, loss: 0.0\n",
      "step: 21517, loss: 3.02789487705013e-07\n",
      "step: 21518, loss: 4.76837147544984e-09\n",
      "step: 21519, loss: 6.914135752822403e-08\n",
      "step: 21520, loss: 0.0\n",
      "step: 21521, loss: 9.53674206272126e-09\n",
      "step: 21522, loss: 1.430511264999268e-08\n",
      "step: 21523, loss: 5.483623155555506e-08\n",
      "step: 21524, loss: 2.0102907001273707e-05\n",
      "step: 21525, loss: 1.99814276129473e-05\n",
      "step: 21526, loss: 9.53674206272126e-09\n",
      "step: 21527, loss: 8.106224669290896e-08\n",
      "step: 21528, loss: 5.483623510826874e-08\n",
      "step: 21529, loss: 6.794874138904561e-07\n",
      "step: 21530, loss: 1.192092646817855e-08\n",
      "step: 21531, loss: 1.668929705544997e-08\n",
      "step: 21532, loss: 1.192092646817855e-08\n",
      "step: 21533, loss: 0.005783936008810997\n",
      "step: 21534, loss: 1.6450816531232704e-07\n",
      "step: 21535, loss: 1.192092824453539e-08\n",
      "step: 21536, loss: 9.369653071189532e-07\n",
      "step: 21537, loss: 0.0011415962362661958\n",
      "step: 21538, loss: 0.0006161891506053507\n",
      "step: 21539, loss: 2.1457603338603803e-07\n",
      "step: 21540, loss: 2.38418573772492e-09\n",
      "step: 21541, loss: 7.152552683464819e-08\n",
      "step: 21542, loss: 1.192092824453539e-08\n",
      "step: 21543, loss: 2.861022352362852e-08\n",
      "step: 21544, loss: 1.192091474422341e-07\n",
      "step: 21545, loss: 4.291529975830599e-08\n",
      "step: 21546, loss: 4.5299508144580614e-08\n",
      "step: 21547, loss: 8.280951988126617e-06\n",
      "step: 21548, loss: 3.576277762817881e-08\n",
      "step: 21549, loss: 1.5020319210634625e-07\n",
      "step: 21550, loss: 0.13349264860153198\n",
      "step: 21551, loss: 1.0728817301242088e-07\n",
      "step: 21552, loss: 4.053114466273655e-08\n",
      "step: 21553, loss: 1.7356119315081742e-06\n",
      "step: 21554, loss: 3.1471020633944136e-07\n",
      "step: 21555, loss: 4.76837103136063e-09\n",
      "step: 21556, loss: 2.288807081640698e-07\n",
      "step: 21557, loss: 8.106216853320802e-08\n",
      "step: 21558, loss: 1.5735585634502058e-07\n",
      "step: 21559, loss: 5.173616841602779e-07\n",
      "step: 21560, loss: 1.907347702001516e-08\n",
      "step: 21561, loss: 2.145766764272139e-08\n",
      "step: 21562, loss: 9.53674117454284e-09\n",
      "step: 21563, loss: 1.692767312988508e-07\n",
      "step: 21564, loss: 4.76837103136063e-09\n",
      "step: 21565, loss: 0.06803871691226959\n",
      "step: 21566, loss: 0.07430215179920197\n",
      "step: 21567, loss: 7.15255632499634e-09\n",
      "step: 21568, loss: 4.76837147544984e-09\n",
      "step: 21569, loss: 3.337858700547258e-08\n",
      "step: 21570, loss: 8.821474040132671e-08\n",
      "step: 21571, loss: 7.867804185934801e-08\n",
      "step: 21572, loss: 9.53674295089968e-09\n",
      "step: 21573, loss: 9.53674206272126e-09\n",
      "step: 21574, loss: 2.1696052954212064e-07\n",
      "step: 21575, loss: 7.15254913075114e-08\n",
      "step: 21576, loss: 2.2411259692489693e-07\n",
      "step: 21577, loss: 2.38418573772492e-09\n",
      "step: 21578, loss: 1.192092824453539e-08\n",
      "step: 21579, loss: 3.0040530418773415e-07\n",
      "step: 21580, loss: 0.0\n",
      "step: 21581, loss: 1.0990943337674253e-06\n",
      "step: 21582, loss: 4.455716862139525e-06\n",
      "step: 21583, loss: 3.6239470091459225e-07\n",
      "step: 21584, loss: 2.8274534997763112e-06\n",
      "step: 21585, loss: 9.765624781721272e-06\n",
      "step: 21586, loss: 1.5735572844732815e-07\n",
      "step: 21587, loss: 2.384185648907078e-08\n",
      "step: 21588, loss: 2.0027097491492896e-07\n",
      "step: 21589, loss: 1.039478092934587e-06\n",
      "step: 21590, loss: 5.006785741556996e-08\n",
      "step: 21591, loss: 8.82147759284635e-08\n",
      "step: 21592, loss: 9.417381079401821e-07\n",
      "step: 21593, loss: 2.861021997091484e-08\n",
      "step: 21594, loss: 1.192092646817855e-08\n",
      "step: 21595, loss: 2.574903987806465e-07\n",
      "step: 21596, loss: 4.76837103136063e-09\n",
      "step: 21597, loss: 1.7404532570708398e-07\n",
      "step: 21598, loss: 2.145766231365087e-08\n",
      "step: 21599, loss: 0.00296864565461874\n",
      "step: 21600, loss: 3.5762766970037774e-08\n",
      "step: 21601, loss: 8.344643731561519e-08\n",
      "step: 21602, loss: 2.145766764272139e-08\n",
      "step: 21603, loss: 1.5258763141901e-07\n",
      "step: 21604, loss: 2.38418529363571e-08\n",
      "step: 21605, loss: 2.38418573772492e-09\n",
      "step: 21606, loss: 4.76837103136063e-09\n",
      "step: 21607, loss: 0.013213062658905983\n",
      "step: 21608, loss: 4.2915313969160707e-08\n",
      "step: 21609, loss: 6.914127226309574e-08\n",
      "step: 21610, loss: 3.1232681863002654e-07\n",
      "step: 21611, loss: 7.867804185934801e-08\n",
      "step: 21612, loss: 2.455699075198936e-07\n",
      "step: 21613, loss: 3.0994389277338996e-08\n",
      "step: 21614, loss: 5.4836210239272987e-08\n",
      "step: 21615, loss: 0.0\n",
      "step: 21616, loss: 0.0001886101090349257\n",
      "step: 21617, loss: 0.05909842997789383\n",
      "step: 21618, loss: 2.8346053113637026e-06\n",
      "step: 21619, loss: 3.986004230682738e-06\n",
      "step: 21620, loss: 3.33785834527589e-08\n",
      "step: 21621, loss: 3.337858700547258e-08\n",
      "step: 21622, loss: 3.814696114545768e-08\n",
      "step: 21623, loss: 0.08364850282669067\n",
      "step: 21624, loss: 3.5762496963798185e-07\n",
      "step: 21625, loss: 1.668929883180681e-08\n",
      "step: 21626, loss: 1.7881360747651343e-07\n",
      "step: 21627, loss: 1.811973220355867e-07\n",
      "step: 21628, loss: 1.192092824453539e-08\n",
      "step: 21629, loss: 1.883498725874233e-07\n",
      "step: 21630, loss: 1.5020344790173112e-07\n",
      "step: 21631, loss: 5.245206935455826e-08\n",
      "step: 21632, loss: 4.0531130451881836e-08\n",
      "step: 21633, loss: 8.153753014994436e-07\n",
      "step: 21634, loss: 2.38418573772492e-09\n",
      "step: 21635, loss: 4.2915317521874385e-08\n",
      "step: 21636, loss: 2.2888053763381322e-07\n",
      "step: 21637, loss: 1.0013567930400313e-07\n",
      "step: 21638, loss: 2.8610209312773804e-08\n",
      "step: 21639, loss: 4.76837147544984e-09\n",
      "step: 21640, loss: 1.2540651823655935e-06\n",
      "step: 21641, loss: 6.238542482606135e-06\n",
      "step: 21642, loss: 4.76837103136063e-09\n",
      "step: 21643, loss: 1.1920899112283223e-07\n",
      "step: 21644, loss: 1.668929883180681e-08\n",
      "step: 21645, loss: 2.38418573772492e-09\n",
      "step: 21646, loss: 0.02205021120607853\n",
      "step: 21647, loss: 1.4066652909150434e-07\n",
      "step: 21648, loss: 1.430511176181426e-08\n",
      "step: 21649, loss: 4.965651896782219e-06\n",
      "step: 21650, loss: 5.483622800284138e-08\n",
      "step: 21651, loss: 4.4345600258566265e-07\n",
      "step: 21652, loss: 1.668929883180681e-08\n",
      "step: 21653, loss: 0.0\n",
      "step: 21654, loss: 4.76837103136063e-09\n",
      "step: 21655, loss: 1.430511264999268e-08\n",
      "step: 21656, loss: 1.8483626263332553e-05\n",
      "step: 21657, loss: 0.0982927605509758\n",
      "step: 21658, loss: 1.1920896980655016e-07\n",
      "step: 21659, loss: 5.483625642455081e-08\n",
      "step: 21660, loss: 4.291489403840387e-07\n",
      "step: 21661, loss: 4.529948327558486e-08\n",
      "step: 21662, loss: 2.384184938364342e-08\n",
      "step: 21663, loss: 1.192092646817855e-08\n",
      "step: 21664, loss: 9.53674206272126e-09\n",
      "step: 21665, loss: 1.430511264999268e-08\n",
      "step: 21666, loss: 2.861021997091484e-08\n",
      "step: 21667, loss: 6.914134331736932e-08\n",
      "step: 21668, loss: 9.53674295089968e-09\n",
      "step: 21669, loss: 7.15255676908555e-09\n",
      "step: 21670, loss: 2.861012831090193e-07\n",
      "step: 21671, loss: 1.668929527909313e-08\n",
      "step: 21672, loss: 7.15255632499634e-09\n",
      "step: 21673, loss: 2.861022529998536e-08\n",
      "step: 21674, loss: 1.668929350273629e-08\n",
      "step: 21675, loss: 2.5033844508470793e-07\n",
      "step: 21676, loss: 0.025784341618418694\n",
      "step: 21677, loss: 2.38418529363571e-08\n",
      "step: 21678, loss: 4.76837103136063e-09\n",
      "step: 21679, loss: 1.5020354737771413e-07\n",
      "step: 21680, loss: 4.76837147544984e-09\n",
      "step: 21681, loss: 7.15255676908555e-09\n",
      "step: 21682, loss: 0.0\n",
      "step: 21683, loss: 1.5735564318219986e-07\n",
      "step: 21684, loss: 2.145766764272139e-08\n",
      "step: 21685, loss: 0.0\n",
      "step: 21686, loss: 1.668929883180681e-08\n",
      "step: 21687, loss: 7.15255676908555e-09\n",
      "step: 21688, loss: 1.668929705544997e-08\n",
      "step: 21689, loss: 0.0\n",
      "step: 21690, loss: 0.052400246262550354\n",
      "step: 21691, loss: 6.461069119723106e-07\n",
      "step: 21692, loss: 1.430511176181426e-08\n",
      "step: 21693, loss: 1.8835011417195346e-07\n",
      "step: 21694, loss: 5.722040796740657e-08\n",
      "step: 21695, loss: 4.053114111002287e-08\n",
      "step: 21696, loss: 6.332989954671575e-08\n",
      "step: 21697, loss: 1.668929527909313e-08\n",
      "step: 21698, loss: 2.0265483158254938e-07\n",
      "step: 21699, loss: 2.145766764272139e-08\n",
      "step: 21700, loss: 0.19869573414325714\n",
      "step: 21701, loss: 4.053114466273655e-08\n",
      "step: 21702, loss: 4.76837103136063e-09\n",
      "step: 21703, loss: 0.0\n",
      "step: 21704, loss: 2.2649655306850036e-07\n",
      "step: 21705, loss: 4.76837147544984e-09\n",
      "step: 21706, loss: 0.07372315973043442\n",
      "step: 21707, loss: 9.53674206272126e-09\n",
      "step: 21708, loss: 1.668927183118285e-07\n",
      "step: 21709, loss: 1.907348234908568e-08\n",
      "step: 21710, loss: 7.152551262379347e-08\n",
      "step: 21711, loss: 9.298310033045709e-08\n",
      "step: 21712, loss: 6.675709585124423e-08\n",
      "step: 21713, loss: 1.430510998545742e-08\n",
      "step: 21714, loss: 5.245204093284883e-08\n",
      "step: 21715, loss: 7.867809159733952e-08\n",
      "step: 21716, loss: 2.38418573772492e-09\n",
      "step: 21717, loss: 4.2915313969160707e-08\n",
      "step: 21718, loss: 0.0\n",
      "step: 21719, loss: 0.0\n",
      "step: 21720, loss: 2.38418573772492e-09\n",
      "step: 21721, loss: 2.38418573772492e-09\n",
      "step: 21722, loss: 9.53674295089968e-09\n",
      "step: 21723, loss: 0.0006626768154092133\n",
      "step: 21724, loss: 4.76837103136063e-09\n",
      "step: 21725, loss: 5.483623155555506e-08\n",
      "step: 21726, loss: 7.15255676908555e-09\n",
      "step: 21727, loss: 5.006788228456571e-08\n",
      "step: 21728, loss: 0.0\n",
      "step: 21729, loss: 5.7220432836402324e-08\n",
      "step: 21730, loss: 6.198879987096007e-08\n",
      "step: 21731, loss: 2.38418573772492e-09\n",
      "step: 21732, loss: 0.00015011432697065175\n",
      "step: 21733, loss: 1.4305086892818508e-07\n",
      "step: 21734, loss: 2.2934807475394337e-06\n",
      "step: 21735, loss: 4.529951169729429e-08\n",
      "step: 21736, loss: 4.9513964768266305e-06\n",
      "step: 21737, loss: 1.430511176181426e-08\n",
      "step: 21738, loss: 1.430510998545742e-08\n",
      "step: 21739, loss: 5.483622800284138e-08\n",
      "step: 21740, loss: 1.430511264999268e-08\n",
      "step: 21741, loss: 6.437297628281158e-08\n",
      "step: 21742, loss: 2.574905124674842e-07\n",
      "step: 21743, loss: 2.5212317268596962e-05\n",
      "step: 21744, loss: 7.86780773864848e-08\n",
      "step: 21745, loss: 2.38418573772492e-09\n",
      "step: 21746, loss: 4.529951169729429e-08\n",
      "step: 21747, loss: 2.38418573772492e-09\n",
      "step: 21748, loss: 0.0697561651468277\n",
      "step: 21749, loss: 7.152551972922083e-08\n",
      "step: 21750, loss: 9.53674295089968e-09\n",
      "step: 21751, loss: 0.0782933160662651\n",
      "step: 21752, loss: 1.0490391844086844e-07\n",
      "step: 21753, loss: 1.9073478796372e-08\n",
      "step: 21754, loss: 7.15255676908555e-09\n",
      "step: 21755, loss: 4.5299508144580614e-08\n",
      "step: 21756, loss: 7.15255632499634e-09\n",
      "step: 21757, loss: 1.4066671383261564e-07\n",
      "step: 21758, loss: 8.225313194998307e-07\n",
      "step: 21759, loss: 2.38418573772492e-09\n",
      "step: 21760, loss: 1.430511264999268e-08\n",
      "step: 21761, loss: 1.907348234908568e-08\n",
      "step: 21762, loss: 5.4836217344700344e-08\n",
      "step: 21763, loss: 2.3841845830929742e-08\n",
      "step: 21764, loss: 1.192092646817855e-08\n",
      "step: 21765, loss: 0.0\n",
      "step: 21766, loss: 3.5762759864610416e-08\n",
      "step: 21767, loss: 1.4066651488064963e-07\n",
      "step: 21768, loss: 2.8848444344475865e-07\n",
      "step: 21769, loss: 1.6212413811445003e-07\n",
      "step: 21770, loss: 3.337834755257063e-07\n",
      "step: 21771, loss: 4.458401008378132e-07\n",
      "step: 21772, loss: 1.2636147062039527e-07\n",
      "step: 21773, loss: 2.38418573772492e-09\n",
      "step: 21774, loss: 3.8385158518394746e-07\n",
      "step: 21775, loss: 9.536724121517182e-08\n",
      "step: 21776, loss: 6.198879276553271e-08\n",
      "step: 21777, loss: 4.76837147544984e-09\n",
      "step: 21778, loss: 0.0\n",
      "step: 21779, loss: 1.430511264999268e-08\n",
      "step: 21780, loss: 2.145766941907823e-08\n",
      "step: 21781, loss: 1.907348412544252e-08\n",
      "step: 21782, loss: 1.2159331674865825e-07\n",
      "step: 21783, loss: 4.7683659687436375e-08\n",
      "step: 21784, loss: 0.0\n",
      "step: 21785, loss: 1.3113005081777374e-07\n",
      "step: 21786, loss: 0.008693280629813671\n",
      "step: 21787, loss: 2.38418573772492e-09\n",
      "step: 21788, loss: 2.264963967490985e-07\n",
      "step: 21789, loss: 4.053114821545023e-08\n",
      "step: 21790, loss: 1.192092824453539e-08\n",
      "step: 21791, loss: 2.0742324124967126e-07\n",
      "step: 21792, loss: 1.0728810906357467e-07\n",
      "step: 21793, loss: 2.38418573772492e-09\n",
      "step: 21794, loss: 7.15255632499634e-09\n",
      "step: 21795, loss: 5.483623510826874e-08\n",
      "step: 21796, loss: 4.76837147544984e-09\n",
      "step: 21797, loss: 0.0\n",
      "step: 21798, loss: 0.1110287755727768\n",
      "step: 21799, loss: 1.43051135381711e-08\n",
      "step: 21800, loss: 3.8146946934602965e-08\n",
      "step: 21801, loss: 2.145766586636455e-08\n",
      "step: 21802, loss: 6.437299049366629e-08\n",
      "step: 21803, loss: 3.337858700547258e-08\n",
      "step: 21804, loss: 0.059407543390989304\n",
      "step: 21805, loss: 1.7404482832716894e-07\n",
      "step: 21806, loss: 8.106216853320802e-08\n",
      "step: 21807, loss: 3.099441059362107e-08\n",
      "step: 21808, loss: 2.622603467727913e-08\n",
      "step: 21809, loss: 5.626608299280633e-07\n",
      "step: 21810, loss: 0.14972542226314545\n",
      "step: 21811, loss: 4.76837103136063e-09\n",
      "step: 21812, loss: 1.263614990421047e-07\n",
      "step: 21813, loss: 1.192092646817855e-08\n",
      "step: 21814, loss: 3.576277762817881e-08\n",
      "step: 21815, loss: 6.9868315222265664e-06\n",
      "step: 21816, loss: 3.576277407546513e-08\n",
      "step: 21817, loss: 0.18620897829532623\n",
      "step: 21818, loss: 2.145760902294569e-07\n",
      "step: 21819, loss: 6.914128647395046e-08\n",
      "step: 21820, loss: 8.273018465843052e-07\n",
      "step: 21821, loss: 2.8610216418201162e-08\n",
      "step: 21822, loss: 0.039087411016225815\n",
      "step: 21823, loss: 9.53674206272126e-09\n",
      "step: 21824, loss: 1.430510998545742e-08\n",
      "step: 21825, loss: 5.245204093284883e-08\n",
      "step: 21826, loss: 3.6000881209474755e-07\n",
      "step: 21827, loss: 0.013589038513600826\n",
      "step: 21828, loss: 0.14112913608551025\n",
      "step: 21829, loss: 7.15255676908555e-09\n",
      "step: 21830, loss: 0.07102127373218536\n",
      "step: 21831, loss: 9.53674295089968e-09\n",
      "step: 21832, loss: 2.8610218194558e-08\n",
      "step: 21833, loss: 7.629380149865028e-08\n",
      "step: 21834, loss: 8.34464017884784e-08\n",
      "step: 21835, loss: 1.430510998545742e-08\n",
      "step: 21836, loss: 1.668929527909313e-08\n",
      "step: 21837, loss: 1.1205407872694195e-06\n",
      "step: 21838, loss: 2.38418573772492e-09\n",
      "step: 21839, loss: 7.39096535085082e-08\n",
      "step: 21840, loss: 2.38418573772492e-09\n",
      "step: 21841, loss: 7.15255632499634e-09\n",
      "step: 21842, loss: 1.430511176181426e-08\n",
      "step: 21843, loss: 2.6226025795494934e-08\n",
      "step: 21844, loss: 6.437298338823894e-08\n",
      "step: 21845, loss: 0.05095440894365311\n",
      "step: 21846, loss: 1.3113003660691902e-07\n",
      "step: 21847, loss: 3.337859055818626e-08\n",
      "step: 21848, loss: 1.907348234908568e-08\n",
      "step: 21849, loss: 2.38418573772492e-09\n",
      "step: 21850, loss: 1.1920911902052467e-07\n",
      "step: 21851, loss: 7.390968903564499e-08\n",
      "step: 21852, loss: 9.53674295089968e-09\n",
      "step: 21853, loss: 2.622603290092229e-08\n",
      "step: 21854, loss: 9.53674206272126e-09\n",
      "step: 21855, loss: 1.430508973498945e-07\n",
      "step: 21856, loss: 2.145766764272139e-08\n",
      "step: 21857, loss: 2.3364891887922568e-07\n",
      "step: 21858, loss: 9.536726963688125e-08\n",
      "step: 21859, loss: 1.192092824453539e-08\n",
      "step: 21860, loss: 5.483619602841827e-08\n",
      "step: 21861, loss: 2.38418573772492e-09\n",
      "step: 21862, loss: 1.502033200040387e-07\n",
      "step: 21863, loss: 1.668929883180681e-08\n",
      "step: 21864, loss: 2.710674834816018e-06\n",
      "step: 21865, loss: 2.38418573772492e-09\n",
      "step: 21866, loss: 1.4781909385419567e-07\n",
      "step: 21867, loss: 2.622604000634965e-08\n",
      "step: 21868, loss: 2.145766764272139e-08\n",
      "step: 21869, loss: 1.668929883180681e-08\n",
      "step: 21870, loss: 7.15255676908555e-09\n",
      "step: 21871, loss: 9.53674295089968e-09\n",
      "step: 21872, loss: 2.384184938364342e-08\n",
      "step: 21873, loss: 3.099441059362107e-08\n",
      "step: 21874, loss: 4.76837147544984e-09\n",
      "step: 21875, loss: 7.152552683464819e-08\n",
      "step: 21876, loss: 9.53674206272126e-09\n",
      "step: 21877, loss: 0.0\n",
      "step: 21878, loss: 9.53674206272126e-09\n",
      "step: 21879, loss: 6.675715980009045e-08\n",
      "step: 21880, loss: 5.7220432836402324e-08\n",
      "step: 21881, loss: 1.1444071645883014e-07\n",
      "step: 21882, loss: 1.1444065250998392e-07\n",
      "step: 21883, loss: 6.341856533254031e-07\n",
      "step: 21884, loss: 1.9550246577182406e-07\n",
      "step: 21885, loss: 1.3828231715251604e-07\n",
      "step: 21886, loss: 4.76837147544984e-09\n",
      "step: 21887, loss: 0.0\n",
      "step: 21888, loss: 0.0\n",
      "step: 21889, loss: 1.5020317789549154e-07\n",
      "step: 21890, loss: 4.329762305133045e-05\n",
      "step: 21891, loss: 4.76837147544984e-09\n",
      "step: 21892, loss: 1.192092558000013e-08\n",
      "step: 21893, loss: 2.38418573772492e-09\n",
      "step: 21894, loss: 8.583050714605633e-08\n",
      "step: 21895, loss: 7.629380860407764e-08\n",
      "step: 21896, loss: 5.602794885817275e-07\n",
      "step: 21897, loss: 7.152551972922083e-08\n",
      "step: 21898, loss: 2.38418573772492e-09\n",
      "step: 21899, loss: 0.02431591972708702\n",
      "step: 21900, loss: 1.6212413811445003e-07\n",
      "step: 21901, loss: 1.192092558000013e-08\n",
      "step: 21902, loss: 6.914129357937782e-08\n",
      "step: 21903, loss: 9.460606634092983e-06\n",
      "step: 21904, loss: 2.479538352417876e-07\n",
      "step: 21905, loss: 1.668929527909313e-08\n",
      "step: 21906, loss: 1.668929350273629e-08\n",
      "step: 21907, loss: 5.811800747324014e-06\n",
      "step: 21908, loss: 0.0\n",
      "step: 21909, loss: 0.0\n",
      "step: 21910, loss: 1.907347702001516e-08\n",
      "step: 21911, loss: 4.76837147544984e-09\n",
      "step: 21912, loss: 2.38418573772492e-09\n",
      "step: 21913, loss: 1.430510998545742e-08\n",
      "step: 21914, loss: 7.15255676908555e-09\n",
      "step: 21915, loss: 1.192092824453539e-08\n",
      "step: 21916, loss: 2.1457660537294032e-08\n",
      "step: 21917, loss: 7.15255676908555e-09\n",
      "step: 21918, loss: 8.838612302497495e-06\n",
      "step: 21919, loss: 2.38418573772492e-09\n",
      "step: 21920, loss: 1.0490395396800523e-07\n",
      "step: 21921, loss: 1.430511264999268e-08\n",
      "step: 21922, loss: 1.1734624649761827e-07\n",
      "step: 21923, loss: 7.15255676908555e-09\n",
      "step: 21924, loss: 2.3841845830929742e-08\n",
      "step: 21925, loss: 0.05408434942364693\n",
      "step: 21926, loss: 4.76837147544984e-09\n",
      "step: 21927, loss: 1.668929527909313e-08\n",
      "step: 21928, loss: 1.430511264999268e-08\n",
      "step: 21929, loss: 2.3841845830929742e-08\n",
      "step: 21930, loss: 5.960456306297601e-08\n",
      "step: 21931, loss: 7.511835065088235e-06\n",
      "step: 21932, loss: 1.8119757783097157e-07\n",
      "step: 21933, loss: 0.0\n",
      "step: 21934, loss: 9.53674295089968e-09\n",
      "step: 21935, loss: 2.622604000634965e-08\n",
      "step: 21936, loss: 2.908685985403281e-07\n",
      "step: 21937, loss: 1.430511264999268e-08\n",
      "step: 21938, loss: 1.573559700318583e-07\n",
      "step: 21939, loss: 2.980218027914816e-07\n",
      "step: 21940, loss: 1.668929527909313e-08\n",
      "step: 21941, loss: 3.8146946934602965e-08\n",
      "step: 21942, loss: 2.38418573772492e-09\n",
      "step: 21943, loss: 4.76837103136063e-09\n",
      "step: 21944, loss: 0.15714941918849945\n",
      "step: 21945, loss: 3.433204938119161e-07\n",
      "step: 21946, loss: 5.9604552404834976e-08\n",
      "step: 21947, loss: 5.078268259239849e-07\n",
      "step: 21948, loss: 5.0067875179138355e-08\n",
      "step: 21949, loss: 2.38418529363571e-08\n",
      "step: 21950, loss: 3.8623446130259254e-07\n",
      "step: 21951, loss: 0.008377041667699814\n",
      "step: 21952, loss: 3.578364157874603e-06\n",
      "step: 21953, loss: 0.0\n",
      "step: 21954, loss: 1.406665859349232e-07\n",
      "step: 21955, loss: 1.0728813037985674e-07\n",
      "step: 21956, loss: 0.00040211118175648153\n",
      "step: 21957, loss: 6.914128647395046e-08\n",
      "step: 21958, loss: 1.430511176181426e-08\n",
      "step: 21959, loss: 9.536731226944539e-08\n",
      "step: 21960, loss: 5.030852116760798e-05\n",
      "step: 21961, loss: 0.06640414893627167\n",
      "step: 21962, loss: 1.192092646817855e-08\n",
      "step: 21963, loss: 4.053114466273655e-08\n",
      "step: 21964, loss: 2.3841844054572903e-08\n",
      "step: 21965, loss: 7.152545578037461e-08\n",
      "step: 21966, loss: 0.09009196609258652\n",
      "step: 21967, loss: 2.8371724170028756e-07\n",
      "step: 21968, loss: 2.861022352362852e-08\n",
      "step: 21969, loss: 8.691209586686455e-06\n",
      "step: 21970, loss: 0.0066901021637022495\n",
      "step: 21971, loss: 1.0514055475141504e-06\n",
      "step: 21972, loss: 1.192092558000013e-08\n",
      "step: 21973, loss: 5.006786452099732e-08\n",
      "step: 21974, loss: 1.0967237074055447e-07\n",
      "step: 21975, loss: 5.507432661033818e-07\n",
      "step: 21976, loss: 9.53674117454284e-09\n",
      "step: 21977, loss: 0.00010424375795992091\n",
      "step: 21978, loss: 8.249120355685591e-07\n",
      "step: 21979, loss: 4.053114821545023e-08\n",
      "step: 21980, loss: 6.437296917738422e-08\n",
      "step: 21981, loss: 4.5060750153425033e-07\n",
      "step: 21982, loss: 1.168248857652543e-07\n",
      "step: 21983, loss: 0.013459899462759495\n",
      "step: 21984, loss: 2.264968799181588e-07\n",
      "step: 21985, loss: 4.601425587225094e-07\n",
      "step: 21986, loss: 8.106221116577217e-08\n",
      "step: 21987, loss: 1.907347702001516e-08\n",
      "step: 21988, loss: 2.5272277071053395e-07\n",
      "step: 21989, loss: 8.535225219929998e-07\n",
      "step: 21990, loss: 6.937867169654055e-07\n",
      "step: 21991, loss: 9.53674117454284e-09\n",
      "step: 21992, loss: 3.099423793173628e-07\n",
      "step: 21993, loss: 4.529951169729429e-08\n",
      "step: 21994, loss: 0.0\n",
      "step: 21995, loss: 1.0251985571585465e-07\n",
      "step: 21996, loss: 1.192092824453539e-08\n",
      "step: 21997, loss: 2.1696014584904333e-07\n",
      "step: 21998, loss: 1.4543502402375452e-07\n",
      "step: 21999, loss: 7.15255632499634e-09\n",
      "step: 22000, loss: 0.029673345386981964\n",
      "step: 22001, loss: 7.15255676908555e-09\n",
      "step: 22002, loss: 2.1695990426451317e-07\n",
      "step: 22003, loss: 1.668929350273629e-08\n",
      "step: 22004, loss: 4.76837103136063e-09\n",
      "step: 22005, loss: 1.5262561646522954e-05\n",
      "step: 22006, loss: 2.384185648907078e-08\n",
      "step: 22007, loss: 0.08453473448753357\n",
      "step: 22008, loss: 3.766980682939902e-07\n",
      "step: 22009, loss: 1.430511264999268e-08\n",
      "step: 22010, loss: 2.384177264502796e-07\n",
      "step: 22011, loss: 5.722037599298346e-08\n",
      "step: 22012, loss: 1.907348234908568e-08\n",
      "step: 22013, loss: 0.0\n",
      "step: 22014, loss: 2.38418573772492e-09\n",
      "step: 22015, loss: 1.9073478796372e-08\n",
      "step: 22016, loss: 2.908685985403281e-07\n",
      "step: 22017, loss: 3.099439993548003e-08\n",
      "step: 22018, loss: 4.649136826628819e-07\n",
      "step: 22019, loss: 1.192092824453539e-08\n",
      "step: 22020, loss: 0.10295553505420685\n",
      "step: 22021, loss: 9.77514886812969e-08\n",
      "step: 22022, loss: 1.9311872279104136e-07\n",
      "step: 22023, loss: 2.145766231365087e-08\n",
      "step: 22024, loss: 7.15255676908555e-09\n",
      "step: 22025, loss: 7.15255676908555e-09\n",
      "step: 22026, loss: 1.2874583887878543e-07\n",
      "step: 22027, loss: 7.39097032464997e-08\n",
      "step: 22028, loss: 8.344637336676897e-08\n",
      "step: 22029, loss: 1.883501568045176e-07\n",
      "step: 22030, loss: 2.622604000634965e-08\n",
      "step: 22031, loss: 0.0958687886595726\n",
      "step: 22032, loss: 4.76837147544984e-09\n",
      "step: 22033, loss: 1.1133905672977562e-06\n",
      "step: 22034, loss: 1.5735582792331115e-07\n",
      "step: 22035, loss: 7.15255676908555e-09\n",
      "step: 22036, loss: 2.38418573772492e-09\n",
      "step: 22037, loss: 3.655667387647554e-05\n",
      "step: 22038, loss: 1.192092824453539e-08\n",
      "step: 22039, loss: 1.3351416328077903e-07\n",
      "step: 22040, loss: 1.31129837654953e-07\n",
      "step: 22041, loss: 2.837167301095178e-07\n",
      "step: 22042, loss: 1.5735571423647343e-07\n",
      "step: 22043, loss: 9.53674117454284e-09\n",
      "step: 22044, loss: 2.38418573772492e-09\n",
      "step: 22045, loss: 1.907348412544252e-08\n",
      "step: 22046, loss: 5.245201961656676e-08\n",
      "step: 22047, loss: 7.15255676908555e-09\n",
      "step: 22048, loss: 7.15255632499634e-09\n",
      "step: 22049, loss: 4.76837103136063e-09\n",
      "step: 22050, loss: 2.1934447147486935e-07\n",
      "step: 22051, loss: 4.053112689916816e-08\n",
      "step: 22052, loss: 0.0\n",
      "step: 22053, loss: 7.15255676908555e-09\n",
      "step: 22054, loss: 0.08248206973075867\n",
      "step: 22055, loss: 1.668929527909313e-08\n",
      "step: 22056, loss: 2.0742351125591085e-07\n",
      "step: 22057, loss: 3.099440704090739e-08\n",
      "step: 22058, loss: 1.2874568255938357e-07\n",
      "step: 22059, loss: 5.245142347121146e-07\n",
      "step: 22060, loss: 2.861022529998536e-08\n",
      "step: 22061, loss: 1.192092646817855e-08\n",
      "step: 22062, loss: 2.0265497369109653e-07\n",
      "step: 22063, loss: 0.17528842389583588\n",
      "step: 22064, loss: 1.8309725646759034e-06\n",
      "step: 22065, loss: 1.668929705544997e-08\n",
      "step: 22066, loss: 1.668929350273629e-08\n",
      "step: 22067, loss: 5.2452033827421474e-08\n",
      "step: 22068, loss: 9.53674206272126e-09\n",
      "step: 22069, loss: 2.145766764272139e-08\n",
      "step: 22070, loss: 2.3841845830929742e-08\n",
      "step: 22071, loss: 1.6689277515524736e-07\n",
      "step: 22072, loss: 0.0\n",
      "step: 22073, loss: 5.00678503101426e-08\n",
      "step: 22074, loss: 9.059896655116972e-08\n",
      "step: 22075, loss: 4.76837147544984e-09\n",
      "step: 22076, loss: 9.53674206272126e-09\n",
      "step: 22077, loss: 3.5762763417324095e-08\n",
      "step: 22078, loss: 1.668929350273629e-08\n",
      "step: 22079, loss: 1.955023236632769e-07\n",
      "step: 22080, loss: 5.722043994182968e-08\n",
      "step: 22081, loss: 8.821480435017293e-08\n",
      "step: 22082, loss: 2.38418573772492e-09\n",
      "step: 22083, loss: 1.192092824453539e-08\n",
      "step: 22084, loss: 3.7669892094527313e-07\n",
      "step: 22085, loss: 2.861021997091484e-08\n",
      "step: 22086, loss: 2.431854966289393e-07\n",
      "step: 22087, loss: 7.390969614107235e-08\n",
      "step: 22088, loss: 1.430511176181426e-08\n",
      "step: 22089, loss: 2.0742378126215044e-07\n",
      "step: 22090, loss: 0.0\n",
      "step: 22091, loss: 5.483626352997817e-08\n",
      "step: 22092, loss: 9.53674295089968e-09\n",
      "step: 22093, loss: 1.668929705544997e-08\n",
      "step: 22094, loss: 1.9788694771705195e-07\n",
      "step: 22095, loss: 3.3378576347331546e-08\n",
      "step: 22096, loss: 1.668929350273629e-08\n",
      "step: 22097, loss: 7.15255632499634e-09\n",
      "step: 22098, loss: 5.960459503739912e-08\n",
      "step: 22099, loss: 5.245206935455826e-08\n",
      "step: 22100, loss: 5.7220432836402324e-08\n",
      "step: 22101, loss: 0.0\n",
      "step: 22102, loss: 3.8146950487316644e-08\n",
      "step: 22103, loss: 2.38418573772492e-09\n",
      "step: 22104, loss: 0.0\n",
      "step: 22105, loss: 1.907348234908568e-08\n",
      "step: 22106, loss: 4.76837103136063e-09\n",
      "step: 22107, loss: 8.106222537662688e-08\n",
      "step: 22108, loss: 5.43587930224021e-07\n",
      "step: 22109, loss: 2.145766764272139e-08\n",
      "step: 22110, loss: 7.15255676908555e-09\n",
      "step: 22111, loss: 9.53674117454284e-09\n",
      "step: 22112, loss: 2.38418573772492e-09\n",
      "step: 22113, loss: 4.76837147544984e-09\n",
      "step: 22114, loss: 1.8834978732229501e-07\n",
      "step: 22115, loss: 9.53674206272126e-09\n",
      "step: 22116, loss: 7.15255676908555e-09\n",
      "step: 22117, loss: 2.38418529363571e-08\n",
      "step: 22118, loss: 0.06984205543994904\n",
      "step: 22119, loss: 8.344642310476047e-08\n",
      "step: 22120, loss: 6.437296207195686e-08\n",
      "step: 22121, loss: 3.576277407546513e-08\n",
      "step: 22122, loss: 2.0884406239929376e-06\n",
      "step: 22123, loss: 2.38418573772492e-09\n",
      "step: 22124, loss: 2.622603467727913e-08\n",
      "step: 22125, loss: 1.668929705544997e-08\n",
      "step: 22126, loss: 1.5497161598432285e-07\n",
      "step: 22127, loss: 4.76837103136063e-09\n",
      "step: 22128, loss: 3.576275631189674e-08\n",
      "step: 22129, loss: 0.0\n",
      "step: 22130, loss: 1.3112990870922658e-07\n",
      "step: 22131, loss: 0.0650157704949379\n",
      "step: 22132, loss: 1.192092824453539e-08\n",
      "step: 22133, loss: 9.77513892053139e-08\n",
      "step: 22134, loss: 0.14907777309417725\n",
      "step: 22135, loss: 9.53674206272126e-09\n",
      "step: 22136, loss: 0.0\n",
      "step: 22137, loss: 2.38418573772492e-09\n",
      "step: 22138, loss: 2.384185116000026e-08\n",
      "step: 22139, loss: 0.0\n",
      "step: 22140, loss: 2.38418573772492e-09\n",
      "step: 22141, loss: 7.15255632499634e-09\n",
      "step: 22142, loss: 5.245204093284883e-08\n",
      "step: 22143, loss: 1.9597057416831376e-06\n",
      "step: 22144, loss: 1.907348412544252e-08\n",
      "step: 22145, loss: 1.883504268107572e-07\n",
      "step: 22146, loss: 0.0\n",
      "step: 22147, loss: 5.2452033827421474e-08\n",
      "step: 22148, loss: 1.396977609147143e-07\n",
      "step: 22149, loss: 3.337858700547258e-08\n",
      "step: 22150, loss: 1.5735601266442245e-07\n",
      "step: 22151, loss: 9.53674206272126e-09\n",
      "step: 22152, loss: 2.38418573772492e-09\n",
      "step: 22153, loss: 0.051171086728572845\n",
      "step: 22154, loss: 2.861022529998536e-08\n",
      "step: 22155, loss: 6.437292654482007e-08\n",
      "step: 22156, loss: 4.291532462730174e-08\n",
      "step: 22157, loss: 1.907348412544252e-08\n",
      "step: 22158, loss: 7.15255632499634e-09\n",
      "step: 22159, loss: 3.814693627646193e-08\n",
      "step: 22160, loss: 7.15255632499634e-09\n",
      "step: 22161, loss: 2.145766586636455e-08\n",
      "step: 22162, loss: 8.821476171760878e-08\n",
      "step: 22163, loss: 4.76837147544984e-09\n",
      "step: 22164, loss: 7.15255676908555e-09\n",
      "step: 22165, loss: 4.76837103136063e-09\n",
      "step: 22166, loss: 4.76837147544984e-09\n",
      "step: 22167, loss: 4.649129152767273e-07\n",
      "step: 22168, loss: 8.106223248205424e-08\n",
      "step: 22169, loss: 0.04995231702923775\n",
      "step: 22170, loss: 1.478191222759051e-07\n",
      "step: 22171, loss: 1.907348234908568e-08\n",
      "step: 22172, loss: 2.38418529363571e-08\n",
      "step: 22173, loss: 4.0531130451881836e-08\n",
      "step: 22174, loss: 1.5974005407315417e-07\n",
      "step: 22175, loss: 5.245201961656676e-08\n",
      "step: 22176, loss: 4.76837147544984e-09\n",
      "step: 22177, loss: 2.38418573772492e-09\n",
      "step: 22178, loss: 7.15255632499634e-09\n",
      "step: 22179, loss: 2.145766586636455e-08\n",
      "step: 22180, loss: 6.675713137838102e-08\n",
      "step: 22181, loss: 4.76837103136063e-09\n",
      "step: 22182, loss: 1.7118022697104607e-06\n",
      "step: 22183, loss: 3.3855164360829804e-07\n",
      "step: 22184, loss: 2.38418573772492e-09\n",
      "step: 22185, loss: 1.096723494242724e-07\n",
      "step: 22186, loss: 4.76837103136063e-09\n",
      "step: 22187, loss: 2.38418573772492e-09\n",
      "step: 22188, loss: 5.006783965200157e-08\n",
      "step: 22189, loss: 1.192092558000013e-08\n",
      "step: 22190, loss: 2.145766764272139e-08\n",
      "step: 22191, loss: 5.483620668655931e-08\n",
      "step: 22192, loss: 0.0984814241528511\n",
      "step: 22193, loss: 1.668929705544997e-08\n",
      "step: 22194, loss: 9.298310743588445e-08\n",
      "step: 22195, loss: 0.06838434934616089\n",
      "step: 22196, loss: 1.430511264999268e-08\n",
      "step: 22197, loss: 2.38418573772492e-09\n",
      "step: 22198, loss: 6.437299049366629e-08\n",
      "step: 22199, loss: 3.337859055818626e-08\n",
      "step: 22200, loss: 2.622604000634965e-08\n",
      "step: 22201, loss: 0.0\n",
      "step: 22202, loss: 3.814696469817136e-08\n",
      "step: 22203, loss: 1.9073478796372e-08\n",
      "step: 22204, loss: 2.38418573772492e-09\n",
      "step: 22205, loss: 1.1444065250998392e-07\n",
      "step: 22206, loss: 1.5258750352131756e-07\n",
      "step: 22207, loss: 1.430510998545742e-08\n",
      "step: 22208, loss: 2.622603467727913e-08\n",
      "step: 22209, loss: 7.15255676908555e-09\n",
      "step: 22210, loss: 2.38418573772492e-09\n",
      "step: 22211, loss: 4.0531134004595515e-08\n",
      "step: 22212, loss: 9.53674117454284e-09\n",
      "step: 22213, loss: 1.57355785290747e-07\n",
      "step: 22214, loss: 5.722037599298346e-08\n",
      "step: 22215, loss: 1.8834991521998745e-07\n",
      "step: 22216, loss: 1.192092558000013e-08\n",
      "step: 22217, loss: 4.76837103136063e-09\n",
      "step: 22218, loss: 4.76837147544984e-09\n",
      "step: 22219, loss: 1.668929883180681e-08\n",
      "step: 22220, loss: 1.192092558000013e-08\n",
      "step: 22221, loss: 6.914130779023253e-08\n",
      "step: 22222, loss: 1.430510998545742e-08\n",
      "step: 22223, loss: 5.00678503101426e-08\n",
      "step: 22224, loss: 4.76837147544984e-09\n",
      "step: 22225, loss: 1.668929527909313e-08\n",
      "step: 22226, loss: 3.5762766970037774e-08\n",
      "step: 22227, loss: 8.821474750675407e-08\n",
      "step: 22228, loss: 3.8146957592744e-08\n",
      "step: 22229, loss: 2.384185116000026e-08\n",
      "step: 22230, loss: 1.192092824453539e-08\n",
      "step: 22231, loss: 4.76837147544984e-09\n",
      "step: 22232, loss: 0.009428650140762329\n",
      "step: 22233, loss: 2.38418573772492e-09\n",
      "step: 22234, loss: 1.430511264999268e-08\n",
      "step: 22235, loss: 0.08855815976858139\n",
      "step: 22236, loss: 2.622603467727913e-08\n",
      "step: 22237, loss: 1.1205654715240598e-07\n",
      "step: 22238, loss: 7.15255676908555e-09\n",
      "step: 22239, loss: 8.821479013931821e-08\n",
      "step: 22240, loss: 2.6226029348208613e-08\n",
      "step: 22241, loss: 9.53674206272126e-09\n",
      "step: 22242, loss: 1.430511264999268e-08\n",
      "step: 22243, loss: 6.914132200108725e-08\n",
      "step: 22244, loss: 0.00017697957810014486\n",
      "step: 22245, loss: 1.1205641214928619e-07\n",
      "step: 22246, loss: 9.53674206272126e-09\n",
      "step: 22247, loss: 8.106216853320802e-08\n",
      "step: 22248, loss: 2.38418573772492e-09\n",
      "step: 22249, loss: 7.15255632499634e-09\n",
      "step: 22250, loss: 4.52994939337259e-08\n",
      "step: 22251, loss: 7.15255676908555e-09\n",
      "step: 22252, loss: 4.76837147544984e-09\n",
      "step: 22253, loss: 0.0\n",
      "step: 22254, loss: 2.38418573772492e-09\n",
      "step: 22255, loss: 1.192092824453539e-08\n",
      "step: 22256, loss: 1.9310991774545982e-06\n",
      "step: 22257, loss: 2.748791530393646e-06\n",
      "step: 22258, loss: 4.76837103136063e-09\n",
      "step: 22259, loss: 8.583050714605633e-08\n",
      "step: 22260, loss: 7.152546999122933e-08\n",
      "step: 22261, loss: 3.576277762817881e-08\n",
      "step: 22262, loss: 9.53674117454284e-09\n",
      "step: 22263, loss: 1.43051135381711e-08\n",
      "step: 22264, loss: 5.483619602841827e-08\n",
      "step: 22265, loss: 4.76837103136063e-09\n",
      "step: 22266, loss: 1.4305066997621907e-07\n",
      "step: 22267, loss: 4.753501343657263e-06\n",
      "step: 22268, loss: 1.192092646817855e-08\n",
      "step: 22269, loss: 6.67571171675263e-08\n",
      "step: 22270, loss: 5.102093041386979e-07\n",
      "step: 22271, loss: 0.06605149805545807\n",
      "step: 22272, loss: 2.694121121749049e-07\n",
      "step: 22273, loss: 2.8610216418201162e-08\n",
      "step: 22274, loss: 9.53674206272126e-09\n",
      "step: 22275, loss: 4.76837147544984e-09\n",
      "step: 22276, loss: 0.0\n",
      "step: 22277, loss: 5.7220432836402324e-08\n",
      "step: 22278, loss: 1.7642925342897797e-07\n",
      "step: 22279, loss: 0.09003119170665741\n",
      "step: 22280, loss: 1.192092646817855e-08\n",
      "step: 22281, loss: 8.320635060954373e-07\n",
      "step: 22282, loss: 0.04351033270359039\n",
      "step: 22283, loss: 4.2915321074588064e-08\n",
      "step: 22284, loss: 1.668929883180681e-08\n",
      "step: 22285, loss: 0.003275635652244091\n",
      "step: 22286, loss: 5.722041507283393e-08\n",
      "step: 22287, loss: 0.0\n",
      "step: 22288, loss: 1.43051135381711e-08\n",
      "step: 22289, loss: 1.907348234908568e-08\n",
      "step: 22290, loss: 1.668929883180681e-08\n",
      "step: 22291, loss: 4.529951525000797e-08\n",
      "step: 22292, loss: 0.0\n",
      "step: 22293, loss: 0.0\n",
      "step: 22294, loss: 2.086070253426442e-06\n",
      "step: 22295, loss: 7.15255676908555e-09\n",
      "step: 22296, loss: 0.2476295530796051\n",
      "step: 22297, loss: 3.0994396382766354e-08\n",
      "step: 22298, loss: 4.7683688109145805e-08\n",
      "step: 22299, loss: 2.622603290092229e-08\n",
      "step: 22300, loss: 2.38418573772492e-09\n",
      "step: 22301, loss: 1.430511176181426e-08\n",
      "step: 22302, loss: 1.907348234908568e-08\n",
      "step: 22303, loss: 0.02572544291615486\n",
      "step: 22304, loss: 1.668929527909313e-08\n",
      "step: 22305, loss: 0.022714242339134216\n",
      "step: 22306, loss: 3.814695404003032e-08\n",
      "step: 22307, loss: 2.38418573772492e-09\n",
      "step: 22308, loss: 1.6212399600590288e-07\n",
      "step: 22309, loss: 1.907347702001516e-08\n",
      "step: 22310, loss: 1.43051135381711e-08\n",
      "step: 22311, loss: 1.668929527909313e-08\n",
      "step: 22312, loss: 3.5762766970037774e-08\n",
      "step: 22313, loss: 1.6927647550346592e-07\n",
      "step: 22314, loss: 9.53674117454284e-09\n",
      "step: 22315, loss: 1.0013565798772106e-07\n",
      "step: 22316, loss: 3.4093625345121836e-07\n",
      "step: 22317, loss: 3.2901610325097863e-07\n",
      "step: 22318, loss: 4.768367034557741e-08\n",
      "step: 22319, loss: 1.907348234908568e-08\n",
      "step: 22320, loss: 9.53674295089968e-09\n",
      "step: 22321, loss: 2.38418573772492e-09\n",
      "step: 22322, loss: 3.337859055818626e-08\n",
      "step: 22323, loss: 0.10374542325735092\n",
      "step: 22324, loss: 4.76837103136063e-09\n",
      "step: 22325, loss: 5.0067846757428924e-08\n",
      "step: 22326, loss: 0.08460202068090439\n",
      "step: 22327, loss: 2.622603290092229e-08\n",
      "step: 22328, loss: 2.38418529363571e-08\n",
      "step: 22329, loss: 3.099441059362107e-08\n",
      "step: 22330, loss: 0.0\n",
      "step: 22331, loss: 1.668929527909313e-08\n",
      "step: 22332, loss: 0.06009335815906525\n",
      "step: 22333, loss: 2.3841844054572903e-08\n",
      "step: 22334, loss: 9.53674206272126e-09\n",
      "step: 22335, loss: 1.668929527909313e-08\n",
      "step: 22336, loss: 3.337858700547258e-08\n",
      "step: 22337, loss: 9.53674117454284e-09\n",
      "step: 22338, loss: 4.76837103136063e-09\n",
      "step: 22339, loss: 3.576277762817881e-08\n",
      "step: 22340, loss: 1.192092824453539e-08\n",
      "step: 22341, loss: 4.76837147544984e-09\n",
      "step: 22342, loss: 1.9073478796372e-08\n",
      "step: 22343, loss: 9.53674206272126e-09\n",
      "step: 22344, loss: 0.0\n",
      "step: 22345, loss: 0.0\n",
      "step: 22346, loss: 0.0\n",
      "step: 22347, loss: 2.622604000634965e-08\n",
      "step: 22348, loss: 1.9073478796372e-08\n",
      "step: 22349, loss: 2.145766764272139e-08\n",
      "step: 22350, loss: 9.53674295089968e-09\n",
      "step: 22351, loss: 1.907348234908568e-08\n",
      "step: 22352, loss: 5.483623510826874e-08\n",
      "step: 22353, loss: 3.814693627646193e-08\n",
      "step: 22354, loss: 1.192092824453539e-08\n",
      "step: 22355, loss: 0.0\n",
      "step: 22356, loss: 1.9073478796372e-08\n",
      "step: 22357, loss: 2.622604000634965e-08\n",
      "step: 22358, loss: 3.8146957592744e-08\n",
      "step: 22359, loss: 3.2663098181728856e-07\n",
      "step: 22360, loss: 1.192092558000013e-08\n",
      "step: 22361, loss: 9.53674206272126e-09\n",
      "step: 22362, loss: 0.10307751595973969\n",
      "step: 22363, loss: 3.5762759864610416e-08\n",
      "step: 22364, loss: 1.8119771993951872e-07\n",
      "step: 22365, loss: 2.3841741381147585e-07\n",
      "step: 22366, loss: 4.0531134004595515e-08\n",
      "step: 22367, loss: 1.668929527909313e-08\n",
      "step: 22368, loss: 1.192092824453539e-08\n",
      "step: 22369, loss: 4.76837147544984e-09\n",
      "step: 22370, loss: 0.0\n",
      "step: 22371, loss: 3.337857279461787e-08\n",
      "step: 22372, loss: 1.0251974913444428e-07\n",
      "step: 22373, loss: 3.5762766970037774e-08\n",
      "step: 22374, loss: 3.725288166833707e-08\n",
      "step: 22375, loss: 2.622603290092229e-08\n",
      "step: 22376, loss: 2.38418573772492e-09\n",
      "step: 22377, loss: 3.8146946934602965e-08\n",
      "step: 22378, loss: 7.15255676908555e-09\n",
      "step: 22379, loss: 3.576277407546513e-08\n",
      "step: 22380, loss: 0.0008069208124652505\n",
      "step: 22381, loss: 5.245206935455826e-08\n",
      "step: 22382, loss: 8.344643731561519e-08\n",
      "step: 22383, loss: 1.5973982669947873e-07\n",
      "step: 22384, loss: 2.38418573772492e-09\n",
      "step: 22385, loss: 1.192092824453539e-08\n",
      "step: 22386, loss: 0.01475850585848093\n",
      "step: 22387, loss: 0.0762949213385582\n",
      "step: 22388, loss: 5.577412957791239e-05\n",
      "step: 22389, loss: 2.145766764272139e-08\n",
      "step: 22390, loss: 0.04393825680017471\n",
      "step: 22391, loss: 9.53674206272126e-09\n",
      "step: 22392, loss: 1.430511264999268e-08\n",
      "step: 22393, loss: 7.15255676908555e-09\n",
      "step: 22394, loss: 0.011191372759640217\n",
      "step: 22395, loss: 1.907347702001516e-08\n",
      "step: 22396, loss: 1.3112631904732552e-06\n",
      "step: 22397, loss: 4.76837103136063e-09\n",
      "step: 22398, loss: 1.0251993387555558e-07\n",
      "step: 22399, loss: 2.38418573772492e-09\n",
      "step: 22400, loss: 5.936549314355943e-07\n",
      "step: 22401, loss: 2.38418573772492e-09\n",
      "step: 22402, loss: 1.668929350273629e-08\n",
      "step: 22403, loss: 9.53674206272126e-09\n",
      "step: 22404, loss: 0.0\n",
      "step: 22405, loss: 7.15255676908555e-09\n",
      "step: 22406, loss: 9.53674295089968e-09\n",
      "step: 22407, loss: 2.38418573772492e-09\n",
      "step: 22408, loss: 2.145766941907823e-08\n",
      "step: 22409, loss: 4.5299508144580614e-08\n",
      "step: 22410, loss: 8.583058530575727e-08\n",
      "step: 22411, loss: 1.192092824453539e-08\n",
      "step: 22412, loss: 1.430511176181426e-08\n",
      "step: 22413, loss: 1.3351400696137716e-07\n",
      "step: 22414, loss: 0.0\n",
      "step: 22415, loss: 0.0\n",
      "step: 22416, loss: 2.38418573772492e-09\n",
      "step: 22417, loss: 7.15254913075114e-08\n",
      "step: 22418, loss: 7.15255676908555e-09\n",
      "step: 22419, loss: 9.53674295089968e-09\n",
      "step: 22420, loss: 4.529949038101222e-08\n",
      "step: 22421, loss: 2.38418573772492e-09\n",
      "step: 22422, loss: 7.15255676908555e-09\n",
      "step: 22423, loss: 3.337858700547258e-08\n",
      "step: 22424, loss: 1.668929883180681e-08\n",
      "step: 22425, loss: 4.76837103136063e-09\n",
      "step: 22426, loss: 1.907348234908568e-08\n",
      "step: 22427, loss: 1.907348234908568e-08\n",
      "step: 22428, loss: 7.15255676908555e-09\n",
      "step: 22429, loss: 0.0\n",
      "step: 22430, loss: 2.38418573772492e-09\n",
      "step: 22431, loss: 5.483625642455081e-08\n",
      "step: 22432, loss: 2.38418573772492e-09\n",
      "step: 22433, loss: 7.15255676908555e-09\n",
      "step: 22434, loss: 6.675714558923573e-08\n",
      "step: 22435, loss: 0.0\n",
      "step: 22436, loss: 0.0\n",
      "step: 22437, loss: 3.099440704090739e-08\n",
      "step: 22438, loss: 1.907339850504286e-07\n",
      "step: 22439, loss: 9.53674206272126e-09\n",
      "step: 22440, loss: 2.38418573772492e-09\n",
      "step: 22441, loss: 3.337859055818626e-08\n",
      "step: 22442, loss: 4.76837103136063e-09\n",
      "step: 22443, loss: 2.622604000634965e-08\n",
      "step: 22444, loss: 6.705611667712219e-06\n",
      "step: 22445, loss: 4.529951169729429e-08\n",
      "step: 22446, loss: 2.1457660537294032e-08\n",
      "step: 22447, loss: 0.0\n",
      "step: 22448, loss: 1.430511176181426e-08\n",
      "step: 22449, loss: 3.337859055818626e-08\n",
      "step: 22450, loss: 9.53674206272126e-09\n",
      "step: 22451, loss: 4.76837147544984e-09\n",
      "step: 22452, loss: 1.1444078040767636e-07\n",
      "step: 22453, loss: 4.053114111002287e-08\n",
      "step: 22454, loss: 2.38418573772492e-09\n",
      "step: 22455, loss: 6.198880697638742e-08\n",
      "step: 22456, loss: 1.192092558000013e-08\n",
      "step: 22457, loss: 4.76837103136063e-09\n",
      "step: 22458, loss: 4.76837147544984e-09\n",
      "step: 22459, loss: 1.430511264999268e-08\n",
      "step: 22460, loss: 5.9604616353681195e-08\n",
      "step: 22461, loss: 3.981552936238586e-07\n",
      "step: 22462, loss: 0.021209735423326492\n",
      "step: 22463, loss: 4.76837103136063e-09\n",
      "step: 22464, loss: 2.145766764272139e-08\n",
      "step: 22465, loss: 4.76837147544984e-09\n",
      "step: 22466, loss: 1.668929527909313e-08\n",
      "step: 22467, loss: 1.9073478796372e-08\n",
      "step: 22468, loss: 7.15255676908555e-09\n",
      "step: 22469, loss: 7.6293815709505e-08\n",
      "step: 22470, loss: 1.907348234908568e-08\n",
      "step: 22471, loss: 7.15255676908555e-09\n",
      "step: 22472, loss: 1.907347702001516e-08\n",
      "step: 22473, loss: 2.8133197815805033e-07\n",
      "step: 22474, loss: 2.38418573772492e-09\n",
      "step: 22475, loss: 1.5735591318843944e-07\n",
      "step: 22476, loss: 0.0\n",
      "step: 22477, loss: 1.430511264999268e-08\n",
      "step: 22478, loss: 2.574905124674842e-07\n",
      "step: 22479, loss: 1.7881346536796627e-07\n",
      "step: 22480, loss: 9.53674206272126e-09\n",
      "step: 22481, loss: 2.38418573772492e-09\n",
      "step: 22482, loss: 9.53674295089968e-09\n",
      "step: 22483, loss: 1.430511264999268e-08\n",
      "step: 22484, loss: 4.768366679286373e-08\n",
      "step: 22485, loss: 7.820019050086557e-07\n",
      "step: 22486, loss: 1.4137738162389724e-06\n",
      "step: 22487, loss: 1.192092824453539e-08\n",
      "step: 22488, loss: 3.576275631189674e-08\n",
      "step: 22489, loss: 1.430511264999268e-08\n",
      "step: 22490, loss: 5.865044272468367e-07\n",
      "step: 22491, loss: 1.7881335168112855e-07\n",
      "step: 22492, loss: 2.145766764272139e-08\n",
      "step: 22493, loss: 2.38418573772492e-09\n",
      "step: 22494, loss: 2.0027137281886098e-07\n",
      "step: 22495, loss: 5.96045985901128e-08\n",
      "step: 22496, loss: 6.675711006209895e-08\n",
      "step: 22497, loss: 0.0\n",
      "step: 22498, loss: 2.38418573772492e-09\n",
      "step: 22499, loss: 9.298303638161087e-08\n",
      "step: 22500, loss: 7.62938654474965e-08\n",
      "step: 22501, loss: 9.53674117454284e-09\n",
      "step: 22502, loss: 1.02519862821282e-07\n",
      "step: 22503, loss: 1.0251977045072636e-07\n",
      "step: 22504, loss: 4.053114111002287e-08\n",
      "step: 22505, loss: 1.835817187156863e-07\n",
      "step: 22506, loss: 5.006786452099732e-08\n",
      "step: 22507, loss: 1.6950849612840102e-06\n",
      "step: 22508, loss: 4.6967960543042864e-07\n",
      "step: 22509, loss: 8.344635915591425e-08\n",
      "step: 22510, loss: 1.9073402768299275e-07\n",
      "step: 22511, loss: 0.0\n",
      "step: 22512, loss: 6.437296207195686e-08\n",
      "step: 22513, loss: 2.145766764272139e-08\n",
      "step: 22514, loss: 0.10023894160985947\n",
      "step: 22515, loss: 4.2676484213188814e-07\n",
      "step: 22516, loss: 1.907347702001516e-08\n",
      "step: 22517, loss: 7.390968903564499e-08\n",
      "step: 22518, loss: 0.0327255018055439\n",
      "step: 22519, loss: 0.0\n",
      "step: 22520, loss: 0.10403880476951599\n",
      "step: 22521, loss: 2.7179552830602915e-07\n",
      "step: 22522, loss: 4.768366679286373e-08\n",
      "step: 22523, loss: 0.0\n",
      "step: 22524, loss: 2.38418573772492e-09\n",
      "step: 22525, loss: 4.76837103136063e-09\n",
      "step: 22526, loss: 4.0531130451881836e-08\n",
      "step: 22527, loss: 7.15255676908555e-09\n",
      "step: 22528, loss: 4.76837103136063e-09\n",
      "step: 22529, loss: 1.907348234908568e-08\n",
      "step: 22530, loss: 2.38418573772492e-09\n",
      "step: 22531, loss: 1.2159323148352996e-07\n",
      "step: 22532, loss: 1.907348234908568e-08\n",
      "step: 22533, loss: 3.5762766970037774e-08\n",
      "step: 22534, loss: 3.099440348819371e-08\n",
      "step: 22535, loss: 2.38418573772492e-09\n",
      "step: 22536, loss: 5.483625642455081e-08\n",
      "step: 22537, loss: 6.675714558923573e-08\n",
      "step: 22538, loss: 0.0\n",
      "step: 22539, loss: 1.740451693876821e-07\n",
      "step: 22540, loss: 1.192092558000013e-08\n",
      "step: 22541, loss: 2.38418529363571e-08\n",
      "step: 22542, loss: 9.53674117454284e-09\n",
      "step: 22543, loss: 4.76837103136063e-09\n",
      "step: 22544, loss: 3.242467983000097e-07\n",
      "step: 22545, loss: 0.0\n",
      "step: 22546, loss: 2.38418573772492e-09\n",
      "step: 22547, loss: 5.245206935455826e-08\n",
      "step: 22548, loss: 4.293601250537904e-06\n",
      "step: 22549, loss: 9.53674206272126e-09\n",
      "step: 22550, loss: 2.145766586636455e-08\n",
      "step: 22551, loss: 0.009174536913633347\n",
      "step: 22552, loss: 0.0\n",
      "step: 22553, loss: 7.15255676908555e-09\n",
      "step: 22554, loss: 1.2159311779669224e-07\n",
      "step: 22555, loss: 9.965691560864798e-07\n",
      "step: 22556, loss: 3.2424836149402836e-07\n",
      "step: 22557, loss: 2.38418573772492e-09\n",
      "step: 22558, loss: 7.15255676908555e-09\n",
      "step: 22559, loss: 1.2636157009637827e-07\n",
      "step: 22560, loss: 0.002311126096174121\n",
      "step: 22561, loss: 5.722041507283393e-08\n",
      "step: 22562, loss: 2.38418573772492e-09\n",
      "step: 22563, loss: 2.0503948405803385e-07\n",
      "step: 22564, loss: 0.0\n",
      "step: 22565, loss: 0.08152926713228226\n",
      "step: 22566, loss: 2.145766764272139e-08\n",
      "step: 22567, loss: 2.69411259523622e-07\n",
      "step: 22568, loss: 1.430511264999268e-08\n",
      "step: 22569, loss: 2.38418573772492e-09\n",
      "step: 22570, loss: 0.05874519422650337\n",
      "step: 22571, loss: 4.053112334645448e-08\n",
      "step: 22572, loss: 9.53674295089968e-09\n",
      "step: 22573, loss: 4.768369876728684e-08\n",
      "step: 22574, loss: 9.53674295089968e-09\n",
      "step: 22575, loss: 3.0517495019921625e-07\n",
      "step: 22576, loss: 0.21474969387054443\n",
      "step: 22577, loss: 4.0531130451881836e-08\n",
      "step: 22578, loss: 2.38418573772492e-09\n",
      "step: 22579, loss: 9.059895234031501e-08\n",
      "step: 22580, loss: 4.053114821545023e-08\n",
      "step: 22581, loss: 3.8146950487316644e-08\n",
      "step: 22582, loss: 3.313991783215897e-07\n",
      "step: 22583, loss: 2.38418573772492e-09\n",
      "step: 22584, loss: 6.914132200108725e-08\n",
      "step: 22585, loss: 2.38418573772492e-09\n",
      "step: 22586, loss: 3.933875518669083e-07\n",
      "step: 22587, loss: 7.15255676908555e-09\n",
      "step: 22588, loss: 3.099439993548003e-08\n",
      "step: 22589, loss: 0.028955990448594093\n",
      "step: 22590, loss: 1.5735589897758473e-07\n",
      "step: 22591, loss: 1.907348234908568e-08\n",
      "step: 22592, loss: 5.96045985901128e-08\n",
      "step: 22593, loss: 0.05527755245566368\n",
      "step: 22594, loss: 8.583051425148369e-08\n",
      "step: 22595, loss: 0.0\n",
      "step: 22596, loss: 9.53674206272126e-09\n",
      "step: 22597, loss: 0.18012109398841858\n",
      "step: 22598, loss: 2.384184938364342e-08\n",
      "step: 22599, loss: 8.344640889390575e-08\n",
      "step: 22600, loss: 5.774196765173656e-08\n",
      "step: 22601, loss: 2.861021997091484e-08\n",
      "step: 22602, loss: 5.483619602841827e-08\n",
      "step: 22603, loss: 9.53674117454284e-09\n",
      "step: 22604, loss: 0.0737907662987709\n",
      "step: 22605, loss: 4.76837147544984e-09\n",
      "step: 22606, loss: 1.430511176181426e-08\n",
      "step: 22607, loss: 2.38418573772492e-09\n",
      "step: 22608, loss: 2.861002883491892e-07\n",
      "step: 22609, loss: 1.1444084435652258e-07\n",
      "step: 22610, loss: 1.597400824948636e-07\n",
      "step: 22611, loss: 0.04350586235523224\n",
      "step: 22612, loss: 4.3153301021447987e-07\n",
      "step: 22613, loss: 1.430511264999268e-08\n",
      "step: 22614, loss: 9.53674295089968e-09\n",
      "step: 22615, loss: 5.483623510826874e-08\n",
      "step: 22616, loss: 3.099441059362107e-08\n",
      "step: 22617, loss: 2.145766231365087e-08\n",
      "step: 22618, loss: 5.981126832921291e-06\n",
      "step: 22619, loss: 6.437296207195686e-08\n",
      "step: 22620, loss: 0.18447601795196533\n",
      "step: 22621, loss: 1.1444066672083864e-07\n",
      "step: 22622, loss: 1.7166088639442023e-07\n",
      "step: 22623, loss: 1.2636144219868584e-07\n",
      "step: 22624, loss: 1.668929883180681e-08\n",
      "step: 22625, loss: 4.76837103136063e-09\n",
      "step: 22626, loss: 2.38418573772492e-09\n",
      "step: 22627, loss: 6.580266926903278e-07\n",
      "step: 22628, loss: 4.768367389829109e-08\n",
      "step: 22629, loss: 3.576277407546513e-08\n",
      "step: 22630, loss: 3.099421235219779e-07\n",
      "step: 22631, loss: 9.53674206272126e-09\n",
      "step: 22632, loss: 1.4305076945220208e-07\n",
      "step: 22633, loss: 3.814696469817136e-08\n",
      "step: 22634, loss: 4.76837147544984e-09\n",
      "step: 22635, loss: 0.0\n",
      "step: 22636, loss: 1.430511264999268e-08\n",
      "step: 22637, loss: 0.10140246152877808\n",
      "step: 22638, loss: 1.0251974913444428e-07\n",
      "step: 22639, loss: 6.198878566010535e-08\n",
      "step: 22640, loss: 7.15255676908555e-09\n",
      "step: 22641, loss: 3.5762763417324095e-08\n",
      "step: 22642, loss: 6.914135042279668e-08\n",
      "step: 22643, loss: 5.722037599298346e-08\n",
      "step: 22644, loss: 4.76837103136063e-09\n",
      "step: 22645, loss: 4.76837147544984e-09\n",
      "step: 22646, loss: 7.390963219222613e-08\n",
      "step: 22647, loss: 3.337859055818626e-08\n",
      "step: 22648, loss: 4.76837147544984e-09\n",
      "step: 22649, loss: 2.9325286732273526e-07\n",
      "step: 22650, loss: 3.337859055818626e-08\n",
      "step: 22651, loss: 3.004058442002133e-07\n",
      "step: 22652, loss: 7.152552683464819e-08\n",
      "step: 22653, loss: 7.629387965835122e-08\n",
      "step: 22654, loss: 7.15255676908555e-09\n",
      "step: 22655, loss: 1.430509684041681e-07\n",
      "step: 22656, loss: 5.483623510826874e-08\n",
      "step: 22657, loss: 7.39097032464997e-08\n",
      "step: 22658, loss: 7.867804185934801e-08\n",
      "step: 22659, loss: 1.668929883180681e-08\n",
      "step: 22660, loss: 1.0728809485271995e-07\n",
      "step: 22661, loss: 1.430511264999268e-08\n",
      "step: 22662, loss: 3.099440704090739e-08\n",
      "step: 22663, loss: 2.38418573772492e-09\n",
      "step: 22664, loss: 4.76837147544984e-09\n",
      "step: 22665, loss: 4.053112334645448e-08\n",
      "step: 22666, loss: 8.583054267319312e-08\n",
      "step: 22667, loss: 6.914135042279668e-08\n",
      "step: 22668, loss: 4.76837147544984e-09\n",
      "step: 22669, loss: 1.668929883180681e-08\n",
      "step: 22670, loss: 5.245206935455826e-08\n",
      "step: 22671, loss: 7.15255676908555e-09\n",
      "step: 22672, loss: 6.675709585124423e-08\n",
      "step: 22673, loss: 2.38418573772492e-09\n",
      "step: 22674, loss: 4.76837103136063e-09\n",
      "step: 22675, loss: 3.576277407546513e-08\n",
      "step: 22676, loss: 2.861005725662835e-07\n",
      "step: 22677, loss: 1.43051135381711e-08\n",
      "step: 22678, loss: 2.4318546820722986e-07\n",
      "step: 22679, loss: 1.0251979176700843e-07\n",
      "step: 22680, loss: 1.430511176181426e-08\n",
      "step: 22681, loss: 1.192092558000013e-08\n",
      "step: 22682, loss: 4.76837147544984e-09\n",
      "step: 22683, loss: 5.9604612800967516e-08\n",
      "step: 22684, loss: 1.668929527909313e-08\n",
      "step: 22685, loss: 0.0\n",
      "step: 22686, loss: 2.38418573772492e-09\n",
      "step: 22687, loss: 3.814693627646193e-08\n",
      "step: 22688, loss: 0.0\n",
      "step: 22689, loss: 0.0\n",
      "step: 22690, loss: 4.529951169729429e-08\n",
      "step: 22691, loss: 0.00011229166557313874\n",
      "step: 22692, loss: 7.15255676908555e-09\n",
      "step: 22693, loss: 9.53674295089968e-09\n",
      "step: 22694, loss: 2.145766764272139e-08\n",
      "step: 22695, loss: 7.152552683464819e-08\n",
      "step: 22696, loss: 3.3140003097287263e-07\n",
      "step: 22697, loss: 1.192092824453539e-08\n",
      "step: 22698, loss: 9.53674295089968e-09\n",
      "step: 22699, loss: 1.692765607685942e-07\n",
      "step: 22700, loss: 9.53674206272126e-09\n",
      "step: 22701, loss: 1.0013566509314842e-07\n",
      "step: 22702, loss: 7.15255632499634e-09\n",
      "step: 22703, loss: 0.0\n",
      "step: 22704, loss: 9.53674295089968e-09\n",
      "step: 22705, loss: 7.15255632499634e-09\n",
      "step: 22706, loss: 0.030833499506115913\n",
      "step: 22707, loss: 2.0265491684767767e-07\n",
      "step: 22708, loss: 2.622603467727913e-08\n",
      "step: 22709, loss: 2.145766231365087e-08\n",
      "step: 22710, loss: 1.430511264999268e-08\n",
      "step: 22711, loss: 2.38418573772492e-09\n",
      "step: 22712, loss: 0.0\n",
      "step: 22713, loss: 4.76837103136063e-09\n",
      "step: 22714, loss: 0.0005062224809080362\n",
      "step: 22715, loss: 2.861021997091484e-08\n",
      "step: 22716, loss: 0.11878630518913269\n",
      "step: 22717, loss: 7.152545578037461e-08\n",
      "step: 22718, loss: 2.145766586636455e-08\n",
      "step: 22719, loss: 5.483623510826874e-08\n",
      "step: 22720, loss: 9.53674206272126e-09\n",
      "step: 22721, loss: 7.15255676908555e-09\n",
      "step: 22722, loss: 3.099441059362107e-08\n",
      "step: 22723, loss: 2.38418573772492e-09\n",
      "step: 22724, loss: 4.160388925811276e-05\n",
      "step: 22725, loss: 7.15255632499634e-09\n",
      "step: 22726, loss: 3.814677711488912e-07\n",
      "step: 22727, loss: 1.3828250189362734e-07\n",
      "step: 22728, loss: 7.15255676908555e-09\n",
      "step: 22729, loss: 1.0728825117212182e-07\n",
      "step: 22730, loss: 2.3841845830929742e-08\n",
      "step: 22731, loss: 6.246481802918424e-07\n",
      "step: 22732, loss: 0.07736137509346008\n",
      "step: 22733, loss: 9.53674295089968e-09\n",
      "step: 22734, loss: 8.344634494505954e-08\n",
      "step: 22735, loss: 0.00016251407214440405\n",
      "step: 22736, loss: 9.53674206272126e-09\n",
      "step: 22737, loss: 2.622603645363597e-08\n",
      "step: 22738, loss: 0.06088137626647949\n",
      "step: 22739, loss: 2.336492315180294e-07\n",
      "step: 22740, loss: 2.38418573772492e-09\n",
      "step: 22741, loss: 2.38418573772492e-09\n",
      "step: 22742, loss: 9.53674206272126e-09\n",
      "step: 22743, loss: 5.00678503101426e-08\n",
      "step: 22744, loss: 2.384185116000026e-08\n",
      "step: 22745, loss: 0.0004546802374534309\n",
      "step: 22746, loss: 6.91408104103175e-07\n",
      "step: 22747, loss: 7.15255676908555e-09\n",
      "step: 22748, loss: 3.049212864425499e-06\n",
      "step: 22749, loss: 2.38418573772492e-09\n",
      "step: 22750, loss: 4.76837147544984e-09\n",
      "step: 22751, loss: 2.861022529998536e-08\n",
      "step: 22752, loss: 2.0265554212528514e-07\n",
      "step: 22753, loss: 2.7656386691887747e-07\n",
      "step: 22754, loss: 0.00012745929416269064\n",
      "step: 22755, loss: 3.0994396382766354e-08\n",
      "step: 22756, loss: 1.1836373232654296e-05\n",
      "step: 22757, loss: 3.749909228645265e-05\n",
      "step: 22758, loss: 1.8596570328099915e-07\n",
      "step: 22759, loss: 5.48362244501277e-08\n",
      "step: 22760, loss: 2.622593058276834e-07\n",
      "step: 22761, loss: 1.668929350273629e-08\n",
      "step: 22762, loss: 2.574905977326125e-07\n",
      "step: 22763, loss: 4.76837103136063e-09\n",
      "step: 22764, loss: 6.914135042279668e-08\n",
      "step: 22765, loss: 2.1265900613798294e-06\n",
      "step: 22766, loss: 1.3589811942438246e-07\n",
      "step: 22767, loss: 6.437296917738422e-08\n",
      "step: 22768, loss: 9.775154552471577e-08\n",
      "step: 22769, loss: 4.484170403884491e-06\n",
      "step: 22770, loss: 0.0768657997250557\n",
      "step: 22771, loss: 5.268982476991368e-07\n",
      "step: 22772, loss: 2.2411288114199124e-07\n",
      "step: 22773, loss: 2.932539189259842e-07\n",
      "step: 22774, loss: 6.198875013296856e-08\n",
      "step: 22775, loss: 2.622603467727913e-08\n",
      "step: 22776, loss: 6.675709585124423e-08\n",
      "step: 22777, loss: 0.0002352981682633981\n",
      "step: 22778, loss: 9.53674206272126e-09\n",
      "step: 22779, loss: 1.3303420018928591e-06\n",
      "step: 22780, loss: 6.914133621194196e-08\n",
      "step: 22781, loss: 0.10931084305047989\n",
      "step: 22782, loss: 7.15255676908555e-09\n",
      "step: 22783, loss: 0.0\n",
      "step: 22784, loss: 7.152546999122933e-08\n",
      "step: 22785, loss: 5.531292117666453e-07\n",
      "step: 22786, loss: 0.0\n",
      "step: 22787, loss: 6.127263532107463e-07\n",
      "step: 22788, loss: 2.264963967490985e-07\n",
      "step: 22789, loss: 3.3614978747209534e-05\n",
      "step: 22790, loss: 1.907348234908568e-08\n",
      "step: 22791, loss: 0.0\n",
      "step: 22792, loss: 7.15255632499634e-09\n",
      "step: 22793, loss: 3.9338897295237985e-07\n",
      "step: 22794, loss: 9.53674117454284e-09\n",
      "step: 22795, loss: 9.53674206272126e-09\n",
      "step: 22796, loss: 6.75814226269722e-06\n",
      "step: 22797, loss: 3.123272733773774e-07\n",
      "step: 22798, loss: 1.611647803656524e-06\n",
      "step: 22799, loss: 1.668929883180681e-08\n",
      "step: 22800, loss: 3.0992887332104146e-06\n",
      "step: 22801, loss: 2.3841844054572903e-08\n",
      "step: 22802, loss: 3.0755870739085367e-07\n",
      "step: 22803, loss: 1.120565116252692e-07\n",
      "step: 22804, loss: 0.0001135363127104938\n",
      "step: 22805, loss: 4.855478255194612e-05\n",
      "step: 22806, loss: 1.2874578203536657e-07\n",
      "step: 22807, loss: 3.099440704090739e-08\n",
      "step: 22808, loss: 1.907347702001516e-08\n",
      "step: 22809, loss: 5.555082225328078e-07\n",
      "step: 22810, loss: 0.023152120411396027\n",
      "step: 22811, loss: 5.006788228456571e-08\n",
      "step: 22812, loss: 0.09250422567129135\n",
      "step: 22813, loss: 1.430511264999268e-08\n",
      "step: 22814, loss: 0.0\n",
      "step: 22815, loss: 6.580305580428103e-07\n",
      "step: 22816, loss: 2.1696041585528292e-07\n",
      "step: 22817, loss: 7.867807028105744e-08\n",
      "step: 22818, loss: 6.600725100724958e-06\n",
      "step: 22819, loss: 4.649135973977536e-07\n",
      "step: 22820, loss: 0.20098638534545898\n",
      "step: 22821, loss: 9.679641834736685e-07\n",
      "step: 22822, loss: 1.6497887145305867e-06\n",
      "step: 22823, loss: 0.0\n",
      "step: 22824, loss: 4.339173642620153e-07\n",
      "step: 22825, loss: 1.0013561535515692e-07\n",
      "step: 22826, loss: 1.4528603742292034e-07\n",
      "step: 22827, loss: 1.907348234908568e-08\n",
      "step: 22828, loss: 0.0\n",
      "step: 22829, loss: 5.722042217826129e-08\n",
      "step: 22830, loss: 1.192092824453539e-08\n",
      "step: 22831, loss: 2.622603645363597e-08\n",
      "step: 22832, loss: 3.6954693882762513e-07\n",
      "step: 22833, loss: 9.53674206272126e-09\n",
      "step: 22834, loss: 2.1457660537294032e-08\n",
      "step: 22835, loss: 3.8623559817096975e-07\n",
      "step: 22836, loss: 8.583057820032991e-08\n",
      "step: 22837, loss: 5.085008979222039e-06\n",
      "step: 22838, loss: 8.583064214917613e-08\n",
      "step: 22839, loss: 2.3364951573512371e-07\n",
      "step: 22840, loss: 7.15255632499634e-09\n",
      "step: 22841, loss: 9.059893102403294e-08\n",
      "step: 22842, loss: 3.790827349803294e-07\n",
      "step: 22843, loss: 3.0994396382766354e-08\n",
      "step: 22844, loss: 0.08847329020500183\n",
      "step: 22845, loss: 5.722038309841082e-08\n",
      "step: 22846, loss: 1.914417225634679e-06\n",
      "step: 22847, loss: 2.538997023293632e-06\n",
      "step: 22848, loss: 1.382822887308066e-07\n",
      "step: 22849, loss: 1.7427643115297542e-06\n",
      "step: 22850, loss: 0.062203336507081985\n",
      "step: 22851, loss: 2.38418573772492e-09\n",
      "step: 22852, loss: 3.981553788889869e-07\n",
      "step: 22853, loss: 8.821481145560028e-08\n",
      "step: 22854, loss: 2.38418529363571e-08\n",
      "step: 22855, loss: 1.1372246717655798e-06\n",
      "step: 22856, loss: 1.9550272156720894e-07\n",
      "step: 22857, loss: 1.6760159269324504e-06\n",
      "step: 22858, loss: 4.291532462730174e-08\n",
      "step: 22859, loss: 3.9338746660178003e-07\n",
      "step: 22860, loss: 2.2172875446813123e-07\n",
      "step: 22861, loss: 1.668929350273629e-08\n",
      "step: 22862, loss: 1.3828257294790092e-07\n",
      "step: 22863, loss: 0.13294845819473267\n",
      "step: 22864, loss: 4.24380971253413e-07\n",
      "step: 22865, loss: 9.775150999757898e-08\n",
      "step: 22866, loss: 4.291499635655782e-07\n",
      "step: 22867, loss: 9.53674206272126e-09\n",
      "step: 22868, loss: 2.3841845830929742e-08\n",
      "step: 22869, loss: 2.38418573772492e-09\n",
      "step: 22870, loss: 7.390971035192706e-08\n",
      "step: 22871, loss: 0.0\n",
      "step: 22872, loss: 8.583052135691105e-08\n",
      "step: 22873, loss: 3.7382558275567135e-06\n",
      "step: 22874, loss: 1.063327999872854e-06\n",
      "step: 22875, loss: 1.8596561801587086e-07\n",
      "step: 22876, loss: 2.38418529363571e-08\n",
      "step: 22877, loss: 8.106215432235331e-08\n",
      "step: 22878, loss: 7.15255632499634e-09\n",
      "step: 22879, loss: 8.106219695491745e-08\n",
      "step: 22880, loss: 4.2915321074588064e-08\n",
      "step: 22881, loss: 0.13694652915000916\n",
      "step: 22882, loss: 8.344639468305104e-08\n",
      "step: 22883, loss: 5.483622800284138e-08\n",
      "step: 22884, loss: 9.53674206272126e-09\n",
      "step: 22885, loss: 1.022790002025431e-06\n",
      "step: 22886, loss: 0.003949938341975212\n",
      "step: 22887, loss: 6.413361006707419e-07\n",
      "step: 22888, loss: 5.245204093284883e-08\n",
      "step: 22889, loss: 4.76837103136063e-09\n",
      "step: 22890, loss: 2.8610218194558e-08\n",
      "step: 22891, loss: 0.0\n",
      "step: 22892, loss: 2.861022529998536e-08\n",
      "step: 22893, loss: 4.5299501039153256e-08\n",
      "step: 22894, loss: 1.6927685919654323e-07\n",
      "step: 22895, loss: 8.439923249170533e-07\n",
      "step: 22896, loss: 1.1205641214928619e-07\n",
      "step: 22897, loss: 0.03729454427957535\n",
      "step: 22898, loss: 5.7220432836402324e-08\n",
      "step: 22899, loss: 4.7683688109145805e-08\n",
      "step: 22900, loss: 1.668929350273629e-08\n",
      "step: 22901, loss: 7.15255676908555e-09\n",
      "step: 22902, loss: 2.145766586636455e-08\n",
      "step: 22903, loss: 6.675713137838102e-08\n",
      "step: 22904, loss: 1.983561105589615e-06\n",
      "step: 22905, loss: 5.722039020383818e-08\n",
      "step: 22906, loss: 1.9311825383283576e-07\n",
      "step: 22907, loss: 5.388187105381803e-07\n",
      "step: 22908, loss: 4.76837147544984e-09\n",
      "step: 22909, loss: 2.288808929051811e-07\n",
      "step: 22910, loss: 6.210322226252174e-06\n",
      "step: 22911, loss: 4.0292434277944267e-07\n",
      "step: 22912, loss: 7.867807028105744e-08\n",
      "step: 22913, loss: 1.3113009345033788e-07\n",
      "step: 22914, loss: 4.053114111002287e-08\n",
      "step: 22915, loss: 5.2452033827421474e-08\n",
      "step: 22916, loss: 1.192092824453539e-08\n",
      "step: 22917, loss: 0.0006453369278460741\n",
      "step: 22918, loss: 1.7642935290496098e-07\n",
      "step: 22919, loss: 4.891759544989327e-06\n",
      "step: 22920, loss: 2.38418573772492e-09\n",
      "step: 22921, loss: 1.3137781024852302e-05\n",
      "step: 22922, loss: 9.53674206272126e-09\n",
      "step: 22923, loss: 4.76837103136063e-09\n",
      "step: 22924, loss: 2.503381608676136e-07\n",
      "step: 22925, loss: 2.1219176460363087e-07\n",
      "step: 22926, loss: 0.0\n",
      "step: 22927, loss: 2.38418573772492e-09\n",
      "step: 22928, loss: 1.6903405821722117e-06\n",
      "step: 22929, loss: 8.855291525833309e-06\n",
      "step: 22930, loss: 1.192092646817855e-08\n",
      "step: 22931, loss: 7.15255676908555e-09\n",
      "step: 22932, loss: 2.6534582957538078e-06\n",
      "step: 22933, loss: 2.8610218194558e-08\n",
      "step: 22934, loss: 8.583062793832141e-08\n",
      "step: 22935, loss: 5.221321544013335e-07\n",
      "step: 22936, loss: 6.127283995738253e-07\n",
      "step: 22937, loss: 3.1709535619484086e-07\n",
      "step: 22938, loss: 4.5299501039153256e-08\n",
      "step: 22939, loss: 4.625267422397883e-07\n",
      "step: 22940, loss: 1.358981478460919e-07\n",
      "step: 22941, loss: 0.06634418666362762\n",
      "step: 22942, loss: 5.006786452099732e-08\n",
      "step: 22943, loss: 4.053112689916816e-08\n",
      "step: 22944, loss: 2.798888090183027e-06\n",
      "step: 22945, loss: 7.009447244854528e-07\n",
      "step: 22946, loss: 1.192092646817855e-08\n",
      "step: 22947, loss: 0.0016896816669031978\n",
      "step: 22948, loss: 9.775154552471577e-08\n",
      "step: 22949, loss: 2.2888133344167727e-07\n",
      "step: 22950, loss: 3.46870342582406e-06\n",
      "step: 22951, loss: 1.5974015354913718e-07\n",
      "step: 22952, loss: 0.07611361145973206\n",
      "step: 22953, loss: 3.5762766970037774e-08\n",
      "step: 22954, loss: 4.76837147544984e-09\n",
      "step: 22955, loss: 2.2411309430481197e-07\n",
      "step: 22956, loss: 0.06724400073289871\n",
      "step: 22957, loss: 0.10612034052610397\n",
      "step: 22958, loss: 0.0030375877395272255\n",
      "step: 22959, loss: 4.76837147544984e-09\n",
      "step: 22960, loss: 3.17310559694306e-06\n",
      "step: 22961, loss: 3.2040925361798145e-06\n",
      "step: 22962, loss: 4.76837103136063e-09\n",
      "step: 22963, loss: 3.0994389277338996e-08\n",
      "step: 22964, loss: 6.789736289647408e-06\n",
      "step: 22965, loss: 1.907348412544252e-08\n",
      "step: 22966, loss: 2.8610209312773804e-08\n",
      "step: 22967, loss: 0.06421893835067749\n",
      "step: 22968, loss: 2.6226029348208613e-08\n",
      "step: 22969, loss: 1.1205647609813241e-07\n",
      "step: 22970, loss: 7.867798501592915e-08\n",
      "step: 22971, loss: 8.795802386885043e-06\n",
      "step: 22972, loss: 2.717957556797046e-07\n",
      "step: 22973, loss: 2.908686838054564e-07\n",
      "step: 22974, loss: 2.1696010321647918e-07\n",
      "step: 22975, loss: 1.332716010438162e-06\n",
      "step: 22976, loss: 5.96045985901128e-08\n",
      "step: 22977, loss: 1.406665859349232e-07\n",
      "step: 22978, loss: 5.936578872933751e-07\n",
      "step: 22979, loss: 1.5544439975201385e-06\n",
      "step: 22980, loss: 5.960378643976583e-07\n",
      "step: 22981, loss: 2.7965120352746453e-06\n",
      "step: 22982, loss: 7.152551972922083e-08\n",
      "step: 22983, loss: 3.9338709711955744e-07\n",
      "step: 22984, loss: 4.768369876728684e-08\n",
      "step: 22985, loss: 1.5020336263660283e-07\n",
      "step: 22986, loss: 0.0\n",
      "step: 22987, loss: 6.448318799812114e-06\n",
      "step: 22988, loss: 4.982908876627334e-07\n",
      "step: 22989, loss: 9.059888839146879e-08\n",
      "step: 22990, loss: 2.622603290092229e-08\n",
      "step: 22991, loss: 1.668929705544997e-08\n",
      "step: 22992, loss: 1.430511264999268e-08\n",
      "step: 22993, loss: 1.430511176181426e-08\n",
      "step: 22994, loss: 2.145762891814229e-07\n",
      "step: 22995, loss: 5.268984750728123e-07\n",
      "step: 22996, loss: 0.09017085283994675\n",
      "step: 22997, loss: 4.291532462730174e-08\n",
      "step: 22998, loss: 0.06501185148954391\n",
      "step: 22999, loss: 9.53674117454284e-09\n",
      "step: 23000, loss: 3.6716210161102936e-07\n",
      "step: 23001, loss: 7.15255676908555e-09\n",
      "step: 23002, loss: 2.4284079700009897e-05\n",
      "step: 23003, loss: 6.651789590250701e-07\n",
      "step: 23004, loss: 1.096698042601929e-06\n",
      "step: 23005, loss: 3.0517384175254847e-07\n",
      "step: 23006, loss: 0.0\n",
      "step: 23007, loss: 2.310144964212668e-06\n",
      "step: 23008, loss: 2.698717480598134e-06\n",
      "step: 23009, loss: 5.2452058696417225e-08\n",
      "step: 23010, loss: 2.3399272322421893e-05\n",
      "step: 23011, loss: 4.3153326600986475e-07\n",
      "step: 23012, loss: 7.152544867494726e-08\n",
      "step: 23013, loss: 6.684158051939448e-06\n",
      "step: 23014, loss: 2.38418529363571e-08\n",
      "step: 23015, loss: 7.234021904878318e-05\n",
      "step: 23016, loss: 2.38418529363571e-08\n",
      "step: 23017, loss: 1.430508973498945e-07\n",
      "step: 23018, loss: 3.0279062457339023e-07\n",
      "step: 23019, loss: 0.0\n",
      "step: 23020, loss: 4.291511856990837e-07\n",
      "step: 23021, loss: 4.76837103136063e-09\n",
      "step: 23022, loss: 6.369950824591797e-06\n",
      "step: 23023, loss: 1.43051135381711e-08\n",
      "step: 23024, loss: 1.625960408091487e-06\n",
      "step: 23025, loss: 9.119991773332004e-06\n",
      "step: 23026, loss: 6.079584409235395e-07\n",
      "step: 23027, loss: 2.38418573772492e-09\n",
      "step: 23028, loss: 1.430510998545742e-08\n",
      "step: 23029, loss: 6.127269216449349e-07\n",
      "step: 23030, loss: 4.7683691661859484e-08\n",
      "step: 23031, loss: 2.717962104270555e-07\n",
      "step: 23032, loss: 5.187390343053266e-06\n",
      "step: 23033, loss: 9.512777410236595e-07\n",
      "step: 23034, loss: 3.123260512438719e-07\n",
      "step: 23035, loss: 4.29153317327291e-08\n",
      "step: 23036, loss: 2.861002599274798e-07\n",
      "step: 23037, loss: 2.932532936483767e-07\n",
      "step: 23038, loss: 9.235265315510333e-05\n",
      "step: 23039, loss: 1.907348234908568e-08\n",
      "step: 23040, loss: 5.7220425730974966e-08\n",
      "step: 23041, loss: 6.198879276553271e-08\n",
      "step: 23042, loss: 7.9973624451668e-06\n",
      "step: 23043, loss: 0.00118060945533216\n",
      "step: 23044, loss: 4.7206413000822067e-07\n",
      "step: 23045, loss: 8.153755857165379e-07\n",
      "step: 23046, loss: 3.433203232816595e-07\n",
      "step: 23047, loss: 7.71421764511615e-06\n",
      "step: 23048, loss: 0.10853385180234909\n",
      "step: 23049, loss: 1.907348234908568e-08\n",
      "step: 23050, loss: 4.935216111334739e-07\n",
      "step: 23051, loss: 1.7404485674887837e-07\n",
      "step: 23052, loss: 5.718223974326975e-07\n",
      "step: 23053, loss: 0.0006521056056953967\n",
      "step: 23054, loss: 1.430511264999268e-08\n",
      "step: 23055, loss: 2.384184938364342e-08\n",
      "step: 23056, loss: 2.2888059447723208e-07\n",
      "step: 23057, loss: 1.0680930699891178e-06\n",
      "step: 23058, loss: 1.6450853479454963e-07\n",
      "step: 23059, loss: 2.358247911615763e-05\n",
      "step: 23060, loss: 1.907348234908568e-08\n",
      "step: 23061, loss: 1.9955143670813413e-06\n",
      "step: 23062, loss: 1.287456257159647e-07\n",
      "step: 23063, loss: 5.722038309841082e-08\n",
      "step: 23064, loss: 9.298302927618352e-08\n",
      "step: 23065, loss: 2.6226025795494934e-08\n",
      "step: 23066, loss: 2.384185116000026e-08\n",
      "step: 23067, loss: 3.5762766970037774e-08\n",
      "step: 23068, loss: 7.390971035192706e-08\n",
      "step: 23069, loss: 9.53674295089968e-09\n",
      "step: 23070, loss: 4.55592680737027e-06\n",
      "step: 23071, loss: 4.3149111661477946e-06\n",
      "step: 23072, loss: 1.192092824453539e-08\n",
      "step: 23073, loss: 1.2278258054720936e-06\n",
      "step: 23074, loss: 1.192092646817855e-08\n",
      "step: 23075, loss: 1.168248857652543e-07\n",
      "step: 23076, loss: 7.629385123664179e-08\n",
      "step: 23077, loss: 0.00014625009498558939\n",
      "step: 23078, loss: 4.76837147544984e-09\n",
      "step: 23079, loss: 3.552406155904464e-07\n",
      "step: 23080, loss: 3.7431544797073e-07\n",
      "step: 23081, loss: 7.390968903564499e-08\n",
      "step: 23082, loss: 2.1934427252290334e-07\n",
      "step: 23083, loss: 2.694123963919992e-07\n",
      "step: 23084, loss: 3.576278118089249e-08\n",
      "step: 23085, loss: 1.668929705544997e-08\n",
      "step: 23086, loss: 1.0251979887243579e-07\n",
      "step: 23087, loss: 9.53674117454284e-09\n",
      "step: 23088, loss: 3.8146946934602965e-08\n",
      "step: 23089, loss: 8.916674687498016e-07\n",
      "step: 23090, loss: 1.0728820853955767e-07\n",
      "step: 23091, loss: 2.38418573772492e-09\n",
      "step: 23092, loss: 0.15047353506088257\n",
      "step: 23093, loss: 6.651792432421644e-07\n",
      "step: 23094, loss: 4.76837103136063e-09\n",
      "step: 23095, loss: 0.0005127232288941741\n",
      "step: 23096, loss: 1.1205647609813241e-07\n",
      "step: 23097, loss: 7.64010565035278e-06\n",
      "step: 23098, loss: 1.9311850962822064e-07\n",
      "step: 23099, loss: 4.887556315225083e-07\n",
      "step: 23100, loss: 7.15255676908555e-09\n",
      "step: 23101, loss: 0.0005085843149572611\n",
      "step: 23102, loss: 1.5353631397374556e-06\n",
      "step: 23103, loss: 4.2915317521874385e-08\n",
      "step: 23104, loss: 4.794029791810317e-06\n",
      "step: 23105, loss: 4.601434966389206e-07\n",
      "step: 23106, loss: 3.80559176846873e-05\n",
      "step: 23107, loss: 6.675715269466309e-08\n",
      "step: 23108, loss: 3.6000881209474755e-07\n",
      "step: 23109, loss: 1.668929527909313e-08\n",
      "step: 23110, loss: 2.38418573772492e-09\n",
      "step: 23111, loss: 4.83986696053762e-07\n",
      "step: 23112, loss: 3.0994389277338996e-08\n",
      "step: 23113, loss: 2.8610211089130644e-08\n",
      "step: 23114, loss: 4.3630328150356945e-07\n",
      "step: 23115, loss: 1.9550233787413163e-07\n",
      "step: 23116, loss: 7.15255632499634e-09\n",
      "step: 23117, loss: 9.059888839146879e-08\n",
      "step: 23118, loss: 4.76837147544984e-09\n",
      "step: 23119, loss: 1.335140495939413e-07\n",
      "step: 23120, loss: 2.1457660537294032e-08\n",
      "step: 23121, loss: 3.67161220538037e-07\n",
      "step: 23122, loss: 3.170954698816786e-07\n",
      "step: 23123, loss: 8.821480435017293e-08\n",
      "step: 23124, loss: 5.4836217344700344e-08\n",
      "step: 23125, loss: 0.001323642092756927\n",
      "step: 23126, loss: 2.145766764272139e-08\n",
      "step: 23127, loss: 0.07516675442457199\n",
      "step: 23128, loss: 0.0\n",
      "step: 23129, loss: 3.814693982917561e-08\n",
      "step: 23130, loss: 2.0383752143970923e-06\n",
      "step: 23131, loss: 0.07773040980100632\n",
      "step: 23132, loss: 7.15255676908555e-09\n",
      "step: 23133, loss: 4.76837147544984e-09\n",
      "step: 23134, loss: 3.3378576347331546e-08\n",
      "step: 23135, loss: 1.430511264999268e-08\n",
      "step: 23136, loss: 2.4627859147585696e-06\n",
      "step: 23137, loss: 4.339173642620153e-07\n",
      "step: 23138, loss: 1.3255651083454723e-06\n",
      "step: 23139, loss: 0.12225838005542755\n",
      "step: 23140, loss: 5.173640147404512e-07\n",
      "step: 23141, loss: 3.337859411089994e-08\n",
      "step: 23142, loss: 1.335141348590696e-07\n",
      "step: 23143, loss: 4.291490824925859e-07\n",
      "step: 23144, loss: 9.6796111392905e-07\n",
      "step: 23145, loss: 0.08376847207546234\n",
      "step: 23146, loss: 1.5020350474514998e-07\n",
      "step: 23147, loss: 9.059895944574237e-08\n",
      "step: 23148, loss: 2.8610218194558e-08\n",
      "step: 23149, loss: 0.0\n",
      "step: 23150, loss: 2.9086902486596955e-07\n",
      "step: 23151, loss: 1.053784899340826e-06\n",
      "step: 23152, loss: 2.622603467727913e-08\n",
      "step: 23153, loss: 3.337859055818626e-08\n",
      "step: 23154, loss: 0.042343106120824814\n",
      "step: 23155, loss: 3.266320334205375e-07\n",
      "step: 23156, loss: 7.15255676908555e-09\n",
      "step: 23157, loss: 1.9382528080313932e-06\n",
      "step: 23158, loss: 4.053114111002287e-08\n",
      "step: 23159, loss: 2.7656366796691145e-07\n",
      "step: 23160, loss: 1.287456257159647e-07\n",
      "step: 23161, loss: 0.007349401246756315\n",
      "step: 23162, loss: 0.03457508236169815\n",
      "step: 23163, loss: 4.839859002458979e-07\n",
      "step: 23164, loss: 0.015912070870399475\n",
      "step: 23165, loss: 4.3630123514049046e-07\n",
      "step: 23166, loss: 0.11497237533330917\n",
      "step: 23167, loss: 3.40937617693271e-07\n",
      "step: 23168, loss: 4.649133984457876e-07\n",
      "step: 23169, loss: 4.2915313969160707e-08\n",
      "step: 23170, loss: 1.5497158756261342e-07\n",
      "step: 23171, loss: 3.3378447028553637e-07\n",
      "step: 23172, loss: 3.337859766361362e-08\n",
      "step: 23173, loss: 1.2846677236666437e-05\n",
      "step: 23174, loss: 4.2199837935186224e-07\n",
      "step: 23175, loss: 0.14031989872455597\n",
      "step: 23176, loss: 7.15255676908555e-09\n",
      "step: 23177, loss: 1.4948328725949978e-06\n",
      "step: 23178, loss: 0.05025845393538475\n",
      "step: 23179, loss: 0.0001323558099102229\n",
      "step: 23180, loss: 8.845183288030967e-07\n",
      "step: 23181, loss: 0.028364159166812897\n",
      "step: 23182, loss: 1.28745710981093e-07\n",
      "step: 23183, loss: 7.152551972922083e-08\n",
      "step: 23184, loss: 8.201449190892163e-07\n",
      "step: 23185, loss: 1.668929883180681e-08\n",
      "step: 23186, loss: 1.3766333722742274e-05\n",
      "step: 23187, loss: 2.7512082851899322e-06\n",
      "step: 23188, loss: 3.085719799855724e-05\n",
      "step: 23189, loss: 1.6569773606534e-06\n",
      "step: 23190, loss: 7.867807028105744e-08\n",
      "step: 23191, loss: 0.20441432297229767\n",
      "step: 23192, loss: 1.003132183541311e-05\n",
      "step: 23193, loss: 0.06007051840424538\n",
      "step: 23194, loss: 0.0005211523384787142\n",
      "step: 23195, loss: 3.354268528710236e-06\n",
      "step: 23196, loss: 4.040960266138427e-05\n",
      "step: 23197, loss: 2.22205781028606e-05\n",
      "step: 23198, loss: 0.00029668130446225405\n",
      "step: 23199, loss: 2.7535527351574274e-06\n",
      "step: 23200, loss: 9.74582799244672e-05\n",
      "step: 23201, loss: 0.001123263267800212\n",
      "step: 23202, loss: 0.0010244500590488315\n",
      "step: 23203, loss: 2.484640572220087e-05\n",
      "step: 23204, loss: 0.0035308038350194693\n",
      "step: 23205, loss: 8.995982170745265e-06\n",
      "step: 23206, loss: 2.6709863959695213e-05\n",
      "step: 23207, loss: 2.4079020022327313e-06\n",
      "step: 23208, loss: 0.03267699107527733\n",
      "step: 23209, loss: 1.279099702835083\n",
      "step: 23210, loss: 0.0003649380523711443\n",
      "step: 23211, loss: 0.0004799125308636576\n",
      "step: 23212, loss: 1.0251986992670936e-07\n",
      "step: 23213, loss: 1.8691264358494664e-06\n",
      "step: 23214, loss: 1.8119784783721116e-07\n",
      "step: 23215, loss: 0.10050633549690247\n",
      "step: 23216, loss: 7.635342626599595e-06\n",
      "step: 23217, loss: 1.3457094610203058e-05\n",
      "step: 23218, loss: 5.102115778754523e-07\n",
      "step: 23219, loss: 0.230621337890625\n",
      "step: 23220, loss: 1.413802920069429e-06\n",
      "step: 23221, loss: 1.192092558000013e-08\n",
      "step: 23222, loss: 0.0017122866120189428\n",
      "step: 23223, loss: 0.2352192997932434\n",
      "step: 23224, loss: 0.0013633141061291099\n",
      "step: 23225, loss: 0.07100162655115128\n",
      "step: 23226, loss: 2.360205144213978e-06\n",
      "step: 23227, loss: 7.831241964595392e-05\n",
      "step: 23228, loss: 0.01158814411610365\n",
      "step: 23229, loss: 9.536720568803503e-08\n",
      "step: 23230, loss: 6.318043119790673e-07\n",
      "step: 23231, loss: 2.145766231365087e-08\n",
      "step: 23232, loss: 1.9263541162217734e-06\n",
      "step: 23233, loss: 5.84117742619128e-07\n",
      "step: 23234, loss: 0.025098755955696106\n",
      "step: 23235, loss: 9.298170198235312e-07\n",
      "step: 23236, loss: 0.054738134145736694\n",
      "step: 23237, loss: 0.0011581785511225462\n",
      "step: 23238, loss: 0.06553900241851807\n",
      "step: 23239, loss: 0.00019523546507116407\n",
      "step: 23240, loss: 1.0217143426416442e-05\n",
      "step: 23241, loss: 5.245201961656676e-08\n",
      "step: 23242, loss: 0.0024992378894239664\n",
      "step: 23243, loss: 0.0002927732712123543\n",
      "step: 23244, loss: 2.38418573772492e-09\n",
      "step: 23245, loss: 1.0251824278384447e-05\n",
      "step: 23246, loss: 5.960456661568969e-08\n",
      "step: 23247, loss: 8.010712235773099e-07\n",
      "step: 23248, loss: 0.0024893409572541714\n",
      "step: 23249, loss: 0.18899111449718475\n",
      "step: 23250, loss: 0.09202299267053604\n",
      "step: 23251, loss: 0.04380829632282257\n",
      "step: 23252, loss: 1.0800137033584178e-06\n",
      "step: 23253, loss: 0.000733731547370553\n",
      "step: 23254, loss: 0.000366839551134035\n",
      "step: 23255, loss: 0.0017465177224949002\n",
      "step: 23256, loss: 0.0002461388648953289\n",
      "step: 23257, loss: 7.15255632499634e-09\n",
      "step: 23258, loss: 1.2816200978704728e-05\n",
      "step: 23259, loss: 2.3453792891814373e-05\n",
      "step: 23260, loss: 1.4543488191520737e-07\n",
      "step: 23261, loss: 0.0007861398626118898\n",
      "step: 23262, loss: 0.05589086934924126\n",
      "step: 23263, loss: 6.250358637771569e-06\n",
      "step: 23264, loss: 0.0010828571394085884\n",
      "step: 23265, loss: 0.10809707641601562\n",
      "step: 23266, loss: 8.106216142778067e-08\n",
      "step: 23267, loss: 0.0017063198611140251\n",
      "step: 23268, loss: 1.6831659195304383e-06\n",
      "step: 23269, loss: 0.018632257357239723\n",
      "step: 23270, loss: 2.3126509063331468e-07\n",
      "step: 23271, loss: 0.0007383256452158093\n",
      "step: 23272, loss: 0.036920975893735886\n",
      "step: 23273, loss: 0.0046356478706002235\n",
      "step: 23274, loss: 1.7904442302096868e-06\n",
      "step: 23275, loss: 0.12873944640159607\n",
      "step: 23276, loss: 1.2254536159161944e-06\n",
      "step: 23277, loss: 1.3780119161310722e-06\n",
      "step: 23278, loss: 0.08835865557193756\n",
      "step: 23279, loss: 3.044506229343824e-06\n",
      "step: 23280, loss: 2.417777432128787e-05\n",
      "step: 23281, loss: 6.341838911794184e-07\n",
      "step: 23282, loss: 0.018713289871811867\n",
      "step: 23283, loss: 0.004097819793969393\n",
      "step: 23284, loss: 1.0426398148410954e-05\n",
      "step: 23285, loss: 2.8610216418201162e-08\n",
      "step: 23286, loss: 0.027376661077141762\n",
      "step: 23287, loss: 0.00032974351779557765\n",
      "step: 23288, loss: 1.430511264999268e-08\n",
      "step: 23289, loss: 1.907339850504286e-07\n",
      "step: 23290, loss: 0.004334890749305487\n",
      "step: 23291, loss: 0.005376594141125679\n",
      "step: 23292, loss: 2.38418573772492e-09\n",
      "step: 23293, loss: 1.192092558000013e-08\n",
      "step: 23294, loss: 4.2979834688594565e-05\n",
      "step: 23295, loss: 8.821470487418992e-08\n",
      "step: 23296, loss: 7.869497494539246e-06\n",
      "step: 23297, loss: 0.047675203531980515\n",
      "step: 23298, loss: 0.0004030590644106269\n",
      "step: 23299, loss: 0.181854709982872\n",
      "step: 23300, loss: 0.07007109373807907\n",
      "step: 23301, loss: 1.3255635167297442e-06\n",
      "step: 23302, loss: 1.430511264999268e-08\n",
      "step: 23303, loss: 0.0\n",
      "step: 23304, loss: 2.38418573772492e-09\n",
      "step: 23305, loss: 0.006879833526909351\n",
      "step: 23306, loss: 3.9028149330988526e-05\n",
      "step: 23307, loss: 1.192092558000013e-08\n",
      "step: 23308, loss: 3.099440704090739e-08\n",
      "step: 23309, loss: 0.1499500721693039\n",
      "step: 23310, loss: 0.0009352591005153954\n",
      "step: 23311, loss: 0.001384574337862432\n",
      "step: 23312, loss: 5.478481398313306e-06\n",
      "step: 23313, loss: 3.0278931717475643e-07\n",
      "step: 23314, loss: 7.4624227863751e-07\n",
      "step: 23315, loss: 3.0611176953243557e-06\n",
      "step: 23316, loss: 2.121920630315799e-07\n",
      "step: 23317, loss: 0.00022157754574436694\n",
      "step: 23318, loss: 0.00033447169698774815\n",
      "step: 23319, loss: 1.4543502402375452e-07\n",
      "step: 23320, loss: 4.053112689916816e-08\n",
      "step: 23321, loss: 2.10990015148127e-06\n",
      "step: 23322, loss: 0.0002591845113784075\n",
      "step: 23323, loss: 0.001695491373538971\n",
      "step: 23324, loss: 0.401774138212204\n",
      "step: 23325, loss: 0.043859075754880905\n",
      "step: 23326, loss: 0.008333859965205193\n",
      "step: 23327, loss: 0.0003814785450231284\n",
      "step: 23328, loss: 6.199084600666538e-05\n",
      "step: 23329, loss: 7.03493151377188e-06\n",
      "step: 23330, loss: 6.747183078914532e-07\n",
      "step: 23331, loss: 0.00044965907000005245\n",
      "step: 23332, loss: 0.06045123562216759\n",
      "step: 23333, loss: 0.060588717460632324\n",
      "step: 23334, loss: 0.17580725252628326\n",
      "step: 23335, loss: 1.0561669796516071e-06\n",
      "step: 23336, loss: 8.819287904771045e-05\n",
      "step: 23337, loss: 0.0033363658003509045\n",
      "step: 23338, loss: 7.390968903564499e-08\n",
      "step: 23339, loss: 1.7319791368208826e-05\n",
      "step: 23340, loss: 1.785715880942007e-06\n",
      "step: 23341, loss: 1.6893203792278655e-05\n",
      "step: 23342, loss: 4.3153306705789873e-07\n",
      "step: 23343, loss: 1.7755064618540928e-05\n",
      "step: 23344, loss: 0.36037686467170715\n",
      "step: 23345, loss: 8.821303367767541e-07\n",
      "step: 23346, loss: 0.3168369233608246\n",
      "step: 23347, loss: 0.5377347469329834\n",
      "step: 23348, loss: 2.837162469404575e-07\n",
      "step: 23349, loss: 0.2579982280731201\n",
      "step: 23350, loss: 6.065155321266502e-05\n",
      "step: 23351, loss: 2.366377884754911e-05\n",
      "step: 23352, loss: 0.0001772536925273016\n",
      "step: 23353, loss: 2.171156847907696e-05\n",
      "step: 23354, loss: 0.0934811532497406\n",
      "step: 23355, loss: 0.1460297703742981\n",
      "step: 23356, loss: 2.946274798887316e-05\n",
      "step: 23357, loss: 0.0002519266272429377\n",
      "step: 23358, loss: 0.00046910534729249775\n",
      "step: 23359, loss: 0.006046636030077934\n",
      "step: 23360, loss: 0.2841755747795105\n",
      "step: 23361, loss: 1.7166097165954852e-07\n",
      "step: 23362, loss: 0.0020984492730349302\n",
      "step: 23363, loss: 0.09886650741100311\n",
      "step: 23364, loss: 1.118151999435213e-06\n",
      "step: 23365, loss: 0.004809993784874678\n",
      "step: 23366, loss: 0.12986841797828674\n",
      "step: 23367, loss: 7.2156590249505825e-06\n",
      "step: 23368, loss: 4.7683684556432127e-08\n",
      "step: 23369, loss: 9.131223350777873e-07\n",
      "step: 23370, loss: 0.0005806444096378982\n",
      "step: 23371, loss: 0.0005248032393865287\n",
      "step: 23372, loss: 0.3464990258216858\n",
      "step: 23373, loss: 2.8507773095043376e-05\n",
      "step: 23374, loss: 0.004766583908349276\n",
      "step: 23375, loss: 0.14365586638450623\n",
      "step: 23376, loss: 0.03223703056573868\n",
      "step: 23377, loss: 7.58539545131498e-06\n",
      "step: 23378, loss: 9.471399971516803e-05\n",
      "step: 23379, loss: 0.0001126037968788296\n",
      "step: 23380, loss: 0.1394384354352951\n",
      "step: 23381, loss: 0.09895246475934982\n",
      "step: 23382, loss: 3.888243099936517e-06\n",
      "step: 23383, loss: 0.0004549296572804451\n",
      "step: 23384, loss: 0.00021973629191052169\n",
      "step: 23385, loss: 0.0013794671976938844\n",
      "step: 23386, loss: 4.76837103136063e-09\n",
      "step: 23387, loss: 0.00044555842760019004\n",
      "step: 23388, loss: 0.00041032672743313015\n",
      "step: 23389, loss: 0.0417463444173336\n",
      "step: 23390, loss: 6.532612815135508e-07\n",
      "step: 23391, loss: 0.10556197166442871\n",
      "step: 23392, loss: 0.3302319049835205\n",
      "step: 23393, loss: 0.036992743611335754\n",
      "step: 23394, loss: 0.012037057429552078\n",
      "step: 23395, loss: 0.0035339645110070705\n",
      "step: 23396, loss: 0.04922550544142723\n",
      "step: 23397, loss: 0.010293602012097836\n",
      "step: 23398, loss: 7.367036118921533e-07\n",
      "step: 23399, loss: 1.6863394193933345e-05\n",
      "step: 23400, loss: 0.02414785325527191\n",
      "step: 23401, loss: 0.2429150640964508\n",
      "step: 23402, loss: 3.089299934799783e-05\n",
      "step: 23403, loss: 8.177640893336502e-07\n",
      "step: 23404, loss: 0.05187535285949707\n",
      "step: 23405, loss: 4.970482677890686e-06\n",
      "step: 23406, loss: 0.00047587102744728327\n",
      "step: 23407, loss: 1.0585540621832479e-06\n",
      "step: 23408, loss: 0.0\n",
      "step: 23409, loss: 1.096705182135338e-05\n",
      "step: 23410, loss: 0.0028622683603316545\n",
      "step: 23411, loss: 0.024228716269135475\n",
      "step: 23412, loss: 0.043262213468551636\n",
      "step: 23413, loss: 0.000973919581156224\n",
      "step: 23414, loss: 0.049045562744140625\n",
      "step: 23415, loss: 0.00595075124874711\n",
      "step: 23416, loss: 9.073469118447974e-05\n",
      "step: 23417, loss: 2.331338146177586e-05\n",
      "step: 23418, loss: 0.1690966635942459\n",
      "step: 23419, loss: 0.05325061455368996\n",
      "step: 23420, loss: 0.19021308422088623\n",
      "step: 23421, loss: 0.0002043530548689887\n",
      "step: 23422, loss: 4.196125473754364e-07\n",
      "step: 23423, loss: 0.1808309108018875\n",
      "step: 23424, loss: 1.9311821120027162e-07\n",
      "step: 23425, loss: 0.0036615885328501463\n",
      "step: 23426, loss: 8.207566861528903e-05\n",
      "step: 23427, loss: 0.7300925254821777\n",
      "step: 23428, loss: 0.00010304625902790576\n",
      "step: 23429, loss: 0.008216790854930878\n",
      "step: 23430, loss: 0.15368056297302246\n",
      "step: 23431, loss: 0.19638767838478088\n",
      "step: 23432, loss: 0.2466437667608261\n",
      "step: 23433, loss: 0.0714934840798378\n",
      "step: 23434, loss: 0.00016327969206031412\n",
      "step: 23435, loss: 0.008081826381385326\n",
      "step: 23436, loss: 0.20424875617027283\n",
      "step: 23437, loss: 0.0564991720020771\n",
      "step: 23438, loss: 1.053165669873124e-05\n",
      "step: 23439, loss: 0.1265006810426712\n",
      "step: 23440, loss: 1.8095150835506502e-06\n",
      "step: 23441, loss: 5.013502686779248e-06\n",
      "step: 23442, loss: 0.01672748290002346\n",
      "step: 23443, loss: 2.892215343308635e-05\n",
      "step: 23444, loss: 6.341836069623241e-07\n",
      "step: 23445, loss: 0.05443798005580902\n",
      "step: 23446, loss: 0.044904276728630066\n",
      "step: 23447, loss: 0.0006888001225888729\n",
      "step: 23448, loss: 0.00171082082670182\n",
      "step: 23449, loss: 0.0008842639508657157\n",
      "step: 23450, loss: 2.937243607448181e-06\n",
      "step: 23451, loss: 0.001930181635543704\n",
      "step: 23452, loss: 0.05142560973763466\n",
      "step: 23453, loss: 0.27126753330230713\n",
      "step: 23454, loss: 2.1457660537294032e-08\n",
      "step: 23455, loss: 0.19430646300315857\n",
      "step: 23456, loss: 1.263614990421047e-07\n",
      "step: 23457, loss: 8.301716297864914e-05\n",
      "step: 23458, loss: 2.622603467727913e-08\n",
      "step: 23459, loss: 0.011589866131544113\n",
      "step: 23460, loss: 0.05345262959599495\n",
      "step: 23461, loss: 7.729831850156188e-05\n",
      "step: 23462, loss: 1.7738066162564792e-06\n",
      "step: 23463, loss: 0.002147996099665761\n",
      "step: 23464, loss: 0.3393515646457672\n",
      "step: 23465, loss: 0.011263017542660236\n",
      "step: 23466, loss: 0.0006021144217811525\n",
      "step: 23467, loss: 0.00012948710354976356\n",
      "step: 23468, loss: 1.1205641214928619e-07\n",
      "step: 23469, loss: 3.394509258214384e-05\n",
      "step: 23470, loss: 0.1988651305437088\n",
      "step: 23471, loss: 0.06200821325182915\n",
      "step: 23472, loss: 1.1205662531210692e-07\n",
      "step: 23473, loss: 1.6689267567926436e-07\n",
      "step: 23474, loss: 0.0003462294989731163\n",
      "step: 23475, loss: 2.81811135209864e-05\n",
      "step: 23476, loss: 0.16105344891548157\n",
      "step: 23477, loss: 0.00011277807061560452\n",
      "step: 23478, loss: 0.14329995214939117\n",
      "step: 23479, loss: 2.4222292267950252e-05\n",
      "step: 23480, loss: 0.31113359332084656\n",
      "step: 23481, loss: 3.3378576347331546e-08\n",
      "step: 23482, loss: 0.09992656856775284\n",
      "step: 23483, loss: 0.06772822141647339\n",
      "step: 23484, loss: 0.0003644387179519981\n",
      "step: 23485, loss: 0.0033203482162207365\n",
      "step: 23486, loss: 0.0004974441835656762\n",
      "step: 23487, loss: 0.00024538126308470964\n",
      "step: 23488, loss: 3.1874567412160104e-06\n",
      "step: 23489, loss: 0.00013310174108482897\n",
      "step: 23490, loss: 0.06805038452148438\n",
      "step: 23491, loss: 0.0001181495317723602\n",
      "step: 23492, loss: 0.04603203758597374\n",
      "step: 23493, loss: 1.835222792578861e-05\n",
      "step: 23494, loss: 0.33929699659347534\n",
      "step: 23495, loss: 1.9025242181669455e-06\n",
      "step: 23496, loss: 0.8254432678222656\n",
      "step: 23497, loss: 0.0034011893440037966\n",
      "step: 23498, loss: 0.0005087803001515567\n",
      "step: 23499, loss: 5.483622800284138e-08\n",
      "step: 23500, loss: 0.0049868361093103886\n",
      "step: 23501, loss: 0.005825095344334841\n",
      "step: 23502, loss: 0.023701123893260956\n",
      "step: 23503, loss: 1.2540288480522577e-05\n",
      "step: 23504, loss: 0.00014842994278296828\n",
      "step: 23505, loss: 0.08415588736534119\n",
      "step: 23506, loss: 7.533967618655879e-07\n",
      "step: 23507, loss: 7.390963929765348e-08\n",
      "step: 23508, loss: 0.0012539072195068002\n",
      "step: 23509, loss: 0.06055581569671631\n",
      "step: 23510, loss: 2.8560534701682627e-06\n",
      "step: 23511, loss: 0.0002488404861651361\n",
      "step: 23512, loss: 2.38418573772492e-09\n",
      "step: 23513, loss: 0.00022241870465222746\n",
      "step: 23514, loss: 2.6778598112287e-05\n",
      "step: 23515, loss: 0.0027991088572889566\n",
      "step: 23516, loss: 1.5497177230372472e-07\n",
      "step: 23517, loss: 2.813320634231786e-07\n",
      "step: 23518, loss: 9.53674206272126e-09\n",
      "step: 23519, loss: 3.3616751693443803e-07\n",
      "step: 23520, loss: 0.0020478267688304186\n",
      "step: 23521, loss: 2.38418573772492e-09\n",
      "step: 23522, loss: 0.005306839477270842\n",
      "step: 23523, loss: 1.4781913648675982e-07\n",
      "step: 23524, loss: 0.07633249461650848\n",
      "step: 23525, loss: 1.835820029327806e-07\n",
      "step: 23526, loss: 2.1457660537294032e-08\n",
      "step: 23527, loss: 6.363070133375004e-05\n",
      "step: 23528, loss: 0.000765131029766053\n",
      "step: 23529, loss: 6.437292654482007e-08\n",
      "step: 23530, loss: 0.020212162286043167\n",
      "step: 23531, loss: 0.02228844352066517\n",
      "step: 23532, loss: 1.0633248166413978e-06\n",
      "step: 23533, loss: 2.38418573772492e-09\n",
      "step: 23534, loss: 5.5879059800645337e-05\n",
      "step: 23535, loss: 1.3207959455030505e-06\n",
      "step: 23536, loss: 0.06506820023059845\n",
      "step: 23537, loss: 0.004255958832800388\n",
      "step: 23538, loss: 0.03301433473825455\n",
      "step: 23539, loss: 0.00026838461053557694\n",
      "step: 23540, loss: 0.0015165216755121946\n",
      "step: 23541, loss: 0.15661239624023438\n",
      "step: 23542, loss: 5.006783965200157e-08\n",
      "step: 23543, loss: 0.036945875734090805\n",
      "step: 23544, loss: 4.52994939337259e-08\n",
      "step: 23545, loss: 0.0001093607788789086\n",
      "step: 23546, loss: 9.67661471804604e-05\n",
      "step: 23547, loss: 8.153770068020094e-07\n",
      "step: 23548, loss: 3.983203714597039e-05\n",
      "step: 23549, loss: 0.2641993761062622\n",
      "step: 23550, loss: 0.08840110898017883\n",
      "step: 23551, loss: 0.08553371578454971\n",
      "step: 23552, loss: 0.0030555170960724354\n",
      "step: 23553, loss: 0.12190407514572144\n",
      "step: 23554, loss: 0.0025410198140889406\n",
      "step: 23555, loss: 5.721977345274354e-07\n",
      "step: 23556, loss: 0.13447895646095276\n",
      "step: 23557, loss: 0.12509173154830933\n",
      "step: 23558, loss: 0.0\n",
      "step: 23559, loss: 6.479348940047203e-06\n",
      "step: 23560, loss: 3.8146957592744e-08\n",
      "step: 23561, loss: 0.9635775089263916\n",
      "step: 23562, loss: 0.0\n",
      "step: 23563, loss: 0.0001311553205596283\n",
      "step: 23564, loss: 3.147105473999545e-07\n",
      "step: 23565, loss: 0.00010223277058685198\n",
      "step: 23566, loss: 0.009011576883494854\n",
      "step: 23567, loss: 4.887534714725916e-07\n",
      "step: 23568, loss: 0.0012215604074299335\n",
      "step: 23569, loss: 0.00033973780227825046\n",
      "step: 23570, loss: 3.337859055818626e-08\n",
      "step: 23571, loss: 7.258523692144081e-05\n",
      "step: 23572, loss: 0.0028068399988114834\n",
      "step: 23573, loss: 0.01150868646800518\n",
      "step: 23574, loss: 5.625890480587259e-06\n",
      "step: 23575, loss: 1.0711682989494875e-05\n",
      "step: 23576, loss: 5.726218660129234e-06\n",
      "step: 23577, loss: 0.010147102177143097\n",
      "step: 23578, loss: 0.0001258994307136163\n",
      "step: 23579, loss: 0.042627885937690735\n",
      "step: 23580, loss: 5.318417152011534e-06\n",
      "step: 23581, loss: 3.006319275300484e-06\n",
      "step: 23582, loss: 0.09026523679494858\n",
      "step: 23583, loss: 0.0625692829489708\n",
      "step: 23584, loss: 1.0418738156658947e-06\n",
      "step: 23585, loss: 0.38478508591651917\n",
      "step: 23586, loss: 0.0031225455459207296\n",
      "step: 23587, loss: 0.40699848532676697\n",
      "step: 23588, loss: 0.06266908347606659\n",
      "step: 23589, loss: 0.06208758056163788\n",
      "step: 23590, loss: 0.0022037283051759005\n",
      "step: 23591, loss: 0.0008600149885751307\n",
      "step: 23592, loss: 0.01855371706187725\n",
      "step: 23593, loss: 7.15255632499634e-09\n",
      "step: 23594, loss: 0.007468752097338438\n",
      "step: 23595, loss: 3.154181968056946e-06\n",
      "step: 23596, loss: 0.15563149750232697\n",
      "step: 23597, loss: 0.007436356041580439\n",
      "step: 23598, loss: 0.08918531239032745\n",
      "step: 23599, loss: 0.38579094409942627\n",
      "step: 23600, loss: 0.0062288022600114346\n",
      "step: 23601, loss: 4.94689720653696e-06\n",
      "step: 23602, loss: 2.38418573772492e-09\n",
      "step: 23603, loss: 0.0009908551583066583\n",
      "step: 23604, loss: 5.290045010042377e-06\n",
      "step: 23605, loss: 0.0005866356659680605\n",
      "step: 23606, loss: 0.0037367427721619606\n",
      "step: 23607, loss: 8.344479738298105e-07\n",
      "step: 23608, loss: 2.3602585770277074e-06\n",
      "step: 23609, loss: 0.00993099994957447\n",
      "step: 23610, loss: 0.09552217274904251\n",
      "step: 23611, loss: 0.003177136182785034\n",
      "step: 23612, loss: 0.0\n",
      "step: 23613, loss: 2.38418573772492e-09\n",
      "step: 23614, loss: 0.014470897614955902\n",
      "step: 23615, loss: 1.430510998545742e-08\n",
      "step: 23616, loss: 2.3054446955939056e-06\n",
      "step: 23617, loss: 1.576571048644837e-05\n",
      "step: 23618, loss: 0.015758413821458817\n",
      "step: 23619, loss: 0.008559092879295349\n",
      "step: 23620, loss: 3.3139949096039345e-07\n",
      "step: 23621, loss: 0.0559750571846962\n",
      "step: 23622, loss: 0.0019421055912971497\n",
      "step: 23623, loss: 0.0006927921785973012\n",
      "step: 23624, loss: 0.13976171612739563\n",
      "step: 23625, loss: 0.051703404635190964\n",
      "step: 23626, loss: 0.14014972746372223\n",
      "step: 23627, loss: 0.0\n",
      "step: 23628, loss: 2.861022352362852e-08\n",
      "step: 23629, loss: 0.032126717269420624\n",
      "step: 23630, loss: 0.0005681792390532792\n",
      "step: 23631, loss: 3.220907046852517e-06\n",
      "step: 23632, loss: 0.3555792570114136\n",
      "step: 23633, loss: 2.76011778623797e-05\n",
      "step: 23634, loss: 1.2302020877541509e-06\n",
      "step: 23635, loss: 0.03976752609014511\n",
      "step: 23636, loss: 0.1987268477678299\n",
      "step: 23637, loss: 0.00012277632777113467\n",
      "step: 23638, loss: 0.012189662083983421\n",
      "step: 23639, loss: 0.35115233063697815\n",
      "step: 23640, loss: 0.0014621560694649816\n",
      "step: 23641, loss: 0.64757239818573\n",
      "step: 23642, loss: 0.08309230953454971\n",
      "step: 23643, loss: 0.016181442886590958\n",
      "step: 23644, loss: 9.53674206272126e-09\n",
      "step: 23645, loss: 9.825587039813399e-05\n",
      "step: 23646, loss: 2.765639237622963e-07\n",
      "step: 23647, loss: 0.33039525151252747\n",
      "step: 23648, loss: 6.461048087658128e-07\n",
      "step: 23649, loss: 2.38418573772492e-09\n",
      "step: 23650, loss: 1.98832162823237e-06\n",
      "step: 23651, loss: 0.04310786351561546\n",
      "step: 23652, loss: 2.4080208049781504e-07\n",
      "step: 23653, loss: 2.307831209691358e-06\n",
      "step: 23654, loss: 2.200538347096881e-06\n",
      "step: 23655, loss: 1.2325948546276777e-06\n",
      "step: 23656, loss: 0.0009234266472049057\n",
      "step: 23657, loss: 5.349399089027429e-06\n",
      "step: 23658, loss: 8.15379621599277e-07\n",
      "step: 23659, loss: 0.07194840162992477\n",
      "step: 23660, loss: 0.13577282428741455\n",
      "step: 23661, loss: 9.298303638161087e-08\n",
      "step: 23662, loss: 1.1205641214928619e-07\n",
      "step: 23663, loss: 0.3733557462692261\n",
      "step: 23664, loss: 4.614661884261295e-05\n",
      "step: 23665, loss: 0.10705801099538803\n",
      "step: 23666, loss: 7.748494681436568e-07\n",
      "step: 23667, loss: 0.05942128971219063\n",
      "step: 23668, loss: 0.0003489741066005081\n",
      "step: 23669, loss: 1.0490390423001372e-07\n",
      "step: 23670, loss: 0.2218150645494461\n",
      "step: 23671, loss: 0.19514112174510956\n",
      "step: 23672, loss: 4.95231943204999e-05\n",
      "step: 23673, loss: 1.6927647550346592e-07\n",
      "step: 23674, loss: 0.006484637036919594\n",
      "step: 23675, loss: 3.172750803059898e-05\n",
      "step: 23676, loss: 0.006468381732702255\n",
      "step: 23677, loss: 0.06903787702322006\n",
      "step: 23678, loss: 0.11089649051427841\n",
      "step: 23679, loss: 4.887554041488329e-07\n",
      "step: 23680, loss: 0.0\n",
      "step: 23681, loss: 0.40674009919166565\n",
      "step: 23682, loss: 0.09181155264377594\n",
      "step: 23683, loss: 0.0008743935613892972\n",
      "step: 23684, loss: 0.19349679350852966\n",
      "step: 23685, loss: 0.005626028403639793\n",
      "step: 23686, loss: 0.0\n",
      "step: 23687, loss: 2.7835718356072903e-05\n",
      "step: 23688, loss: 0.00201602466404438\n",
      "step: 23689, loss: 1.1944428024435183e-06\n",
      "step: 23690, loss: 0.1471950113773346\n",
      "step: 23691, loss: 0.00010554079199209809\n",
      "step: 23692, loss: 1.4305072681963793e-07\n",
      "step: 23693, loss: 0.3765442967414856\n",
      "step: 23694, loss: 5.006786452099732e-08\n",
      "step: 23695, loss: 0.07204601913690567\n",
      "step: 23696, loss: 0.008033474907279015\n",
      "step: 23697, loss: 0.2917380630970001\n",
      "step: 23698, loss: 0.0031484300270676613\n",
      "step: 23699, loss: 0.270426869392395\n",
      "step: 23700, loss: 5.4836217344700344e-08\n",
      "step: 23701, loss: 0.16314063966274261\n",
      "step: 23702, loss: 3.249396104365587e-05\n",
      "step: 23703, loss: 0.0\n",
      "step: 23704, loss: 0.0010493950685486197\n",
      "step: 23705, loss: 1.0251979176700843e-07\n",
      "step: 23706, loss: 0.004357616417109966\n",
      "step: 23707, loss: 1.9073478796372e-08\n",
      "step: 23708, loss: 4.670161615649704e-06\n",
      "step: 23709, loss: 0.0\n",
      "step: 23710, loss: 0.01799183152616024\n",
      "step: 23711, loss: 5.587708074017428e-05\n",
      "step: 23712, loss: 0.039044689387083054\n",
      "step: 23713, loss: 0.03929120674729347\n",
      "step: 23714, loss: 0.01919623464345932\n",
      "step: 23715, loss: 0.10606029629707336\n",
      "step: 23716, loss: 3.081138129346073e-05\n",
      "step: 23717, loss: 0.00471089081838727\n",
      "step: 23718, loss: 4.896859172731638e-05\n",
      "step: 23719, loss: 0.008555221371352673\n",
      "step: 23720, loss: 3.576275631189674e-08\n",
      "step: 23721, loss: 0.004880879539996386\n",
      "step: 23722, loss: 2.8371613325361977e-07\n",
      "step: 23723, loss: 1.6689232040789648e-07\n",
      "step: 23724, loss: 5.2452033827421474e-08\n",
      "step: 23725, loss: 0.004822995513677597\n",
      "step: 23726, loss: 0.03062250465154648\n",
      "step: 23727, loss: 7.65254026191542e-06\n",
      "step: 23728, loss: 0.00036981378798373044\n",
      "step: 23729, loss: 3.63338335773733e-06\n",
      "step: 23730, loss: 9.033650485434919e-07\n",
      "step: 23731, loss: 2.0169195522612426e-06\n",
      "step: 23732, loss: 7.13397457730025e-05\n",
      "step: 23733, loss: 0.0\n",
      "step: 23734, loss: 4.442814315552823e-05\n",
      "step: 23735, loss: 1.7642573766352143e-06\n",
      "step: 23736, loss: 0.003887645900249481\n",
      "step: 23737, loss: 1.430511264999268e-08\n",
      "step: 23738, loss: 8.106216853320802e-08\n",
      "step: 23739, loss: 5.388207000578404e-07\n",
      "step: 23740, loss: 3.087296136072837e-06\n",
      "step: 23741, loss: 1.6593244254181627e-06\n",
      "step: 23742, loss: 4.52994939337259e-08\n",
      "step: 23743, loss: 4.9662870878819376e-05\n",
      "step: 23744, loss: 5.7879833548213355e-06\n",
      "step: 23745, loss: 9.53674206272126e-09\n",
      "step: 23746, loss: 3.6905462366121355e-06\n",
      "step: 23747, loss: 1.668929527909313e-08\n",
      "step: 23748, loss: 9.775138209988654e-08\n",
      "step: 23749, loss: 0.0\n",
      "step: 23750, loss: 0.18255586922168732\n",
      "step: 23751, loss: 2.145766231365087e-08\n",
      "step: 23752, loss: 2.4695158572285436e-05\n",
      "step: 23753, loss: 0.06413447111845016\n",
      "step: 23754, loss: 1.0013562246058427e-07\n",
      "step: 23755, loss: 0.00012581284681800753\n",
      "step: 23756, loss: 3.576275631189674e-08\n",
      "step: 23757, loss: 0.011952982284128666\n",
      "step: 23758, loss: 0.17619159817695618\n",
      "step: 23759, loss: 1.0013420705945464e-06\n",
      "step: 23760, loss: 5.125933739691391e-07\n",
      "step: 23761, loss: 5.531268243430532e-07\n",
      "step: 23762, loss: 1.668929527909313e-08\n",
      "step: 23763, loss: 4.76837147544984e-09\n",
      "step: 23764, loss: 1.5735564318219986e-07\n",
      "step: 23765, loss: 0.0\n",
      "step: 23766, loss: 7.460253254976124e-05\n",
      "step: 23767, loss: 5.945732482359745e-06\n",
      "step: 23768, loss: 1.0424824722576886e-05\n",
      "step: 23769, loss: 0.0001569917076267302\n",
      "step: 23770, loss: 1.08954361621727e-06\n",
      "step: 23771, loss: 0.0001524370745755732\n",
      "step: 23772, loss: 3.0705955396115314e-06\n",
      "step: 23773, loss: 0.00791364349424839\n",
      "step: 23774, loss: 0.0\n",
      "step: 23775, loss: 1.2538866030809004e-05\n",
      "step: 23776, loss: 2.0956392745574703e-06\n",
      "step: 23777, loss: 0.025927584618330002\n",
      "step: 23778, loss: 0.10861217230558395\n",
      "step: 23779, loss: 9.083542522603238e-07\n",
      "step: 23780, loss: 0.00012061313464073464\n",
      "step: 23781, loss: 0.0007690881029702723\n",
      "step: 23782, loss: 2.384184938364342e-08\n",
      "step: 23783, loss: 8.106221827119953e-08\n",
      "step: 23784, loss: 2.1457660537294032e-08\n",
      "step: 23785, loss: 0.015742972493171692\n",
      "step: 23786, loss: 2.2124168026493862e-06\n",
      "step: 23787, loss: 0.4568347930908203\n",
      "step: 23788, loss: 1.401425924996147e-05\n",
      "step: 23789, loss: 0.07740473002195358\n",
      "step: 23790, loss: 1.459073246223852e-06\n",
      "step: 23791, loss: 0.00365445320494473\n",
      "step: 23792, loss: 3.5762485595114413e-07\n",
      "step: 23793, loss: 0.2162102311849594\n",
      "step: 23794, loss: 1.7779013433028013e-05\n",
      "step: 23795, loss: 1.1157684411955415e-06\n",
      "step: 23796, loss: 6.241809751372784e-05\n",
      "step: 23797, loss: 2.707526618905831e-05\n",
      "step: 23798, loss: 0.00014139262202661484\n",
      "step: 23799, loss: 0.0636664405465126\n",
      "step: 23800, loss: 0.00016763810708653182\n",
      "step: 23801, loss: 3.966891654272331e-06\n",
      "step: 23802, loss: 2.0980782267088216e-07\n",
      "step: 23803, loss: 1.1444078751310371e-07\n",
      "step: 23804, loss: 7.78090088715544e-06\n",
      "step: 23805, loss: 0.00939683523029089\n",
      "step: 23806, loss: 4.393577455630293e-06\n",
      "step: 23807, loss: 2.7417058845458087e-06\n",
      "step: 23808, loss: 0.00757468631491065\n",
      "step: 23809, loss: 6.0287270571279805e-06\n",
      "step: 23810, loss: 1.4471680515271146e-05\n",
      "step: 23811, loss: 5.0740196456899866e-05\n",
      "step: 23812, loss: 2.6226025795494934e-08\n",
      "step: 23813, loss: 0.009610697627067566\n",
      "step: 23814, loss: 1.6356001651729457e-05\n",
      "step: 23815, loss: 2.9633235953951953e-06\n",
      "step: 23816, loss: 7.15255676908555e-09\n",
      "step: 23817, loss: 0.0\n",
      "step: 23818, loss: 2.1695971952340187e-07\n",
      "step: 23819, loss: 4.291529975830599e-08\n",
      "step: 23820, loss: 0.0\n",
      "step: 23821, loss: 0.3761347830295563\n",
      "step: 23822, loss: 0.0037528551183640957\n",
      "step: 23823, loss: 4.0292516700901615e-07\n",
      "step: 23824, loss: 2.145766764272139e-08\n",
      "step: 23825, loss: 0.011084910482168198\n",
      "step: 23826, loss: 6.675711006209895e-08\n",
      "step: 23827, loss: 6.031943939888151e-07\n",
      "step: 23828, loss: 1.5163085436142865e-06\n",
      "step: 23829, loss: 7.15255632499634e-09\n",
      "step: 23830, loss: 0.11088494956493378\n",
      "step: 23831, loss: 3.814693982917561e-08\n",
      "step: 23832, loss: 9.453707207285333e-06\n",
      "step: 23833, loss: 0.0\n",
      "step: 23834, loss: 4.179779352853075e-05\n",
      "step: 23835, loss: 4.2915321074588064e-08\n",
      "step: 23836, loss: 2.38418573772492e-09\n",
      "step: 23837, loss: 8.292862730741035e-06\n",
      "step: 23838, loss: 7.843817684261012e-07\n",
      "step: 23839, loss: 2.9325275363589753e-07\n",
      "step: 23840, loss: 1.5830393067517434e-06\n",
      "step: 23841, loss: 4.053114111002287e-08\n",
      "step: 23842, loss: 7.15255632499634e-09\n",
      "step: 23843, loss: 0.00012739276280626655\n",
      "step: 23844, loss: 7.411745627905475e-06\n",
      "step: 23845, loss: 0.11476278305053711\n",
      "step: 23846, loss: 2.38418573772492e-09\n",
      "step: 23847, loss: 0.4851556122303009\n",
      "step: 23848, loss: 4.874270234722644e-05\n",
      "step: 23849, loss: 0.09001235663890839\n",
      "step: 23850, loss: 4.768340886585065e-07\n",
      "step: 23851, loss: 3.576277407546513e-08\n",
      "step: 23852, loss: 0.19292941689491272\n",
      "step: 23853, loss: 0.0029970109462738037\n",
      "step: 23854, loss: 5.099123427498853e-06\n",
      "step: 23855, loss: 0.0001515477488283068\n",
      "step: 23856, loss: 0.002510318998247385\n",
      "step: 23857, loss: 0.22350731492042542\n",
      "step: 23858, loss: 0.08191550523042679\n",
      "step: 23859, loss: 0.00021954085968900472\n",
      "step: 23860, loss: 0.08925680071115494\n",
      "step: 23861, loss: 0.30796894431114197\n",
      "step: 23862, loss: 0.2910081744194031\n",
      "step: 23863, loss: 0.004860256798565388\n",
      "step: 23864, loss: 0.025951499119400978\n",
      "step: 23865, loss: 0.00012411516217980534\n",
      "step: 23866, loss: 0.0001825462095439434\n",
      "step: 23867, loss: 8.595526196586434e-06\n",
      "step: 23868, loss: 1.4602763258153573e-05\n",
      "step: 23869, loss: 0.03844568878412247\n",
      "step: 23870, loss: 1.1181521131220507e-06\n",
      "step: 23871, loss: 1.5973982669947873e-07\n",
      "step: 23872, loss: 0.023435689508914948\n",
      "step: 23873, loss: 1.311349024035735e-05\n",
      "step: 23874, loss: 0.0008724251529201865\n",
      "step: 23875, loss: 3.26630811287032e-07\n",
      "step: 23876, loss: 0.0\n",
      "step: 23877, loss: 5.2452058696417225e-08\n",
      "step: 23878, loss: 0.0033721160143613815\n",
      "step: 23879, loss: 2.1877312974538654e-05\n",
      "step: 23880, loss: 5.245205159098987e-08\n",
      "step: 23881, loss: 0.001287570339627564\n",
      "step: 23882, loss: 1.668929527909313e-08\n",
      "step: 23883, loss: 0.005570851732045412\n",
      "step: 23884, loss: 2.3315978978644125e-06\n",
      "step: 23885, loss: 0.15202844142913818\n",
      "step: 23886, loss: 1.3351396432881302e-07\n",
      "step: 23887, loss: 0.003528479253873229\n",
      "step: 23888, loss: 6.675711006209895e-08\n",
      "step: 23889, loss: 1.4305065576536435e-07\n",
      "step: 23890, loss: 0.1171930804848671\n",
      "step: 23891, loss: 0.17852796614170074\n",
      "step: 23892, loss: 0.02951577864587307\n",
      "step: 23893, loss: 2.0742349704505614e-07\n",
      "step: 23894, loss: 0.00011592946975724772\n",
      "step: 23895, loss: 9.611448149371427e-06\n",
      "step: 23896, loss: 2.171878122680937e-06\n",
      "step: 23897, loss: 0.00013761856826022267\n",
      "step: 23898, loss: 0.016431383788585663\n",
      "step: 23899, loss: 0.02385813742876053\n",
      "step: 23900, loss: 0.0\n",
      "step: 23901, loss: 0.03531794622540474\n",
      "step: 23902, loss: 0.0007044018129818141\n",
      "step: 23903, loss: 2.38418573772492e-09\n",
      "step: 23904, loss: 0.007719065994024277\n",
      "step: 23905, loss: 0.0006859967834316194\n",
      "step: 23906, loss: 6.914129357937782e-08\n",
      "step: 23907, loss: 0.0015828730538487434\n",
      "step: 23908, loss: 0.0004471598658710718\n",
      "step: 23909, loss: 4.76837147544984e-09\n",
      "step: 23910, loss: 0.00011995907698292285\n",
      "step: 23911, loss: 9.210406460624654e-06\n",
      "step: 23912, loss: 2.4951621526270173e-05\n",
      "step: 23913, loss: 0.00013013853458687663\n",
      "step: 23914, loss: 4.1722969967850077e-07\n",
      "step: 23915, loss: 0.0\n",
      "step: 23916, loss: 6.089343150961213e-05\n",
      "step: 23917, loss: 4.159971467743162e-06\n",
      "step: 23918, loss: 0.0\n",
      "step: 23919, loss: 3.766978693420242e-07\n",
      "step: 23920, loss: 0.12892568111419678\n",
      "step: 23921, loss: 0.13970847427845\n",
      "step: 23922, loss: 0.0002336123725399375\n",
      "step: 23923, loss: 3.099440348819371e-08\n",
      "step: 23924, loss: 1.2397744342251826e-07\n",
      "step: 23925, loss: 2.9488472137018107e-05\n",
      "step: 23926, loss: 3.3612599509069696e-05\n",
      "step: 23927, loss: 2.3841844054572903e-08\n",
      "step: 23928, loss: 0.0005642207688651979\n",
      "step: 23929, loss: 1.430510998545742e-08\n",
      "step: 23930, loss: 2.38418573772492e-09\n",
      "step: 23931, loss: 7.274732342921197e-05\n",
      "step: 23932, loss: 1.192092646817855e-08\n",
      "step: 23933, loss: 1.3980752555653453e-05\n",
      "step: 23934, loss: 5.4836210239272987e-08\n",
      "step: 23935, loss: 3.8146950487316644e-08\n",
      "step: 23936, loss: 0.014750883914530277\n",
      "step: 23937, loss: 0.013410593383014202\n",
      "step: 23938, loss: 0.01425390038639307\n",
      "step: 23939, loss: 1.3041131978752674e-06\n",
      "step: 23940, loss: 4.76837103136063e-09\n",
      "step: 23941, loss: 0.15375608205795288\n",
      "step: 23942, loss: 9.059801300281833e-07\n",
      "step: 23943, loss: 7.152553394007555e-08\n",
      "step: 23944, loss: 2.1696024532502634e-07\n",
      "step: 23945, loss: 0.0002919570542871952\n",
      "step: 23946, loss: 5.078252343082568e-07\n",
      "step: 23947, loss: 2.1575731352641014e-06\n",
      "step: 23948, loss: 4.065196480951272e-05\n",
      "step: 23949, loss: 0.2002752721309662\n",
      "step: 23950, loss: 8.143409104377497e-06\n",
      "step: 23951, loss: 0.00010002498311223462\n",
      "step: 23952, loss: 2.3364900414435397e-07\n",
      "step: 23953, loss: 2.2195540623215493e-06\n",
      "step: 23954, loss: 0.0\n",
      "step: 23955, loss: 7.265686235768953e-06\n",
      "step: 23956, loss: 8.195627998475175e-08\n",
      "step: 23957, loss: 0.0002295647864229977\n",
      "step: 23958, loss: 1.335142343350526e-07\n",
      "step: 23959, loss: 0.0\n",
      "step: 23960, loss: 1.2254339480932686e-06\n",
      "step: 23961, loss: 0.1208619773387909\n",
      "step: 23962, loss: 0.0005291995476000011\n",
      "step: 23963, loss: 0.0\n",
      "step: 23964, loss: 1.8596598749809345e-07\n",
      "step: 23965, loss: 0.0\n",
      "step: 23966, loss: 0.11586418002843857\n",
      "step: 23967, loss: 0.00014812234439887106\n",
      "step: 23968, loss: 6.719492375850677e-05\n",
      "step: 23969, loss: 2.38418573772492e-09\n",
      "step: 23970, loss: 2.38418573772492e-09\n",
      "step: 23971, loss: 1.144405885611377e-07\n",
      "step: 23972, loss: 3.5762627703661565e-07\n",
      "step: 23973, loss: 1.907347702001516e-08\n",
      "step: 23974, loss: 0.0\n",
      "step: 23975, loss: 7.39802053431049e-05\n",
      "step: 23976, loss: 7.438527518388582e-07\n",
      "step: 23977, loss: 5.960456306297601e-08\n",
      "step: 23978, loss: 3.075589347645291e-07\n",
      "step: 23979, loss: 1.3195132851251401e-05\n",
      "step: 23980, loss: 2.622603290092229e-08\n",
      "step: 23981, loss: 0.0\n",
      "step: 23982, loss: 1.008485241982271e-06\n",
      "step: 23983, loss: 1.244524810317671e-06\n",
      "step: 23984, loss: 2.5272285597566224e-07\n",
      "step: 23985, loss: 0.0\n",
      "step: 23986, loss: 4.6014557142370904e-07\n",
      "step: 23987, loss: 5.984218205412617e-07\n",
      "step: 23988, loss: 0.0\n",
      "step: 23989, loss: 0.0001350154198007658\n",
      "step: 23990, loss: 0.0008340414497070014\n",
      "step: 23991, loss: 6.103015493863495e-06\n",
      "step: 23992, loss: 1.3589811942438246e-07\n",
      "step: 23993, loss: 4.76837147544984e-09\n",
      "step: 23994, loss: 0.032072845846414566\n",
      "step: 23995, loss: 3.910037094101426e-07\n",
      "step: 23996, loss: 4.333980086812517e-06\n",
      "step: 23997, loss: 5.674322665072395e-07\n",
      "step: 23998, loss: 1.4305064155450964e-07\n",
      "step: 23999, loss: 0.0\n",
      "step: 24000, loss: 0.041493479162454605\n",
      "step: 24001, loss: 0.01983761042356491\n",
      "step: 24002, loss: 5.459712610900169e-07\n",
      "step: 24003, loss: 1.1444065250998392e-07\n",
      "step: 24004, loss: 0.2874651253223419\n",
      "step: 24005, loss: 4.572379566525342e-06\n",
      "step: 24006, loss: 4.458392197648209e-07\n",
      "step: 24007, loss: 2.2172804392539547e-07\n",
      "step: 24008, loss: 4.529951169729429e-08\n",
      "step: 24009, loss: 7.80299014877528e-05\n",
      "step: 24010, loss: 3.4331981169088976e-07\n",
      "step: 24011, loss: 0.0\n",
      "step: 24012, loss: 1.4066651488064963e-07\n",
      "step: 24013, loss: 0.015314937569200993\n",
      "step: 24014, loss: 7.86779779105018e-08\n",
      "step: 24015, loss: 3.5762607808464963e-07\n",
      "step: 24016, loss: 1.1634494967438513e-06\n",
      "step: 24017, loss: 9.822703077588812e-07\n",
      "step: 24018, loss: 4.744502177800314e-07\n",
      "step: 24019, loss: 4.0531130451881836e-08\n",
      "step: 24020, loss: 4.346121386333834e-06\n",
      "step: 24021, loss: 5.423297352535883e-06\n",
      "step: 24022, loss: 2.38418573772492e-09\n",
      "step: 24023, loss: 2.38418573772492e-09\n",
      "step: 24024, loss: 0.0\n",
      "step: 24025, loss: 0.06777232885360718\n",
      "step: 24026, loss: 1.267032257601386e-05\n",
      "step: 24027, loss: 2.145766586636455e-08\n",
      "step: 24028, loss: 1.6450815110147232e-07\n",
      "step: 24029, loss: 0.0023060033563524485\n",
      "step: 24030, loss: 1.9140239601256326e-05\n",
      "step: 24031, loss: 5.9604552404834976e-08\n",
      "step: 24032, loss: 1.4447879266299424e-06\n",
      "step: 24033, loss: 1.972894460777752e-05\n",
      "step: 24034, loss: 3.0181536203599535e-06\n",
      "step: 24035, loss: 1.430511176181426e-08\n",
      "step: 24036, loss: 6.535597640322521e-05\n",
      "step: 24037, loss: 3.0994389277338996e-08\n",
      "step: 24038, loss: 8.344633073420482e-08\n",
      "step: 24039, loss: 0.0\n",
      "step: 24040, loss: 2.8610209312773804e-08\n",
      "step: 24041, loss: 0.005304065532982349\n",
      "step: 24042, loss: 0.0\n",
      "step: 24043, loss: 0.0\n",
      "step: 24044, loss: 0.017602426931262016\n",
      "step: 24045, loss: 3.0373701065400383e-06\n",
      "step: 24046, loss: 0.0028256899677217007\n",
      "step: 24047, loss: 0.0001771845418261364\n",
      "step: 24048, loss: 0.030496938154101372\n",
      "step: 24049, loss: 3.099439993548003e-08\n",
      "step: 24050, loss: 1.192092558000013e-08\n",
      "step: 24051, loss: 5.003783826396102e-06\n",
      "step: 24052, loss: 3.6971319786971435e-05\n",
      "step: 24053, loss: 9.727270935400156e-07\n",
      "step: 24054, loss: 5.722039020383818e-08\n",
      "step: 24055, loss: 7.15255632499634e-09\n",
      "step: 24056, loss: 0.000611428520642221\n",
      "step: 24057, loss: 4.76837103136063e-09\n",
      "step: 24058, loss: 0.2006317377090454\n",
      "step: 24059, loss: 2.1718876723753056e-06\n",
      "step: 24060, loss: 1.0442395250720438e-05\n",
      "step: 24061, loss: 5.4836217344700344e-08\n",
      "step: 24062, loss: 0.0\n",
      "step: 24063, loss: 8.924748726713005e-06\n",
      "step: 24064, loss: 2.38418573772492e-09\n",
      "step: 24065, loss: 0.0\n",
      "step: 24066, loss: 0.010037805885076523\n",
      "step: 24067, loss: 9.536720568803503e-08\n",
      "step: 24068, loss: 4.053111624102712e-08\n",
      "step: 24069, loss: 1.1050036846427247e-05\n",
      "step: 24070, loss: 2.38418573772492e-09\n",
      "step: 24071, loss: 2.4460337044729386e-06\n",
      "step: 24072, loss: 6.890186909913609e-07\n",
      "step: 24073, loss: 3.0278931717475643e-07\n",
      "step: 24074, loss: 0.24744652211666107\n",
      "step: 24075, loss: 1.192092558000013e-08\n",
      "step: 24076, loss: 0.00012743323168251663\n",
      "step: 24077, loss: 2.38418573772492e-09\n",
      "step: 24078, loss: 3.37673191097565e-05\n",
      "step: 24079, loss: 6.330474570859224e-05\n",
      "step: 24080, loss: 0.0011047855950891972\n",
      "step: 24081, loss: 0.001453641103580594\n",
      "step: 24082, loss: 2.951410351670347e-06\n",
      "step: 24083, loss: 2.1457660537294032e-08\n",
      "step: 24084, loss: 7.15255676908555e-09\n",
      "step: 24085, loss: 7.15255632499634e-09\n",
      "step: 24086, loss: 0.12209150195121765\n",
      "step: 24087, loss: 2.4419236069661565e-05\n",
      "step: 24088, loss: 0.0018700292566791177\n",
      "step: 24089, loss: 2.3841844054572903e-08\n",
      "step: 24090, loss: 1.4543005590894609e-06\n",
      "step: 24091, loss: 1.5019523743831087e-05\n",
      "step: 24092, loss: 6.687418499495834e-05\n",
      "step: 24093, loss: 2.7177870833838824e-06\n",
      "step: 24094, loss: 0.0\n",
      "step: 24095, loss: 0.10045976936817169\n",
      "step: 24096, loss: 0.004513835068792105\n",
      "step: 24097, loss: 0.0\n",
      "step: 24098, loss: 3.809578174696071e-06\n",
      "step: 24099, loss: 1.430511264999268e-08\n",
      "step: 24100, loss: 1.7642906868786667e-07\n",
      "step: 24101, loss: 4.76837103136063e-09\n",
      "step: 24102, loss: 3.0040541787457187e-07\n",
      "step: 24103, loss: 7.15255632499634e-09\n",
      "step: 24104, loss: 0.0\n",
      "step: 24105, loss: 5.531252327273251e-07\n",
      "step: 24106, loss: 0.013621709309518337\n",
      "step: 24107, loss: 4.76837103136063e-09\n",
      "step: 24108, loss: 0.0\n",
      "step: 24109, loss: 0.08884064853191376\n",
      "step: 24110, loss: 4.053074746934726e-07\n",
      "step: 24111, loss: 1.8835017101537233e-07\n",
      "step: 24112, loss: 1.2444158528523985e-05\n",
      "step: 24113, loss: 3.576275631189674e-08\n",
      "step: 24114, loss: 0.0\n",
      "step: 24115, loss: 9.52255868469365e-06\n",
      "step: 24116, loss: 0.006737865041941404\n",
      "step: 24117, loss: 0.00042861775727942586\n",
      "step: 24118, loss: 0.12730666995048523\n",
      "step: 24119, loss: 1.2516585456978646e-06\n",
      "step: 24120, loss: 3.0994389277338996e-08\n",
      "step: 24121, loss: 1.430511264999268e-08\n",
      "step: 24122, loss: 0.0\n",
      "step: 24123, loss: 1.2159310358583753e-07\n",
      "step: 24124, loss: 0.0\n",
      "step: 24125, loss: 2.38418573772492e-09\n",
      "step: 24126, loss: 6.651768558185722e-07\n",
      "step: 24127, loss: 0.0008845084230415523\n",
      "step: 24128, loss: 4.76837103136063e-09\n",
      "step: 24129, loss: 3.8146950487316644e-08\n",
      "step: 24130, loss: 2.3911982225399697e-06\n",
      "step: 24131, loss: 2.38418573772492e-09\n",
      "step: 24132, loss: 1.2778882592101581e-06\n",
      "step: 24133, loss: 1.2636147062039527e-07\n",
      "step: 24134, loss: 2.0814393792534247e-05\n",
      "step: 24135, loss: 0.0\n",
      "step: 24136, loss: 1.4543142015099875e-06\n",
      "step: 24137, loss: 0.12409895658493042\n",
      "step: 24138, loss: 0.01189707312732935\n",
      "step: 24139, loss: 0.0\n",
      "step: 24140, loss: 0.056568145751953125\n",
      "step: 24141, loss: 8.344476327692973e-07\n",
      "step: 24142, loss: 4.713359157904051e-05\n",
      "step: 24143, loss: 0.00020300765754655004\n",
      "step: 24144, loss: 2.0265477473913052e-07\n",
      "step: 24145, loss: 3.149285021208925e-06\n",
      "step: 24146, loss: 2.319678287676652e-06\n",
      "step: 24147, loss: 0.029399145394563675\n",
      "step: 24148, loss: 0.06141948327422142\n",
      "step: 24149, loss: 0.0\n",
      "step: 24150, loss: 0.0\n",
      "step: 24151, loss: 4.76837147544984e-09\n",
      "step: 24152, loss: 7.15255676908555e-09\n",
      "step: 24153, loss: 0.002349699614569545\n",
      "step: 24154, loss: 0.18747323751449585\n",
      "step: 24155, loss: 5.054414486949099e-07\n",
      "step: 24156, loss: 7.05885713614407e-06\n",
      "step: 24157, loss: 3.814693982917561e-08\n",
      "step: 24158, loss: 0.00013901446072850376\n",
      "step: 24159, loss: 0.0\n",
      "step: 24160, loss: 0.12586744129657745\n",
      "step: 24161, loss: 4.529947972287118e-08\n",
      "step: 24162, loss: 0.05411035940051079\n",
      "step: 24163, loss: 1.1444080172395843e-07\n",
      "step: 24164, loss: 3.886186448198714e-07\n",
      "step: 24165, loss: 0.0\n",
      "step: 24166, loss: 0.011646338738501072\n",
      "step: 24167, loss: 0.0007750639342702925\n",
      "step: 24168, loss: 1.0633186775521608e-06\n",
      "step: 24169, loss: 3.0794319172855467e-05\n",
      "step: 24170, loss: 0.13543246686458588\n",
      "step: 24171, loss: 4.7989142331061885e-06\n",
      "step: 24172, loss: 1.672313555900473e-05\n",
      "step: 24173, loss: 8.868974532560969e-07\n",
      "step: 24174, loss: 9.53674206272126e-09\n",
      "step: 24175, loss: 4.76837103136063e-09\n",
      "step: 24176, loss: 9.22468097996898e-06\n",
      "step: 24177, loss: 1.4972143844715902e-06\n",
      "step: 24178, loss: 1.668929350273629e-08\n",
      "step: 24179, loss: 0.0\n",
      "step: 24180, loss: 4.76837147544984e-09\n",
      "step: 24181, loss: 0.07505831867456436\n",
      "step: 24182, loss: 1.0989579379838688e-07\n",
      "step: 24183, loss: 1.6927688761825266e-07\n",
      "step: 24184, loss: 2.6226025795494934e-08\n",
      "step: 24185, loss: 1.3112979502238886e-07\n",
      "step: 24186, loss: 0.0\n",
      "step: 24187, loss: 0.0016071037389338017\n",
      "step: 24188, loss: 6.556407470270642e-07\n",
      "step: 24189, loss: 3.576277052275145e-08\n",
      "step: 24190, loss: 1.3112986607666244e-07\n",
      "step: 24191, loss: 2.1457660537294032e-08\n",
      "step: 24192, loss: 0.0\n",
      "step: 24193, loss: 9.53674117454284e-09\n",
      "step: 24194, loss: 1.192092558000013e-08\n",
      "step: 24195, loss: 1.399494522047462e-06\n",
      "step: 24196, loss: 2.38418573772492e-09\n",
      "step: 24197, loss: 1.5735582792331115e-07\n",
      "step: 24198, loss: 4.52994939337259e-08\n",
      "step: 24199, loss: 4.813295618077973e-06\n",
      "step: 24200, loss: 1.3113005081777374e-07\n",
      "step: 24201, loss: 0.0\n",
      "step: 24202, loss: 1.1205656846868806e-07\n",
      "step: 24203, loss: 1.2159316042925639e-07\n",
      "step: 24204, loss: 1.192092646817855e-08\n",
      "step: 24205, loss: 4.7683688109145805e-08\n",
      "step: 24206, loss: 4.291530331101967e-08\n",
      "step: 24207, loss: 3.0517344384861644e-07\n",
      "step: 24208, loss: 2.47944285547419e-06\n",
      "step: 24209, loss: 0.0\n",
      "step: 24210, loss: 2.1934407357093733e-07\n",
      "step: 24211, loss: 2.38418573772492e-09\n",
      "step: 24212, loss: 3.576277407546513e-08\n",
      "step: 24213, loss: 0.0\n",
      "step: 24214, loss: 0.0\n",
      "step: 24215, loss: 4.0054021610558266e-07\n",
      "step: 24216, loss: 0.001889987150207162\n",
      "step: 24217, loss: 1.4066660014577792e-07\n",
      "step: 24218, loss: 2.03124091058271e-06\n",
      "step: 24219, loss: 8.463736662633892e-07\n",
      "step: 24220, loss: 9.779080573935062e-05\n",
      "step: 24221, loss: 0.0\n",
      "step: 24222, loss: 4.410696874401765e-07\n",
      "step: 24223, loss: 0.00010432405542815104\n",
      "step: 24224, loss: 1.995465481741121e-06\n",
      "step: 24225, loss: 2.5813606043811888e-05\n",
      "step: 24226, loss: 2.38418573772492e-09\n",
      "step: 24227, loss: 0.016722485423088074\n",
      "step: 24228, loss: 0.0\n",
      "step: 24229, loss: 2.1696017427075276e-07\n",
      "step: 24230, loss: 1.192092824453539e-08\n",
      "step: 24231, loss: 1.907347702001516e-08\n",
      "step: 24232, loss: 2.38418573772492e-09\n",
      "step: 24233, loss: 0.0\n",
      "step: 24234, loss: 1.3851649782736786e-06\n",
      "step: 24235, loss: 2.38418573772492e-09\n",
      "step: 24236, loss: 6.901027063577203e-06\n",
      "step: 24237, loss: 4.434536435837799e-07\n",
      "step: 24238, loss: 1.907347702001516e-08\n",
      "step: 24239, loss: 0.0\n",
      "step: 24240, loss: 4.76837103136063e-09\n",
      "step: 24241, loss: 0.23398888111114502\n",
      "step: 24242, loss: 7.15255676908555e-09\n",
      "step: 24243, loss: 0.0\n",
      "step: 24244, loss: 1.1598084711295087e-05\n",
      "step: 24245, loss: 0.0009005937608890235\n",
      "step: 24246, loss: 4.76837147544984e-09\n",
      "step: 24247, loss: 2.098074958212237e-07\n",
      "step: 24248, loss: 7.867798501592915e-08\n",
      "step: 24249, loss: 4.053111624102712e-08\n",
      "step: 24250, loss: 4.7683161596978607e-07\n",
      "step: 24251, loss: 6.19887430275412e-08\n",
      "step: 24252, loss: 7.867798501592915e-08\n",
      "step: 24253, loss: 1.9311833909796405e-07\n",
      "step: 24254, loss: 4.768367389829109e-08\n",
      "step: 24255, loss: 1.907347702001516e-08\n",
      "step: 24256, loss: 0.0\n",
      "step: 24257, loss: 1.1491446230138536e-06\n",
      "step: 24258, loss: 2.3604263333254494e-05\n",
      "step: 24259, loss: 0.009534213691949844\n",
      "step: 24260, loss: 0.038940344005823135\n",
      "step: 24261, loss: 5.273161605146015e-06\n",
      "step: 24262, loss: 0.0\n",
      "step: 24263, loss: 2.145766586636455e-08\n",
      "step: 24264, loss: 2.7894787990589975e-07\n",
      "step: 24265, loss: 3.099439993548003e-08\n",
      "step: 24266, loss: 0.0\n",
      "step: 24267, loss: 0.08353840559720993\n",
      "step: 24268, loss: 5.5735708883730695e-06\n",
      "step: 24269, loss: 0.0\n",
      "step: 24270, loss: 6.675709585124423e-08\n",
      "step: 24271, loss: 0.05532803386449814\n",
      "step: 24272, loss: 0.00021750754967797548\n",
      "step: 24273, loss: 2.38418573772492e-09\n",
      "step: 24274, loss: 1.1083400750067085e-05\n",
      "step: 24275, loss: 0.0\n",
      "step: 24276, loss: 0.0001857379247667268\n",
      "step: 24277, loss: 2.622603290092229e-08\n",
      "step: 24278, loss: 2.112278934873757e-06\n",
      "step: 24279, loss: 0.0\n",
      "step: 24280, loss: 9.53674206272126e-09\n",
      "step: 24281, loss: 2.058400059468113e-05\n",
      "step: 24282, loss: 4.364967935543973e-06\n",
      "step: 24283, loss: 4.3630123514049046e-07\n",
      "step: 24284, loss: 2.9384949812083505e-05\n",
      "step: 24285, loss: 5.721988145523937e-07\n",
      "step: 24286, loss: 0.0011948267929255962\n",
      "step: 24287, loss: 6.390603084582835e-05\n",
      "step: 24288, loss: 0.0\n",
      "step: 24289, loss: 6.783443677704781e-05\n",
      "step: 24290, loss: 0.0\n",
      "step: 24291, loss: 2.38418573772492e-09\n",
      "step: 24292, loss: 2.2649683728559467e-07\n",
      "step: 24293, loss: 7.899636693764478e-05\n",
      "step: 24294, loss: 0.0\n",
      "step: 24295, loss: 0.15679581463336945\n",
      "step: 24296, loss: 0.0\n",
      "step: 24297, loss: 0.0529208667576313\n",
      "step: 24298, loss: 0.0006640248466283083\n",
      "step: 24299, loss: 0.04874330013990402\n",
      "step: 24300, loss: 0.0\n",
      "step: 24301, loss: 0.0\n",
      "step: 24302, loss: 4.76837103136063e-09\n",
      "step: 24303, loss: 0.0\n",
      "step: 24304, loss: 2.232973747595679e-05\n",
      "step: 24305, loss: 6.651796411460964e-07\n",
      "step: 24306, loss: 5.483622800284138e-08\n",
      "step: 24307, loss: 9.63190018410387e-07\n",
      "step: 24308, loss: 0.0\n",
      "step: 24309, loss: 5.10210895754426e-07\n",
      "step: 24310, loss: 2.38418573772492e-09\n",
      "step: 24311, loss: 0.0007435368024744093\n",
      "step: 24312, loss: 2.3364906098777283e-07\n",
      "step: 24313, loss: 2.38418573772492e-09\n",
      "step: 24314, loss: 2.293703073519282e-05\n",
      "step: 24315, loss: 6.437296207195686e-08\n",
      "step: 24316, loss: 1.3160272374079796e-06\n",
      "step: 24317, loss: 0.0\n",
      "step: 24318, loss: 1.9550276419977308e-07\n",
      "step: 24319, loss: 8.636046914034523e-06\n",
      "step: 24320, loss: 0.0\n",
      "step: 24321, loss: 0.0\n",
      "step: 24322, loss: 4.725162852992071e-06\n",
      "step: 24323, loss: 1.1920908349338788e-07\n",
      "step: 24324, loss: 6.429144377761986e-06\n",
      "step: 24325, loss: 3.8146623637658195e-07\n",
      "step: 24326, loss: 1.0171981557505205e-05\n",
      "step: 24327, loss: 7.39096535085082e-08\n",
      "step: 24328, loss: 0.00918577890843153\n",
      "step: 24329, loss: 1.5687327277191798e-06\n",
      "step: 24330, loss: 6.048285740689607e-06\n",
      "step: 24331, loss: 5.340505140338792e-07\n",
      "step: 24332, loss: 0.04242975637316704\n",
      "step: 24333, loss: 6.624572961300146e-06\n",
      "step: 24334, loss: 0.005132926627993584\n",
      "step: 24335, loss: 0.09510326385498047\n",
      "step: 24336, loss: 2.3082322513801046e-05\n",
      "step: 24337, loss: 0.11223010718822479\n",
      "step: 24338, loss: 0.07032675296068192\n",
      "step: 24339, loss: 0.0\n",
      "step: 24340, loss: 5.4836210239272987e-08\n",
      "step: 24341, loss: 6.212249445525231e-06\n",
      "step: 24342, loss: 4.76837103136063e-09\n",
      "step: 24343, loss: 1.3827898328599986e-06\n",
      "step: 24344, loss: 9.53674206272126e-09\n",
      "step: 24345, loss: 4.222136794851394e-06\n",
      "step: 24346, loss: 5.507425271389366e-07\n",
      "step: 24347, loss: 0.0\n",
      "step: 24348, loss: 0.025131577625870705\n",
      "step: 24349, loss: 1.3559772924054414e-05\n",
      "step: 24350, loss: 4.76837147544984e-09\n",
      "step: 24351, loss: 1.192092558000013e-08\n",
      "step: 24352, loss: 8.087908099696506e-06\n",
      "step: 24353, loss: 2.0217394194332883e-05\n",
      "step: 24354, loss: 0.003978549037128687\n",
      "step: 24355, loss: 0.010022200644016266\n",
      "step: 24356, loss: 4.7683691661859484e-08\n",
      "step: 24357, loss: 2.38418573772492e-09\n",
      "step: 24358, loss: 7.15255632499634e-09\n",
      "step: 24359, loss: 1.144405885611377e-07\n",
      "step: 24360, loss: 5.245202316928044e-08\n",
      "step: 24361, loss: 9.298312875216652e-08\n",
      "step: 24362, loss: 1.1989170161541551e-05\n",
      "step: 24363, loss: 1.8834990100913274e-07\n",
      "step: 24364, loss: 1.668929883180681e-08\n",
      "step: 24365, loss: 0.12098482251167297\n",
      "step: 24366, loss: 1.4352316384247388e-06\n",
      "step: 24367, loss: 0.006031930446624756\n",
      "step: 24368, loss: 4.76837103136063e-09\n",
      "step: 24369, loss: 1.668929705544997e-08\n",
      "step: 24370, loss: 1.0084863788506482e-06\n",
      "step: 24371, loss: 5.1587157940957695e-06\n",
      "step: 24372, loss: 1.668929527909313e-08\n",
      "step: 24373, loss: 0.0\n",
      "step: 24374, loss: 0.013596250675618649\n",
      "step: 24375, loss: 4.196130305444967e-07\n",
      "step: 24376, loss: 3.5285785315863905e-07\n",
      "step: 24377, loss: 1.0466302455824916e-06\n",
      "step: 24378, loss: 1.3425751603790559e-05\n",
      "step: 24379, loss: 2.486553512426326e-06\n",
      "step: 24380, loss: 2.9802220069541363e-07\n",
      "step: 24381, loss: 0.09659480303525925\n",
      "step: 24382, loss: 4.76837147544984e-09\n",
      "step: 24383, loss: 2.8133274554420495e-07\n",
      "step: 24384, loss: 0.0\n",
      "step: 24385, loss: 0.060150183737277985\n",
      "step: 24386, loss: 1.6092812984425109e-06\n",
      "step: 24387, loss: 0.0\n",
      "step: 24388, loss: 7.104753763087501e-07\n",
      "step: 24389, loss: 2.38418573772492e-09\n",
      "step: 24390, loss: 7.15255632499634e-09\n",
      "step: 24391, loss: 1.4948312809792696e-06\n",
      "step: 24392, loss: 0.0023213329259306192\n",
      "step: 24393, loss: 0.0\n",
      "step: 24394, loss: 0.0\n",
      "step: 24395, loss: 0.04555768519639969\n",
      "step: 24396, loss: 0.035544730722904205\n",
      "step: 24397, loss: 4.0511557017453015e-05\n",
      "step: 24398, loss: 9.53674117454284e-09\n",
      "step: 24399, loss: 8.487646709909313e-07\n",
      "step: 24400, loss: 0.0003976418811362237\n",
      "step: 24401, loss: 7.15255676908555e-09\n",
      "step: 24402, loss: 5.841179699928034e-07\n",
      "step: 24403, loss: 0.0\n",
      "step: 24404, loss: 4.529951169729429e-08\n",
      "step: 24405, loss: 0.0\n",
      "step: 24406, loss: 6.016779934725491e-06\n",
      "step: 24407, loss: 1.43051135381711e-08\n",
      "step: 24408, loss: 3.7252898543727042e-09\n",
      "step: 24409, loss: 4.76837103136063e-09\n",
      "step: 24410, loss: 0.0\n",
      "step: 24411, loss: 0.0\n",
      "step: 24412, loss: 8.320636197822751e-07\n",
      "step: 24413, loss: 4.76837147544984e-09\n",
      "step: 24414, loss: 9.53674117454284e-09\n",
      "step: 24415, loss: 5.2452055143703546e-08\n",
      "step: 24416, loss: 1.597400824948636e-07\n",
      "step: 24417, loss: 0.0\n",
      "step: 24418, loss: 1.192091971802256e-07\n",
      "step: 24419, loss: 7.15255632499634e-09\n",
      "step: 24420, loss: 0.0\n",
      "step: 24421, loss: 0.0\n",
      "step: 24422, loss: 1.0728551842476008e-06\n",
      "step: 24423, loss: 0.0\n",
      "step: 24424, loss: 6.914128647395046e-08\n",
      "step: 24425, loss: 0.0\n",
      "step: 24426, loss: 3.862343476157548e-07\n",
      "step: 24427, loss: 2.2457834347733296e-06\n",
      "step: 24428, loss: 0.0\n",
      "step: 24429, loss: 0.0\n",
      "step: 24430, loss: 0.0\n",
      "step: 24431, loss: 2.74179512871342e-07\n",
      "step: 24432, loss: 9.53674206272126e-09\n",
      "step: 24433, loss: 3.8146950487316644e-08\n",
      "step: 24434, loss: 6.914127226309574e-08\n",
      "step: 24435, loss: 2.38418573772492e-09\n",
      "step: 24436, loss: 3.0279045404313365e-07\n",
      "step: 24437, loss: 0.0\n",
      "step: 24438, loss: 0.0\n",
      "step: 24439, loss: 0.00024216476595029235\n",
      "step: 24440, loss: 0.0745123103260994\n",
      "step: 24441, loss: 9.53674206272126e-09\n",
      "step: 24442, loss: 4.6729522296118375e-07\n",
      "step: 24443, loss: 4.29153317327291e-08\n",
      "step: 24444, loss: 4.0531130451881836e-08\n",
      "step: 24445, loss: 9.53674206272126e-09\n",
      "step: 24446, loss: 0.05923660844564438\n",
      "step: 24447, loss: 2.1457660537294032e-08\n",
      "step: 24448, loss: 4.5299501039153256e-08\n",
      "step: 24449, loss: 0.0\n",
      "step: 24450, loss: 3.5141667922289344e-06\n",
      "step: 24451, loss: 4.911369160254253e-07\n",
      "step: 24452, loss: 7.15255632499634e-09\n",
      "step: 24453, loss: 4.76837103136063e-09\n",
      "step: 24454, loss: 3.576277052275145e-08\n",
      "step: 24455, loss: 0.0\n",
      "step: 24456, loss: 4.7683659687436375e-08\n",
      "step: 24457, loss: 0.0\n",
      "step: 24458, loss: 2.3841844054572903e-08\n",
      "step: 24459, loss: 0.0\n",
      "step: 24460, loss: 8.724216968403198e-06\n",
      "step: 24461, loss: 2.8632023258978734e-06\n",
      "step: 24462, loss: 8.353914745384827e-05\n",
      "step: 24463, loss: 0.06732181459665298\n",
      "step: 24464, loss: 0.0\n",
      "step: 24465, loss: 3.862343476157548e-07\n",
      "step: 24466, loss: 0.0\n",
      "step: 24467, loss: 5.587756731983973e-06\n",
      "step: 24468, loss: 4.93520360578259e-07\n",
      "step: 24469, loss: 1.303255521634128e-05\n",
      "step: 24470, loss: 1.192092558000013e-08\n",
      "step: 24471, loss: 3.886192700974789e-07\n",
      "step: 24472, loss: 2.38418573772492e-09\n",
      "step: 24473, loss: 0.0\n",
      "step: 24474, loss: 0.10115446150302887\n",
      "step: 24475, loss: 0.0\n",
      "step: 24476, loss: 1.3828231715251604e-07\n",
      "step: 24477, loss: 7.15255632499634e-09\n",
      "step: 24478, loss: 5.006785741556996e-08\n",
      "step: 24479, loss: 0.0020720248576253653\n",
      "step: 24480, loss: 1.2636161272894242e-07\n",
      "step: 24481, loss: 1.0728806643101052e-07\n",
      "step: 24482, loss: 0.08497905731201172\n",
      "step: 24483, loss: 0.11476954817771912\n",
      "step: 24484, loss: 2.38418573772492e-09\n",
      "step: 24485, loss: 0.0\n",
      "step: 24486, loss: 5.606819286185782e-06\n",
      "step: 24487, loss: 0.0\n",
      "step: 24488, loss: 4.467465259949677e-06\n",
      "step: 24489, loss: 3.027908803687751e-07\n",
      "step: 24490, loss: 2.38418573772492e-09\n",
      "step: 24491, loss: 4.76837147544984e-09\n",
      "step: 24492, loss: 2.813322055317258e-07\n",
      "step: 24493, loss: 1.8166689415011206e-06\n",
      "step: 24494, loss: 0.0006797218229621649\n",
      "step: 24495, loss: 0.0\n",
      "step: 24496, loss: 2.38418573772492e-09\n",
      "step: 24497, loss: 0.0\n",
      "step: 24498, loss: 3.9694650695309974e-06\n",
      "step: 24499, loss: 2.9086857011861866e-07\n",
      "step: 24500, loss: 8.871972568158526e-06\n",
      "step: 24501, loss: 0.005962206516414881\n",
      "step: 24502, loss: 6.198879276553271e-08\n",
      "step: 24503, loss: 3.886007107212208e-06\n",
      "step: 24504, loss: 1.192092646817855e-08\n",
      "step: 24505, loss: 7.653111993022321e-07\n",
      "step: 24506, loss: 0.0983256995677948\n",
      "step: 24507, loss: 7.15255676908555e-09\n",
      "step: 24508, loss: 4.434536435837799e-07\n",
      "step: 24509, loss: 8.583050714605633e-08\n",
      "step: 24510, loss: 2.408013699550793e-07\n",
      "step: 24511, loss: 0.0\n",
      "step: 24512, loss: 2.38418573772492e-09\n",
      "step: 24513, loss: 4.76837103136063e-09\n",
      "step: 24514, loss: 0.0\n",
      "step: 24515, loss: 5.245202316928044e-08\n",
      "step: 24516, loss: 0.000755357148591429\n",
      "step: 24517, loss: 1.0049459888250567e-05\n",
      "step: 24518, loss: 4.053111624102712e-08\n",
      "step: 24519, loss: 1.3136534562363522e-06\n",
      "step: 24520, loss: 2.38418573772492e-09\n",
      "step: 24521, loss: 9.321951210949919e-07\n",
      "step: 24522, loss: 3.337858700547258e-08\n",
      "step: 24523, loss: 0.0\n",
      "step: 24524, loss: 0.0006717419601045549\n",
      "step: 24525, loss: 7.15255632499634e-09\n",
      "step: 24526, loss: 8.779302152106538e-05\n",
      "step: 24527, loss: 0.0\n",
      "step: 24528, loss: 0.0\n",
      "step: 24529, loss: 5.722040796740657e-08\n",
      "step: 24530, loss: 4.1961322949646274e-07\n",
      "step: 24531, loss: 0.00015655392780900002\n",
      "step: 24532, loss: 5.483623510826874e-08\n",
      "step: 24533, loss: 1.668929883180681e-08\n",
      "step: 24534, loss: 4.76837147544984e-09\n",
      "step: 24535, loss: 2.38418573772492e-09\n",
      "step: 24536, loss: 2.38418573772492e-09\n",
      "step: 24537, loss: 5.531259148483514e-07\n",
      "step: 24538, loss: 9.059891681317822e-08\n",
      "step: 24539, loss: 2.3841845830929742e-08\n",
      "step: 24540, loss: 0.21323803067207336\n",
      "step: 24541, loss: 0.0\n",
      "step: 24542, loss: 0.0\n",
      "step: 24543, loss: 2.861022529998536e-08\n",
      "step: 24544, loss: 2.4556965172450873e-07\n",
      "step: 24545, loss: 0.0\n",
      "step: 24546, loss: 9.059895234031501e-08\n",
      "step: 24547, loss: 2.3841844054572903e-08\n",
      "step: 24548, loss: 2.145766764272139e-08\n",
      "step: 24549, loss: 0.0\n",
      "step: 24550, loss: 0.02770543284714222\n",
      "step: 24551, loss: 6.675713848380838e-08\n",
      "step: 24552, loss: 1.7856754084277782e-06\n",
      "step: 24553, loss: 0.0001782729523256421\n",
      "step: 24554, loss: 2.0980742476695013e-07\n",
      "step: 24555, loss: 4.5299501039153256e-08\n",
      "step: 24556, loss: 1.1229200254092575e-06\n",
      "step: 24557, loss: 0.0\n",
      "step: 24558, loss: 4.768314738612389e-07\n",
      "step: 24559, loss: 0.0\n",
      "step: 24560, loss: 0.0\n",
      "step: 24561, loss: 0.0\n",
      "step: 24562, loss: 8.85380450199591e-06\n",
      "step: 24563, loss: 1.9580855223466642e-05\n",
      "step: 24564, loss: 1.2874561150511e-07\n",
      "step: 24565, loss: 0.0013815517304465175\n",
      "step: 24566, loss: 2.622603290092229e-08\n",
      "step: 24567, loss: 1.9073478796372e-08\n",
      "step: 24568, loss: 0.0008239886956289411\n",
      "step: 24569, loss: 1.192092558000013e-08\n",
      "step: 24570, loss: 0.0\n",
      "step: 24571, loss: 0.00015785792493261397\n",
      "step: 24572, loss: 2.1620036932290532e-05\n",
      "step: 24573, loss: 0.004034238401800394\n",
      "step: 24574, loss: 3.2186304110837227e-07\n",
      "step: 24575, loss: 2.0265478894998523e-07\n",
      "step: 24576, loss: 9.528078226139769e-05\n",
      "step: 24577, loss: 0.13777919113636017\n",
      "step: 24578, loss: 0.0\n",
      "step: 24579, loss: 1.907348234908568e-08\n",
      "step: 24580, loss: 4.76837103136063e-09\n",
      "step: 24581, loss: 0.017758682370185852\n",
      "step: 24582, loss: 4.76837103136063e-09\n",
      "step: 24583, loss: 6.794813884880568e-07\n",
      "step: 24584, loss: 5.721964271288016e-07\n",
      "step: 24585, loss: 5.960460214282648e-08\n",
      "step: 24586, loss: 7.414685114781605e-07\n",
      "step: 24587, loss: 5.006785741556996e-08\n",
      "step: 24588, loss: 0.0\n",
      "step: 24589, loss: 2.6225953320135886e-07\n",
      "step: 24590, loss: 1.192092558000013e-08\n",
      "step: 24591, loss: 9.53674117454284e-09\n",
      "step: 24592, loss: 3.6239319456399244e-07\n",
      "step: 24593, loss: 0.0\n",
      "step: 24594, loss: 2.38418573772492e-09\n",
      "step: 24595, loss: 4.053112334645448e-08\n",
      "step: 24596, loss: 4.8683403292670846e-05\n",
      "step: 24597, loss: 0.12187045067548752\n",
      "step: 24598, loss: 2.7179561357115745e-07\n",
      "step: 24599, loss: 0.2692273259162903\n",
      "step: 24600, loss: 1.5735565739305457e-07\n",
      "step: 24601, loss: 0.0\n",
      "step: 24602, loss: 0.0894581601023674\n",
      "step: 24603, loss: 9.53674117454284e-09\n",
      "step: 24604, loss: 0.0\n",
      "step: 24605, loss: 1.7166064480989007e-07\n",
      "step: 24606, loss: 0.0\n",
      "step: 24607, loss: 0.0732480064034462\n",
      "step: 24608, loss: 2.38418573772492e-09\n",
      "step: 24609, loss: 2.384184938364342e-08\n",
      "step: 24610, loss: 8.916664455682621e-07\n",
      "step: 24611, loss: 0.0\n",
      "step: 24612, loss: 4.353276835900033e-06\n",
      "step: 24613, loss: 2.1696017427075276e-07\n",
      "step: 24614, loss: 0.09568598121404648\n",
      "step: 24615, loss: 3.0153911211527884e-05\n",
      "step: 24616, loss: 0.0\n",
      "step: 24617, loss: 1.430511264999268e-08\n",
      "step: 24618, loss: 1.1790052667493e-05\n",
      "step: 24619, loss: 5.602761916634336e-07\n",
      "step: 24620, loss: 2.3841844054572903e-08\n",
      "step: 24621, loss: 4.959046009389567e-07\n",
      "step: 24622, loss: 0.0\n",
      "step: 24623, loss: 2.3841844054572903e-08\n",
      "step: 24624, loss: 1.430510998545742e-08\n",
      "step: 24625, loss: 2.5938265935110394e-06\n",
      "step: 24626, loss: 2.1457660537294032e-08\n",
      "step: 24627, loss: 6.675715269466309e-08\n",
      "step: 24628, loss: 2.38418573772492e-09\n",
      "step: 24629, loss: 8.821471908504463e-08\n",
      "step: 24630, loss: 0.0\n",
      "step: 24631, loss: 5.00678503101426e-08\n",
      "step: 24632, loss: 7.15255632499634e-09\n",
      "step: 24633, loss: 1.1205641214928619e-07\n",
      "step: 24634, loss: 1.0076585112983594e-06\n",
      "step: 24635, loss: 0.0\n",
      "step: 24636, loss: 4.76837103136063e-09\n",
      "step: 24637, loss: 6.437291233396536e-08\n",
      "step: 24638, loss: 0.0\n",
      "step: 24639, loss: 0.0\n",
      "step: 24640, loss: 0.0\n",
      "step: 24641, loss: 2.38418573772492e-09\n",
      "step: 24642, loss: 0.0\n",
      "step: 24643, loss: 1.430511264999268e-08\n",
      "step: 24644, loss: 0.0854518786072731\n",
      "step: 24645, loss: 0.0\n",
      "step: 24646, loss: 2.384184938364342e-08\n",
      "step: 24647, loss: 9.775138209988654e-08\n",
      "step: 24648, loss: 0.0\n",
      "step: 24649, loss: 1.192092558000013e-08\n",
      "step: 24650, loss: 3.147106042433734e-07\n",
      "step: 24651, loss: 1.0728806643101052e-07\n",
      "step: 24652, loss: 2.0027067648697994e-07\n",
      "step: 24653, loss: 2.455696233027993e-07\n",
      "step: 24654, loss: 9.77513892053139e-08\n",
      "step: 24655, loss: 0.04045271873474121\n",
      "step: 24656, loss: 0.0\n",
      "step: 24657, loss: 2.598750938886951e-07\n",
      "step: 24658, loss: 1.668929705544997e-08\n",
      "step: 24659, loss: 0.0\n",
      "step: 24660, loss: 7.152551972922083e-08\n",
      "step: 24661, loss: 0.0\n",
      "step: 24662, loss: 7.15255676908555e-09\n",
      "step: 24663, loss: 4.76837103136063e-09\n",
      "step: 24664, loss: 4.76837147544984e-09\n",
      "step: 24665, loss: 0.0\n",
      "step: 24666, loss: 1.842904794102651e-06\n",
      "step: 24667, loss: 0.01924358494579792\n",
      "step: 24668, loss: 8.344633073420482e-08\n",
      "step: 24669, loss: 2.8918109364894917e-06\n",
      "step: 24670, loss: 1.430510998545742e-08\n",
      "step: 24671, loss: 2.38418573772492e-09\n",
      "step: 24672, loss: 0.0011267983354628086\n",
      "step: 24673, loss: 9.894126833387418e-07\n",
      "step: 24674, loss: 9.775149578672426e-08\n",
      "step: 24675, loss: 4.76837103136063e-09\n",
      "step: 24676, loss: 1.5162863746809307e-06\n",
      "step: 24677, loss: 0.0\n",
      "step: 24678, loss: 6.84249584992358e-07\n",
      "step: 24679, loss: 0.0\n",
      "step: 24680, loss: 2.38418573772492e-09\n",
      "step: 24681, loss: 1.1205653294155127e-07\n",
      "step: 24682, loss: 2.38418573772492e-09\n",
      "step: 24683, loss: 2.38418573772492e-09\n",
      "step: 24684, loss: 0.0\n",
      "step: 24685, loss: 2.1457660537294032e-08\n",
      "step: 24686, loss: 0.0002382102538831532\n",
      "step: 24687, loss: 1.907347702001516e-08\n",
      "step: 24688, loss: 1.2945798744112835e-06\n",
      "step: 24689, loss: 1.3827807379129808e-06\n",
      "step: 24690, loss: 6.437296207195686e-08\n",
      "step: 24691, loss: 9.53674206272126e-09\n",
      "step: 24692, loss: 7.15255676908555e-09\n",
      "step: 24693, loss: 0.0\n",
      "step: 24694, loss: 3.33785834527589e-08\n",
      "step: 24695, loss: 5.245201961656676e-08\n",
      "step: 24696, loss: 0.0\n",
      "step: 24697, loss: 0.0\n",
      "step: 24698, loss: 1.430510998545742e-08\n",
      "step: 24699, loss: 1.668929350273629e-08\n",
      "step: 24700, loss: 7.867802054306594e-08\n",
      "step: 24701, loss: 2.3935922399687115e-06\n",
      "step: 24702, loss: 5.722042217826129e-08\n",
      "step: 24703, loss: 2.145766231365087e-08\n",
      "step: 24704, loss: 0.0021320339292287827\n",
      "step: 24705, loss: 2.276817440360901e-06\n",
      "step: 24706, loss: 1.668929883180681e-08\n",
      "step: 24707, loss: 4.053111624102712e-08\n",
      "step: 24708, loss: 0.06727829575538635\n",
      "step: 24709, loss: 0.012210797518491745\n",
      "step: 24710, loss: 0.0\n",
      "step: 24711, loss: 0.0\n",
      "step: 24712, loss: 7.303446909645572e-05\n",
      "step: 24713, loss: 3.0994389277338996e-08\n",
      "step: 24714, loss: 1.6212408127103117e-07\n",
      "step: 24715, loss: 1.430510998545742e-08\n",
      "step: 24716, loss: 2.0027069069783465e-07\n",
      "step: 24717, loss: 2.0265477473913052e-07\n",
      "step: 24718, loss: 8.583059951661198e-08\n",
      "step: 24719, loss: 1.187289285553561e-06\n",
      "step: 24720, loss: 1.1634583643171936e-06\n",
      "step: 24721, loss: 4.76837103136063e-09\n",
      "step: 24722, loss: 2.145766231365087e-08\n",
      "step: 24723, loss: 0.0\n",
      "step: 24724, loss: 7.15255632499634e-09\n",
      "step: 24725, loss: 1.0728817301242088e-07\n",
      "step: 24726, loss: 6.437292654482007e-08\n",
      "step: 24727, loss: 1.668929527909313e-08\n",
      "step: 24728, loss: 0.0\n",
      "step: 24729, loss: 4.76837147544984e-09\n",
      "step: 24730, loss: 0.0\n",
      "step: 24731, loss: 8.255096872744616e-06\n",
      "step: 24732, loss: 4.76837103136063e-09\n",
      "step: 24733, loss: 0.0\n",
      "step: 24734, loss: 6.532575866913248e-07\n",
      "step: 24735, loss: 0.0\n",
      "step: 24736, loss: 7.15255676908555e-09\n",
      "step: 24737, loss: 0.0001174120043287985\n",
      "step: 24738, loss: 7.15255632499634e-09\n",
      "step: 24739, loss: 1.1402694326534402e-05\n",
      "step: 24740, loss: 1.0251840194541728e-06\n",
      "step: 24741, loss: 0.0\n",
      "step: 24742, loss: 0.0\n",
      "step: 24743, loss: 7.152545578037461e-08\n",
      "step: 24744, loss: 1.3112999397435487e-07\n",
      "step: 24745, loss: 9.53674117454284e-09\n",
      "step: 24746, loss: 1.430511264999268e-08\n",
      "step: 24747, loss: 1.5497158756261342e-07\n",
      "step: 24748, loss: 1.2663487723330036e-05\n",
      "step: 24749, loss: 0.18837061524391174\n",
      "step: 24750, loss: 2.38418573772492e-09\n",
      "step: 24751, loss: 3.838520683530078e-07\n",
      "step: 24752, loss: 1.430511176181426e-08\n",
      "step: 24753, loss: 0.0\n",
      "step: 24754, loss: 0.0\n",
      "step: 24755, loss: 0.050821997225284576\n",
      "step: 24756, loss: 4.76837147544984e-09\n",
      "step: 24757, loss: 9.53674206272126e-09\n",
      "step: 24758, loss: 2.6226029348208613e-08\n",
      "step: 24759, loss: 3.1232590913532476e-07\n",
      "step: 24760, loss: 1.668929350273629e-08\n",
      "step: 24761, loss: 9.53674117454284e-09\n",
      "step: 24762, loss: 4.159979653195478e-06\n",
      "step: 24763, loss: 2.024081140916678e-06\n",
      "step: 24764, loss: 0.0\n",
      "step: 24765, loss: 7.15255676908555e-09\n",
      "step: 24766, loss: 9.298307901417502e-08\n",
      "step: 24767, loss: 3.814693627646193e-08\n",
      "step: 24768, loss: 3.6477834441939194e-07\n",
      "step: 24769, loss: 1.8619628008309519e-06\n",
      "step: 24770, loss: 4.053111624102712e-08\n",
      "step: 24771, loss: 2.6226025795494934e-08\n",
      "step: 24772, loss: 1.907348234908568e-08\n",
      "step: 24773, loss: 0.0\n",
      "step: 24774, loss: 0.11339984089136124\n",
      "step: 24775, loss: 5.006783965200157e-08\n",
      "step: 24776, loss: 0.0\n",
      "step: 24777, loss: 0.02394459769129753\n",
      "step: 24778, loss: 0.0\n",
      "step: 24779, loss: 3.6000889735987585e-07\n",
      "step: 24780, loss: 2.551066700107185e-07\n",
      "step: 24781, loss: 0.0\n",
      "step: 24782, loss: 9.53674117454284e-09\n",
      "step: 24783, loss: 3.814695404003032e-08\n",
      "step: 24784, loss: 4.76837103136063e-09\n",
      "step: 24785, loss: 2.1457660537294032e-08\n",
      "step: 24786, loss: 4.0531130451881836e-08\n",
      "step: 24787, loss: 0.12081385403871536\n",
      "step: 24788, loss: 9.53674206272126e-09\n",
      "step: 24789, loss: 2.38418573772492e-09\n",
      "step: 24790, loss: 0.00010797630238812417\n",
      "step: 24791, loss: 2.38418573772492e-09\n",
      "step: 24792, loss: 4.291529975830599e-08\n",
      "step: 24793, loss: 0.00017244646733161062\n",
      "step: 24794, loss: 3.576275631189674e-08\n",
      "step: 24795, loss: 0.0\n",
      "step: 24796, loss: 9.918101113726152e-07\n",
      "step: 24797, loss: 0.0\n",
      "step: 24798, loss: 1.430511176181426e-08\n",
      "step: 24799, loss: 1.192092824453539e-08\n",
      "step: 24800, loss: 0.0\n",
      "step: 24801, loss: 1.2636144219868584e-07\n",
      "step: 24802, loss: 1.3589838943062205e-07\n",
      "step: 24803, loss: 1.192092558000013e-08\n",
      "step: 24804, loss: 2.384184938364342e-08\n",
      "step: 24805, loss: 9.53674117454284e-09\n",
      "step: 24806, loss: 1.192092558000013e-08\n",
      "step: 24807, loss: 3.814685385350458e-07\n",
      "step: 24808, loss: 0.0\n",
      "step: 24809, loss: 0.08881646394729614\n",
      "step: 24810, loss: 0.05783578380942345\n",
      "step: 24811, loss: 0.0\n",
      "step: 24812, loss: 2.3841845830929742e-08\n",
      "step: 24813, loss: 0.0\n",
      "step: 24814, loss: 5.2452055143703546e-08\n",
      "step: 24815, loss: 5.9604552404834976e-08\n",
      "step: 24816, loss: 0.0\n",
      "step: 24817, loss: 0.0\n",
      "step: 24818, loss: 0.0\n",
      "step: 24819, loss: 8.583057820032991e-08\n",
      "step: 24820, loss: 3.623636075644754e-06\n",
      "step: 24821, loss: 0.0\n",
      "step: 24822, loss: 1.668929350273629e-08\n",
      "step: 24823, loss: 0.0\n",
      "step: 24824, loss: 2.145766764272139e-08\n",
      "step: 24825, loss: 9.53674206272126e-09\n",
      "step: 24826, loss: 2.38418573772492e-09\n",
      "step: 24827, loss: 0.0\n",
      "step: 24828, loss: 2.38418573772492e-09\n",
      "step: 24829, loss: 5.483620668655931e-08\n",
      "step: 24830, loss: 1.907347702001516e-08\n",
      "step: 24831, loss: 1.192092646817855e-08\n",
      "step: 24832, loss: 0.0\n",
      "step: 24833, loss: 0.0\n",
      "step: 24834, loss: 2.38418573772492e-09\n",
      "step: 24835, loss: 1.192092646817855e-08\n",
      "step: 24836, loss: 0.0\n",
      "step: 24837, loss: 1.192092824453539e-08\n",
      "step: 24838, loss: 0.13933506608009338\n",
      "step: 24839, loss: 0.0\n",
      "step: 24840, loss: 0.0\n",
      "step: 24841, loss: 0.0\n",
      "step: 24842, loss: 2.2610756786889397e-05\n",
      "step: 24843, loss: 3.3328151403111406e-06\n",
      "step: 24844, loss: 0.012032304890453815\n",
      "step: 24845, loss: 0.0\n",
      "step: 24846, loss: 0.0\n",
      "step: 24847, loss: 0.0\n",
      "step: 24848, loss: 4.76837103136063e-09\n",
      "step: 24849, loss: 1.668929883180681e-08\n",
      "step: 24850, loss: 4.76837103136063e-09\n",
      "step: 24851, loss: 1.0251973492358957e-07\n",
      "step: 24852, loss: 4.7683659687436375e-08\n",
      "step: 24853, loss: 7.15255676908555e-09\n",
      "step: 24854, loss: 4.274391358194407e-06\n",
      "step: 24855, loss: 0.07529767602682114\n",
      "step: 24856, loss: 1.0275689419358969e-06\n",
      "step: 24857, loss: 0.0\n",
      "step: 24858, loss: 6.437298338823894e-08\n",
      "step: 24859, loss: 0.0\n",
      "step: 24860, loss: 0.0\n",
      "step: 24861, loss: 4.768366679286373e-08\n",
      "step: 24862, loss: 3.8146957592744e-08\n",
      "step: 24863, loss: 6.914127226309574e-08\n",
      "step: 24864, loss: 7.15255632499634e-09\n",
      "step: 24865, loss: 5.006785741556996e-08\n",
      "step: 24866, loss: 1.263615985180877e-07\n",
      "step: 24867, loss: 7.15255676908555e-09\n",
      "step: 24868, loss: 1.9048737840421381e-06\n",
      "step: 24869, loss: 2.38418573772492e-09\n",
      "step: 24870, loss: 2.38418573772492e-09\n",
      "step: 24871, loss: 0.0\n",
      "step: 24872, loss: 0.0\n",
      "step: 24873, loss: 1.192092646817855e-08\n",
      "step: 24874, loss: 4.76837147544984e-09\n",
      "step: 24875, loss: 1.907347702001516e-08\n",
      "step: 24876, loss: 0.0\n",
      "step: 24877, loss: 4.76837147544984e-09\n",
      "step: 24878, loss: 4.76837103136063e-09\n",
      "step: 24879, loss: 2.38418573772492e-09\n",
      "step: 24880, loss: 1.668929527909313e-08\n",
      "step: 24881, loss: 0.0\n",
      "step: 24882, loss: 0.0\n",
      "step: 24883, loss: 6.389550435415003e-07\n",
      "step: 24884, loss: 1.0013559403887484e-07\n",
      "step: 24885, loss: 8.988179729385593e-07\n",
      "step: 24886, loss: 3.814693627646193e-08\n",
      "step: 24887, loss: 0.0\n",
      "step: 24888, loss: 4.0531134004595515e-08\n",
      "step: 24889, loss: 2.38418573772492e-09\n",
      "step: 24890, loss: 0.0\n",
      "step: 24891, loss: 0.0\n",
      "step: 24892, loss: 0.051916468888521194\n",
      "step: 24893, loss: 7.15255632499634e-09\n",
      "step: 24894, loss: 0.0\n",
      "step: 24895, loss: 2.38418573772492e-09\n",
      "step: 24896, loss: 0.0\n",
      "step: 24897, loss: 0.0\n",
      "step: 24898, loss: 0.02130531333386898\n",
      "step: 24899, loss: 1.7051577742677182e-05\n",
      "step: 24900, loss: 1.192092824453539e-08\n",
      "step: 24901, loss: 0.0\n",
      "step: 24902, loss: 0.0\n",
      "step: 24903, loss: 0.0\n",
      "step: 24904, loss: 0.0\n",
      "step: 24905, loss: 1.835781858972041e-06\n",
      "step: 24906, loss: 1.192092558000013e-08\n",
      "step: 24907, loss: 6.437291233396536e-08\n",
      "step: 24908, loss: 4.76837103136063e-09\n",
      "step: 24909, loss: 0.0338037833571434\n",
      "step: 24910, loss: 0.0\n",
      "step: 24911, loss: 2.38418573772492e-09\n",
      "step: 24912, loss: 0.0\n",
      "step: 24913, loss: 5.72200349324703e-07\n",
      "step: 24914, loss: 2.384184938364342e-08\n",
      "step: 24915, loss: 4.76837103136063e-09\n",
      "step: 24916, loss: 2.38418573772492e-09\n",
      "step: 24917, loss: 0.0\n",
      "step: 24918, loss: 4.76837147544984e-09\n",
      "step: 24919, loss: 3.8146950487316644e-08\n",
      "step: 24920, loss: 2.584290314189275e-06\n",
      "step: 24921, loss: 2.145766586636455e-08\n",
      "step: 24922, loss: 0.0\n",
      "step: 24923, loss: 4.76837103136063e-09\n",
      "step: 24924, loss: 4.529949038101222e-08\n",
      "step: 24925, loss: 0.0\n",
      "step: 24926, loss: 6.437291943939272e-08\n",
      "step: 24927, loss: 1.1205643346556826e-07\n",
      "step: 24928, loss: 4.76837147544984e-09\n",
      "step: 24929, loss: 1.406664864589402e-07\n",
      "step: 24930, loss: 1.57355685814764e-07\n",
      "step: 24931, loss: 2.38418573772492e-09\n",
      "step: 24932, loss: 3.8146652059367625e-07\n",
      "step: 24933, loss: 5.483623510826874e-08\n",
      "step: 24934, loss: 0.07002834975719452\n",
      "step: 24935, loss: 5.483622800284138e-08\n",
      "step: 24936, loss: 0.0\n",
      "step: 24937, loss: 2.38418573772492e-09\n",
      "step: 24938, loss: 2.861002599274798e-07\n",
      "step: 24939, loss: 0.0\n",
      "step: 24940, loss: 6.91412793685231e-08\n",
      "step: 24941, loss: 9.775138209988654e-08\n",
      "step: 24942, loss: 0.0\n",
      "step: 24943, loss: 0.0\n",
      "step: 24944, loss: 0.0\n",
      "step: 24945, loss: 0.0\n",
      "step: 24946, loss: 0.0\n",
      "step: 24947, loss: 1.6450815110147232e-07\n",
      "step: 24948, loss: 7.15255632499634e-09\n",
      "step: 24949, loss: 9.53674117454284e-09\n",
      "step: 24950, loss: 2.38418573772492e-09\n",
      "step: 24951, loss: 0.0\n",
      "step: 24952, loss: 4.053112334645448e-08\n",
      "step: 24953, loss: 2.384184938364342e-08\n",
      "step: 24954, loss: 6.437293365024743e-08\n",
      "step: 24955, loss: 8.106218274406274e-08\n",
      "step: 24956, loss: 1.430511176181426e-08\n",
      "step: 24957, loss: 4.76837103136063e-09\n",
      "step: 24958, loss: 2.38418573772492e-09\n",
      "step: 24959, loss: 0.0\n",
      "step: 24960, loss: 2.38418573772492e-09\n",
      "step: 24961, loss: 0.0\n",
      "step: 24962, loss: 2.38418573772492e-09\n",
      "step: 24963, loss: 1.668929527909313e-08\n",
      "step: 24964, loss: 8.583060662203934e-08\n",
      "step: 24965, loss: 2.38418573772492e-09\n",
      "step: 24966, loss: 3.9815506625018315e-07\n",
      "step: 24967, loss: 4.982888981430733e-07\n",
      "step: 24968, loss: 7.15255632499634e-09\n",
      "step: 24969, loss: 0.0\n",
      "step: 24970, loss: 4.76837147544984e-09\n",
      "step: 24971, loss: 6.877196028653998e-06\n",
      "step: 24972, loss: 3.337859055818626e-08\n",
      "step: 24973, loss: 0.0\n",
      "step: 24974, loss: 4.76837103136063e-09\n",
      "step: 24975, loss: 1.430511264999268e-08\n",
      "step: 24976, loss: 0.0\n",
      "step: 24977, loss: 4.76837103136063e-09\n",
      "step: 24978, loss: 3.647785717930674e-07\n",
      "step: 24979, loss: 2.6226025795494934e-08\n",
      "step: 24980, loss: 7.104749215613992e-07\n",
      "step: 24981, loss: 1.273114776267903e-06\n",
      "step: 24982, loss: 4.76837103136063e-09\n",
      "step: 24983, loss: 0.03622831404209137\n",
      "step: 24984, loss: 2.8133197815805033e-07\n",
      "step: 24985, loss: 0.0\n",
      "step: 24986, loss: 0.045639023184776306\n",
      "step: 24987, loss: 0.0\n",
      "step: 24988, loss: 0.0\n",
      "step: 24989, loss: 9.53674295089968e-09\n",
      "step: 24990, loss: 1.263614990421047e-07\n",
      "step: 24991, loss: 2.38418573772492e-09\n",
      "step: 24992, loss: 0.09065365791320801\n",
      "step: 24993, loss: 5.006786096828364e-08\n",
      "step: 24994, loss: 5.4836210239272987e-08\n",
      "step: 24995, loss: 0.0\n",
      "step: 24996, loss: 0.0\n",
      "step: 24997, loss: 2.1457660537294032e-08\n",
      "step: 24998, loss: 0.0\n",
      "step: 24999, loss: 2.1695976215596602e-07\n",
      "step: 25000, loss: 0.0\n",
      "step: 25001, loss: 0.0\n",
      "step: 25002, loss: 7.15255676908555e-09\n",
      "step: 25003, loss: 2.38418573772492e-09\n",
      "step: 25004, loss: 0.0\n",
      "step: 25005, loss: 1.668929705544997e-08\n",
      "step: 25006, loss: 0.0\n",
      "step: 25007, loss: 1.3589817626780132e-07\n",
      "step: 25008, loss: 1.430511264999268e-08\n",
      "step: 25009, loss: 0.0\n",
      "step: 25010, loss: 0.0\n",
      "step: 25011, loss: 0.026433981955051422\n",
      "step: 25012, loss: 0.0\n",
      "step: 25013, loss: 0.0\n",
      "step: 25014, loss: 2.38418573772492e-09\n",
      "step: 25015, loss: 3.33785834527589e-08\n",
      "step: 25016, loss: 1.52587531943027e-07\n",
      "step: 25017, loss: 9.298316427930331e-08\n",
      "step: 25018, loss: 0.0\n",
      "step: 25019, loss: 1.192092646817855e-08\n",
      "step: 25020, loss: 4.0531134004595515e-08\n",
      "step: 25021, loss: 2.6226025795494934e-08\n",
      "step: 25022, loss: 1.0967236363512711e-07\n",
      "step: 25023, loss: 2.38418573772492e-09\n",
      "step: 25024, loss: 1.2397727289226168e-07\n",
      "step: 25025, loss: 7.15255632499634e-09\n",
      "step: 25026, loss: 1.6689236304046062e-07\n",
      "step: 25027, loss: 0.0\n",
      "step: 25028, loss: 2.38418573772492e-09\n",
      "step: 25029, loss: 2.38418573772492e-09\n",
      "step: 25030, loss: 9.53674117454284e-09\n",
      "step: 25031, loss: 1.430511176181426e-08\n",
      "step: 25032, loss: 5.483622800284138e-08\n",
      "step: 25033, loss: 0.056681204587221146\n",
      "step: 25034, loss: 1.668929527909313e-08\n",
      "step: 25035, loss: 5.4836210239272987e-08\n",
      "step: 25036, loss: 0.0\n",
      "step: 25037, loss: 0.0\n",
      "step: 25038, loss: 9.536728384773596e-08\n",
      "step: 25039, loss: 0.021289963275194168\n",
      "step: 25040, loss: 7.152545578037461e-08\n",
      "step: 25041, loss: 6.914127226309574e-08\n",
      "step: 25042, loss: 0.0\n",
      "step: 25043, loss: 7.15255676908555e-09\n",
      "step: 25044, loss: 1.883498583765686e-07\n",
      "step: 25045, loss: 8.638424333184958e-06\n",
      "step: 25046, loss: 0.0\n",
      "step: 25047, loss: 2.622603467727913e-08\n",
      "step: 25048, loss: 0.0\n",
      "step: 25049, loss: 2.1457660537294032e-08\n",
      "step: 25050, loss: 7.15255676908555e-09\n",
      "step: 25051, loss: 0.09223677963018417\n",
      "step: 25052, loss: 0.06690645962953568\n",
      "step: 25053, loss: 1.6212398179504817e-07\n",
      "step: 25054, loss: 7.15255632499634e-09\n",
      "step: 25055, loss: 1.1150118552905042e-05\n",
      "step: 25056, loss: 3.719295591508853e-07\n",
      "step: 25057, loss: 1.668929350273629e-08\n",
      "step: 25058, loss: 2.7417962655817973e-07\n",
      "step: 25059, loss: 3.9100279991544085e-07\n",
      "step: 25060, loss: 4.76837147544984e-09\n",
      "step: 25061, loss: 1.192092646817855e-08\n",
      "step: 25062, loss: 1.502032347389104e-07\n",
      "step: 25063, loss: 2.38418573772492e-09\n",
      "step: 25064, loss: 0.0\n",
      "step: 25065, loss: 5.0067875179138355e-08\n",
      "step: 25066, loss: 2.622603290092229e-08\n",
      "step: 25067, loss: 0.0\n",
      "step: 25068, loss: 0.0\n",
      "step: 25069, loss: 0.0\n",
      "step: 25070, loss: 7.629382992035971e-08\n",
      "step: 25071, loss: 4.005394202977186e-07\n",
      "step: 25072, loss: 1.192092824453539e-08\n",
      "step: 25073, loss: 2.38418573772492e-09\n",
      "step: 25074, loss: 0.10133136063814163\n",
      "step: 25075, loss: 4.76837103136063e-09\n",
      "step: 25076, loss: 1.192092824453539e-08\n",
      "step: 25077, loss: 0.10202792286872864\n",
      "step: 25078, loss: 0.0\n",
      "step: 25079, loss: 0.0891646072268486\n",
      "step: 25080, loss: 2.38418573772492e-09\n",
      "step: 25081, loss: 1.2874365893367212e-06\n",
      "step: 25082, loss: 0.08809222280979156\n",
      "step: 25083, loss: 3.0279036877800536e-07\n",
      "step: 25084, loss: 6.675714558923573e-08\n",
      "step: 25085, loss: 0.0\n",
      "step: 25086, loss: 2.235172580355993e-08\n",
      "step: 25087, loss: 2.338234116905369e-05\n",
      "step: 25088, loss: 0.00016641583351884037\n",
      "step: 25089, loss: 4.053111624102712e-08\n",
      "step: 25090, loss: 2.38418573772492e-09\n",
      "step: 25091, loss: 0.0\n",
      "step: 25092, loss: 7.15255632499634e-09\n",
      "step: 25093, loss: 4.4841522139904555e-06\n",
      "step: 25094, loss: 0.11544976383447647\n",
      "step: 25095, loss: 0.14525671303272247\n",
      "step: 25096, loss: 0.0\n",
      "step: 25097, loss: 0.0\n",
      "step: 25098, loss: 1.192092558000013e-08\n",
      "step: 25099, loss: 1.668929527909313e-08\n",
      "step: 25100, loss: 0.0\n",
      "step: 25101, loss: 0.0\n",
      "step: 25102, loss: 2.38418573772492e-09\n",
      "step: 25103, loss: 0.0\n",
      "step: 25104, loss: 5.9604580826544407e-08\n",
      "step: 25105, loss: 7.15255676908555e-09\n",
      "step: 25106, loss: 6.675714558923573e-08\n",
      "step: 25107, loss: 2.38418573772492e-09\n",
      "step: 25108, loss: 1.3351420591334318e-07\n",
      "step: 25109, loss: 0.0\n",
      "step: 25110, loss: 4.053112334645448e-08\n",
      "step: 25111, loss: 3.3378576347331546e-08\n",
      "step: 25112, loss: 1.6212406706017646e-07\n",
      "step: 25113, loss: 4.76837103136063e-09\n",
      "step: 25114, loss: 9.53674206272126e-09\n",
      "step: 25115, loss: 7.46241482829646e-07\n",
      "step: 25116, loss: 1.828586960073153e-06\n",
      "step: 25117, loss: 0.0008208674844354391\n",
      "step: 25118, loss: 6.437293365024743e-08\n",
      "step: 25119, loss: 1.6164334510904155e-06\n",
      "step: 25120, loss: 4.76837103136063e-09\n",
      "step: 25121, loss: 0.0\n",
      "step: 25122, loss: 0.0\n",
      "step: 25123, loss: 0.0\n",
      "step: 25124, loss: 3.7908341710135574e-07\n",
      "step: 25125, loss: 0.0\n",
      "step: 25126, loss: 6.675714558923573e-08\n",
      "step: 25127, loss: 2.38418573772492e-09\n",
      "step: 25128, loss: 6.198877855467799e-08\n",
      "step: 25129, loss: 1.192092558000013e-08\n",
      "step: 25130, loss: 1.952558022821904e-06\n",
      "step: 25131, loss: 1.430511176181426e-08\n",
      "step: 25132, loss: 7.86780773864848e-08\n",
      "step: 25133, loss: 2.3841844054572903e-08\n",
      "step: 25134, loss: 1.7166064480989007e-07\n",
      "step: 25135, loss: 0.0\n",
      "step: 25136, loss: 1.192092646817855e-08\n",
      "step: 25137, loss: 0.0\n",
      "step: 25138, loss: 1.907347702001516e-08\n",
      "step: 25139, loss: 1.192092558000013e-08\n",
      "step: 25140, loss: 3.5524067243386526e-07\n",
      "step: 25141, loss: 7.15255676908555e-09\n",
      "step: 25142, loss: 3.3378579900045224e-08\n",
      "step: 25143, loss: 1.9788650718055578e-07\n",
      "step: 25144, loss: 2.6226025795494934e-08\n",
      "step: 25145, loss: 2.1457660537294032e-08\n",
      "step: 25146, loss: 0.0\n",
      "step: 25147, loss: 0.0\n",
      "step: 25148, loss: 0.059176694601774216\n",
      "step: 25149, loss: 1.192092646817855e-08\n",
      "step: 25150, loss: 4.76837147544984e-09\n",
      "step: 25151, loss: 2.38418573772492e-09\n",
      "step: 25152, loss: 9.53674117454284e-09\n",
      "step: 25153, loss: 4.291529975830599e-08\n",
      "step: 25154, loss: 4.76837103136063e-09\n",
      "step: 25155, loss: 4.76837103136063e-09\n",
      "step: 25156, loss: 0.0\n",
      "step: 25157, loss: 8.821478303389085e-08\n",
      "step: 25158, loss: 2.38418573772492e-09\n",
      "step: 25159, loss: 4.76837147544984e-09\n",
      "step: 25160, loss: 8.82146906633352e-08\n",
      "step: 25161, loss: 1.907347702001516e-08\n",
      "step: 25162, loss: 0.0\n",
      "step: 25163, loss: 9.53674117454284e-09\n",
      "step: 25164, loss: 4.76837103136063e-09\n",
      "step: 25165, loss: 6.858126653241925e-06\n",
      "step: 25166, loss: 7.15255676908555e-09\n",
      "step: 25167, loss: 1.192092558000013e-08\n",
      "step: 25168, loss: 1.1444060277199242e-07\n",
      "step: 25169, loss: 3.337859055818626e-08\n",
      "step: 25170, loss: 1.4709885363117792e-06\n",
      "step: 25171, loss: 4.76837103136063e-09\n",
      "step: 25172, loss: 5.245204803827619e-08\n",
      "step: 25173, loss: 8.821468355790785e-08\n",
      "step: 25174, loss: 2.2411227007523848e-07\n",
      "step: 25175, loss: 0.0\n",
      "step: 25176, loss: 0.07006516307592392\n",
      "step: 25177, loss: 0.15463322401046753\n",
      "step: 25178, loss: 9.53674117454284e-09\n",
      "step: 25179, loss: 0.0\n",
      "step: 25180, loss: 4.76837103136063e-09\n",
      "step: 25181, loss: 6.437294786110215e-08\n",
      "step: 25182, loss: 7.15255632499634e-09\n",
      "step: 25183, loss: 2.670270760063431e-07\n",
      "step: 25184, loss: 6.198873592211385e-08\n",
      "step: 25185, loss: 2.38418573772492e-09\n",
      "step: 25186, loss: 1.9073478796372e-08\n",
      "step: 25187, loss: 8.090292794804554e-06\n",
      "step: 25188, loss: 7.867805607020273e-08\n",
      "step: 25189, loss: 9.536730516401803e-08\n",
      "step: 25190, loss: 4.76837103136063e-09\n",
      "step: 25191, loss: 6.675709585124423e-08\n",
      "step: 25192, loss: 0.0\n",
      "step: 25193, loss: 2.145766586636455e-08\n",
      "step: 25194, loss: 9.53674117454284e-09\n",
      "step: 25195, loss: 7.15255632499634e-09\n",
      "step: 25196, loss: 0.0\n",
      "step: 25197, loss: 8.129908906084893e-07\n",
      "step: 25198, loss: 0.0\n",
      "step: 25199, loss: 4.76837103136063e-09\n",
      "step: 25200, loss: 0.18119901418685913\n",
      "step: 25201, loss: 0.0\n",
      "step: 25202, loss: 5.48362244501277e-08\n",
      "step: 25203, loss: 2.8610211089130644e-08\n",
      "step: 25204, loss: 0.0\n",
      "step: 25205, loss: 0.0\n",
      "step: 25206, loss: 0.0639016330242157\n",
      "step: 25207, loss: 0.0\n",
      "step: 25208, loss: 4.76837147544984e-09\n",
      "step: 25209, loss: 1.3589811942438246e-07\n",
      "step: 25210, loss: 0.0\n",
      "step: 25211, loss: 4.76837103136063e-09\n",
      "step: 25212, loss: 2.861021997091484e-08\n",
      "step: 25213, loss: 0.0\n",
      "step: 25214, loss: 4.768368100371845e-08\n",
      "step: 25215, loss: 4.76837147544984e-09\n",
      "step: 25216, loss: 2.622603290092229e-08\n",
      "step: 25217, loss: 9.53672767423086e-08\n",
      "step: 25218, loss: 4.76837103136063e-09\n",
      "step: 25219, loss: 9.53674206272126e-09\n",
      "step: 25220, loss: 0.0\n",
      "step: 25221, loss: 2.38418573772492e-09\n",
      "step: 25222, loss: 2.38418573772492e-09\n",
      "step: 25223, loss: 0.07395781576633453\n",
      "step: 25224, loss: 2.145766764272139e-08\n",
      "step: 25225, loss: 8.58306137274667e-08\n",
      "step: 25226, loss: 4.76837103136063e-09\n",
      "step: 25227, loss: 5.388199610933952e-07\n",
      "step: 25228, loss: 2.38418573772492e-09\n",
      "step: 25229, loss: 2.38418573772492e-09\n",
      "step: 25230, loss: 0.0\n",
      "step: 25231, loss: 4.76837103136063e-09\n",
      "step: 25232, loss: 3.337858700547258e-08\n",
      "step: 25233, loss: 1.668929527909313e-08\n",
      "step: 25234, loss: 0.0\n",
      "step: 25235, loss: 5.604445959761506e-06\n",
      "step: 25236, loss: 4.76837103136063e-09\n",
      "step: 25237, loss: 9.53674117454284e-09\n",
      "step: 25238, loss: 1.1682478628927129e-07\n",
      "step: 25239, loss: 2.622603467727913e-08\n",
      "step: 25240, loss: 0.0\n",
      "step: 25241, loss: 7.15255676908555e-09\n",
      "step: 25242, loss: 0.0\n",
      "step: 25243, loss: 2.38418573772492e-09\n",
      "step: 25244, loss: 9.53674117454284e-09\n",
      "step: 25245, loss: 9.53674117454284e-09\n",
      "step: 25246, loss: 1.6927647550346592e-07\n",
      "step: 25247, loss: 2.38418573772492e-09\n",
      "step: 25248, loss: 0.0\n",
      "step: 25249, loss: 1.192092646817855e-08\n",
      "step: 25250, loss: 0.0\n",
      "step: 25251, loss: 0.0008235767018049955\n",
      "step: 25252, loss: 3.170945603869768e-07\n",
      "step: 25253, loss: 4.7683688109145805e-08\n",
      "step: 25254, loss: 2.4080145522020757e-07\n",
      "step: 25255, loss: 1.430511264999268e-08\n",
      "step: 25256, loss: 7.390962508679877e-08\n",
      "step: 25257, loss: 2.38418573772492e-09\n",
      "step: 25258, loss: 0.0\n",
      "step: 25259, loss: 2.38418573772492e-09\n",
      "step: 25260, loss: 1.907348234908568e-08\n",
      "step: 25261, loss: 4.76837103136063e-09\n",
      "step: 25262, loss: 0.0\n",
      "step: 25263, loss: 2.38418573772492e-09\n",
      "step: 25264, loss: 2.38418573772492e-09\n",
      "step: 25265, loss: 0.0\n",
      "step: 25266, loss: 4.7683688109145805e-08\n",
      "step: 25267, loss: 0.0\n",
      "step: 25268, loss: 3.814693627646193e-08\n",
      "step: 25269, loss: 2.38418573772492e-09\n",
      "step: 25270, loss: 2.38418573772492e-09\n",
      "step: 25271, loss: 2.38418573772492e-09\n",
      "step: 25272, loss: 0.0\n",
      "step: 25273, loss: 0.07298608124256134\n",
      "step: 25274, loss: 1.907348234908568e-08\n",
      "step: 25275, loss: 2.38418573772492e-09\n",
      "step: 25276, loss: 0.021383214741945267\n",
      "step: 25277, loss: 1.356560119347705e-06\n",
      "step: 25278, loss: 0.0\n",
      "step: 25279, loss: 1.668929527909313e-08\n",
      "step: 25280, loss: 1.430510998545742e-08\n",
      "step: 25281, loss: 4.76837147544984e-09\n",
      "step: 25282, loss: 7.343157903960673e-07\n",
      "step: 25283, loss: 0.03486800566315651\n",
      "step: 25284, loss: 1.0728809485271995e-07\n",
      "step: 25285, loss: 7.15255632499634e-09\n",
      "step: 25286, loss: 0.017242036759853363\n",
      "step: 25287, loss: 1.4211865163815673e-05\n",
      "step: 25288, loss: 2.38418573772492e-09\n",
      "step: 25289, loss: 1.668929350273629e-08\n",
      "step: 25290, loss: 2.38418573772492e-09\n",
      "step: 25291, loss: 4.76837147544984e-09\n",
      "step: 25292, loss: 0.0\n",
      "step: 25293, loss: 0.0\n",
      "step: 25294, loss: 7.15255632499634e-09\n",
      "step: 25295, loss: 2.38418573772492e-09\n",
      "step: 25296, loss: 7.15255632499634e-09\n",
      "step: 25297, loss: 3.576275631189674e-08\n",
      "step: 25298, loss: 2.38418573772492e-09\n",
      "step: 25299, loss: 2.38418573772492e-09\n",
      "step: 25300, loss: 2.574906545760314e-07\n",
      "step: 25301, loss: 1.1682475786756186e-07\n",
      "step: 25302, loss: 0.0\n",
      "step: 25303, loss: 0.053011149168014526\n",
      "step: 25304, loss: 4.76837103136063e-09\n",
      "step: 25305, loss: 0.1607586145401001\n",
      "step: 25306, loss: 0.0\n",
      "step: 25307, loss: 0.0\n",
      "step: 25308, loss: 6.437291233396536e-08\n",
      "step: 25309, loss: 4.76837103136063e-09\n",
      "step: 25310, loss: 0.13948935270309448\n",
      "step: 25311, loss: 0.12551316618919373\n",
      "step: 25312, loss: 1.8626450382086546e-09\n",
      "step: 25313, loss: 0.0\n",
      "step: 25314, loss: 6.19887430275412e-08\n",
      "step: 25315, loss: 2.38418573772492e-09\n",
      "step: 25316, loss: 0.0\n",
      "step: 25317, loss: 3.814693982917561e-08\n",
      "step: 25318, loss: 7.15255632499634e-09\n",
      "step: 25319, loss: 4.76837147544984e-09\n",
      "step: 25320, loss: 1.430510998545742e-08\n",
      "step: 25321, loss: 7.15255676908555e-09\n",
      "step: 25322, loss: 4.291530331101967e-08\n",
      "step: 25323, loss: 0.0\n",
      "step: 25324, loss: 4.76837103136063e-09\n",
      "step: 25325, loss: 3.3139929200842744e-07\n",
      "step: 25326, loss: 2.38418573772492e-09\n",
      "step: 25327, loss: 0.0\n",
      "step: 25328, loss: 2.38418573772492e-09\n",
      "step: 25329, loss: 3.8146957592744e-08\n",
      "step: 25330, loss: 2.145766764272139e-08\n",
      "step: 25331, loss: 0.0\n",
      "step: 25332, loss: 2.312647353619468e-07\n",
      "step: 25333, loss: 2.38418573772492e-09\n",
      "step: 25334, loss: 1.192092558000013e-08\n",
      "step: 25335, loss: 0.0\n",
      "step: 25336, loss: 4.887521640739578e-07\n",
      "step: 25337, loss: 1.2636147062039527e-07\n",
      "step: 25338, loss: 4.053114466273655e-08\n",
      "step: 25339, loss: 0.0\n",
      "step: 25340, loss: 2.38418573772492e-09\n",
      "step: 25341, loss: 0.0\n",
      "step: 25342, loss: 3.576277052275145e-08\n",
      "step: 25343, loss: 5.960460924825384e-08\n",
      "step: 25344, loss: 9.53674117454284e-09\n",
      "step: 25345, loss: 0.0\n",
      "step: 25346, loss: 1.668929350273629e-08\n",
      "step: 25347, loss: 0.0\n",
      "step: 25348, loss: 1.192092646817855e-08\n",
      "step: 25349, loss: 0.0\n",
      "step: 25350, loss: 0.0\n",
      "step: 25351, loss: 0.005738009698688984\n",
      "step: 25352, loss: 7.629382992035971e-08\n",
      "step: 25353, loss: 0.0\n",
      "step: 25354, loss: 0.0\n",
      "step: 25355, loss: 0.0\n",
      "step: 25356, loss: 1.192092558000013e-08\n",
      "step: 25357, loss: 1.1205641214928619e-07\n",
      "step: 25358, loss: 6.198875013296856e-08\n",
      "step: 25359, loss: 0.0\n",
      "step: 25360, loss: 0.0\n",
      "step: 25361, loss: 1.2889524441561662e-05\n",
      "step: 25362, loss: 0.0\n",
      "step: 25363, loss: 4.76837103136063e-09\n",
      "step: 25364, loss: 2.38418573772492e-09\n",
      "step: 25365, loss: 1.430510998545742e-08\n",
      "step: 25366, loss: 7.15255632499634e-09\n",
      "step: 25367, loss: 0.0\n",
      "step: 25368, loss: 0.0\n",
      "step: 25369, loss: 7.15255632499634e-09\n",
      "step: 25370, loss: 9.53674206272126e-09\n",
      "step: 25371, loss: 0.047666896134614944\n",
      "step: 25372, loss: 4.76837103136063e-09\n",
      "step: 25373, loss: 2.145766586636455e-08\n",
      "step: 25374, loss: 7.15255632499634e-09\n",
      "step: 25375, loss: 5.960456306297601e-08\n",
      "step: 25376, loss: 6.91412793685231e-08\n",
      "step: 25377, loss: 0.0\n",
      "step: 25378, loss: 0.0\n",
      "step: 25379, loss: 2.38418573772492e-09\n",
      "step: 25380, loss: 0.0603000670671463\n",
      "step: 25381, loss: 7.15255676908555e-09\n",
      "step: 25382, loss: 9.53674117454284e-09\n",
      "step: 25383, loss: 0.0\n",
      "step: 25384, loss: 9.53674117454284e-09\n",
      "step: 25385, loss: 5.483620668655931e-08\n",
      "step: 25386, loss: 4.76837103136063e-09\n",
      "step: 25387, loss: 0.0\n",
      "step: 25388, loss: 2.38418573772492e-09\n",
      "step: 25389, loss: 3.337857279461787e-08\n",
      "step: 25390, loss: 1.192092824453539e-08\n",
      "step: 25391, loss: 2.050389724672641e-07\n",
      "step: 25392, loss: 9.059889549689615e-08\n",
      "step: 25393, loss: 2.38418573772492e-09\n",
      "step: 25394, loss: 9.53674295089968e-09\n",
      "step: 25395, loss: 3.0994389277338996e-08\n",
      "step: 25396, loss: 0.04039883241057396\n",
      "step: 25397, loss: 2.38418573772492e-09\n",
      "step: 25398, loss: 1.192092558000013e-08\n",
      "step: 25399, loss: 0.0\n",
      "step: 25400, loss: 2.38418573772492e-09\n",
      "step: 25401, loss: 7.629387255292386e-08\n",
      "step: 25402, loss: 0.0\n",
      "step: 25403, loss: 0.0\n",
      "step: 25404, loss: 7.15255632499634e-09\n",
      "step: 25405, loss: 0.0\n",
      "step: 25406, loss: 2.38418573772492e-09\n",
      "step: 25407, loss: 0.0\n",
      "step: 25408, loss: 1.2012687875539996e-05\n",
      "step: 25409, loss: 6.939167633390753e-06\n",
      "step: 25410, loss: 8.82146906633352e-08\n",
      "step: 25411, loss: 0.01834527961909771\n",
      "step: 25412, loss: 2.38418573772492e-09\n",
      "step: 25413, loss: 2.38418573772492e-09\n",
      "step: 25414, loss: 0.0\n",
      "step: 25415, loss: 0.028420301154255867\n",
      "step: 25416, loss: 0.0\n",
      "step: 25417, loss: 0.0\n",
      "step: 25418, loss: 0.0\n",
      "step: 25419, loss: 1.192092824453539e-08\n",
      "step: 25420, loss: 0.0\n",
      "step: 25421, loss: 0.08203496783971786\n",
      "step: 25422, loss: 0.0015402967110276222\n",
      "step: 25423, loss: 2.38418573772492e-09\n",
      "step: 25424, loss: 8.821468355790785e-08\n",
      "step: 25425, loss: 0.0\n",
      "step: 25426, loss: 2.38418573772492e-09\n",
      "step: 25427, loss: 7.15255676908555e-09\n",
      "step: 25428, loss: 2.74179711823308e-07\n",
      "step: 25429, loss: 0.0008064721478149295\n",
      "step: 25430, loss: 0.0\n",
      "step: 25431, loss: 9.53674117454284e-09\n",
      "step: 25432, loss: 2.384184938364342e-08\n",
      "step: 25433, loss: 4.76837103136063e-09\n",
      "step: 25434, loss: 7.15255676908555e-09\n",
      "step: 25435, loss: 1.9073478796372e-08\n",
      "step: 25436, loss: 1.0251974913444428e-07\n",
      "step: 25437, loss: 0.0\n",
      "step: 25438, loss: 2.38418573772492e-09\n",
      "step: 25439, loss: 0.0\n",
      "step: 25440, loss: 7.152548420208404e-08\n",
      "step: 25441, loss: 5.054411644778156e-07\n",
      "step: 25442, loss: 2.38418573772492e-09\n",
      "step: 25443, loss: 4.7683688109145805e-08\n",
      "step: 25444, loss: 0.0\n",
      "step: 25445, loss: 0.0\n",
      "step: 25446, loss: 0.0\n",
      "step: 25447, loss: 2.38418573772492e-09\n",
      "step: 25448, loss: 0.0014921380206942558\n",
      "step: 25449, loss: 0.0\n",
      "step: 25450, loss: 9.53674117454284e-09\n",
      "step: 25451, loss: 1.907347702001516e-08\n",
      "step: 25452, loss: 1.192092558000013e-08\n",
      "step: 25453, loss: 2.38418573772492e-09\n",
      "step: 25454, loss: 0.0\n",
      "step: 25455, loss: 2.38418573772492e-09\n",
      "step: 25456, loss: 0.0858125388622284\n",
      "step: 25457, loss: 2.38418573772492e-09\n",
      "step: 25458, loss: 2.38418573772492e-09\n",
      "step: 25459, loss: 9.53674117454284e-09\n",
      "step: 25460, loss: 2.384185116000026e-08\n",
      "step: 25461, loss: 0.0\n",
      "step: 25462, loss: 1.203981923936226e-06\n",
      "step: 25463, loss: 9.53674117454284e-09\n",
      "step: 25464, loss: 7.033228826003324e-07\n",
      "step: 25465, loss: 3.0994389277338996e-08\n",
      "step: 25466, loss: 1.2636144219868584e-07\n",
      "step: 25467, loss: 2.38418573772492e-09\n",
      "step: 25468, loss: 0.0\n",
      "step: 25469, loss: 1.668929527909313e-08\n",
      "step: 25470, loss: 2.145766231365087e-08\n",
      "step: 25471, loss: 0.00021108187502250075\n",
      "step: 25472, loss: 0.043108440935611725\n",
      "step: 25473, loss: 2.38418573772492e-09\n",
      "step: 25474, loss: 7.629380149865028e-08\n",
      "step: 25475, loss: 0.0\n",
      "step: 25476, loss: 3.8146950487316644e-08\n",
      "step: 25477, loss: 1.2778828022419475e-06\n",
      "step: 25478, loss: 2.3841844054572903e-08\n",
      "step: 25479, loss: 9.536721279346239e-08\n",
      "step: 25480, loss: 8.106216142778067e-08\n",
      "step: 25481, loss: 3.8146946934602965e-08\n",
      "step: 25482, loss: 5.674281737810816e-07\n",
      "step: 25483, loss: 7.15255632499634e-09\n",
      "step: 25484, loss: 9.53674206272126e-09\n",
      "step: 25485, loss: 3.0994396382766354e-08\n",
      "step: 25486, loss: 9.53674117454284e-09\n",
      "step: 25487, loss: 0.0771031528711319\n",
      "step: 25488, loss: 1.0728819432870296e-07\n",
      "step: 25489, loss: 0.11608348041772842\n",
      "step: 25490, loss: 2.9086885433571297e-07\n",
      "step: 25491, loss: 1.1205641214928619e-07\n",
      "step: 25492, loss: 2.7179535777577257e-07\n",
      "step: 25493, loss: 0.13705353438854218\n",
      "step: 25494, loss: 4.291529975830599e-08\n",
      "step: 25495, loss: 0.0\n",
      "step: 25496, loss: 0.0\n",
      "step: 25497, loss: 8.821470487418992e-08\n",
      "step: 25498, loss: 2.38418573772492e-09\n",
      "step: 25499, loss: 2.1695976215596602e-07\n",
      "step: 25500, loss: 1.907348234908568e-08\n",
      "step: 25501, loss: 9.77514886812969e-08\n",
      "step: 25502, loss: 0.0\n",
      "step: 25503, loss: 3.576275631189674e-08\n",
      "step: 25504, loss: 1.0347135912525118e-06\n",
      "step: 25505, loss: 4.76837103136063e-09\n",
      "step: 25506, loss: 2.38418573772492e-09\n",
      "step: 25507, loss: 4.76837147544984e-09\n",
      "step: 25508, loss: 3.337857279461787e-08\n",
      "step: 25509, loss: 2.861022529998536e-08\n",
      "step: 25510, loss: 3.337857279461787e-08\n",
      "step: 25511, loss: 4.76837103136063e-09\n",
      "step: 25512, loss: 2.38418573772492e-09\n",
      "step: 25513, loss: 2.38418573772492e-09\n",
      "step: 25514, loss: 7.15255676908555e-09\n",
      "step: 25515, loss: 2.145766764272139e-08\n",
      "step: 25516, loss: 4.053111624102712e-08\n",
      "step: 25517, loss: 2.38418573772492e-09\n",
      "step: 25518, loss: 0.0\n",
      "step: 25519, loss: 4.76837103136063e-09\n",
      "step: 25520, loss: 2.38418573772492e-09\n",
      "step: 25521, loss: 0.0\n",
      "step: 25522, loss: 7.15255632499634e-09\n",
      "step: 25523, loss: 2.38418573772492e-09\n",
      "step: 25524, loss: 2.1551882127823774e-06\n",
      "step: 25525, loss: 2.622603645363597e-08\n",
      "step: 25526, loss: 0.0\n",
      "step: 25527, loss: 9.298302927618352e-08\n",
      "step: 25528, loss: 4.76837103136063e-09\n",
      "step: 25529, loss: 4.196143379431305e-07\n",
      "step: 25530, loss: 4.76837147544984e-09\n",
      "step: 25531, loss: 2.6867969609156717e-06\n",
      "step: 25532, loss: 7.15255632499634e-09\n",
      "step: 25533, loss: 1.192092558000013e-08\n",
      "step: 25534, loss: 1.192092824453539e-08\n",
      "step: 25535, loss: 7.15255632499634e-09\n",
      "step: 25536, loss: 0.06648216396570206\n",
      "step: 25537, loss: 3.254148850828642e-06\n",
      "step: 25538, loss: 5.587935003603661e-09\n",
      "step: 25539, loss: 0.0\n",
      "step: 25540, loss: 2.574903987806465e-07\n",
      "step: 25541, loss: 6.675714558923573e-08\n",
      "step: 25542, loss: 2.38418573772492e-09\n",
      "step: 25543, loss: 3.814695404003032e-08\n",
      "step: 25544, loss: 4.529949748643958e-08\n",
      "step: 25545, loss: 0.0\n",
      "step: 25546, loss: 4.7683688109145805e-08\n",
      "step: 25547, loss: 0.00048404288827441633\n",
      "step: 25548, loss: 9.53674206272126e-09\n",
      "step: 25549, loss: 0.0\n",
      "step: 25550, loss: 1.668929527909313e-08\n",
      "step: 25551, loss: 0.0\n",
      "step: 25552, loss: 7.15255676908555e-09\n",
      "step: 25553, loss: 4.2915317521874385e-08\n",
      "step: 25554, loss: 9.775139631074126e-08\n",
      "step: 25555, loss: 5.245201961656676e-08\n",
      "step: 25556, loss: 4.76837103136063e-09\n",
      "step: 25557, loss: 1.2636157009637827e-07\n",
      "step: 25558, loss: 0.0\n",
      "step: 25559, loss: 1.478191222759051e-07\n",
      "step: 25560, loss: 1.502031352629274e-07\n",
      "step: 25561, loss: 0.0012804587604478002\n",
      "step: 25562, loss: 7.15255632499634e-09\n",
      "step: 25563, loss: 4.76837147544984e-09\n",
      "step: 25564, loss: 0.0\n",
      "step: 25565, loss: 0.0\n",
      "step: 25566, loss: 4.76837103136063e-09\n",
      "step: 25567, loss: 0.0\n",
      "step: 25568, loss: 4.053112689916816e-08\n",
      "step: 25569, loss: 7.15255632499634e-09\n",
      "step: 25570, loss: 0.0\n",
      "step: 25571, loss: 0.0\n",
      "step: 25572, loss: 2.5749110932338226e-07\n",
      "step: 25573, loss: 5.2452033827421474e-08\n",
      "step: 25574, loss: 2.38418573772492e-09\n",
      "step: 25575, loss: 0.0\n",
      "step: 25576, loss: 2.0550628505588975e-06\n",
      "step: 25577, loss: 1.430510998545742e-08\n",
      "step: 25578, loss: 4.76837103136063e-09\n",
      "step: 25579, loss: 2.38418573772492e-09\n",
      "step: 25580, loss: 2.38418573772492e-09\n",
      "step: 25581, loss: 6.437292654482007e-08\n",
      "step: 25582, loss: 0.0\n",
      "step: 25583, loss: 0.0\n",
      "step: 25584, loss: 4.959054535902396e-07\n",
      "step: 25585, loss: 0.18497467041015625\n",
      "step: 25586, loss: 0.0\n",
      "step: 25587, loss: 4.529951169729429e-08\n",
      "step: 25588, loss: 1.5639649291188107e-06\n",
      "step: 25589, loss: 0.007824183441698551\n",
      "step: 25590, loss: 1.192092558000013e-08\n",
      "step: 25591, loss: 2.264964109599532e-07\n",
      "step: 25592, loss: 0.0\n",
      "step: 25593, loss: 7.15255676908555e-09\n",
      "step: 25594, loss: 1.907347702001516e-08\n",
      "step: 25595, loss: 9.53674206272126e-09\n",
      "step: 25596, loss: 2.45569736989637e-07\n",
      "step: 25597, loss: 1.208745629810437e-06\n",
      "step: 25598, loss: 2.38418573772492e-09\n",
      "step: 25599, loss: 0.0015488140052184463\n",
      "step: 25600, loss: 5.483622800284138e-08\n",
      "step: 25601, loss: 1.430511176181426e-08\n",
      "step: 25602, loss: 2.38418573772492e-09\n",
      "step: 25603, loss: 1.668929883180681e-08\n",
      "step: 25604, loss: 1.1205646899270505e-07\n",
      "step: 25605, loss: 1.6212403863846703e-07\n",
      "step: 25606, loss: 7.15255676908555e-09\n",
      "step: 25607, loss: 0.10078219324350357\n",
      "step: 25608, loss: 2.38418573772492e-09\n",
      "step: 25609, loss: 2.384184938364342e-08\n",
      "step: 25610, loss: 0.04155649617314339\n",
      "step: 25611, loss: 2.861022529998536e-08\n",
      "step: 25612, loss: 9.53674117454284e-09\n",
      "step: 25613, loss: 6.008057766848651e-07\n",
      "step: 25614, loss: 2.38418573772492e-09\n",
      "step: 25615, loss: 7.15255632499634e-09\n",
      "step: 25616, loss: 1.430510998545742e-08\n",
      "step: 25617, loss: 0.0\n",
      "step: 25618, loss: 6.198873592211385e-08\n",
      "step: 25619, loss: 0.0\n",
      "step: 25620, loss: 1.907347702001516e-08\n",
      "step: 25621, loss: 7.15255632499634e-09\n",
      "step: 25622, loss: 2.38418573772492e-09\n",
      "step: 25623, loss: 4.76837103136063e-09\n",
      "step: 25624, loss: 0.0\n",
      "step: 25625, loss: 0.06785821914672852\n",
      "step: 25626, loss: 9.53674295089968e-09\n",
      "step: 25627, loss: 3.099440704090739e-08\n",
      "step: 25628, loss: 2.38418573772492e-09\n",
      "step: 25629, loss: 1.668929527909313e-08\n",
      "step: 25630, loss: 1.907347702001516e-08\n",
      "step: 25631, loss: 8.82146906633352e-08\n",
      "step: 25632, loss: 1.668929705544997e-08\n",
      "step: 25633, loss: 7.15255632499634e-09\n",
      "step: 25634, loss: 2.384184938364342e-08\n",
      "step: 25635, loss: 0.0\n",
      "step: 25636, loss: 9.53674206272126e-09\n",
      "step: 25637, loss: 4.76837147544984e-09\n",
      "step: 25638, loss: 2.38418573772492e-09\n",
      "step: 25639, loss: 0.0\n",
      "step: 25640, loss: 0.0\n",
      "step: 25641, loss: 0.0\n",
      "step: 25642, loss: 1.192092558000013e-08\n",
      "step: 25643, loss: 0.0\n",
      "step: 25644, loss: 4.005394202977186e-07\n",
      "step: 25645, loss: 0.0\n",
      "step: 25646, loss: 9.53674206272126e-09\n",
      "step: 25647, loss: 0.0\n",
      "step: 25648, loss: 2.38418573772492e-09\n",
      "step: 25649, loss: 7.15255676908555e-09\n",
      "step: 25650, loss: 2.38418573772492e-09\n",
      "step: 25651, loss: 0.0\n",
      "step: 25652, loss: 5.960457727383073e-08\n",
      "step: 25653, loss: 0.0\n",
      "step: 25654, loss: 4.76837103136063e-09\n",
      "step: 25655, loss: 2.38418573772492e-09\n",
      "step: 25656, loss: 0.037326227873563766\n",
      "step: 25657, loss: 0.0\n",
      "step: 25658, loss: 0.0\n",
      "step: 25659, loss: 2.38418573772492e-09\n",
      "step: 25660, loss: 4.053111624102712e-08\n",
      "step: 25661, loss: 8.106215432235331e-08\n",
      "step: 25662, loss: 0.0\n",
      "step: 25663, loss: 4.5299501039153256e-08\n",
      "step: 25664, loss: 0.0\n",
      "step: 25665, loss: 2.38418573772492e-09\n",
      "step: 25666, loss: 0.0\n",
      "step: 25667, loss: 0.0\n",
      "step: 25668, loss: 0.0\n",
      "step: 25669, loss: 0.0\n",
      "step: 25670, loss: 0.0\n",
      "step: 25671, loss: 0.1619875580072403\n",
      "step: 25672, loss: 1.192092558000013e-08\n",
      "step: 25673, loss: 4.5299501039153256e-08\n",
      "step: 25674, loss: 2.38418573772492e-09\n",
      "step: 25675, loss: 0.0\n",
      "step: 25676, loss: 8.6782478092573e-07\n",
      "step: 25677, loss: 7.15255632499634e-09\n",
      "step: 25678, loss: 0.03757433965802193\n",
      "step: 25679, loss: 3.3378579900045224e-08\n",
      "step: 25680, loss: 0.0\n",
      "step: 25681, loss: 4.76837147544984e-09\n",
      "step: 25682, loss: 0.002349206944927573\n",
      "step: 25683, loss: 0.0\n",
      "step: 25684, loss: 3.0994396382766354e-08\n",
      "step: 25685, loss: 4.76837103136063e-09\n",
      "step: 25686, loss: 7.15255676908555e-09\n",
      "step: 25687, loss: 9.53674206272126e-09\n",
      "step: 25688, loss: 2.38418573772492e-09\n",
      "step: 25689, loss: 0.0\n",
      "step: 25690, loss: 0.0\n",
      "step: 25691, loss: 0.0\n",
      "step: 25692, loss: 9.53674206272126e-09\n",
      "step: 25693, loss: 4.291529975830599e-08\n",
      "step: 25694, loss: 0.0\n",
      "step: 25695, loss: 2.38418573772492e-09\n",
      "step: 25696, loss: 7.15255676908555e-09\n",
      "step: 25697, loss: 0.04993118718266487\n",
      "step: 25698, loss: 2.38418573772492e-09\n",
      "step: 25699, loss: 7.15255676908555e-09\n",
      "step: 25700, loss: 0.0\n",
      "step: 25701, loss: 1.2111305522921612e-06\n",
      "step: 25702, loss: 0.0\n",
      "step: 25703, loss: 0.0\n",
      "step: 25704, loss: 0.0\n",
      "step: 25705, loss: 0.0\n",
      "step: 25706, loss: 9.53674206272126e-09\n",
      "step: 25707, loss: 0.0\n",
      "step: 25708, loss: 0.0027486884500831366\n",
      "step: 25709, loss: 0.08829439431428909\n",
      "step: 25710, loss: 0.12855952978134155\n",
      "step: 25711, loss: 0.06462036818265915\n",
      "step: 25712, loss: 1.668929705544997e-08\n",
      "step: 25713, loss: 2.38418573772492e-09\n",
      "step: 25714, loss: 2.38418573772492e-09\n",
      "step: 25715, loss: 4.76837147544984e-09\n",
      "step: 25716, loss: 4.768366679286373e-08\n",
      "step: 25717, loss: 0.0\n",
      "step: 25718, loss: 4.529947972287118e-08\n",
      "step: 25719, loss: 9.775138209988654e-08\n",
      "step: 25720, loss: 9.53674117454284e-09\n",
      "step: 25721, loss: 2.38418573772492e-09\n",
      "step: 25722, loss: 3.33785834527589e-08\n",
      "step: 25723, loss: 4.76837103136063e-09\n",
      "step: 25724, loss: 2.1695976215596602e-07\n",
      "step: 25725, loss: 7.15255676908555e-09\n",
      "step: 25726, loss: 1.3875500144422404e-06\n",
      "step: 25727, loss: 2.38418573772492e-09\n",
      "step: 25728, loss: 0.0\n",
      "step: 25729, loss: 4.5299501039153256e-08\n",
      "step: 25730, loss: 0.0\n",
      "step: 25731, loss: 0.0\n",
      "step: 25732, loss: 2.38418573772492e-09\n",
      "step: 25733, loss: 0.0\n",
      "step: 25734, loss: 0.0\n",
      "step: 25735, loss: 3.33785834527589e-08\n",
      "step: 25736, loss: 2.622603467727913e-08\n",
      "step: 25737, loss: 5.722038309841082e-08\n",
      "step: 25738, loss: 1.430511176181426e-08\n",
      "step: 25739, loss: 0.0\n",
      "step: 25740, loss: 5.2452055143703546e-08\n",
      "step: 25741, loss: 7.15255632499634e-09\n",
      "step: 25742, loss: 5.125937718730711e-07\n",
      "step: 25743, loss: 1.907348412544252e-08\n",
      "step: 25744, loss: 7.15255676908555e-09\n",
      "step: 25745, loss: 1.430511176181426e-08\n",
      "step: 25746, loss: 0.0\n",
      "step: 25747, loss: 4.52994939337259e-08\n",
      "step: 25748, loss: 0.07322049885988235\n",
      "step: 25749, loss: 0.0\n",
      "step: 25750, loss: 2.38418573772492e-09\n",
      "step: 25751, loss: 7.15255632499634e-09\n",
      "step: 25752, loss: 7.15255632499634e-09\n",
      "step: 25753, loss: 9.059888839146879e-08\n",
      "step: 25754, loss: 4.7683659687436375e-08\n",
      "step: 25755, loss: 4.76837147544984e-09\n",
      "step: 25756, loss: 1.907348234908568e-08\n",
      "step: 25757, loss: 1.2397734394653526e-07\n",
      "step: 25758, loss: 0.0\n",
      "step: 25759, loss: 0.0\n",
      "step: 25760, loss: 0.0\n",
      "step: 25761, loss: 0.0\n",
      "step: 25762, loss: 0.0\n",
      "step: 25763, loss: 0.05563807114958763\n",
      "step: 25764, loss: 9.313223969797946e-09\n",
      "step: 25765, loss: 1.430511264999268e-08\n",
      "step: 25766, loss: 2.38418573772492e-09\n",
      "step: 25767, loss: 0.0\n",
      "step: 25768, loss: 0.0019419010495766997\n",
      "step: 25769, loss: 0.0\n",
      "step: 25770, loss: 2.38418573772492e-09\n",
      "step: 25771, loss: 0.012995021417737007\n",
      "step: 25772, loss: 9.53674117454284e-09\n",
      "step: 25773, loss: 0.0\n",
      "step: 25774, loss: 2.38418573772492e-09\n",
      "step: 25775, loss: 7.15255632499634e-09\n",
      "step: 25776, loss: 2.2411221323181962e-07\n",
      "step: 25777, loss: 1.192092646817855e-08\n",
      "step: 25778, loss: 2.38418573772492e-09\n",
      "step: 25779, loss: 0.0\n",
      "step: 25780, loss: 2.861021997091484e-08\n",
      "step: 25781, loss: 0.0\n",
      "step: 25782, loss: 0.0\n",
      "step: 25783, loss: 5.9604552404834976e-08\n",
      "step: 25784, loss: 1.192092646817855e-08\n",
      "step: 25785, loss: 1.6402525488956599e-06\n",
      "step: 25786, loss: 7.7722944524794e-07\n",
      "step: 25787, loss: 1.192092558000013e-08\n",
      "step: 25788, loss: 9.298305769789295e-08\n",
      "step: 25789, loss: 9.53674206272126e-09\n",
      "step: 25790, loss: 0.08741338551044464\n",
      "step: 25791, loss: 0.0\n",
      "step: 25792, loss: 0.0\n",
      "step: 25793, loss: 2.38418573772492e-09\n",
      "step: 25794, loss: 1.6450827899916476e-07\n",
      "step: 25795, loss: 1.430510998545742e-08\n",
      "step: 25796, loss: 0.0\n",
      "step: 25797, loss: 1.430510998545742e-08\n",
      "step: 25798, loss: 1.430510998545742e-08\n",
      "step: 25799, loss: 0.0\n",
      "step: 25800, loss: 0.0\n",
      "step: 25801, loss: 2.38418573772492e-09\n",
      "step: 25802, loss: 2.145766586636455e-08\n",
      "step: 25803, loss: 7.15255676908555e-09\n",
      "step: 25804, loss: 0.030961014330387115\n",
      "step: 25805, loss: 7.15255632499634e-09\n",
      "step: 25806, loss: 0.0\n",
      "step: 25807, loss: 6.67571669055178e-08\n",
      "step: 25808, loss: 2.38418573772492e-09\n",
      "step: 25809, loss: 2.908685985403281e-07\n",
      "step: 25810, loss: 0.0\n",
      "step: 25811, loss: 0.0\n",
      "step: 25812, loss: 3.337857279461787e-08\n",
      "step: 25813, loss: 7.629380149865028e-08\n",
      "step: 25814, loss: 9.53674117454284e-09\n",
      "step: 25815, loss: 4.529949038101222e-08\n",
      "step: 25816, loss: 0.0\n",
      "step: 25817, loss: 2.38418573772492e-09\n",
      "step: 25818, loss: 5.245202316928044e-08\n",
      "step: 25819, loss: 9.441153565603599e-07\n",
      "step: 25820, loss: 5.2452033827421474e-08\n",
      "step: 25821, loss: 0.0\n",
      "step: 25822, loss: 2.38418573772492e-09\n",
      "step: 25823, loss: 6.437296917738422e-08\n",
      "step: 25824, loss: 0.0\n",
      "step: 25825, loss: 4.76837103136063e-09\n",
      "step: 25826, loss: 5.006786096828364e-08\n",
      "step: 25827, loss: 3.5762759864610416e-08\n",
      "step: 25828, loss: 4.76837147544984e-09\n",
      "step: 25829, loss: 2.145766586636455e-08\n",
      "step: 25830, loss: 1.1062315934395883e-06\n",
      "step: 25831, loss: 2.38418573772492e-09\n",
      "step: 25832, loss: 2.38418573772492e-09\n",
      "step: 25833, loss: 1.1896739806616097e-06\n",
      "step: 25834, loss: 1.1205641214928619e-07\n",
      "step: 25835, loss: 0.0\n",
      "step: 25836, loss: 1.907348234908568e-08\n",
      "step: 25837, loss: 1.192092646817855e-08\n",
      "step: 25838, loss: 0.05071357637643814\n",
      "step: 25839, loss: 0.0\n",
      "step: 25840, loss: 0.0\n",
      "step: 25841, loss: 2.38418573772492e-09\n",
      "step: 25842, loss: 1.7642932448325155e-07\n",
      "step: 25843, loss: 0.11039338260889053\n",
      "step: 25844, loss: 0.09424367547035217\n",
      "step: 25845, loss: 0.033856455236673355\n",
      "step: 25846, loss: 2.38418573772492e-09\n",
      "step: 25847, loss: 9.991258411901072e-05\n",
      "step: 25848, loss: 4.76837103136063e-09\n",
      "step: 25849, loss: 5.006785741556996e-08\n",
      "step: 25850, loss: 3.099440704090739e-08\n",
      "step: 25851, loss: 4.76837103136063e-09\n",
      "step: 25852, loss: 0.0\n",
      "step: 25853, loss: 0.0009567495435476303\n",
      "step: 25854, loss: 2.861022352362852e-08\n",
      "step: 25855, loss: 0.028627116233110428\n",
      "step: 25856, loss: 0.0\n",
      "step: 25857, loss: 1.0728810906357467e-07\n",
      "step: 25858, loss: 7.15255632499634e-09\n",
      "step: 25859, loss: 2.38418573772492e-09\n",
      "step: 25860, loss: 2.38418573772492e-09\n",
      "step: 25861, loss: 6.877195119159296e-06\n",
      "step: 25862, loss: 4.76837103136063e-09\n",
      "step: 25863, loss: 0.0\n",
      "step: 25864, loss: 1.192092646817855e-08\n",
      "step: 25865, loss: 2.38418573772492e-09\n",
      "step: 25866, loss: 1.4305086892818508e-07\n",
      "step: 25867, loss: 6.19887430275412e-08\n",
      "step: 25868, loss: 3.3378579900045224e-08\n",
      "step: 25869, loss: 0.0\n",
      "step: 25870, loss: 1.6927650392517535e-07\n",
      "step: 25871, loss: 0.0\n",
      "step: 25872, loss: 2.145766586636455e-08\n",
      "step: 25873, loss: 2.38418573772492e-09\n",
      "step: 25874, loss: 4.0531130451881836e-08\n",
      "step: 25875, loss: 0.07995600998401642\n",
      "step: 25876, loss: 0.0\n",
      "step: 25877, loss: 4.76837103136063e-09\n",
      "step: 25878, loss: 3.576275631189674e-08\n",
      "step: 25879, loss: 0.0\n",
      "step: 25880, loss: 1.5782694617882953e-06\n",
      "step: 25881, loss: 6.675711006209895e-08\n",
      "step: 25882, loss: 0.0\n",
      "step: 25883, loss: 7.15255676908555e-09\n",
      "step: 25884, loss: 4.76837103136063e-09\n",
      "step: 25885, loss: 3.5762766970037774e-08\n",
      "step: 25886, loss: 0.07031531631946564\n",
      "step: 25887, loss: 0.212053120136261\n",
      "step: 25888, loss: 4.76837147544984e-09\n",
      "step: 25889, loss: 7.15255676908555e-09\n",
      "step: 25890, loss: 8.583053556776576e-08\n",
      "step: 25891, loss: 0.0\n",
      "step: 25892, loss: 2.622603290092229e-08\n",
      "step: 25893, loss: 2.38418573772492e-09\n",
      "step: 25894, loss: 0.0\n",
      "step: 25895, loss: 1.668929527909313e-08\n",
      "step: 25896, loss: 7.15255632499634e-09\n",
      "step: 25897, loss: 4.76837103136063e-09\n",
      "step: 25898, loss: 5.197459245209757e-07\n",
      "step: 25899, loss: 4.291529975830599e-08\n",
      "step: 25900, loss: 4.76837147544984e-09\n",
      "step: 25901, loss: 4.76837147544984e-09\n",
      "step: 25902, loss: 0.0\n",
      "step: 25903, loss: 0.0\n",
      "step: 25904, loss: 0.0706852376461029\n",
      "step: 25905, loss: 0.0\n",
      "step: 25906, loss: 3.814695404003032e-08\n",
      "step: 25907, loss: 5.006783965200157e-08\n",
      "step: 25908, loss: 6.675711006209895e-08\n",
      "step: 25909, loss: 0.0\n",
      "step: 25910, loss: 9.29830648033203e-08\n",
      "step: 25911, loss: 2.8610216418201162e-08\n",
      "step: 25912, loss: 0.0\n",
      "step: 25913, loss: 7.390962508679877e-08\n",
      "step: 25914, loss: 3.8146950487316644e-08\n",
      "step: 25915, loss: 3.5762763417324095e-08\n",
      "step: 25916, loss: 1.430511264999268e-08\n",
      "step: 25917, loss: 7.15255676908555e-09\n",
      "step: 25918, loss: 2.38418573772492e-09\n",
      "step: 25919, loss: 1.430511264999268e-08\n",
      "step: 25920, loss: 2.8610209312773804e-08\n",
      "step: 25921, loss: 7.15255632499634e-09\n",
      "step: 25922, loss: 3.814693982917561e-08\n",
      "step: 25923, loss: 0.0\n",
      "step: 25924, loss: 4.0531130451881836e-08\n",
      "step: 25925, loss: 2.38418573772492e-09\n",
      "step: 25926, loss: 2.1457557863868715e-07\n",
      "step: 25927, loss: 4.76837103136063e-09\n",
      "step: 25928, loss: 0.0\n",
      "step: 25929, loss: 2.0980762371891615e-07\n",
      "step: 25930, loss: 1.668929527909313e-08\n",
      "step: 25931, loss: 0.0\n",
      "step: 25932, loss: 1.0251984150499993e-07\n",
      "step: 25933, loss: 7.867804896477537e-08\n",
      "step: 25934, loss: 0.0\n",
      "step: 25935, loss: 2.6226025795494934e-08\n",
      "step: 25936, loss: 6.604085456274333e-07\n",
      "step: 25937, loss: 4.2915317521874385e-08\n",
      "step: 25938, loss: 2.3841844054572903e-08\n",
      "step: 25939, loss: 0.0\n",
      "step: 25940, loss: 3.099440704090739e-08\n",
      "step: 25941, loss: 1.668929350273629e-08\n",
      "step: 25942, loss: 7.15255632499634e-09\n",
      "step: 25943, loss: 1.192092824453539e-08\n",
      "step: 25944, loss: 1.907348234908568e-08\n",
      "step: 25945, loss: 4.76837103136063e-09\n",
      "step: 25946, loss: 7.15255676908555e-09\n",
      "step: 25947, loss: 9.53674206272126e-09\n",
      "step: 25948, loss: 0.0\n",
      "step: 25949, loss: 2.38418573772492e-09\n",
      "step: 25950, loss: 0.0\n",
      "step: 25951, loss: 0.0\n",
      "step: 25952, loss: 0.0\n",
      "step: 25953, loss: 0.0\n",
      "step: 25954, loss: 5.006786452099732e-08\n",
      "step: 25955, loss: 2.38418573772492e-09\n",
      "step: 25956, loss: 4.76837147544984e-09\n",
      "step: 25957, loss: 0.0\n",
      "step: 25958, loss: 7.15255632499634e-09\n",
      "step: 25959, loss: 0.0\n",
      "step: 25960, loss: 0.0\n",
      "step: 25961, loss: 1.3589811942438246e-07\n",
      "step: 25962, loss: 0.0\n",
      "step: 25963, loss: 0.0\n",
      "step: 25964, loss: 9.53674206272126e-09\n",
      "step: 25965, loss: 0.05708801746368408\n",
      "step: 25966, loss: 1.668929527909313e-08\n",
      "step: 25967, loss: 7.15255632499634e-09\n",
      "step: 25968, loss: 2.38418573772492e-09\n",
      "step: 25969, loss: 4.76837147544984e-09\n",
      "step: 25970, loss: 7.15255676908555e-09\n",
      "step: 25971, loss: 0.0\n",
      "step: 25972, loss: 2.38418573772492e-09\n",
      "step: 25973, loss: 1.192092824453539e-08\n",
      "step: 25974, loss: 0.0008054732461459935\n",
      "step: 25975, loss: 1.430510998545742e-08\n",
      "step: 25976, loss: 1.192092646817855e-08\n",
      "step: 25977, loss: 2.5987455387621594e-07\n",
      "step: 25978, loss: 5.245204093284883e-08\n",
      "step: 25979, loss: 7.15255676908555e-09\n",
      "step: 25980, loss: 0.0\n",
      "step: 25981, loss: 5.054411644778156e-07\n",
      "step: 25982, loss: 4.053114111002287e-08\n",
      "step: 25983, loss: 1.435229478374822e-06\n",
      "step: 25984, loss: 2.1457660537294032e-08\n",
      "step: 25985, loss: 0.06264929473400116\n",
      "step: 25986, loss: 4.76837103136063e-09\n",
      "step: 25987, loss: 0.0\n",
      "step: 25988, loss: 3.0994396382766354e-08\n",
      "step: 25989, loss: 4.76837103136063e-09\n",
      "step: 25990, loss: 2.793966658032332e-08\n",
      "step: 25991, loss: 3.1421116091223666e-06\n",
      "step: 25992, loss: 2.38418573772492e-09\n",
      "step: 25993, loss: 0.0\n",
      "step: 25994, loss: 0.0\n",
      "step: 25995, loss: 6.914132200108725e-08\n",
      "step: 25996, loss: 4.76837147544984e-09\n",
      "step: 25997, loss: 1.1491445093270158e-06\n",
      "step: 25998, loss: 0.0\n",
      "step: 25999, loss: 0.0\n",
      "step: 26000, loss: 3.0994396382766354e-08\n",
      "step: 26001, loss: 5.0067868073711e-08\n",
      "step: 26002, loss: 1.668929350273629e-08\n",
      "step: 26003, loss: 2.2411231270780263e-07\n",
      "step: 26004, loss: 6.365675631059275e-07\n",
      "step: 26005, loss: 5.006783965200157e-08\n",
      "step: 26006, loss: 2.4794012460915837e-06\n",
      "step: 26007, loss: 3.5762766970037774e-08\n",
      "step: 26008, loss: 2.145766764272139e-08\n",
      "step: 26009, loss: 1.907347702001516e-08\n",
      "step: 26010, loss: 4.7683688109145805e-08\n",
      "step: 26011, loss: 9.53674206272126e-09\n",
      "step: 26012, loss: 4.053112689916816e-08\n",
      "step: 26013, loss: 7.15255676908555e-09\n",
      "step: 26014, loss: 2.38418573772492e-09\n",
      "step: 26015, loss: 0.00584788341075182\n",
      "step: 26016, loss: 2.38418573772492e-09\n",
      "step: 26017, loss: 0.0\n",
      "step: 26018, loss: 0.0\n",
      "step: 26019, loss: 7.15255632499634e-09\n",
      "step: 26020, loss: 0.0\n",
      "step: 26021, loss: 0.0\n",
      "step: 26022, loss: 0.0\n",
      "step: 26023, loss: 0.0\n",
      "step: 26024, loss: 2.38418573772492e-09\n",
      "step: 26025, loss: 0.0\n",
      "step: 26026, loss: 0.0\n",
      "step: 26027, loss: 4.5299508144580614e-08\n",
      "step: 26028, loss: 2.837163322055858e-07\n",
      "step: 26029, loss: 9.298302927618352e-08\n",
      "step: 26030, loss: 0.0\n",
      "step: 26031, loss: 0.0\n",
      "step: 26032, loss: 1.9788650718055578e-07\n",
      "step: 26033, loss: 0.0\n",
      "step: 26034, loss: 2.8226879749126965e-06\n",
      "step: 26035, loss: 0.0012341794790700078\n",
      "step: 26036, loss: 9.53674117454284e-09\n",
      "step: 26037, loss: 3.8861850271132425e-07\n",
      "step: 26038, loss: 5.483623510826874e-08\n",
      "step: 26039, loss: 7.152546999122933e-08\n",
      "step: 26040, loss: 0.0\n",
      "step: 26041, loss: 2.38418573772492e-09\n",
      "step: 26042, loss: 2.38418573772492e-09\n",
      "step: 26043, loss: 8.177601671377488e-07\n",
      "step: 26044, loss: 0.13927900791168213\n",
      "step: 26045, loss: 2.38418573772492e-09\n",
      "step: 26046, loss: 1.3589836100891262e-07\n",
      "step: 26047, loss: 2.38418573772492e-09\n",
      "step: 26048, loss: 4.76837103136063e-09\n",
      "step: 26049, loss: 1.907347702001516e-08\n",
      "step: 26050, loss: 4.76837103136063e-09\n",
      "step: 26051, loss: 8.821471908504463e-08\n",
      "step: 26052, loss: 3.814693627646193e-08\n",
      "step: 26053, loss: 1.4066650066979491e-07\n",
      "step: 26054, loss: 2.38418573772492e-09\n",
      "step: 26055, loss: 2.38418573772492e-09\n",
      "step: 26056, loss: 0.0\n",
      "step: 26057, loss: 9.53674206272126e-09\n",
      "step: 26058, loss: 7.15255632499634e-09\n",
      "step: 26059, loss: 3.0994389277338996e-08\n",
      "step: 26060, loss: 3.695455177421536e-07\n",
      "step: 26061, loss: 5.006783965200157e-08\n",
      "step: 26062, loss: 3.5524067243386526e-07\n",
      "step: 26063, loss: 0.0\n",
      "step: 26064, loss: 0.0\n",
      "step: 26065, loss: 2.38418573772492e-09\n",
      "step: 26066, loss: 7.15255632499634e-09\n",
      "step: 26067, loss: 0.0\n",
      "step: 26068, loss: 3.8146950487316644e-08\n",
      "step: 26069, loss: 1.430511176181426e-08\n",
      "step: 26070, loss: 7.15255676908555e-09\n",
      "step: 26071, loss: 0.0\n",
      "step: 26072, loss: 0.0\n",
      "step: 26073, loss: 0.0\n",
      "step: 26074, loss: 0.0\n",
      "step: 26075, loss: 0.0\n",
      "step: 26076, loss: 0.0711485967040062\n",
      "step: 26077, loss: 2.38418573772492e-09\n",
      "step: 26078, loss: 0.0\n",
      "step: 26079, loss: 1.907348234908568e-08\n",
      "step: 26080, loss: 1.668929705544997e-08\n",
      "step: 26081, loss: 1.430510998545742e-08\n",
      "step: 26082, loss: 4.76837147544984e-09\n",
      "step: 26083, loss: 0.0\n",
      "step: 26084, loss: 2.6226029348208613e-08\n",
      "step: 26085, loss: 0.10647613555192947\n",
      "step: 26086, loss: 2.38418573772492e-09\n",
      "step: 26087, loss: 3.814693627646193e-08\n",
      "step: 26088, loss: 1.192092558000013e-08\n",
      "step: 26089, loss: 0.0\n",
      "step: 26090, loss: 2.6226025795494934e-08\n",
      "step: 26091, loss: 0.0026467465795576572\n",
      "step: 26092, loss: 9.53674117454284e-09\n",
      "step: 26093, loss: 1.907348234908568e-08\n",
      "step: 26094, loss: 1.430510998545742e-08\n",
      "step: 26095, loss: 9.53674295089968e-09\n",
      "step: 26096, loss: 2.38418573772492e-09\n",
      "step: 26097, loss: 4.76837103136063e-09\n",
      "step: 26098, loss: 2.503379334939382e-07\n",
      "step: 26099, loss: 1.192092646817855e-08\n",
      "step: 26100, loss: 1.192092558000013e-08\n",
      "step: 26101, loss: 7.15255676908555e-09\n",
      "step: 26102, loss: 4.76837103136063e-09\n",
      "step: 26103, loss: 1.668929883180681e-08\n",
      "step: 26104, loss: 9.059894523488765e-08\n",
      "step: 26105, loss: 3.5762759864610416e-08\n",
      "step: 26106, loss: 4.76837147544984e-09\n",
      "step: 26107, loss: 0.0\n",
      "step: 26108, loss: 4.76837147544984e-09\n",
      "step: 26109, loss: 3.2186275689127797e-07\n",
      "step: 26110, loss: 9.53674117454284e-09\n",
      "step: 26111, loss: 3.8146950487316644e-08\n",
      "step: 26112, loss: 0.0\n",
      "step: 26113, loss: 0.0\n",
      "step: 26114, loss: 0.0\n",
      "step: 26115, loss: 0.1473456174135208\n",
      "step: 26116, loss: 0.0\n",
      "step: 26117, loss: 1.192092646817855e-08\n",
      "step: 26118, loss: 4.76837103136063e-09\n",
      "step: 26119, loss: 8.106215432235331e-08\n",
      "step: 26120, loss: 7.15255632499634e-09\n",
      "step: 26121, loss: 0.0\n",
      "step: 26122, loss: 0.0\n",
      "step: 26123, loss: 7.15255676908555e-09\n",
      "step: 26124, loss: 2.38418573772492e-09\n",
      "step: 26125, loss: 0.006120853591710329\n",
      "step: 26126, loss: 3.576275631189674e-08\n",
      "step: 26127, loss: 1.192092558000013e-08\n",
      "step: 26128, loss: 1.192092824453539e-08\n",
      "step: 26129, loss: 9.0598852864332e-08\n",
      "step: 26130, loss: 2.6702701916292426e-07\n",
      "step: 26131, loss: 2.861021997091484e-08\n",
      "step: 26132, loss: 1.192092646817855e-08\n",
      "step: 26133, loss: 1.192092646817855e-08\n",
      "step: 26134, loss: 0.0\n",
      "step: 26135, loss: 0.007467042189091444\n",
      "step: 26136, loss: 1.907347702001516e-08\n",
      "step: 26137, loss: 8.583057820032991e-08\n",
      "step: 26138, loss: 4.76837103136063e-09\n",
      "step: 26139, loss: 7.15255632499634e-09\n",
      "step: 26140, loss: 0.0\n",
      "step: 26141, loss: 1.502032347389104e-07\n",
      "step: 26142, loss: 2.38418573772492e-09\n",
      "step: 26143, loss: 1.668929883180681e-08\n",
      "step: 26144, loss: 9.53674206272126e-09\n",
      "step: 26145, loss: 8.583050714605633e-08\n",
      "step: 26146, loss: 4.76837103136063e-09\n",
      "step: 26147, loss: 0.0\n",
      "step: 26148, loss: 1.430511264999268e-08\n",
      "step: 26149, loss: 2.2411221323181962e-07\n",
      "step: 26150, loss: 3.480881787254475e-07\n",
      "step: 26151, loss: 1.668929883180681e-08\n",
      "step: 26152, loss: 3.3378579900045224e-08\n",
      "step: 26153, loss: 0.0\n",
      "step: 26154, loss: 0.0\n",
      "step: 26155, loss: 0.0\n",
      "step: 26156, loss: 2.622603645363597e-08\n",
      "step: 26157, loss: 0.06751887500286102\n",
      "step: 26158, loss: 0.0\n",
      "step: 26159, loss: 1.430510998545742e-08\n",
      "step: 26160, loss: 9.53674117454284e-09\n",
      "step: 26161, loss: 4.529949038101222e-08\n",
      "step: 26162, loss: 7.15255676908555e-09\n",
      "step: 26163, loss: 1.168235485238256e-06\n",
      "step: 26164, loss: 0.002334744669497013\n",
      "step: 26165, loss: 1.668929705544997e-08\n",
      "step: 26166, loss: 2.38418573772492e-09\n",
      "step: 26167, loss: 4.76837103136063e-09\n",
      "step: 26168, loss: 1.3041072861597058e-06\n",
      "step: 26169, loss: 4.0531130451881836e-08\n",
      "step: 26170, loss: 2.38418573772492e-09\n",
      "step: 26171, loss: 8.106219695491745e-08\n",
      "step: 26172, loss: 2.38418573772492e-09\n",
      "step: 26173, loss: 2.12191409332263e-07\n",
      "step: 26174, loss: 0.0\n",
      "step: 26175, loss: 0.0002666297950781882\n",
      "step: 26176, loss: 2.503381608676136e-07\n",
      "step: 26177, loss: 3.671622152978671e-07\n",
      "step: 26178, loss: 0.10374337434768677\n",
      "step: 26179, loss: 0.12334217876195908\n",
      "step: 26180, loss: 7.15255676908555e-09\n",
      "step: 26181, loss: 1.192092646817855e-08\n",
      "step: 26182, loss: 0.0\n",
      "step: 26183, loss: 0.0\n",
      "step: 26184, loss: 0.0\n",
      "step: 26185, loss: 0.0\n",
      "step: 26186, loss: 9.53674206272126e-09\n",
      "step: 26187, loss: 7.977292261784896e-05\n",
      "step: 26188, loss: 2.38418573772492e-09\n",
      "step: 26189, loss: 2.38418573772492e-09\n",
      "step: 26190, loss: 5.960456306297601e-08\n",
      "step: 26191, loss: 0.0\n",
      "step: 26192, loss: 1.192092558000013e-08\n",
      "step: 26193, loss: 2.38418573772492e-09\n",
      "step: 26194, loss: 0.03778667747974396\n",
      "step: 26195, loss: 4.76837103136063e-09\n",
      "step: 26196, loss: 0.0\n",
      "step: 26197, loss: 4.76837103136063e-09\n",
      "step: 26198, loss: 1.4781900858906738e-07\n",
      "step: 26199, loss: 2.38418573772492e-09\n",
      "step: 26200, loss: 2.38418573772492e-09\n",
      "step: 26201, loss: 0.0\n",
      "step: 26202, loss: 5.4836245766409775e-08\n",
      "step: 26203, loss: 2.38418573772492e-09\n",
      "step: 26204, loss: 1.4781919333017868e-07\n",
      "step: 26205, loss: 4.76837147544984e-09\n",
      "step: 26206, loss: 4.76837147544984e-09\n",
      "step: 26207, loss: 4.76837103136063e-09\n",
      "step: 26208, loss: 0.0\n",
      "step: 26209, loss: 1.6974682921500062e-06\n",
      "step: 26210, loss: 0.09386993199586868\n",
      "step: 26211, loss: 0.06446672230958939\n",
      "step: 26212, loss: 0.0\n",
      "step: 26213, loss: 4.76837103136063e-09\n",
      "step: 26214, loss: 2.8610209312773804e-08\n",
      "step: 26215, loss: 0.0\n",
      "step: 26216, loss: 3.7252898543727042e-09\n",
      "step: 26217, loss: 0.01810312271118164\n",
      "step: 26218, loss: 1.1205643346556826e-07\n",
      "step: 26219, loss: 0.0\n",
      "step: 26220, loss: 0.0\n",
      "step: 26221, loss: 0.025567002594470978\n",
      "step: 26222, loss: 1.2159311779669224e-07\n",
      "step: 26223, loss: 9.53674117454284e-09\n",
      "step: 26224, loss: 0.040482159703969955\n",
      "step: 26225, loss: 0.0\n",
      "step: 26226, loss: 0.0\n",
      "step: 26227, loss: 4.76837147544984e-09\n",
      "step: 26228, loss: 1.668929350273629e-08\n",
      "step: 26229, loss: 2.38418573772492e-09\n",
      "step: 26230, loss: 2.38418573772492e-09\n",
      "step: 26231, loss: 7.15255676908555e-09\n",
      "step: 26232, loss: 6.91412793685231e-08\n",
      "step: 26233, loss: 5.006785741556996e-08\n",
      "step: 26234, loss: 9.53674206272126e-09\n",
      "step: 26235, loss: 9.53674206272126e-09\n",
      "step: 26236, loss: 4.76837103136063e-09\n",
      "step: 26237, loss: 2.38418573772492e-09\n",
      "step: 26238, loss: 0.01399241667240858\n",
      "step: 26239, loss: 0.0\n",
      "step: 26240, loss: 0.0\n",
      "step: 26241, loss: 0.0\n",
      "step: 26242, loss: 2.6226029348208613e-08\n",
      "step: 26243, loss: 9.083542522603238e-07\n",
      "step: 26244, loss: 9.53674206272126e-09\n",
      "step: 26245, loss: 0.0\n",
      "step: 26246, loss: 9.53674206272126e-09\n",
      "step: 26247, loss: 7.15255632499634e-09\n",
      "step: 26248, loss: 0.0\n",
      "step: 26249, loss: 1.192092646817855e-08\n",
      "step: 26250, loss: 0.0\n",
      "step: 26251, loss: 4.76837103136063e-09\n",
      "step: 26252, loss: 1.668929883180681e-08\n",
      "step: 26253, loss: 0.0\n",
      "step: 26254, loss: 0.0\n",
      "step: 26255, loss: 1.1920908349338788e-07\n",
      "step: 26256, loss: 2.6226025795494934e-08\n",
      "step: 26257, loss: 0.0\n",
      "step: 26258, loss: 7.15255676908555e-09\n",
      "step: 26259, loss: 4.76837103136063e-09\n",
      "step: 26260, loss: 3.5762766970037774e-08\n",
      "step: 26261, loss: 5.413753115135478e-06\n",
      "step: 26262, loss: 4.76837103136063e-09\n",
      "step: 26263, loss: 0.0\n",
      "step: 26264, loss: 1.907348412544252e-08\n",
      "step: 26265, loss: 1.668929527909313e-08\n",
      "step: 26266, loss: 7.15255676908555e-09\n",
      "step: 26267, loss: 4.76837147544984e-09\n",
      "step: 26268, loss: 0.0\n",
      "step: 26269, loss: 2.38418573772492e-09\n",
      "step: 26270, loss: 2.38418573772492e-09\n",
      "step: 26271, loss: 1.192092646817855e-08\n",
      "step: 26272, loss: 2.8894244223920396e-06\n",
      "step: 26273, loss: 1.192092646817855e-08\n",
      "step: 26274, loss: 2.8610209312773804e-08\n",
      "step: 26275, loss: 9.53674117454284e-09\n",
      "step: 26276, loss: 0.0\n",
      "step: 26277, loss: 2.38418573772492e-09\n",
      "step: 26278, loss: 7.15255676908555e-09\n",
      "step: 26279, loss: 0.0\n",
      "step: 26280, loss: 2.38418573772492e-09\n",
      "step: 26281, loss: 2.38418573772492e-09\n",
      "step: 26282, loss: 1.430510998545742e-08\n",
      "step: 26283, loss: 1.0728806643101052e-07\n",
      "step: 26284, loss: 0.0\n",
      "step: 26285, loss: 2.384184938364342e-08\n",
      "step: 26286, loss: 0.10545632988214493\n",
      "step: 26287, loss: 4.76837103136063e-09\n",
      "step: 26288, loss: 7.15255632499634e-09\n",
      "step: 26289, loss: 5.030569241171179e-07\n",
      "step: 26290, loss: 0.0\n",
      "step: 26291, loss: 7.15255632499634e-09\n",
      "step: 26292, loss: 0.052422452718019485\n",
      "step: 26293, loss: 2.38418573772492e-09\n",
      "step: 26294, loss: 9.0598852864332e-08\n",
      "step: 26295, loss: 4.76837103136063e-09\n",
      "step: 26296, loss: 9.53674206272126e-09\n",
      "step: 26297, loss: 1.430510998545742e-08\n",
      "step: 26298, loss: 0.0011198072461411357\n",
      "step: 26299, loss: 0.16601978242397308\n",
      "step: 26300, loss: 4.76837147544984e-09\n",
      "step: 26301, loss: 2.38418573772492e-09\n",
      "step: 26302, loss: 0.08509867638349533\n",
      "step: 26303, loss: 0.0\n",
      "step: 26304, loss: 0.0\n",
      "step: 26305, loss: 1.907348234908568e-08\n",
      "step: 26306, loss: 2.38418573772492e-09\n",
      "step: 26307, loss: 2.1695990426451317e-07\n",
      "step: 26308, loss: 7.15255632499634e-09\n",
      "step: 26309, loss: 0.0\n",
      "step: 26310, loss: 9.775146025958747e-08\n",
      "step: 26311, loss: 7.15255676908555e-09\n",
      "step: 26312, loss: 1.6450815110147232e-07\n",
      "step: 26313, loss: 2.38418573772492e-09\n",
      "step: 26314, loss: 1.5020316368463682e-07\n",
      "step: 26315, loss: 1.430511264999268e-08\n",
      "step: 26316, loss: 0.0659247413277626\n",
      "step: 26317, loss: 1.740448709597331e-07\n",
      "step: 26318, loss: 0.0\n",
      "step: 26319, loss: 2.12191409332263e-07\n",
      "step: 26320, loss: 0.0\n",
      "step: 26321, loss: 1.192092646817855e-08\n",
      "step: 26322, loss: 4.053112334645448e-08\n",
      "step: 26323, loss: 4.768366679286373e-08\n",
      "step: 26324, loss: 1.430510998545742e-08\n",
      "step: 26325, loss: 1.907347702001516e-08\n",
      "step: 26326, loss: 0.0\n",
      "step: 26327, loss: 3.576275631189674e-08\n",
      "step: 26328, loss: 2.38418573772492e-09\n",
      "step: 26329, loss: 2.38418573772492e-09\n",
      "step: 26330, loss: 1.7166064480989007e-07\n",
      "step: 26331, loss: 1.4781898016735795e-07\n",
      "step: 26332, loss: 4.053112334645448e-08\n",
      "step: 26333, loss: 0.0\n",
      "step: 26334, loss: 7.15255676908555e-09\n",
      "step: 26335, loss: 9.298312164673916e-08\n",
      "step: 26336, loss: 0.0\n",
      "step: 26337, loss: 0.0\n",
      "step: 26338, loss: 3.280373448433238e-06\n",
      "step: 26339, loss: 0.0\n",
      "step: 26340, loss: 0.0\n",
      "step: 26341, loss: 1.4280765299190534e-06\n",
      "step: 26342, loss: 3.1232693231686426e-07\n",
      "step: 26343, loss: 2.38418573772492e-09\n",
      "step: 26344, loss: 2.38418573772492e-09\n",
      "step: 26345, loss: 4.291530686373335e-08\n",
      "step: 26346, loss: 0.0\n",
      "step: 26347, loss: 5.96045985901128e-08\n",
      "step: 26348, loss: 2.6226025795494934e-08\n",
      "step: 26349, loss: 4.529947972287118e-08\n",
      "step: 26350, loss: 3.621250243668328e-06\n",
      "step: 26351, loss: 2.38418573772492e-09\n",
      "step: 26352, loss: 0.0\n",
      "step: 26353, loss: 7.843819389563578e-07\n",
      "step: 26354, loss: 4.76837103136063e-09\n",
      "step: 26355, loss: 1.4042589100426994e-06\n",
      "step: 26356, loss: 1.192092558000013e-08\n",
      "step: 26357, loss: 2.38418573772492e-09\n",
      "step: 26358, loss: 2.9086857011861866e-07\n",
      "step: 26359, loss: 1.1444060277199242e-07\n",
      "step: 26360, loss: 3.0040521892260585e-07\n",
      "step: 26361, loss: 0.0\n",
      "step: 26362, loss: 0.1382581740617752\n",
      "step: 26363, loss: 7.15255632499634e-09\n",
      "step: 26364, loss: 0.0\n",
      "step: 26365, loss: 4.76837103136063e-09\n",
      "step: 26366, loss: 4.76837147544984e-09\n",
      "step: 26367, loss: 9.53674117454284e-09\n",
      "step: 26368, loss: 2.38418573772492e-09\n",
      "step: 26369, loss: 2.1457559284954186e-07\n",
      "step: 26370, loss: 8.344633073420482e-08\n",
      "step: 26371, loss: 0.0\n",
      "step: 26372, loss: 0.0\n",
      "step: 26373, loss: 2.38418573772492e-09\n",
      "step: 26374, loss: 4.76837103136063e-09\n",
      "step: 26375, loss: 5.9604552404834976e-08\n",
      "step: 26376, loss: 4.410697158618859e-07\n",
      "step: 26377, loss: 2.622603290092229e-08\n",
      "step: 26378, loss: 2.38418573772492e-09\n",
      "step: 26379, loss: 1.2874561150511e-07\n",
      "step: 26380, loss: 4.76837147544984e-09\n",
      "step: 26381, loss: 7.152546288580197e-08\n",
      "step: 26382, loss: 3.576275631189674e-08\n",
      "step: 26383, loss: 2.908689964442601e-07\n",
      "step: 26384, loss: 0.0003227845882065594\n",
      "step: 26385, loss: 2.38418573772492e-09\n",
      "step: 26386, loss: 3.814693627646193e-08\n",
      "step: 26387, loss: 5.006785741556996e-08\n",
      "step: 26388, loss: 3.0994389277338996e-08\n",
      "step: 26389, loss: 6.675713137838102e-08\n",
      "step: 26390, loss: 2.0027060543270636e-07\n",
      "step: 26391, loss: 0.08060529083013535\n",
      "step: 26392, loss: 0.0\n",
      "step: 26393, loss: 1.1920899822825959e-07\n",
      "step: 26394, loss: 3.337859055818626e-08\n",
      "step: 26395, loss: 4.76837147544984e-09\n",
      "step: 26396, loss: 2.6226029348208613e-08\n",
      "step: 26397, loss: 0.0\n",
      "step: 26398, loss: 0.0\n",
      "step: 26399, loss: 1.0673546057660133e-05\n",
      "step: 26400, loss: 0.0\n",
      "step: 26401, loss: 0.07618991285562515\n",
      "step: 26402, loss: 5.364352659853466e-07\n",
      "step: 26403, loss: 1.9073446821948892e-07\n",
      "step: 26404, loss: 0.0\n",
      "step: 26405, loss: 2.38418573772492e-09\n",
      "step: 26406, loss: 0.0\n",
      "step: 26407, loss: 1.0967224284286203e-07\n",
      "step: 26408, loss: 1.668929883180681e-08\n",
      "step: 26409, loss: 0.0\n",
      "step: 26410, loss: 4.76837147544984e-09\n",
      "step: 26411, loss: 0.0\n",
      "step: 26412, loss: 4.76837103136063e-09\n",
      "step: 26413, loss: 3.6954537563360645e-07\n",
      "step: 26414, loss: 4.768367034557741e-08\n",
      "step: 26415, loss: 2.38418573772492e-09\n",
      "step: 26416, loss: 0.018677182495594025\n",
      "step: 26417, loss: 1.907348234908568e-08\n",
      "step: 26418, loss: 9.53674117454284e-09\n",
      "step: 26419, loss: 2.38418573772492e-09\n",
      "step: 26420, loss: 1.0728813037985674e-07\n",
      "step: 26421, loss: 2.9418699796224246e-06\n",
      "step: 26422, loss: 0.10108540207147598\n",
      "step: 26423, loss: 4.2434003262314945e-06\n",
      "step: 26424, loss: 2.38418573772492e-09\n",
      "step: 26425, loss: 9.53674206272126e-09\n",
      "step: 26426, loss: 0.2297508865594864\n",
      "step: 26427, loss: 0.0\n",
      "step: 26428, loss: 0.0\n",
      "step: 26429, loss: 5.769656468146422e-07\n",
      "step: 26430, loss: 2.38418573772492e-09\n",
      "step: 26431, loss: 7.15255676908555e-09\n",
      "step: 26432, loss: 6.198877855467799e-08\n",
      "step: 26433, loss: 0.0\n",
      "step: 26434, loss: 2.1457660537294032e-08\n",
      "step: 26435, loss: 4.76837103136063e-09\n",
      "step: 26436, loss: 2.38418573772492e-09\n",
      "step: 26437, loss: 0.0\n",
      "step: 26438, loss: 4.76837147544984e-09\n",
      "step: 26439, loss: 0.0\n",
      "step: 26440, loss: 5.722041507283393e-08\n",
      "step: 26441, loss: 1.5854218418098753e-06\n",
      "step: 26442, loss: 1.434232075325781e-07\n",
      "step: 26443, loss: 1.3637077245221008e-06\n",
      "step: 26444, loss: 2.408013699550793e-07\n",
      "step: 26445, loss: 7.15255676908555e-09\n",
      "step: 26446, loss: 2.38418529363571e-08\n",
      "step: 26447, loss: 0.0\n",
      "step: 26448, loss: 7.15255632499634e-09\n",
      "step: 26449, loss: 7.15255632499634e-09\n",
      "step: 26450, loss: 0.0\n",
      "step: 26451, loss: 0.0\n",
      "step: 26452, loss: 4.291530331101967e-08\n",
      "step: 26453, loss: 0.0\n",
      "step: 26454, loss: 2.0027069069783465e-07\n",
      "step: 26455, loss: 0.0\n",
      "step: 26456, loss: 2.1934388882982603e-07\n",
      "step: 26457, loss: 9.53674117454284e-09\n",
      "step: 26458, loss: 2.38418573772492e-09\n",
      "step: 26459, loss: 9.53674117454284e-09\n",
      "step: 26460, loss: 7.390966771936291e-08\n",
      "step: 26461, loss: 0.0032719692680984735\n",
      "step: 26462, loss: 9.53674206272126e-09\n",
      "step: 26463, loss: 3.337857279461787e-08\n",
      "step: 26464, loss: 0.01983931101858616\n",
      "step: 26465, loss: 0.0\n",
      "step: 26466, loss: 0.0\n",
      "step: 26467, loss: 9.53674206272126e-09\n",
      "step: 26468, loss: 1.192092558000013e-08\n",
      "step: 26469, loss: 0.0\n",
      "step: 26470, loss: 0.0\n",
      "step: 26471, loss: 9.53674117454284e-09\n",
      "step: 26472, loss: 1.8240238205180503e-05\n",
      "step: 26473, loss: 1.9073449664119835e-07\n",
      "step: 26474, loss: 9.53674117454284e-09\n",
      "step: 26475, loss: 3.409356565953203e-07\n",
      "step: 26476, loss: 4.76837103136063e-09\n",
      "step: 26477, loss: 1.192092558000013e-08\n",
      "step: 26478, loss: 2.38418573772492e-09\n",
      "step: 26479, loss: 2.3841844054572903e-08\n",
      "step: 26480, loss: 3.409370492590824e-07\n",
      "step: 26481, loss: 0.059071511030197144\n",
      "step: 26482, loss: 0.0\n",
      "step: 26483, loss: 3.337857279461787e-08\n",
      "step: 26484, loss: 8.344633073420482e-08\n",
      "step: 26485, loss: 0.0\n",
      "step: 26486, loss: 1.1205641214928619e-07\n",
      "step: 26487, loss: 4.76837147544984e-09\n",
      "step: 26488, loss: 3.433200390645652e-07\n",
      "step: 26489, loss: 9.53674117454284e-09\n",
      "step: 26490, loss: 0.0\n",
      "step: 26491, loss: 0.0\n",
      "step: 26492, loss: 0.0\n",
      "step: 26493, loss: 1.4781919333017868e-07\n",
      "step: 26494, loss: 7.15255676908555e-09\n",
      "step: 26495, loss: 1.9073439716521534e-07\n",
      "step: 26496, loss: 6.246471571103029e-07\n",
      "step: 26497, loss: 2.384184938364342e-08\n",
      "step: 26498, loss: 3.0994389277338996e-08\n",
      "step: 26499, loss: 2.8610209312773804e-08\n",
      "step: 26500, loss: 0.14017514884471893\n",
      "step: 26501, loss: 2.38418573772492e-09\n",
      "step: 26502, loss: 0.0\n",
      "step: 26503, loss: 2.38418573772492e-09\n",
      "step: 26504, loss: 2.145766764272139e-08\n",
      "step: 26505, loss: 0.0\n",
      "step: 26506, loss: 1.5497167282774171e-07\n",
      "step: 26507, loss: 4.2915313969160707e-08\n",
      "step: 26508, loss: 1.668929527909313e-08\n",
      "step: 26509, loss: 0.07051247358322144\n",
      "step: 26510, loss: 4.76837147544984e-09\n",
      "step: 26511, loss: 0.0\n",
      "step: 26512, loss: 0.0\n",
      "step: 26513, loss: 2.38418573772492e-09\n",
      "step: 26514, loss: 9.53674117454284e-09\n",
      "step: 26515, loss: 1.668929883180681e-08\n",
      "step: 26516, loss: 7.15255676908555e-09\n",
      "step: 26517, loss: 3.33785834527589e-08\n",
      "step: 26518, loss: 2.8610209312773804e-08\n",
      "step: 26519, loss: 2.38418573772492e-09\n",
      "step: 26520, loss: 2.38418573772492e-09\n",
      "step: 26521, loss: 3.0994389277338996e-08\n",
      "step: 26522, loss: 7.462367648258805e-07\n",
      "step: 26523, loss: 2.38418573772492e-09\n",
      "step: 26524, loss: 0.23584851622581482\n",
      "step: 26525, loss: 4.053112689916816e-08\n",
      "step: 26526, loss: 7.15255676908555e-09\n",
      "step: 26527, loss: 2.38418573772492e-09\n",
      "step: 26528, loss: 2.38418573772492e-09\n",
      "step: 26529, loss: 7.629380149865028e-08\n",
      "step: 26530, loss: 0.0\n",
      "step: 26531, loss: 0.0006745659629814327\n",
      "step: 26532, loss: 0.0\n",
      "step: 26533, loss: 0.0\n",
      "step: 26534, loss: 0.0\n",
      "step: 26535, loss: 2.38418573772492e-09\n",
      "step: 26536, loss: 7.15255632499634e-09\n",
      "step: 26537, loss: 4.2676606426539365e-07\n",
      "step: 26538, loss: 2.38418573772492e-09\n",
      "step: 26539, loss: 9.53674206272126e-09\n",
      "step: 26540, loss: 4.76837103136063e-09\n",
      "step: 26541, loss: 0.0\n",
      "step: 26542, loss: 0.026376930996775627\n",
      "step: 26543, loss: 0.0\n",
      "step: 26544, loss: 7.15255676908555e-09\n",
      "step: 26545, loss: 0.0\n",
      "step: 26546, loss: 3.33785834527589e-08\n",
      "step: 26547, loss: 2.3841844054572903e-08\n",
      "step: 26548, loss: 2.145766764272139e-08\n",
      "step: 26549, loss: 0.0\n",
      "step: 26550, loss: 0.0\n",
      "step: 26551, loss: 0.0\n",
      "step: 26552, loss: 3.5762766970037774e-08\n",
      "step: 26553, loss: 3.337858700547258e-08\n",
      "step: 26554, loss: 9.53674117454284e-09\n",
      "step: 26555, loss: 7.15255676908555e-09\n",
      "step: 26556, loss: 3.8146957592744e-08\n",
      "step: 26557, loss: 2.38418573772492e-09\n",
      "step: 26558, loss: 0.0\n",
      "step: 26559, loss: 7.15255676908555e-09\n",
      "step: 26560, loss: 2.574903987806465e-07\n",
      "step: 26561, loss: 1.668929350273629e-08\n",
      "step: 26562, loss: 2.38418573772492e-09\n",
      "step: 26563, loss: 0.0\n",
      "step: 26564, loss: 1.668929527909313e-08\n",
      "step: 26565, loss: 9.53674206272126e-09\n",
      "step: 26566, loss: 0.0\n",
      "step: 26567, loss: 0.0\n",
      "step: 26568, loss: 0.0\n",
      "step: 26569, loss: 0.0\n",
      "step: 26570, loss: 0.0\n",
      "step: 26571, loss: 1.4305066997621907e-07\n",
      "step: 26572, loss: 0.06417537480592728\n",
      "step: 26573, loss: 0.0\n",
      "step: 26574, loss: 2.38418573772492e-09\n",
      "step: 26575, loss: 0.0\n",
      "step: 26576, loss: 0.0\n",
      "step: 26577, loss: 0.0\n",
      "step: 26578, loss: 0.11774661391973495\n",
      "step: 26579, loss: 0.0\n",
      "step: 26580, loss: 2.3841844054572903e-08\n",
      "step: 26581, loss: 2.38418573772492e-09\n",
      "step: 26582, loss: 2.38418573772492e-09\n",
      "step: 26583, loss: 0.0\n",
      "step: 26584, loss: 0.0\n",
      "step: 26585, loss: 5.5215383326867595e-05\n",
      "step: 26586, loss: 2.38418573772492e-09\n",
      "step: 26587, loss: 1.59914925461635e-05\n",
      "step: 26588, loss: 0.0\n",
      "step: 26589, loss: 2.38418573772492e-09\n",
      "step: 26590, loss: 6.270310564104875e-07\n",
      "step: 26591, loss: 0.0\n",
      "step: 26592, loss: 2.3841844054572903e-08\n",
      "step: 26593, loss: 1.3351396432881302e-07\n",
      "step: 26594, loss: 0.08618266880512238\n",
      "step: 26595, loss: 0.0\n",
      "step: 26596, loss: 9.53674206272126e-09\n",
      "step: 26597, loss: 1.430510998545742e-08\n",
      "step: 26598, loss: 0.0\n",
      "step: 26599, loss: 0.0\n",
      "step: 26600, loss: 4.386853902360599e-07\n",
      "step: 26601, loss: 7.15255676908555e-09\n",
      "step: 26602, loss: 1.8358169029397686e-07\n",
      "step: 26603, loss: 0.0\n",
      "step: 26604, loss: 9.53674206272126e-09\n",
      "step: 26605, loss: 7.15255632499634e-09\n",
      "step: 26606, loss: 0.0\n",
      "step: 26607, loss: 3.8146946934602965e-08\n",
      "step: 26608, loss: 0.0\n",
      "step: 26609, loss: 2.1457660537294032e-08\n",
      "step: 26610, loss: 0.0\n",
      "step: 26611, loss: 2.38418573772492e-09\n",
      "step: 26612, loss: 0.07354091107845306\n",
      "step: 26613, loss: 0.0\n",
      "step: 26614, loss: 7.15255632499634e-09\n",
      "step: 26615, loss: 2.3841845830929742e-08\n",
      "step: 26616, loss: 0.04860997572541237\n",
      "step: 26617, loss: 2.38418573772492e-09\n",
      "step: 26618, loss: 1.192092646817855e-08\n",
      "step: 26619, loss: 0.0\n",
      "step: 26620, loss: 2.0265477473913052e-07\n",
      "step: 26621, loss: 7.15255676908555e-09\n",
      "step: 26622, loss: 0.0\n",
      "step: 26623, loss: 2.38418573772492e-09\n",
      "step: 26624, loss: 4.76837103136063e-09\n",
      "step: 26625, loss: 0.0\n",
      "step: 26626, loss: 0.022922944277524948\n",
      "step: 26627, loss: 0.0\n",
      "step: 26628, loss: 1.192092646817855e-08\n",
      "step: 26629, loss: 2.38418573772492e-09\n",
      "step: 26630, loss: 9.536725542602653e-08\n",
      "step: 26631, loss: 7.15255632499634e-09\n",
      "step: 26632, loss: 0.0\n",
      "step: 26633, loss: 0.05950939655303955\n",
      "step: 26634, loss: 4.298231033317279e-06\n",
      "step: 26635, loss: 4.744475177176355e-07\n",
      "step: 26636, loss: 2.38418573772492e-09\n",
      "step: 26637, loss: 0.07126570492982864\n",
      "step: 26638, loss: 0.04849828779697418\n",
      "step: 26639, loss: 2.38418573772492e-09\n",
      "step: 26640, loss: 1.192092558000013e-08\n",
      "step: 26641, loss: 0.002648649737238884\n",
      "step: 26642, loss: 1.192092558000013e-08\n",
      "step: 26643, loss: 1.192092824453539e-08\n",
      "step: 26644, loss: 0.0\n",
      "step: 26645, loss: 7.15255632499634e-09\n",
      "step: 26646, loss: 5.0067878731852034e-08\n",
      "step: 26647, loss: 4.76837147544984e-09\n",
      "step: 26648, loss: 1.1205647609813241e-07\n",
      "step: 26649, loss: 7.009444402683584e-07\n",
      "step: 26650, loss: 0.0\n",
      "step: 26651, loss: 9.53674206272126e-09\n",
      "step: 26652, loss: 2.38418573772492e-09\n",
      "step: 26653, loss: 2.38418573772492e-09\n",
      "step: 26654, loss: 1.668929527909313e-08\n",
      "step: 26655, loss: 4.76837147544984e-09\n",
      "step: 26656, loss: 1.907347702001516e-08\n",
      "step: 26657, loss: 4.291530331101967e-08\n",
      "step: 26658, loss: 9.53674117454284e-09\n",
      "step: 26659, loss: 6.246484076655179e-07\n",
      "step: 26660, loss: 3.125426701444667e-06\n",
      "step: 26661, loss: 0.0\n",
      "step: 26662, loss: 0.0\n",
      "step: 26663, loss: 9.53674117454284e-09\n",
      "step: 26664, loss: 2.1457660537294032e-08\n",
      "step: 26665, loss: 1.6212423759043304e-07\n",
      "step: 26666, loss: 2.622604000634965e-08\n",
      "step: 26667, loss: 3.0994396382766354e-08\n",
      "step: 26668, loss: 9.313205850958184e-08\n",
      "step: 26669, loss: 4.91136233904399e-07\n",
      "step: 26670, loss: 1.5544289908575593e-06\n",
      "step: 26671, loss: 7.15255676908555e-09\n",
      "step: 26672, loss: 2.38418573772492e-09\n",
      "step: 26673, loss: 4.76837103136063e-09\n",
      "step: 26674, loss: 4.76837103136063e-09\n",
      "step: 26675, loss: 1.430510998545742e-08\n",
      "step: 26676, loss: 4.76837103136063e-09\n",
      "step: 26677, loss: 8.821474750675407e-08\n",
      "step: 26678, loss: 1.9550272156720894e-07\n",
      "step: 26679, loss: 1.4066655751321377e-07\n",
      "step: 26680, loss: 4.76837103136063e-09\n",
      "step: 26681, loss: 0.0\n",
      "step: 26682, loss: 0.053481072187423706\n",
      "step: 26683, loss: 0.0\n",
      "step: 26684, loss: 9.178924074149108e-07\n",
      "step: 26685, loss: 1.192092646817855e-08\n",
      "step: 26686, loss: 0.0011794345919042826\n",
      "step: 26687, loss: 4.76837103136063e-09\n",
      "step: 26688, loss: 1.192092646817855e-08\n",
      "step: 26689, loss: 4.76837103136063e-09\n",
      "step: 26690, loss: 1.370864538330352e-06\n",
      "step: 26691, loss: 0.0\n",
      "step: 26692, loss: 0.0\n",
      "step: 26693, loss: 2.6226025795494934e-08\n",
      "step: 26694, loss: 4.76837103136063e-09\n",
      "step: 26695, loss: 2.8610209312773804e-08\n",
      "step: 26696, loss: 0.22561433911323547\n",
      "step: 26697, loss: 7.15255676908555e-09\n",
      "step: 26698, loss: 1.430510998545742e-08\n",
      "step: 26699, loss: 9.53674117454284e-09\n",
      "step: 26700, loss: 0.0\n",
      "step: 26701, loss: 0.0\n",
      "step: 26702, loss: 0.0\n",
      "step: 26703, loss: 2.38418573772492e-09\n",
      "step: 26704, loss: 7.15255676908555e-09\n",
      "step: 26705, loss: 1.3351402117223188e-07\n",
      "step: 26706, loss: 0.0\n",
      "step: 26707, loss: 3.743137426681642e-07\n",
      "step: 26708, loss: 2.646428640673548e-07\n",
      "step: 26709, loss: 6.198879276553271e-08\n",
      "step: 26710, loss: 0.0\n",
      "step: 26711, loss: 2.38418573772492e-09\n",
      "step: 26712, loss: 4.5299501039153256e-08\n",
      "step: 26713, loss: 3.099439993548003e-08\n",
      "step: 26714, loss: 0.0\n",
      "step: 26715, loss: 7.15255676908555e-09\n",
      "step: 26716, loss: 7.6293815709505e-08\n",
      "step: 26717, loss: 0.05582135170698166\n",
      "step: 26718, loss: 3.0040530418773415e-07\n",
      "step: 26719, loss: 4.76837103136063e-09\n",
      "step: 26720, loss: 1.3541790622184635e-06\n",
      "step: 26721, loss: 9.53674295089968e-09\n",
      "step: 26722, loss: 4.76837103136063e-09\n",
      "step: 26723, loss: 8.049782991292886e-06\n",
      "step: 26724, loss: 2.38418573772492e-09\n",
      "step: 26725, loss: 7.390963219222613e-08\n",
      "step: 26726, loss: 1.1205647609813241e-07\n",
      "step: 26727, loss: 2.38418573772492e-09\n",
      "step: 26728, loss: 0.0\n",
      "step: 26729, loss: 0.0\n",
      "step: 26730, loss: 9.53674117454284e-09\n",
      "step: 26731, loss: 9.53674206272126e-09\n",
      "step: 26732, loss: 4.434540130660025e-07\n",
      "step: 26733, loss: 2.38418573772492e-09\n",
      "step: 26734, loss: 5.722037599298346e-08\n",
      "step: 26735, loss: 1.6402545952587388e-06\n",
      "step: 26736, loss: 3.337857279461787e-08\n",
      "step: 26737, loss: 0.0\n",
      "step: 26738, loss: 2.3841844054572903e-08\n",
      "step: 26739, loss: 1.430511264999268e-08\n",
      "step: 26740, loss: 0.0\n",
      "step: 26741, loss: 4.053112689916816e-08\n",
      "step: 26742, loss: 2.38418573772492e-09\n",
      "step: 26743, loss: 0.08419936895370483\n",
      "step: 26744, loss: 2.8610209312773804e-08\n",
      "step: 26745, loss: 0.07352085411548615\n",
      "step: 26746, loss: 9.53674206272126e-09\n",
      "step: 26747, loss: 7.15255632499634e-09\n",
      "step: 26748, loss: 1.001355514063107e-07\n",
      "step: 26749, loss: 2.145766764272139e-08\n",
      "step: 26750, loss: 4.6252668539636943e-07\n",
      "step: 26751, loss: 3.862344328808831e-07\n",
      "step: 26752, loss: 1.907348234908568e-08\n",
      "step: 26753, loss: 2.38418573772492e-09\n",
      "step: 26754, loss: 2.38418573772492e-09\n",
      "step: 26755, loss: 0.0\n",
      "step: 26756, loss: 0.0\n",
      "step: 26757, loss: 8.821468355790785e-08\n",
      "step: 26758, loss: 7.86779779105018e-08\n",
      "step: 26759, loss: 3.0994396382766354e-08\n",
      "step: 26760, loss: 0.0\n",
      "step: 26761, loss: 0.0\n",
      "step: 26762, loss: 9.393489222020435e-07\n",
      "step: 26763, loss: 4.768367389829109e-08\n",
      "step: 26764, loss: 1.430511176181426e-08\n",
      "step: 26765, loss: 2.3841844054572903e-08\n",
      "step: 26766, loss: 0.0\n",
      "step: 26767, loss: 2.38418573772492e-09\n",
      "step: 26768, loss: 8.55501821206417e-06\n",
      "step: 26769, loss: 2.574903987806465e-07\n",
      "step: 26770, loss: 0.0\n",
      "step: 26771, loss: 4.291529975830599e-08\n",
      "step: 26772, loss: 0.0\n",
      "step: 26773, loss: 3.8146950487316644e-08\n",
      "step: 26774, loss: 5.0067868073711e-08\n",
      "step: 26775, loss: 0.0\n",
      "step: 26776, loss: 4.76837103136063e-09\n",
      "step: 26777, loss: 4.424559847393539e-06\n",
      "step: 26778, loss: 0.05871659889817238\n",
      "step: 26779, loss: 0.0\n",
      "step: 26780, loss: 1.192092824453539e-08\n",
      "step: 26781, loss: 0.00017347699031233788\n",
      "step: 26782, loss: 4.76837103136063e-09\n",
      "step: 26783, loss: 0.0\n",
      "step: 26784, loss: 1.5020336263660283e-07\n",
      "step: 26785, loss: 4.76837103136063e-09\n",
      "step: 26786, loss: 7.390962508679877e-08\n",
      "step: 26787, loss: 1.0132570196219604e-06\n",
      "step: 26788, loss: 1.668929527909313e-08\n",
      "step: 26789, loss: 2.384184938364342e-08\n",
      "step: 26790, loss: 4.76837103136063e-09\n",
      "step: 26791, loss: 0.0\n",
      "step: 26792, loss: 5.9604580826544407e-08\n",
      "step: 26793, loss: 2.6226025795494934e-08\n",
      "step: 26794, loss: 2.8610216418201162e-08\n",
      "step: 26795, loss: 2.0742314177368826e-07\n",
      "step: 26796, loss: 2.145766231365087e-08\n",
      "step: 26797, loss: 1.907348412544252e-08\n",
      "step: 26798, loss: 2.38418573772492e-09\n",
      "step: 26799, loss: 3.6000889735987585e-07\n",
      "step: 26800, loss: 1.430511264999268e-08\n",
      "step: 26801, loss: 0.09626869112253189\n",
      "step: 26802, loss: 5.245203738013515e-08\n",
      "step: 26803, loss: 2.38418573772492e-09\n",
      "step: 26804, loss: 0.0\n",
      "step: 26805, loss: 1.668929527909313e-08\n",
      "step: 26806, loss: 2.384185116000026e-08\n",
      "step: 26807, loss: 4.6968224864940566e-07\n",
      "step: 26808, loss: 0.0\n",
      "step: 26809, loss: 2.980211206704553e-07\n",
      "step: 26810, loss: 1.192092558000013e-08\n",
      "step: 26811, loss: 0.00012221824727021158\n",
      "step: 26812, loss: 2.38418573772492e-09\n",
      "step: 26813, loss: 1.668929527909313e-08\n",
      "step: 26814, loss: 4.053111624102712e-08\n",
      "step: 26815, loss: 4.76837103136063e-09\n",
      "step: 26816, loss: 2.38418573772492e-09\n",
      "step: 26817, loss: 4.76837103136063e-09\n",
      "step: 26818, loss: 3.218637232293986e-07\n",
      "step: 26819, loss: 2.145766231365087e-08\n",
      "step: 26820, loss: 0.0\n",
      "step: 26821, loss: 0.0\n",
      "step: 26822, loss: 0.0\n",
      "step: 26823, loss: 2.38418573772492e-09\n",
      "step: 26824, loss: 4.76837103136063e-09\n",
      "step: 26825, loss: 1.0967224284286203e-07\n",
      "step: 26826, loss: 2.288805234229585e-07\n",
      "step: 26827, loss: 1.5497158756261342e-07\n",
      "step: 26828, loss: 1.430511264999268e-08\n",
      "step: 26829, loss: 1.1682485734354486e-07\n",
      "step: 26830, loss: 4.7683684556432127e-08\n",
      "step: 26831, loss: 9.53674206272126e-09\n",
      "step: 26832, loss: 1.668929527909313e-08\n",
      "step: 26833, loss: 4.76837103136063e-09\n",
      "step: 26834, loss: 1.430511176181426e-08\n",
      "step: 26835, loss: 7.152550551836612e-08\n",
      "step: 26836, loss: 7.15255632499634e-09\n",
      "step: 26837, loss: 0.1282205432653427\n",
      "step: 26838, loss: 7.15255676908555e-09\n",
      "step: 26839, loss: 8.106219695491745e-08\n",
      "step: 26840, loss: 1.192092824453539e-08\n",
      "step: 26841, loss: 0.0\n",
      "step: 26842, loss: 2.5272211701121705e-07\n",
      "step: 26843, loss: 4.727281520899851e-06\n",
      "step: 26844, loss: 9.53674117454284e-09\n",
      "step: 26845, loss: 1.6211898810070124e-06\n",
      "step: 26846, loss: 5.483619602841827e-08\n",
      "step: 26847, loss: 0.0\n",
      "step: 26848, loss: 0.0\n",
      "step: 26849, loss: 1.430510998545742e-08\n",
      "step: 26850, loss: 0.06764763593673706\n",
      "step: 26851, loss: 0.0\n",
      "step: 26852, loss: 1.9073478796372e-08\n",
      "step: 26853, loss: 0.0\n",
      "step: 26854, loss: 7.15255676908555e-09\n",
      "step: 26855, loss: 7.15255632499634e-09\n",
      "step: 26856, loss: 0.040219321846961975\n",
      "step: 26857, loss: 7.152551262379347e-08\n",
      "step: 26858, loss: 0.0\n",
      "step: 26859, loss: 4.7683659687436375e-08\n",
      "step: 26860, loss: 7.438521265612508e-07\n",
      "step: 26861, loss: 0.1521318256855011\n",
      "step: 26862, loss: 4.76837103136063e-09\n",
      "step: 26863, loss: 0.0\n",
      "step: 26864, loss: 0.0\n",
      "step: 26865, loss: 2.38418573772492e-09\n",
      "step: 26866, loss: 2.38418573772492e-09\n",
      "step: 26867, loss: 0.0\n",
      "step: 26868, loss: 0.07487958669662476\n",
      "step: 26869, loss: 2.38418573772492e-09\n",
      "step: 26870, loss: 1.430510998545742e-08\n",
      "step: 26871, loss: 1.4042362863619928e-06\n",
      "step: 26872, loss: 4.376915967441164e-06\n",
      "step: 26873, loss: 2.38418573772492e-09\n",
      "step: 26874, loss: 0.0\n",
      "step: 26875, loss: 1.192092558000013e-08\n",
      "step: 26876, loss: 0.0\n",
      "step: 26877, loss: 0.0\n",
      "step: 26878, loss: 2.38418573772492e-09\n",
      "step: 26879, loss: 0.0\n",
      "step: 26880, loss: 2.38418573772492e-09\n",
      "step: 26881, loss: 2.38418573772492e-09\n",
      "step: 26882, loss: 7.15255676908555e-09\n",
      "step: 26883, loss: 3.4332063592046325e-07\n",
      "step: 26884, loss: 2.38418573772492e-09\n",
      "step: 26885, loss: 0.0\n",
      "step: 26886, loss: 0.0\n",
      "step: 26887, loss: 2.38418573772492e-09\n",
      "step: 26888, loss: 0.011523272842168808\n",
      "step: 26889, loss: 9.53674117454284e-09\n",
      "step: 26890, loss: 7.629388676377857e-08\n",
      "step: 26891, loss: 7.15255676908555e-09\n",
      "step: 26892, loss: 9.53674117454284e-09\n",
      "step: 26893, loss: 0.0\n",
      "step: 26894, loss: 7.4505797087454084e-09\n",
      "step: 26895, loss: 2.38418573772492e-09\n",
      "step: 26896, loss: 0.0\n",
      "step: 26897, loss: 0.0\n",
      "step: 26898, loss: 0.0\n",
      "step: 26899, loss: 2.38418573772492e-09\n",
      "step: 26900, loss: 3.552420935193368e-07\n",
      "step: 26901, loss: 1.9073395662871917e-07\n",
      "step: 26902, loss: 2.38418573772492e-09\n",
      "step: 26903, loss: 1.549714738757757e-07\n",
      "step: 26904, loss: 7.867798501592915e-08\n",
      "step: 26905, loss: 0.0\n",
      "step: 26906, loss: 1.192092646817855e-08\n",
      "step: 26907, loss: 9.536734779658218e-08\n",
      "step: 26908, loss: 4.76837103136063e-09\n",
      "step: 26909, loss: 0.18027368187904358\n",
      "step: 26910, loss: 2.622603467727913e-08\n",
      "step: 26911, loss: 0.0\n",
      "step: 26912, loss: 2.956378466478782e-07\n",
      "step: 26913, loss: 2.1934391725153546e-07\n",
      "step: 26914, loss: 3.337857279461787e-08\n",
      "step: 26915, loss: 2.38418573772492e-09\n",
      "step: 26916, loss: 0.0003885079931933433\n",
      "step: 26917, loss: 6.675713137838102e-08\n",
      "step: 26918, loss: 0.0\n",
      "step: 26919, loss: 0.08535856008529663\n",
      "step: 26920, loss: 2.6226025795494934e-08\n",
      "step: 26921, loss: 2.8610218194558e-08\n",
      "step: 26922, loss: 1.192092824453539e-08\n",
      "step: 26923, loss: 0.0\n",
      "step: 26924, loss: 4.76837103136063e-09\n",
      "step: 26925, loss: 4.76837103136063e-09\n",
      "step: 26926, loss: 0.0\n",
      "step: 26927, loss: 0.0\n",
      "step: 26928, loss: 0.0\n",
      "step: 26929, loss: 0.0\n",
      "step: 26930, loss: 2.3841844054572903e-08\n",
      "step: 26931, loss: 0.0\n",
      "step: 26932, loss: 0.0\n",
      "step: 26933, loss: 2.38418573772492e-09\n",
      "step: 26934, loss: 0.0\n",
      "step: 26935, loss: 0.06612708419561386\n",
      "step: 26936, loss: 1.430511176181426e-08\n",
      "step: 26937, loss: 4.5299501039153256e-08\n",
      "step: 26938, loss: 4.76837103136063e-09\n",
      "step: 26939, loss: 1.0728808774729259e-07\n",
      "step: 26940, loss: 0.0\n",
      "step: 26941, loss: 9.53674206272126e-09\n",
      "step: 26942, loss: 2.38418573772492e-09\n",
      "step: 26943, loss: 2.8610209312773804e-08\n",
      "step: 26944, loss: 0.0\n",
      "step: 26945, loss: 1.668929350273629e-08\n",
      "step: 26946, loss: 9.53674206272126e-09\n",
      "step: 26947, loss: 9.281870006816462e-06\n",
      "step: 26948, loss: 8.821468355790785e-08\n",
      "step: 26949, loss: 9.53674117454284e-09\n",
      "step: 26950, loss: 0.0\n",
      "step: 26951, loss: 0.0\n",
      "step: 26952, loss: 4.76837103136063e-09\n",
      "step: 26953, loss: 0.0\n",
      "step: 26954, loss: 2.622603290092229e-08\n",
      "step: 26955, loss: 0.0\n",
      "step: 26956, loss: 0.0\n",
      "step: 26957, loss: 2.38418573772492e-09\n",
      "step: 26958, loss: 0.00017066550208255649\n",
      "step: 26959, loss: 2.38418573772492e-09\n",
      "step: 26960, loss: 0.0\n",
      "step: 26961, loss: 2.384185116000026e-08\n",
      "step: 26962, loss: 9.53674206272126e-09\n",
      "step: 26963, loss: 0.0\n",
      "step: 26964, loss: 2.1457577759065316e-07\n",
      "step: 26965, loss: 0.0\n",
      "step: 26966, loss: 4.76837103136063e-09\n",
      "step: 26967, loss: 4.76837103136063e-09\n",
      "step: 26968, loss: 1.3660926470038248e-06\n",
      "step: 26969, loss: 1.192092646817855e-08\n",
      "step: 26970, loss: 0.0\n",
      "step: 26971, loss: 5.9604552404834976e-08\n",
      "step: 26972, loss: 7.533888037869474e-07\n",
      "step: 26973, loss: 0.0\n",
      "step: 26974, loss: 2.38418573772492e-09\n",
      "step: 26975, loss: 3.576277407546513e-08\n",
      "step: 26976, loss: 3.337857279461787e-08\n",
      "step: 26977, loss: 6.198875013296856e-08\n",
      "step: 26978, loss: 4.529949038101222e-08\n",
      "step: 26979, loss: 7.510046202696685e-07\n",
      "step: 26980, loss: 1.192092646817855e-08\n",
      "step: 26981, loss: 4.291529975830599e-08\n",
      "step: 26982, loss: 5.006785741556996e-08\n",
      "step: 26983, loss: 0.0\n",
      "step: 26984, loss: 4.2438063019289984e-07\n",
      "step: 26985, loss: 2.38418573772492e-09\n",
      "step: 26986, loss: 0.0\n",
      "step: 26987, loss: 4.76837103136063e-09\n",
      "step: 26988, loss: 1.430510998545742e-08\n",
      "step: 26989, loss: 4.76837147544984e-09\n",
      "step: 26990, loss: 0.0\n",
      "step: 26991, loss: 5.48362244501277e-08\n",
      "step: 26992, loss: 4.76837147544984e-09\n",
      "step: 26993, loss: 0.0\n",
      "step: 26994, loss: 9.53674206272126e-09\n",
      "step: 26995, loss: 1.192092646817855e-08\n",
      "step: 26996, loss: 4.76837103136063e-09\n",
      "step: 26997, loss: 1.430511176181426e-08\n",
      "step: 26998, loss: 3.6000881209474755e-07\n",
      "step: 26999, loss: 3.5762759864610416e-08\n",
      "step: 27000, loss: 2.38418573772492e-09\n",
      "step: 27001, loss: 9.53674206272126e-09\n",
      "step: 27002, loss: 2.1457660537294032e-08\n",
      "step: 27003, loss: 2.2888114870056597e-07\n",
      "step: 27004, loss: 2.8610211089130644e-08\n",
      "step: 27005, loss: 4.053112334645448e-08\n",
      "step: 27006, loss: 0.03469216078519821\n",
      "step: 27007, loss: 7.15255676908555e-09\n",
      "step: 27008, loss: 0.0\n",
      "step: 27009, loss: 0.0\n",
      "step: 27010, loss: 0.0\n",
      "step: 27011, loss: 2.145766231365087e-08\n",
      "step: 27012, loss: 0.03995956480503082\n",
      "step: 27013, loss: 7.15255676908555e-09\n",
      "step: 27014, loss: 4.76837103136063e-09\n",
      "step: 27015, loss: 0.0\n",
      "step: 27016, loss: 2.8610209312773804e-08\n",
      "step: 27017, loss: 4.76837103136063e-09\n",
      "step: 27018, loss: 9.77514389433054e-08\n",
      "step: 27019, loss: 0.0\n",
      "step: 27020, loss: 0.0549250952899456\n",
      "step: 27021, loss: 1.907347702001516e-08\n",
      "step: 27022, loss: 2.38418573772492e-09\n",
      "step: 27023, loss: 0.16779693961143494\n",
      "step: 27024, loss: 0.0\n",
      "step: 27025, loss: 1.668929527909313e-08\n",
      "step: 27026, loss: 1.192092558000013e-08\n",
      "step: 27027, loss: 2.38418573772492e-09\n",
      "step: 27028, loss: 2.622603290092229e-08\n",
      "step: 27029, loss: 9.53674117454284e-09\n",
      "step: 27030, loss: 2.38418573772492e-09\n",
      "step: 27031, loss: 4.76837103136063e-09\n",
      "step: 27032, loss: 1.430510998545742e-08\n",
      "step: 27033, loss: 9.53674206272126e-09\n",
      "step: 27034, loss: 1.907347702001516e-08\n",
      "step: 27035, loss: 0.0\n",
      "step: 27036, loss: 6.198873592211385e-08\n",
      "step: 27037, loss: 0.12478702515363693\n",
      "step: 27038, loss: 0.0\n",
      "step: 27039, loss: 0.0\n",
      "step: 27040, loss: 0.07784909754991531\n",
      "step: 27041, loss: 2.8610209312773804e-08\n",
      "step: 27042, loss: 2.38418573772492e-09\n",
      "step: 27043, loss: 0.0\n",
      "step: 27044, loss: 2.38418573772492e-09\n",
      "step: 27045, loss: 0.10651390254497528\n",
      "step: 27046, loss: 4.76837147544984e-09\n",
      "step: 27047, loss: 0.0\n",
      "step: 27048, loss: 9.53674206272126e-09\n",
      "step: 27049, loss: 0.0\n",
      "step: 27050, loss: 0.0\n",
      "step: 27051, loss: 4.76837103136063e-09\n",
      "step: 27052, loss: 2.1695976215596602e-07\n",
      "step: 27053, loss: 0.0\n",
      "step: 27054, loss: 4.76837103136063e-09\n",
      "step: 27055, loss: 0.0\n",
      "step: 27056, loss: 4.76837147544984e-09\n",
      "step: 27057, loss: 2.38418573772492e-09\n",
      "step: 27058, loss: 2.38418573772492e-09\n",
      "step: 27059, loss: 2.38418573772492e-09\n",
      "step: 27060, loss: 4.6729522296118375e-07\n",
      "step: 27061, loss: 2.38418573772492e-09\n",
      "step: 27062, loss: 0.0\n",
      "step: 27063, loss: 0.0\n",
      "step: 27064, loss: 2.38418573772492e-09\n",
      "step: 27065, loss: 7.867799212135651e-08\n",
      "step: 27066, loss: 4.76837147544984e-09\n",
      "step: 27067, loss: 0.0\n",
      "step: 27068, loss: 0.0\n",
      "step: 27069, loss: 0.0\n",
      "step: 27070, loss: 1.668929705544997e-08\n",
      "step: 27071, loss: 2.38418573772492e-09\n",
      "step: 27072, loss: 0.056078966706991196\n",
      "step: 27073, loss: 1.9073478796372e-08\n",
      "step: 27074, loss: 3.3378579900045224e-08\n",
      "step: 27075, loss: 0.07300425320863724\n",
      "step: 27076, loss: 0.0\n",
      "step: 27077, loss: 2.336489473009351e-07\n",
      "step: 27078, loss: 0.0036393774207681417\n",
      "step: 27079, loss: 0.0\n",
      "step: 27080, loss: 0.0\n",
      "step: 27081, loss: 1.9073478796372e-08\n",
      "step: 27082, loss: 4.76837147544984e-09\n",
      "step: 27083, loss: 0.0\n",
      "step: 27084, loss: 7.15255676908555e-09\n",
      "step: 27085, loss: 4.76837103136063e-09\n",
      "step: 27086, loss: 4.76837147544984e-09\n",
      "step: 27087, loss: 1.323183141721529e-06\n",
      "step: 27088, loss: 5.960456661568969e-08\n",
      "step: 27089, loss: 4.2915321074588064e-08\n",
      "step: 27090, loss: 5.9604552404834976e-08\n",
      "step: 27091, loss: 1.430510998545742e-08\n",
      "step: 27092, loss: 4.953724783263169e-06\n",
      "step: 27093, loss: 4.76837103136063e-09\n",
      "step: 27094, loss: 0.0\n",
      "step: 27095, loss: 1.192092558000013e-08\n",
      "step: 27096, loss: 4.76837103136063e-09\n",
      "step: 27097, loss: 0.0\n",
      "step: 27098, loss: 4.529949038101222e-08\n",
      "step: 27099, loss: 0.0\n",
      "step: 27100, loss: 5.245205159098987e-08\n",
      "step: 27101, loss: 7.15255632499634e-09\n",
      "step: 27102, loss: 0.10557682812213898\n",
      "step: 27103, loss: 3.5762766970037774e-08\n",
      "step: 27104, loss: 6.437293365024743e-08\n",
      "step: 27105, loss: 1.430511176181426e-08\n",
      "step: 27106, loss: 0.01773465797305107\n",
      "step: 27107, loss: 9.53674117454284e-09\n",
      "step: 27108, loss: 7.86779779105018e-08\n",
      "step: 27109, loss: 7.15255632499634e-09\n",
      "step: 27110, loss: 0.0\n",
      "step: 27111, loss: 1.668929527909313e-08\n",
      "step: 27112, loss: 0.0\n",
      "step: 27113, loss: 2.38418573772492e-09\n",
      "step: 27114, loss: 9.29830648033203e-08\n",
      "step: 27115, loss: 2.38418573772492e-09\n",
      "step: 27116, loss: 2.6226025795494934e-08\n",
      "step: 27117, loss: 0.0\n",
      "step: 27118, loss: 5.292822038427403e-07\n",
      "step: 27119, loss: 0.16821900010108948\n",
      "step: 27120, loss: 0.0\n",
      "step: 27121, loss: 0.0\n",
      "step: 27122, loss: 0.0\n",
      "step: 27123, loss: 4.76837103136063e-09\n",
      "step: 27124, loss: 9.53674206272126e-09\n",
      "step: 27125, loss: 1.430511176181426e-08\n",
      "step: 27126, loss: 1.430511264999268e-08\n",
      "step: 27127, loss: 1.192092646817855e-08\n",
      "step: 27128, loss: 0.0\n",
      "step: 27129, loss: 2.38418573772492e-09\n",
      "step: 27130, loss: 0.06647085398435593\n",
      "step: 27131, loss: 2.38418573772492e-09\n",
      "step: 27132, loss: 2.4556996436331247e-07\n",
      "step: 27133, loss: 0.0\n",
      "step: 27134, loss: 9.775144604873276e-08\n",
      "step: 27135, loss: 4.76837103136063e-09\n",
      "step: 27136, loss: 0.0\n",
      "step: 27137, loss: 2.145766586636455e-08\n",
      "step: 27138, loss: 1.668929350273629e-08\n",
      "step: 27139, loss: 0.0\n",
      "step: 27140, loss: 9.53674206272126e-09\n",
      "step: 27141, loss: 0.05328712984919548\n",
      "step: 27142, loss: 4.76837103136063e-09\n",
      "step: 27143, loss: 9.53674206272126e-09\n",
      "step: 27144, loss: 4.124599115584715e-07\n",
      "step: 27145, loss: 0.0\n",
      "step: 27146, loss: 2.3841844054572903e-08\n",
      "step: 27147, loss: 0.00033030431950464845\n",
      "step: 27148, loss: 0.0\n",
      "step: 27149, loss: 4.76837103136063e-09\n",
      "step: 27150, loss: 9.53674206272126e-09\n",
      "step: 27151, loss: 1.2159320306182053e-07\n",
      "step: 27152, loss: 7.15255632499634e-09\n",
      "step: 27153, loss: 0.0\n",
      "step: 27154, loss: 2.38418573772492e-09\n",
      "step: 27155, loss: 1.192092646817855e-08\n",
      "step: 27156, loss: 2.38418573772492e-09\n",
      "step: 27157, loss: 0.0018184296786785126\n",
      "step: 27158, loss: 2.38418573772492e-09\n",
      "step: 27159, loss: 4.219964750973304e-07\n",
      "step: 27160, loss: 1.8119747835498856e-07\n",
      "step: 27161, loss: 1.430510998545742e-08\n",
      "step: 27162, loss: 1.668929705544997e-08\n",
      "step: 27163, loss: 2.38418573772492e-09\n",
      "step: 27164, loss: 3.3378579900045224e-08\n",
      "step: 27165, loss: 1.192092558000013e-08\n",
      "step: 27166, loss: 1.7404494201400666e-07\n",
      "step: 27167, loss: 7.6293815709505e-08\n",
      "step: 27168, loss: 4.76837147544984e-09\n",
      "step: 27169, loss: 1.192092646817855e-08\n",
      "step: 27170, loss: 2.145766231365087e-08\n",
      "step: 27171, loss: 0.0\n",
      "step: 27172, loss: 0.0025708747562021017\n",
      "step: 27173, loss: 0.0\n",
      "step: 27174, loss: 1.430511176181426e-08\n",
      "step: 27175, loss: 7.15255632499634e-09\n",
      "step: 27176, loss: 0.0\n",
      "step: 27177, loss: 0.0\n",
      "step: 27178, loss: 1.0251982729414522e-07\n",
      "step: 27179, loss: 7.15255676908555e-09\n",
      "step: 27180, loss: 0.0\n",
      "step: 27181, loss: 0.0\n",
      "step: 27182, loss: 0.0\n",
      "step: 27183, loss: 2.38418573772492e-09\n",
      "step: 27184, loss: 4.053111624102712e-08\n",
      "step: 27185, loss: 1.327957534158486e-06\n",
      "step: 27186, loss: 0.0\n",
      "step: 27187, loss: 2.38418573772492e-09\n",
      "step: 27188, loss: 2.38418573772492e-09\n",
      "step: 27189, loss: 0.0\n",
      "step: 27190, loss: 1.192092646817855e-08\n",
      "step: 27191, loss: 0.0\n",
      "step: 27192, loss: 4.76837103136063e-09\n",
      "step: 27193, loss: 0.0\n",
      "step: 27194, loss: 7.15255632499634e-09\n",
      "step: 27195, loss: 1.6810998204164207e-05\n",
      "step: 27196, loss: 2.38418573772492e-09\n",
      "step: 27197, loss: 0.0\n",
      "step: 27198, loss: 2.384185116000026e-08\n",
      "step: 27199, loss: 0.13944898545742035\n",
      "step: 27200, loss: 2.622604000634965e-08\n",
      "step: 27201, loss: 1.8358161923970329e-07\n",
      "step: 27202, loss: 0.0\n",
      "step: 27203, loss: 1.668929527909313e-08\n",
      "step: 27204, loss: 3.8146950487316644e-08\n",
      "step: 27205, loss: 0.0\n",
      "step: 27206, loss: 0.0005690112011507154\n",
      "step: 27207, loss: 0.0\n",
      "step: 27208, loss: 4.76837103136063e-09\n",
      "step: 27209, loss: 0.0\n",
      "step: 27210, loss: 2.1457559284954186e-07\n",
      "step: 27211, loss: 0.0\n",
      "step: 27212, loss: 7.15255632499634e-09\n",
      "step: 27213, loss: 2.38418573772492e-09\n",
      "step: 27214, loss: 7.15255632499634e-09\n",
      "step: 27215, loss: 2.38418573772492e-09\n",
      "step: 27216, loss: 1.0728808774729259e-07\n",
      "step: 27217, loss: 0.0\n",
      "step: 27218, loss: 0.0\n",
      "step: 27219, loss: 7.15255676908555e-09\n",
      "step: 27220, loss: 0.0\n",
      "step: 27221, loss: 2.622604000634965e-08\n",
      "step: 27222, loss: 0.0\n",
      "step: 27223, loss: 0.0\n",
      "step: 27224, loss: 0.060279279947280884\n",
      "step: 27225, loss: 0.0\n",
      "step: 27226, loss: 2.38418573772492e-09\n",
      "step: 27227, loss: 7.15255676908555e-09\n",
      "step: 27228, loss: 0.053226493299007416\n",
      "step: 27229, loss: 0.09052607417106628\n",
      "step: 27230, loss: 0.0\n",
      "step: 27231, loss: 0.0\n",
      "step: 27232, loss: 0.0\n",
      "step: 27233, loss: 2.3841845830929742e-08\n",
      "step: 27234, loss: 1.668929527909313e-08\n",
      "step: 27235, loss: 0.0\n",
      "step: 27236, loss: 1.192092558000013e-08\n",
      "step: 27237, loss: 4.76837103136063e-09\n",
      "step: 27238, loss: 3.814693627646193e-08\n",
      "step: 27239, loss: 1.0013556561716541e-07\n",
      "step: 27240, loss: 0.0\n",
      "step: 27241, loss: 1.430510998545742e-08\n",
      "step: 27242, loss: 0.0\n",
      "step: 27243, loss: 7.15255632499634e-09\n",
      "step: 27244, loss: 0.1656111776828766\n",
      "step: 27245, loss: 0.0\n",
      "step: 27246, loss: 0.0\n",
      "step: 27247, loss: 4.76837103136063e-09\n",
      "step: 27248, loss: 5.149775574864179e-07\n",
      "step: 27249, loss: 1.430511176181426e-08\n",
      "step: 27250, loss: 4.76837147544984e-09\n",
      "step: 27251, loss: 0.0\n",
      "step: 27252, loss: 1.430510998545742e-08\n",
      "step: 27253, loss: 1.3970884538139217e-06\n",
      "step: 27254, loss: 2.3841844054572903e-08\n",
      "step: 27255, loss: 0.0\n",
      "step: 27256, loss: 0.0\n",
      "step: 27257, loss: 0.0\n",
      "step: 27258, loss: 0.08394952863454819\n",
      "step: 27259, loss: 2.38418573772492e-09\n",
      "step: 27260, loss: 0.0\n",
      "step: 27261, loss: 7.15255632499634e-09\n",
      "step: 27262, loss: 7.15255632499634e-09\n",
      "step: 27263, loss: 0.0\n",
      "step: 27264, loss: 1.907348234908568e-08\n",
      "step: 27265, loss: 0.0\n",
      "step: 27266, loss: 5.2659743232652545e-06\n",
      "step: 27267, loss: 0.0\n",
      "step: 27268, loss: 2.38418573772492e-09\n",
      "step: 27269, loss: 3.814693982917561e-08\n",
      "step: 27270, loss: 7.15255676908555e-09\n",
      "step: 27271, loss: 1.907348234908568e-08\n",
      "step: 27272, loss: 0.0\n",
      "step: 27273, loss: 0.0\n",
      "step: 27274, loss: 7.15255676908555e-09\n",
      "step: 27275, loss: 0.0\n",
      "step: 27276, loss: 0.062489934265613556\n",
      "step: 27277, loss: 1.907348234908568e-08\n",
      "step: 27278, loss: 2.38418573772492e-09\n",
      "step: 27279, loss: 3.313993488518463e-07\n",
      "step: 27280, loss: 2.384184938364342e-08\n",
      "step: 27281, loss: 2.38418573772492e-09\n",
      "step: 27282, loss: 0.0\n",
      "step: 27283, loss: 0.0\n",
      "step: 27284, loss: 0.0\n",
      "step: 27285, loss: 0.0\n",
      "step: 27286, loss: 2.38418573772492e-09\n",
      "step: 27287, loss: 0.045897554606199265\n",
      "step: 27288, loss: 2.38418573772492e-09\n",
      "step: 27289, loss: 2.622587373934948e-07\n",
      "step: 27290, loss: 1.0251973492358957e-07\n",
      "step: 27291, loss: 0.0\n",
      "step: 27292, loss: 2.38418573772492e-09\n",
      "step: 27293, loss: 2.38418573772492e-09\n",
      "step: 27294, loss: 1.192092824453539e-08\n",
      "step: 27295, loss: 5.245205159098987e-08\n",
      "step: 27296, loss: 1.5258730456935155e-07\n",
      "step: 27297, loss: 0.0\n",
      "step: 27298, loss: 1.2063617305102525e-06\n",
      "step: 27299, loss: 3.8146950487316644e-08\n",
      "step: 27300, loss: 2.38418573772492e-09\n",
      "step: 27301, loss: 0.0\n",
      "step: 27302, loss: 4.672950808526366e-07\n",
      "step: 27303, loss: 1.430510998545742e-08\n",
      "step: 27304, loss: 3.576277407546513e-08\n",
      "step: 27305, loss: 3.8575075450353324e-05\n",
      "step: 27306, loss: 0.13564945757389069\n",
      "step: 27307, loss: 4.76837147544984e-09\n",
      "step: 27308, loss: 2.38418573772492e-09\n",
      "step: 27309, loss: 0.0\n",
      "step: 27310, loss: 0.0\n",
      "step: 27311, loss: 2.38418573772492e-09\n",
      "step: 27312, loss: 1.430510998545742e-08\n",
      "step: 27313, loss: 2.38418573772492e-09\n",
      "step: 27314, loss: 0.0\n",
      "step: 27315, loss: 1.8119764888524514e-07\n",
      "step: 27316, loss: 0.13517078757286072\n",
      "step: 27317, loss: 0.066437266767025\n",
      "step: 27318, loss: 7.15255676908555e-09\n",
      "step: 27319, loss: 1.7881336589198327e-07\n",
      "step: 27320, loss: 0.0\n",
      "step: 27321, loss: 1.406665859349232e-07\n",
      "step: 27322, loss: 1.931182822545452e-07\n",
      "step: 27323, loss: 5.29282317529578e-07\n",
      "step: 27324, loss: 4.76837147544984e-09\n",
      "step: 27325, loss: 9.53674206272126e-09\n",
      "step: 27326, loss: 9.53674206272126e-09\n",
      "step: 27327, loss: 4.76837103136063e-09\n",
      "step: 27328, loss: 0.0\n",
      "step: 27329, loss: 2.217280723471049e-07\n",
      "step: 27330, loss: 2.145766764272139e-08\n",
      "step: 27331, loss: 0.0\n",
      "step: 27332, loss: 0.0\n",
      "step: 27333, loss: 2.38418573772492e-09\n",
      "step: 27334, loss: 2.646428640673548e-07\n",
      "step: 27335, loss: 0.0\n",
      "step: 27336, loss: 2.1457660537294032e-08\n",
      "step: 27337, loss: 2.38418573772492e-09\n",
      "step: 27338, loss: 9.53674206272126e-09\n",
      "step: 27339, loss: 4.76837147544984e-09\n",
      "step: 27340, loss: 2.861022352362852e-08\n",
      "step: 27341, loss: 2.38418573772492e-09\n",
      "step: 27342, loss: 1.668929350273629e-08\n",
      "step: 27343, loss: 4.768366679286373e-08\n",
      "step: 27344, loss: 2.384185116000026e-08\n",
      "step: 27345, loss: 7.15255632499634e-09\n",
      "step: 27346, loss: 0.05040593072772026\n",
      "step: 27347, loss: 8.821476171760878e-08\n",
      "step: 27348, loss: 5.245201961656676e-08\n",
      "step: 27349, loss: 1.430510998545742e-08\n",
      "step: 27350, loss: 9.53674117454284e-09\n",
      "step: 27351, loss: 1.0490391844086844e-07\n",
      "step: 27352, loss: 0.0\n",
      "step: 27353, loss: 1.430511176181426e-08\n",
      "step: 27354, loss: 2.38418573772492e-09\n",
      "step: 27355, loss: 0.0\n",
      "step: 27356, loss: 0.0\n",
      "step: 27357, loss: 2.384184938364342e-08\n",
      "step: 27358, loss: 2.38418529363571e-08\n",
      "step: 27359, loss: 2.38418573772492e-09\n",
      "step: 27360, loss: 2.38418573772492e-09\n",
      "step: 27361, loss: 3.576277407546513e-08\n",
      "step: 27362, loss: 0.060833774507045746\n",
      "step: 27363, loss: 4.76837103136063e-09\n",
      "step: 27364, loss: 9.53674206272126e-09\n",
      "step: 27365, loss: 2.38418573772492e-09\n",
      "step: 27366, loss: 2.38418573772492e-09\n",
      "step: 27367, loss: 7.152551972922083e-08\n",
      "step: 27368, loss: 3.147100642308942e-07\n",
      "step: 27369, loss: 1.9073478796372e-08\n",
      "step: 27370, loss: 0.0\n",
      "step: 27371, loss: 1.192092824453539e-08\n",
      "step: 27372, loss: 1.430511176181426e-08\n",
      "step: 27373, loss: 9.53674206272126e-09\n",
      "step: 27374, loss: 1.192092646817855e-08\n",
      "step: 27375, loss: 2.1457660537294032e-08\n",
      "step: 27376, loss: 3.8146946934602965e-08\n",
      "step: 27377, loss: 1.907348234908568e-08\n",
      "step: 27378, loss: 0.0001180117396870628\n",
      "step: 27379, loss: 1.907348234908568e-08\n",
      "step: 27380, loss: 4.76837147544984e-09\n",
      "step: 27381, loss: 4.527056717051892e-06\n",
      "step: 27382, loss: 2.145766586636455e-08\n",
      "step: 27383, loss: 2.38418573772492e-09\n",
      "step: 27384, loss: 9.53674206272126e-09\n",
      "step: 27385, loss: 6.675711006209895e-08\n",
      "step: 27386, loss: 2.38418573772492e-09\n",
      "step: 27387, loss: 2.8610209312773804e-08\n",
      "step: 27388, loss: 0.01547202467918396\n",
      "step: 27389, loss: 2.38418573772492e-09\n",
      "step: 27390, loss: 2.38418573772492e-09\n",
      "step: 27391, loss: 9.53674117454284e-09\n",
      "step: 27392, loss: 4.76837103136063e-09\n",
      "step: 27393, loss: 4.531255763140507e-05\n",
      "step: 27394, loss: 6.914127226309574e-08\n",
      "step: 27395, loss: 0.0\n",
      "step: 27396, loss: 1.907348234908568e-08\n",
      "step: 27397, loss: 0.0\n",
      "step: 27398, loss: 2.38418573772492e-09\n",
      "step: 27399, loss: 0.0003391211212147027\n",
      "step: 27400, loss: 3.859627668134635e-06\n",
      "step: 27401, loss: 0.0\n",
      "step: 27402, loss: 2.38418573772492e-09\n",
      "step: 27403, loss: 9.53674117454284e-09\n",
      "step: 27404, loss: 0.0\n",
      "step: 27405, loss: 2.2172812919052376e-07\n",
      "step: 27406, loss: 5.531234705813404e-07\n",
      "step: 27407, loss: 1.430510998545742e-08\n",
      "step: 27408, loss: 4.76837147544984e-09\n",
      "step: 27409, loss: 1.430510998545742e-08\n",
      "step: 27410, loss: 2.6226025795494934e-08\n",
      "step: 27411, loss: 0.0\n",
      "step: 27412, loss: 2.6226025795494934e-08\n",
      "step: 27413, loss: 9.321951210949919e-07\n",
      "step: 27414, loss: 1.0728818011784824e-07\n",
      "step: 27415, loss: 2.145766231365087e-08\n",
      "step: 27416, loss: 4.76837147544984e-09\n",
      "step: 27417, loss: 2.38418573772492e-09\n",
      "step: 27418, loss: 5.722041507283393e-08\n",
      "step: 27419, loss: 2.38418573772492e-09\n",
      "step: 27420, loss: 7.15255632499634e-09\n",
      "step: 27421, loss: 0.0\n",
      "step: 27422, loss: 4.053112334645448e-08\n",
      "step: 27423, loss: 7.86780276484933e-08\n",
      "step: 27424, loss: 4.76837103136063e-09\n",
      "step: 27425, loss: 1.2564305507112294e-06\n",
      "step: 27426, loss: 1.3112979502238886e-07\n",
      "step: 27427, loss: 0.13016945123672485\n",
      "step: 27428, loss: 1.0967232810799032e-07\n",
      "step: 27429, loss: 0.0\n",
      "step: 27430, loss: 0.00020259419397916645\n",
      "step: 27431, loss: 4.52994939337259e-08\n",
      "step: 27432, loss: 0.0\n",
      "step: 27433, loss: 0.0\n",
      "step: 27434, loss: 0.0\n",
      "step: 27435, loss: 1.1205644057099562e-07\n",
      "step: 27436, loss: 0.0\n",
      "step: 27437, loss: 2.38418573772492e-09\n",
      "step: 27438, loss: 0.0\n",
      "step: 27439, loss: 0.0\n",
      "step: 27440, loss: 4.60144775615845e-07\n",
      "step: 27441, loss: 7.629380149865028e-08\n",
      "step: 27442, loss: 0.204948291182518\n",
      "step: 27443, loss: 0.21910524368286133\n",
      "step: 27444, loss: 7.15255676908555e-09\n",
      "step: 27445, loss: 2.145766764272139e-08\n",
      "step: 27446, loss: 0.0\n",
      "step: 27447, loss: 0.033391665667295456\n",
      "step: 27448, loss: 2.6226025795494934e-08\n",
      "step: 27449, loss: 2.38418573772492e-09\n",
      "step: 27450, loss: 6.794814453314757e-07\n",
      "step: 27451, loss: 2.38418573772492e-09\n",
      "step: 27452, loss: 2.38418573772492e-09\n",
      "step: 27453, loss: 7.15255632499634e-09\n",
      "step: 27454, loss: 6.675713137838102e-08\n",
      "step: 27455, loss: 0.0\n",
      "step: 27456, loss: 0.0\n",
      "step: 27457, loss: 0.0\n",
      "step: 27458, loss: 0.0\n",
      "step: 27459, loss: 1.430511264999268e-08\n",
      "step: 27460, loss: 1.16824800500126e-07\n",
      "step: 27461, loss: 1.430511264999268e-08\n",
      "step: 27462, loss: 7.15255632499634e-09\n",
      "step: 27463, loss: 9.775139631074126e-08\n",
      "step: 27464, loss: 7.15255676908555e-09\n",
      "step: 27465, loss: 1.0728812327442938e-07\n",
      "step: 27466, loss: 0.0\n",
      "step: 27467, loss: 1.430511264999268e-08\n",
      "step: 27468, loss: 0.0\n",
      "step: 27469, loss: 0.0\n",
      "step: 27470, loss: 2.38418573772492e-09\n",
      "step: 27471, loss: 0.0\n",
      "step: 27472, loss: 2.145766231365087e-08\n",
      "step: 27473, loss: 1.430510998545742e-08\n",
      "step: 27474, loss: 2.38418573772492e-09\n",
      "step: 27475, loss: 9.53674117454284e-09\n",
      "step: 27476, loss: 7.15255676908555e-09\n",
      "step: 27477, loss: 2.38418573772492e-09\n",
      "step: 27478, loss: 0.0\n",
      "step: 27479, loss: 2.38418573772492e-09\n",
      "step: 27480, loss: 0.08121892064809799\n",
      "step: 27481, loss: 0.0\n",
      "step: 27482, loss: 7.15255676908555e-09\n",
      "step: 27483, loss: 0.0\n",
      "step: 27484, loss: 2.38418573772492e-09\n",
      "step: 27485, loss: 2.8610216418201162e-08\n",
      "step: 27486, loss: 4.529948327558486e-08\n",
      "step: 27487, loss: 0.0\n",
      "step: 27488, loss: 0.05699869617819786\n",
      "step: 27489, loss: 3.337858700547258e-08\n",
      "step: 27490, loss: 0.0\n",
      "step: 27491, loss: 0.0\n",
      "step: 27492, loss: 0.0\n",
      "step: 27493, loss: 0.0\n",
      "step: 27494, loss: 0.0\n",
      "step: 27495, loss: 0.0\n",
      "step: 27496, loss: 2.38418573772492e-09\n",
      "step: 27497, loss: 5.722039020383818e-08\n",
      "step: 27498, loss: 0.0\n",
      "step: 27499, loss: 4.363016330444225e-07\n",
      "step: 27500, loss: 0.0\n",
      "step: 27501, loss: 7.15255632499634e-09\n",
      "step: 27502, loss: 0.0\n",
      "step: 27503, loss: 0.004835445899516344\n",
      "step: 27504, loss: 1.668929527909313e-08\n",
      "step: 27505, loss: 4.76837147544984e-09\n",
      "step: 27506, loss: 2.38418573772492e-09\n",
      "step: 27507, loss: 2.38418573772492e-09\n",
      "step: 27508, loss: 0.1534581184387207\n",
      "step: 27509, loss: 9.536730516401803e-08\n",
      "step: 27510, loss: 5.00678503101426e-08\n",
      "step: 27511, loss: 2.38418573772492e-09\n",
      "step: 27512, loss: 0.0\n",
      "step: 27513, loss: 2.8610209312773804e-08\n",
      "step: 27514, loss: 4.76837147544984e-09\n",
      "step: 27515, loss: 2.908692806613544e-07\n",
      "step: 27516, loss: 2.38418573772492e-09\n",
      "step: 27517, loss: 4.76837103136063e-09\n",
      "step: 27518, loss: 2.38418573772492e-09\n",
      "step: 27519, loss: 1.430511176181426e-08\n",
      "step: 27520, loss: 0.0\n",
      "step: 27521, loss: 1.668929527909313e-08\n",
      "step: 27522, loss: 7.15255632499634e-09\n",
      "step: 27523, loss: 7.15255676908555e-09\n",
      "step: 27524, loss: 9.53674206272126e-09\n",
      "step: 27525, loss: 0.0\n",
      "step: 27526, loss: 3.0994389277338996e-08\n",
      "step: 27527, loss: 1.192092558000013e-08\n",
      "step: 27528, loss: 6.198873592211385e-08\n",
      "step: 27529, loss: 1.0013556561716541e-07\n",
      "step: 27530, loss: 4.76837147544984e-09\n",
      "step: 27531, loss: 4.76837147544984e-09\n",
      "step: 27532, loss: 0.0\n",
      "step: 27533, loss: 1.4781925017359754e-07\n",
      "step: 27534, loss: 0.0\n",
      "step: 27535, loss: 5.0067878731852034e-08\n",
      "step: 27536, loss: 1.668929527909313e-08\n",
      "step: 27537, loss: 1.907347702001516e-08\n",
      "step: 27538, loss: 0.018769070506095886\n",
      "step: 27539, loss: 0.0\n",
      "step: 27540, loss: 7.15255676908555e-09\n",
      "step: 27541, loss: 3.4570399520816864e-07\n",
      "step: 27542, loss: 7.15255676908555e-09\n",
      "step: 27543, loss: 2.38418573772492e-09\n",
      "step: 27544, loss: 0.0\n",
      "step: 27545, loss: 2.6226025795494934e-08\n",
      "step: 27546, loss: 9.53674117454284e-09\n",
      "step: 27547, loss: 0.0\n",
      "step: 27548, loss: 0.0\n",
      "step: 27549, loss: 2.14575635482106e-07\n",
      "step: 27550, loss: 1.5735565739305457e-07\n",
      "step: 27551, loss: 3.0994396382766354e-08\n",
      "step: 27552, loss: 3.5762766970037774e-08\n",
      "step: 27553, loss: 2.622604000634965e-08\n",
      "step: 27554, loss: 2.3841844054572903e-08\n",
      "step: 27555, loss: 2.38418573772492e-09\n",
      "step: 27556, loss: 2.38418573772492e-09\n",
      "step: 27557, loss: 1.430511176181426e-08\n",
      "step: 27558, loss: 2.38418573772492e-09\n",
      "step: 27559, loss: 0.0\n",
      "step: 27560, loss: 1.907347702001516e-08\n",
      "step: 27561, loss: 0.0\n",
      "step: 27562, loss: 7.15255676908555e-09\n",
      "step: 27563, loss: 4.76837103136063e-09\n",
      "step: 27564, loss: 3.576275631189674e-08\n",
      "step: 27565, loss: 5.00672740599839e-07\n",
      "step: 27566, loss: 2.38418573772492e-09\n",
      "step: 27567, loss: 9.53674117454284e-09\n",
      "step: 27568, loss: 7.15255676908555e-09\n",
      "step: 27569, loss: 2.674877805475262e-06\n",
      "step: 27570, loss: 1.9073478796372e-08\n",
      "step: 27571, loss: 9.53674117454284e-09\n",
      "step: 27572, loss: 0.0\n",
      "step: 27573, loss: 0.0\n",
      "step: 27574, loss: 2.38418573772492e-09\n",
      "step: 27575, loss: 0.0\n",
      "step: 27576, loss: 2.38418573772492e-09\n",
      "step: 27577, loss: 1.120565116252692e-07\n",
      "step: 27578, loss: 0.0\n",
      "step: 27579, loss: 0.0\n",
      "step: 27580, loss: 2.38418529363571e-08\n",
      "step: 27581, loss: 4.76837103136063e-09\n",
      "step: 27582, loss: 2.38418573772492e-09\n",
      "step: 27583, loss: 2.38418573772492e-09\n",
      "step: 27584, loss: 2.861021997091484e-08\n",
      "step: 27585, loss: 4.76837147544984e-09\n",
      "step: 27586, loss: 5.2452058696417225e-08\n",
      "step: 27587, loss: 1.430511264999268e-08\n",
      "step: 27588, loss: 0.0\n",
      "step: 27589, loss: 1.668929527909313e-08\n",
      "step: 27590, loss: 1.907347702001516e-08\n",
      "step: 27591, loss: 5.245202316928044e-08\n",
      "step: 27592, loss: 2.38418573772492e-09\n",
      "step: 27593, loss: 4.76837147544984e-09\n",
      "step: 27594, loss: 7.15255676908555e-09\n",
      "step: 27595, loss: 0.0\n",
      "step: 27596, loss: 9.536733358572747e-08\n",
      "step: 27597, loss: 0.0\n",
      "step: 27598, loss: 4.76837147544984e-09\n",
      "step: 27599, loss: 9.53674206272126e-09\n",
      "step: 27600, loss: 1.192092558000013e-08\n",
      "step: 27601, loss: 0.0\n",
      "step: 27602, loss: 2.38418573772492e-09\n",
      "step: 27603, loss: 2.4388732526858803e-06\n",
      "step: 27604, loss: 0.0\n",
      "step: 27605, loss: 2.38418529363571e-08\n",
      "step: 27606, loss: 3.0994396382766354e-08\n",
      "step: 27607, loss: 1.192092558000013e-08\n",
      "step: 27608, loss: 4.76837147544984e-09\n",
      "step: 27609, loss: 0.0\n",
      "step: 27610, loss: 0.0\n",
      "step: 27611, loss: 3.0994396382766354e-08\n",
      "step: 27612, loss: 2.38418573772492e-09\n",
      "step: 27613, loss: 7.15255676908555e-09\n",
      "step: 27614, loss: 5.269007488095667e-07\n",
      "step: 27615, loss: 5.0067875179138355e-08\n",
      "step: 27616, loss: 0.0\n",
      "step: 27617, loss: 4.76837103136063e-09\n",
      "step: 27618, loss: 2.38418573772492e-09\n",
      "step: 27619, loss: 1.1872926961586927e-06\n",
      "step: 27620, loss: 3.814693627646193e-08\n",
      "step: 27621, loss: 1.192092558000013e-08\n",
      "step: 27622, loss: 1.144405885611377e-07\n",
      "step: 27623, loss: 0.0\n",
      "step: 27624, loss: 0.0\n",
      "step: 27625, loss: 2.38418573772492e-09\n",
      "step: 27626, loss: 4.76837103136063e-09\n",
      "step: 27627, loss: 2.38418573772492e-09\n",
      "step: 27628, loss: 0.0\n",
      "step: 27629, loss: 1.907348234908568e-08\n",
      "step: 27630, loss: 8.868979080034478e-07\n",
      "step: 27631, loss: 0.0\n",
      "step: 27632, loss: 1.668929350273629e-08\n",
      "step: 27633, loss: 5.60280113859335e-07\n",
      "step: 27634, loss: 1.668929527909313e-08\n",
      "step: 27635, loss: 4.768367389829109e-08\n",
      "step: 27636, loss: 0.0\n",
      "step: 27637, loss: 3.576277052275145e-08\n",
      "step: 27638, loss: 2.8610209312773804e-08\n",
      "step: 27639, loss: 3.0994396382766354e-08\n",
      "step: 27640, loss: 0.0018160290783271194\n",
      "step: 27641, loss: 0.0\n",
      "step: 27642, loss: 7.15255676908555e-09\n",
      "step: 27643, loss: 9.536720568803503e-08\n",
      "step: 27644, loss: 4.76837147544984e-09\n",
      "step: 27645, loss: 9.53674206272126e-09\n",
      "step: 27646, loss: 2.193439598840996e-07\n",
      "step: 27647, loss: 2.8610211089130644e-08\n",
      "step: 27648, loss: 2.622603467727913e-08\n",
      "step: 27649, loss: 0.0\n",
      "step: 27650, loss: 4.76837103136063e-09\n",
      "step: 27651, loss: 2.38418573772492e-09\n",
      "step: 27652, loss: 0.0\n",
      "step: 27653, loss: 0.0\n",
      "step: 27654, loss: 0.0\n",
      "step: 27655, loss: 7.152546288580197e-08\n",
      "step: 27656, loss: 2.145766586636455e-08\n",
      "step: 27657, loss: 7.15255676908555e-09\n",
      "step: 27658, loss: 0.0\n",
      "step: 27659, loss: 7.15255632499634e-09\n",
      "step: 27660, loss: 2.0742331230394484e-07\n",
      "step: 27661, loss: 0.08794886618852615\n",
      "step: 27662, loss: 1.907347702001516e-08\n",
      "step: 27663, loss: 0.0\n",
      "step: 27664, loss: 4.583349800668657e-05\n",
      "step: 27665, loss: 0.15038679540157318\n",
      "step: 27666, loss: 0.0\n",
      "step: 27667, loss: 1.192092646817855e-08\n",
      "step: 27668, loss: 3.814693627646193e-08\n",
      "step: 27669, loss: 5.722038309841082e-08\n",
      "step: 27670, loss: 1.192092558000013e-08\n",
      "step: 27671, loss: 1.907347702001516e-08\n",
      "step: 27672, loss: 7.15255676908555e-09\n",
      "step: 27673, loss: 0.0\n",
      "step: 27674, loss: 1.0251987703213672e-07\n",
      "step: 27675, loss: 0.0\n",
      "step: 27676, loss: 0.0\n",
      "step: 27677, loss: 0.0\n",
      "step: 27678, loss: 0.00010642084089340642\n",
      "step: 27679, loss: 2.6226025795494934e-08\n",
      "step: 27680, loss: 0.0\n",
      "step: 27681, loss: 1.430511176181426e-08\n",
      "step: 27682, loss: 4.76837147544984e-09\n",
      "step: 27683, loss: 2.0980725423669355e-07\n",
      "step: 27684, loss: 7.15255632499634e-09\n",
      "step: 27685, loss: 2.38418573772492e-09\n",
      "step: 27686, loss: 0.0\n",
      "step: 27687, loss: 0.0\n",
      "step: 27688, loss: 0.14365994930267334\n",
      "step: 27689, loss: 2.3841845830929742e-08\n",
      "step: 27690, loss: 7.15255676908555e-09\n",
      "step: 27691, loss: 2.38418573772492e-09\n",
      "step: 27692, loss: 0.12386076897382736\n",
      "step: 27693, loss: 0.0\n",
      "step: 27694, loss: 1.7404485674887837e-07\n",
      "step: 27695, loss: 0.0\n",
      "step: 27696, loss: 0.0\n",
      "step: 27697, loss: 1.0275576869389624e-06\n",
      "step: 27698, loss: 2.38418573772492e-09\n",
      "step: 27699, loss: 2.8610209312773804e-08\n",
      "step: 27700, loss: 2.38418573772492e-09\n",
      "step: 27701, loss: 4.053111624102712e-08\n",
      "step: 27702, loss: 7.15255632499634e-09\n",
      "step: 27703, loss: 0.09591551870107651\n",
      "step: 27704, loss: 2.38418573772492e-09\n",
      "step: 27705, loss: 0.0\n",
      "step: 27706, loss: 2.38418573772492e-09\n",
      "step: 27707, loss: 2.38418573772492e-09\n",
      "step: 27708, loss: 8.821468355790785e-08\n",
      "step: 27709, loss: 4.76837103136063e-09\n",
      "step: 27710, loss: 4.76837103136063e-09\n",
      "step: 27711, loss: 2.38418573772492e-09\n",
      "step: 27712, loss: 0.0\n",
      "step: 27713, loss: 2.38418573772492e-09\n",
      "step: 27714, loss: 1.907348412544252e-08\n",
      "step: 27715, loss: 2.384185116000026e-08\n",
      "step: 27716, loss: 1.192092646817855e-08\n",
      "step: 27717, loss: 0.0\n",
      "step: 27718, loss: 0.0\n",
      "step: 27719, loss: 0.0\n",
      "step: 27720, loss: 1.668929527909313e-08\n",
      "step: 27721, loss: 8.559043749301054e-07\n",
      "step: 27722, loss: 0.0\n",
      "step: 27723, loss: 0.0\n",
      "step: 27724, loss: 0.0034490497782826424\n",
      "step: 27725, loss: 2.38418573772492e-09\n",
      "step: 27726, loss: 9.775138209988654e-08\n",
      "step: 27727, loss: 0.0\n",
      "step: 27728, loss: 0.07950129359960556\n",
      "step: 27729, loss: 7.15255676908555e-09\n",
      "step: 27730, loss: 0.0003875491092912853\n",
      "step: 27731, loss: 0.05248744785785675\n",
      "step: 27732, loss: 2.38418573772492e-09\n",
      "step: 27733, loss: 4.76837147544984e-09\n",
      "step: 27734, loss: 2.38418573772492e-09\n",
      "step: 27735, loss: 0.0\n",
      "step: 27736, loss: 1.907347702001516e-08\n",
      "step: 27737, loss: 0.0\n",
      "step: 27738, loss: 0.0\n",
      "step: 27739, loss: 2.38418573772492e-09\n",
      "step: 27740, loss: 0.0\n",
      "step: 27741, loss: 7.15255676908555e-09\n",
      "step: 27742, loss: 2.145766231365087e-08\n",
      "step: 27743, loss: 0.07219020277261734\n",
      "step: 27744, loss: 0.05124718323349953\n",
      "step: 27745, loss: 0.0\n",
      "step: 27746, loss: 2.145766231365087e-08\n",
      "step: 27747, loss: 4.76837103136063e-09\n",
      "step: 27748, loss: 0.06449417024850845\n",
      "step: 27749, loss: 2.38418573772492e-09\n",
      "step: 27750, loss: 9.53674206272126e-09\n",
      "step: 27751, loss: 2.38418573772492e-09\n",
      "step: 27752, loss: 0.0\n",
      "step: 27753, loss: 2.1457660537294032e-08\n",
      "step: 27754, loss: 2.38418573772492e-09\n",
      "step: 27755, loss: 0.0\n",
      "step: 27756, loss: 2.1457660537294032e-08\n",
      "step: 27757, loss: 2.38418573772492e-09\n",
      "step: 27758, loss: 2.145766231365087e-08\n",
      "step: 27759, loss: 4.76837147544984e-09\n",
      "step: 27760, loss: 2.38418573772492e-09\n",
      "step: 27761, loss: 2.38418573772492e-09\n",
      "step: 27762, loss: 0.0\n",
      "step: 27763, loss: 1.9073478796372e-08\n",
      "step: 27764, loss: 5.483619602841827e-08\n",
      "step: 27765, loss: 0.0\n",
      "step: 27766, loss: 0.0\n",
      "step: 27767, loss: 7.15255676908555e-09\n",
      "step: 27768, loss: 7.15255676908555e-09\n",
      "step: 27769, loss: 1.1682491418696372e-07\n",
      "step: 27770, loss: 0.0\n",
      "step: 27771, loss: 0.0\n",
      "step: 27772, loss: 1.192092558000013e-08\n",
      "step: 27773, loss: 0.0\n",
      "step: 27774, loss: 7.15255676908555e-09\n",
      "step: 27775, loss: 3.0994389277338996e-08\n",
      "step: 27776, loss: 4.76837103136063e-09\n",
      "step: 27777, loss: 0.0\n",
      "step: 27778, loss: 7.152546288580197e-08\n",
      "step: 27779, loss: 7.15255632499634e-09\n",
      "step: 27780, loss: 7.15255676908555e-09\n",
      "step: 27781, loss: 0.08710664510726929\n",
      "step: 27782, loss: 0.061905037611722946\n",
      "step: 27783, loss: 1.192092824453539e-08\n",
      "step: 27784, loss: 1.6760195649112575e-06\n",
      "step: 27785, loss: 4.768367389829109e-08\n",
      "step: 27786, loss: 1.430511264999268e-08\n",
      "step: 27787, loss: 1.43051135381711e-08\n",
      "step: 27788, loss: 9.53674117454284e-09\n",
      "step: 27789, loss: 7.15255676908555e-09\n",
      "step: 27790, loss: 9.53674117454284e-09\n",
      "step: 27791, loss: 2.38418573772492e-09\n",
      "step: 27792, loss: 1.0967224284286203e-07\n",
      "step: 27793, loss: 7.15255676908555e-09\n",
      "step: 27794, loss: 0.0\n",
      "step: 27795, loss: 7.15255676908555e-09\n",
      "step: 27796, loss: 1.1444070224797542e-07\n",
      "step: 27797, loss: 2.145766764272139e-08\n",
      "step: 27798, loss: 1.0430790808868551e-07\n",
      "step: 27799, loss: 2.145766231365087e-08\n",
      "step: 27800, loss: 3.8146950487316644e-08\n",
      "step: 27801, loss: 3.0994396382766354e-08\n",
      "step: 27802, loss: 0.0\n",
      "step: 27803, loss: 2.2411227007523848e-07\n",
      "step: 27804, loss: 0.0\n",
      "step: 27805, loss: 5.2141920605208725e-05\n",
      "step: 27806, loss: 0.04939596727490425\n",
      "step: 27807, loss: 1.430510998545742e-08\n",
      "step: 27808, loss: 5.316668989507889e-07\n",
      "step: 27809, loss: 2.1457660537294032e-08\n",
      "step: 27810, loss: 5.080919072497636e-05\n",
      "step: 27811, loss: 2.38418573772492e-09\n",
      "step: 27812, loss: 1.192092646817855e-08\n",
      "step: 27813, loss: 4.76837103136063e-09\n",
      "step: 27814, loss: 2.38418573772492e-09\n",
      "step: 27815, loss: 4.696792927916249e-07\n",
      "step: 27816, loss: 5.960456306297601e-08\n",
      "step: 27817, loss: 1.668929527909313e-08\n",
      "step: 27818, loss: 0.0\n",
      "step: 27819, loss: 5.9604580826544407e-08\n",
      "step: 27820, loss: 0.0\n",
      "step: 27821, loss: 5.722037599298346e-08\n",
      "step: 27822, loss: 5.2452033827421474e-08\n",
      "step: 27823, loss: 0.0\n",
      "step: 27824, loss: 0.16160058975219727\n",
      "step: 27825, loss: 1.430510998545742e-08\n",
      "step: 27826, loss: 0.08313066512346268\n",
      "step: 27827, loss: 2.38418573772492e-09\n",
      "step: 27828, loss: 1.192092558000013e-08\n",
      "step: 27829, loss: 0.0\n",
      "step: 27830, loss: 0.11571183800697327\n",
      "step: 27831, loss: 4.76837103136063e-09\n",
      "step: 27832, loss: 2.38418573772492e-09\n",
      "step: 27833, loss: 0.0\n",
      "step: 27834, loss: 0.0\n",
      "step: 27835, loss: 1.31129837654953e-07\n",
      "step: 27836, loss: 0.0\n",
      "step: 27837, loss: 1.668929705544997e-08\n",
      "step: 27838, loss: 2.38418573772492e-09\n",
      "step: 27839, loss: 1.430511264999268e-08\n",
      "step: 27840, loss: 2.145766941907823e-08\n",
      "step: 27841, loss: 0.0\n",
      "step: 27842, loss: 9.775145315416012e-08\n",
      "step: 27843, loss: 7.15255676908555e-09\n",
      "step: 27844, loss: 2.38418573772492e-09\n",
      "step: 27845, loss: 2.145766764272139e-08\n",
      "step: 27846, loss: 8.034547249735624e-07\n",
      "step: 27847, loss: 2.6226025795494934e-08\n",
      "step: 27848, loss: 0.0\n",
      "step: 27849, loss: 0.0\n",
      "step: 27850, loss: 2.6226025795494934e-08\n",
      "step: 27851, loss: 4.76837103136063e-09\n",
      "step: 27852, loss: 0.0\n",
      "step: 27853, loss: 2.38418573772492e-09\n",
      "step: 27854, loss: 2.38418573772492e-09\n",
      "step: 27855, loss: 1.4543488191520737e-07\n",
      "step: 27856, loss: 1.430510998545742e-08\n",
      "step: 27857, loss: 0.006029015872627497\n",
      "step: 27858, loss: 0.08910644799470901\n",
      "step: 27859, loss: 1.430511264999268e-08\n",
      "step: 27860, loss: 7.15255632499634e-09\n",
      "step: 27861, loss: 0.0\n",
      "step: 27862, loss: 1.668929527909313e-08\n",
      "step: 27863, loss: 6.437291233396536e-08\n",
      "step: 27864, loss: 0.0\n",
      "step: 27865, loss: 2.384184938364342e-08\n",
      "step: 27866, loss: 4.76837147544984e-09\n",
      "step: 27867, loss: 1.430510998545742e-08\n",
      "step: 27868, loss: 4.053111624102712e-08\n",
      "step: 27869, loss: 2.38418573772492e-09\n",
      "step: 27870, loss: 3.099440704090739e-08\n",
      "step: 27871, loss: 4.76837103136063e-09\n",
      "step: 27872, loss: 1.525873472019157e-07\n",
      "step: 27873, loss: 0.0\n",
      "step: 27874, loss: 0.0\n",
      "step: 27875, loss: 7.15255676908555e-09\n",
      "step: 27876, loss: 0.0745435506105423\n",
      "step: 27877, loss: 0.0\n",
      "step: 27878, loss: 0.0\n",
      "step: 27879, loss: 0.08646123111248016\n",
      "step: 27880, loss: 6.198830533321598e-07\n",
      "step: 27881, loss: 0.0\n",
      "step: 27882, loss: 1.668929883180681e-08\n",
      "step: 27883, loss: 7.15255676908555e-09\n",
      "step: 27884, loss: 0.0\n",
      "step: 27885, loss: 1.9073478796372e-08\n",
      "step: 27886, loss: 0.0\n",
      "step: 27887, loss: 2.38418573772492e-09\n",
      "step: 27888, loss: 0.0\n",
      "step: 27889, loss: 0.0\n",
      "step: 27890, loss: 0.0\n",
      "step: 27891, loss: 4.76837103136063e-09\n",
      "step: 27892, loss: 2.3841844054572903e-08\n",
      "step: 27893, loss: 0.0\n",
      "step: 27894, loss: 0.0\n",
      "step: 27895, loss: 2.417418272671057e-06\n",
      "step: 27896, loss: 4.76837103136063e-09\n",
      "step: 27897, loss: 2.38418573772492e-09\n",
      "step: 27898, loss: 0.0\n",
      "step: 27899, loss: 7.15255676908555e-09\n",
      "step: 27900, loss: 3.5762759864610416e-08\n",
      "step: 27901, loss: 4.76837147544984e-09\n",
      "step: 27902, loss: 2.38418573772492e-09\n",
      "step: 27903, loss: 0.0\n",
      "step: 27904, loss: 2.38418573772492e-09\n",
      "step: 27905, loss: 1.192092824453539e-08\n",
      "step: 27906, loss: 0.0\n",
      "step: 27907, loss: 1.192092824453539e-08\n",
      "step: 27908, loss: 0.0725974440574646\n",
      "step: 27909, loss: 0.0\n",
      "step: 27910, loss: 0.0\n",
      "step: 27911, loss: 2.38418573772492e-09\n",
      "step: 27912, loss: 0.07987645268440247\n",
      "step: 27913, loss: 5.006786452099732e-08\n",
      "step: 27914, loss: 1.1229200254092575e-06\n",
      "step: 27915, loss: 7.15255676908555e-09\n",
      "step: 27916, loss: 2.38418573772492e-09\n",
      "step: 27917, loss: 0.0\n",
      "step: 27918, loss: 0.0\n",
      "step: 27919, loss: 5.0067875179138355e-08\n",
      "step: 27920, loss: 4.76837103136063e-09\n",
      "step: 27921, loss: 9.53674117454284e-09\n",
      "step: 27922, loss: 7.15255676908555e-09\n",
      "step: 27923, loss: 2.38418573772492e-09\n",
      "step: 27924, loss: 3.0994389277338996e-08\n",
      "step: 27925, loss: 0.00017571047646924853\n",
      "step: 27926, loss: 0.0\n",
      "step: 27927, loss: 7.15255632499634e-09\n",
      "step: 27928, loss: 1.8047471712634433e-06\n",
      "step: 27929, loss: 2.38418573772492e-09\n",
      "step: 27930, loss: 1.430511176181426e-08\n",
      "step: 27931, loss: 9.53674117454284e-09\n",
      "step: 27932, loss: 4.76837103136063e-09\n",
      "step: 27933, loss: 0.0\n",
      "step: 27934, loss: 0.03298288956284523\n",
      "step: 27935, loss: 2.38418573772492e-09\n",
      "step: 27936, loss: 0.0\n",
      "step: 27937, loss: 0.04057125002145767\n",
      "step: 27938, loss: 5.9604580826544407e-08\n",
      "step: 27939, loss: 4.410696874401765e-07\n",
      "step: 27940, loss: 5.722037599298346e-08\n",
      "step: 27941, loss: 1.668929350273629e-08\n",
      "step: 27942, loss: 4.0769182874100807e-07\n",
      "step: 27943, loss: 1.1205646188727769e-07\n",
      "step: 27944, loss: 0.0\n",
      "step: 27945, loss: 1.192092558000013e-08\n",
      "step: 27946, loss: 0.0\n",
      "step: 27947, loss: 9.53674117454284e-09\n",
      "step: 27948, loss: 1.7642896921188367e-07\n",
      "step: 27949, loss: 3.0994396382766354e-08\n",
      "step: 27950, loss: 1.4066655751321377e-07\n",
      "step: 27951, loss: 0.1184680387377739\n",
      "step: 27952, loss: 7.15255676908555e-09\n",
      "step: 27953, loss: 0.0\n",
      "step: 27954, loss: 1.1086165159213124e-06\n",
      "step: 27955, loss: 2.6226029348208613e-08\n",
      "step: 27956, loss: 1.0967227126457146e-07\n",
      "step: 27957, loss: 1.5497151650833985e-07\n",
      "step: 27958, loss: 0.0\n",
      "step: 27959, loss: 0.0\n",
      "step: 27960, loss: 3.4570427942526294e-07\n",
      "step: 27961, loss: 0.0\n",
      "step: 27962, loss: 7.15255632499634e-09\n",
      "step: 27963, loss: 1.430511176181426e-08\n",
      "step: 27964, loss: 0.0\n",
      "step: 27965, loss: 2.7179532935406314e-07\n",
      "step: 27966, loss: 2.38418573772492e-09\n",
      "step: 27967, loss: 2.38418573772492e-09\n",
      "step: 27968, loss: 0.0\n",
      "step: 27969, loss: 0.0\n",
      "step: 27970, loss: 2.14575635482106e-07\n",
      "step: 27971, loss: 7.15255676908555e-09\n",
      "step: 27972, loss: 0.0\n",
      "step: 27973, loss: 1.430511264999268e-08\n",
      "step: 27974, loss: 5.722037599298346e-08\n",
      "step: 27975, loss: 2.9325269679247867e-07\n",
      "step: 27976, loss: 4.0531130451881836e-08\n",
      "step: 27977, loss: 2.38418573772492e-09\n",
      "step: 27978, loss: 4.768367389829109e-08\n",
      "step: 27979, loss: 6.008057766848651e-07\n",
      "step: 27980, loss: 1.430510998545742e-08\n",
      "step: 27981, loss: 4.76837147544984e-09\n",
      "step: 27982, loss: 7.152551262379347e-08\n",
      "step: 27983, loss: 8.106221827119953e-08\n",
      "step: 27984, loss: 2.38418573772492e-09\n",
      "step: 27985, loss: 0.0\n",
      "step: 27986, loss: 4.76837103136063e-09\n",
      "step: 27987, loss: 0.0\n",
      "step: 27988, loss: 9.059893102403294e-08\n",
      "step: 27989, loss: 2.956368518880481e-07\n",
      "step: 27990, loss: 7.15255676908555e-09\n",
      "step: 27991, loss: 4.76837147544984e-09\n",
      "step: 27992, loss: 2.38418573772492e-09\n",
      "step: 27993, loss: 0.0\n",
      "step: 27994, loss: 0.0\n",
      "step: 27995, loss: 0.0\n",
      "step: 27996, loss: 7.390962508679877e-08\n",
      "step: 27997, loss: 9.53674117454284e-09\n",
      "step: 27998, loss: 1.430511176181426e-08\n",
      "step: 27999, loss: 0.0\n",
      "step: 28000, loss: 0.0\n",
      "step: 28001, loss: 1.430510998545742e-08\n",
      "step: 28002, loss: 2.1457660537294032e-08\n",
      "step: 28003, loss: 9.53674206272126e-09\n",
      "step: 28004, loss: 0.0\n",
      "step: 28005, loss: 0.0\n",
      "step: 28006, loss: 2.622603467727913e-08\n",
      "step: 28007, loss: 2.384185116000026e-08\n",
      "step: 28008, loss: 2.3841844054572903e-08\n",
      "step: 28009, loss: 3.576278118089249e-08\n",
      "step: 28010, loss: 1.0490390423001372e-07\n",
      "step: 28011, loss: 0.0\n",
      "step: 28012, loss: 5.245204093284883e-08\n",
      "step: 28013, loss: 2.6226025795494934e-08\n",
      "step: 28014, loss: 2.8371613325361977e-07\n",
      "step: 28015, loss: 0.14013631641864777\n",
      "step: 28016, loss: 0.0\n",
      "step: 28017, loss: 1.544908059258887e-06\n",
      "step: 28018, loss: 9.53674206272126e-09\n",
      "step: 28019, loss: 2.38418573772492e-09\n",
      "step: 28020, loss: 0.0\n",
      "step: 28021, loss: 0.0\n",
      "step: 28022, loss: 2.3841844054572903e-08\n",
      "step: 28023, loss: 0.0\n",
      "step: 28024, loss: 0.0007613292546011508\n",
      "step: 28025, loss: 0.0\n",
      "step: 28026, loss: 3.0278999929578276e-07\n",
      "step: 28027, loss: 2.38418573772492e-09\n",
      "step: 28028, loss: 0.07560459524393082\n",
      "step: 28029, loss: 9.870287840385572e-07\n",
      "step: 28030, loss: 2.38418573772492e-09\n",
      "step: 28031, loss: 1.1038481488867546e-06\n",
      "step: 28032, loss: 0.0\n",
      "step: 28033, loss: 7.15255676908555e-09\n",
      "step: 28034, loss: 1.001356508822937e-07\n",
      "step: 28035, loss: 0.0\n",
      "step: 28036, loss: 6.198875013296856e-08\n",
      "step: 28037, loss: 2.38418573772492e-09\n",
      "step: 28038, loss: 8.821476171760878e-08\n",
      "step: 28039, loss: 1.192092558000013e-08\n",
      "step: 28040, loss: 1.907347702001516e-08\n",
      "step: 28041, loss: 9.53674117454284e-09\n",
      "step: 28042, loss: 1.907347702001516e-08\n",
      "step: 28043, loss: 3.598550028982572e-05\n",
      "step: 28044, loss: 2.38418573772492e-09\n",
      "step: 28045, loss: 4.76837103136063e-09\n",
      "step: 28046, loss: 2.38418573772492e-09\n",
      "step: 28047, loss: 0.0\n",
      "step: 28048, loss: 0.06853672862052917\n",
      "step: 28049, loss: 7.15255632499634e-09\n",
      "step: 28050, loss: 2.38418573772492e-09\n",
      "step: 28051, loss: 2.38418573772492e-09\n",
      "step: 28052, loss: 2.145766231365087e-08\n",
      "step: 28053, loss: 1.192092824453539e-08\n",
      "step: 28054, loss: 0.0\n",
      "step: 28055, loss: 1.192092558000013e-08\n",
      "step: 28056, loss: 1.192092646817855e-08\n",
      "step: 28057, loss: 0.0\n",
      "step: 28058, loss: 0.0\n",
      "step: 28059, loss: 2.38418573772492e-09\n",
      "step: 28060, loss: 4.7444868300772214e-07\n",
      "step: 28061, loss: 4.76837103136063e-09\n",
      "step: 28062, loss: 0.0\n",
      "step: 28063, loss: 2.38418573772492e-09\n",
      "step: 28064, loss: 2.6226029348208613e-08\n",
      "step: 28065, loss: 2.38418573772492e-09\n",
      "step: 28066, loss: 4.76837147544984e-09\n",
      "step: 28067, loss: 0.0018457415280863643\n",
      "step: 28068, loss: 9.53674117454284e-09\n",
      "step: 28069, loss: 0.0\n",
      "step: 28070, loss: 4.76837147544984e-09\n",
      "step: 28071, loss: 0.0\n",
      "step: 28072, loss: 0.0\n",
      "step: 28073, loss: 0.0\n",
      "step: 28074, loss: 1.8596561801587086e-07\n",
      "step: 28075, loss: 4.386853902360599e-07\n",
      "step: 28076, loss: 2.38418573772492e-09\n",
      "step: 28077, loss: 7.15255632499634e-09\n",
      "step: 28078, loss: 7.15255676908555e-09\n",
      "step: 28079, loss: 1.192092558000013e-08\n",
      "step: 28080, loss: 2.38418573772492e-09\n",
      "step: 28081, loss: 0.0\n",
      "step: 28082, loss: 0.0\n",
      "step: 28083, loss: 8.106219695491745e-08\n",
      "step: 28084, loss: 4.76837103136063e-09\n",
      "step: 28085, loss: 2.38418573772492e-09\n",
      "step: 28086, loss: 4.76837103136063e-09\n",
      "step: 28087, loss: 0.0\n",
      "step: 28088, loss: 2.384184938364342e-08\n",
      "step: 28089, loss: 0.06781528145074844\n",
      "step: 28090, loss: 8.320699862451875e-07\n",
      "step: 28091, loss: 4.76837103136063e-09\n",
      "step: 28092, loss: 1.192092558000013e-08\n",
      "step: 28093, loss: 1.192092824453539e-08\n",
      "step: 28094, loss: 0.1581244319677353\n",
      "step: 28095, loss: 2.38418573772492e-09\n",
      "step: 28096, loss: 2.1457557863868715e-07\n",
      "step: 28097, loss: 0.0\n",
      "step: 28098, loss: 1.430511176181426e-08\n",
      "step: 28099, loss: 0.0\n",
      "step: 28100, loss: 1.192092558000013e-08\n",
      "step: 28101, loss: 0.0\n",
      "step: 28102, loss: 9.53674206272126e-09\n",
      "step: 28103, loss: 2.264965814902098e-07\n",
      "step: 28104, loss: 0.0\n",
      "step: 28105, loss: 1.192092558000013e-08\n",
      "step: 28106, loss: 2.38418573772492e-09\n",
      "step: 28107, loss: 2.38418573772492e-09\n",
      "step: 28108, loss: 9.53674206272126e-09\n",
      "step: 28109, loss: 0.0\n",
      "step: 28110, loss: 7.6293815709505e-08\n",
      "step: 28111, loss: 1.0967232810799032e-07\n",
      "step: 28112, loss: 1.907347702001516e-08\n",
      "step: 28113, loss: 4.76837103136063e-09\n",
      "step: 28114, loss: 2.5914421257766662e-06\n",
      "step: 28115, loss: 0.0\n",
      "step: 28116, loss: 3.337857279461787e-08\n",
      "step: 28117, loss: 2.622603467727913e-08\n",
      "step: 28118, loss: 1.192092646817855e-08\n",
      "step: 28119, loss: 0.0\n",
      "step: 28120, loss: 0.0\n",
      "step: 28121, loss: 0.0\n",
      "step: 28122, loss: 5.9604580826544407e-08\n",
      "step: 28123, loss: 1.1444061698284713e-07\n",
      "step: 28124, loss: 0.0\n",
      "step: 28125, loss: 4.76837103136063e-09\n",
      "step: 28126, loss: 4.76837147544984e-09\n",
      "step: 28127, loss: 0.0\n",
      "step: 28128, loss: 3.576277052275145e-08\n",
      "step: 28129, loss: 0.09871700406074524\n",
      "step: 28130, loss: 0.0\n",
      "step: 28131, loss: 0.15749575197696686\n",
      "step: 28132, loss: 2.3841844054572903e-08\n",
      "step: 28133, loss: 2.527220885895076e-07\n",
      "step: 28134, loss: 1.192092558000013e-08\n",
      "step: 28135, loss: 7.15255632499634e-09\n",
      "step: 28136, loss: 2.145766764272139e-08\n",
      "step: 28137, loss: 2.0980725423669355e-07\n",
      "step: 28138, loss: 2.0980725423669355e-07\n",
      "step: 28139, loss: 9.53674117454284e-09\n",
      "step: 28140, loss: 0.0\n",
      "step: 28141, loss: 0.0\n",
      "step: 28142, loss: 7.15255676908555e-09\n",
      "step: 28143, loss: 1.430511264999268e-08\n",
      "step: 28144, loss: 2.38418573772492e-09\n",
      "step: 28145, loss: 0.0\n",
      "step: 28146, loss: 0.0\n",
      "step: 28147, loss: 1.6450856321625906e-07\n",
      "step: 28148, loss: 0.0\n",
      "step: 28149, loss: 1.6450815110147232e-07\n",
      "step: 28150, loss: 7.15255632499634e-09\n",
      "step: 28151, loss: 2.622603467727913e-08\n",
      "step: 28152, loss: 0.0\n",
      "step: 28153, loss: 0.0\n",
      "step: 28154, loss: 2.38418573772492e-09\n",
      "step: 28155, loss: 0.0\n",
      "step: 28156, loss: 7.15255632499634e-09\n",
      "step: 28157, loss: 0.0\n",
      "step: 28158, loss: 0.0\n",
      "step: 28159, loss: 1.0490391133544108e-07\n",
      "step: 28160, loss: 7.15255676908555e-09\n",
      "step: 28161, loss: 1.192092824453539e-08\n",
      "step: 28162, loss: 2.38418573772492e-09\n",
      "step: 28163, loss: 0.0\n",
      "step: 28164, loss: 4.291529975830599e-08\n",
      "step: 28165, loss: 4.458035618881695e-06\n",
      "step: 28166, loss: 7.86779779105018e-08\n",
      "step: 28167, loss: 0.0\n",
      "step: 28168, loss: 9.53674117454284e-09\n",
      "step: 28169, loss: 5.960460214282648e-08\n",
      "step: 28170, loss: 0.0\n",
      "step: 28171, loss: 0.0\n",
      "step: 28172, loss: 0.0\n",
      "step: 28173, loss: 5.960456306297601e-08\n",
      "step: 28174, loss: 1.668929883180681e-08\n",
      "step: 28175, loss: 0.0\n",
      "step: 28176, loss: 2.38418573772492e-09\n",
      "step: 28177, loss: 1.192092558000013e-08\n",
      "step: 28178, loss: 4.76837103136063e-09\n",
      "step: 28179, loss: 0.0\n",
      "step: 28180, loss: 4.0769162978904205e-07\n",
      "step: 28181, loss: 9.53674206272126e-09\n",
      "step: 28182, loss: 0.0\n",
      "step: 28183, loss: 2.38418573772492e-09\n",
      "step: 28184, loss: 7.15255632499634e-09\n",
      "step: 28185, loss: 1.668929705544997e-08\n",
      "step: 28186, loss: 9.53674206272126e-09\n",
      "step: 28187, loss: 4.76837103136063e-09\n",
      "step: 28188, loss: 2.4318546820722986e-07\n",
      "step: 28189, loss: 1.907347702001516e-08\n",
      "step: 28190, loss: 2.38418573772492e-09\n",
      "step: 28191, loss: 0.0\n",
      "step: 28192, loss: 9.53674206272126e-09\n",
      "step: 28193, loss: 0.0\n",
      "step: 28194, loss: 4.76837103136063e-09\n",
      "step: 28195, loss: 0.0\n",
      "step: 28196, loss: 4.76837147544984e-09\n",
      "step: 28197, loss: 2.38418573772492e-09\n",
      "step: 28198, loss: 9.53674117454284e-09\n",
      "step: 28199, loss: 2.38418573772492e-09\n",
      "step: 28200, loss: 2.503379334939382e-07\n",
      "step: 28201, loss: 1.907348412544252e-08\n",
      "step: 28202, loss: 7.390962508679877e-08\n",
      "step: 28203, loss: 1.430511176181426e-08\n",
      "step: 28204, loss: 0.0\n",
      "step: 28205, loss: 4.0531130451881836e-08\n",
      "step: 28206, loss: 2.38418573772492e-09\n",
      "step: 28207, loss: 0.14607186615467072\n",
      "step: 28208, loss: 2.38418529363571e-08\n",
      "step: 28209, loss: 0.033162184059619904\n",
      "step: 28210, loss: 7.15255676908555e-09\n",
      "step: 28211, loss: 1.8119739308986027e-07\n",
      "step: 28212, loss: 7.15255632499634e-09\n",
      "step: 28213, loss: 2.8610216418201162e-08\n",
      "step: 28214, loss: 0.0\n",
      "step: 28215, loss: 0.0\n",
      "step: 28216, loss: 3.0994389277338996e-08\n",
      "step: 28217, loss: 6.675715980009045e-08\n",
      "step: 28218, loss: 1.478190227999221e-07\n",
      "step: 28219, loss: 0.0\n",
      "step: 28220, loss: 7.15255676908555e-09\n",
      "step: 28221, loss: 9.53674206272126e-09\n",
      "step: 28222, loss: 2.8610209312773804e-08\n",
      "step: 28223, loss: 0.0\n",
      "step: 28224, loss: 2.1457660537294032e-08\n",
      "step: 28225, loss: 1.907347702001516e-08\n",
      "step: 28226, loss: 5.96045985901128e-08\n",
      "step: 28227, loss: 4.76837103136063e-09\n",
      "step: 28228, loss: 0.0\n",
      "step: 28229, loss: 4.2915313969160707e-08\n",
      "step: 28230, loss: 2.38418573772492e-09\n",
      "step: 28231, loss: 0.0\n",
      "step: 28232, loss: 0.22058330476284027\n",
      "step: 28233, loss: 1.192092824453539e-08\n",
      "step: 28234, loss: 0.1268930286169052\n",
      "step: 28235, loss: 2.38418573772492e-09\n",
      "step: 28236, loss: 0.0\n",
      "step: 28237, loss: 2.38418573772492e-09\n",
      "step: 28238, loss: 1.907347702001516e-08\n",
      "step: 28239, loss: 0.006753657944500446\n",
      "step: 28240, loss: 1.430510998545742e-08\n",
      "step: 28241, loss: 4.76837103136063e-09\n",
      "step: 28242, loss: 2.145766586636455e-08\n",
      "step: 28243, loss: 4.76837103136063e-09\n",
      "step: 28244, loss: 1.8480821381672285e-05\n",
      "step: 28245, loss: 0.0\n",
      "step: 28246, loss: 0.0\n",
      "step: 28247, loss: 7.152544867494726e-08\n",
      "step: 28248, loss: 0.0\n",
      "step: 28249, loss: 2.38418573772492e-09\n",
      "step: 28250, loss: 1.3038510715546181e-08\n",
      "step: 28251, loss: 2.38418573772492e-09\n",
      "step: 28252, loss: 4.76837103136063e-09\n",
      "step: 28253, loss: 0.0\n",
      "step: 28254, loss: 1.1444060277199242e-07\n",
      "step: 28255, loss: 0.0\n",
      "step: 28256, loss: 7.390962508679877e-08\n",
      "step: 28257, loss: 0.0003046254569198936\n",
      "step: 28258, loss: 2.3841845830929742e-08\n",
      "step: 28259, loss: 0.0\n",
      "step: 28260, loss: 0.0\n",
      "step: 28261, loss: 2.38418573772492e-09\n",
      "step: 28262, loss: 2.38418573772492e-09\n",
      "step: 28263, loss: 0.0\n",
      "step: 28264, loss: 1.668929527909313e-08\n",
      "step: 28265, loss: 1.192092558000013e-08\n",
      "step: 28266, loss: 8.821480435017293e-08\n",
      "step: 28267, loss: 4.76837103136063e-09\n",
      "step: 28268, loss: 2.38418573772492e-09\n",
      "step: 28269, loss: 0.0\n",
      "step: 28270, loss: 2.2649652464679093e-07\n",
      "step: 28271, loss: 1.0728814459071145e-07\n",
      "step: 28272, loss: 0.005974658764898777\n",
      "step: 28273, loss: 7.15255632499634e-09\n",
      "step: 28274, loss: 0.0\n",
      "step: 28275, loss: 0.0\n",
      "step: 28276, loss: 2.38418573772492e-09\n",
      "step: 28277, loss: 0.0\n",
      "step: 28278, loss: 1.811973362464414e-07\n",
      "step: 28279, loss: 0.0\n",
      "step: 28280, loss: 0.0\n",
      "step: 28281, loss: 0.0\n",
      "step: 28282, loss: 0.0\n",
      "step: 28283, loss: 0.0005797335179522634\n",
      "step: 28284, loss: 2.38418573772492e-09\n",
      "step: 28285, loss: 1.430511176181426e-08\n",
      "step: 28286, loss: 0.0\n",
      "step: 28287, loss: 4.8547532060183585e-05\n",
      "step: 28288, loss: 1.239773865790994e-07\n",
      "step: 28289, loss: 4.768368100371845e-08\n",
      "step: 28290, loss: 0.0\n",
      "step: 28291, loss: 2.38418573772492e-09\n",
      "step: 28292, loss: 0.0\n",
      "step: 28293, loss: 2.1219146617568185e-07\n",
      "step: 28294, loss: 1.192092646817855e-08\n",
      "step: 28295, loss: 0.06103542447090149\n",
      "step: 28296, loss: 2.38418573772492e-09\n",
      "step: 28297, loss: 0.0\n",
      "step: 28298, loss: 4.76837147544984e-09\n",
      "step: 28299, loss: 2.38418573772492e-09\n",
      "step: 28300, loss: 2.38418573772492e-09\n",
      "step: 28301, loss: 0.0\n",
      "step: 28302, loss: 2.622603467727913e-08\n",
      "step: 28303, loss: 3.337858700547258e-08\n",
      "step: 28304, loss: 0.0\n",
      "step: 28305, loss: 1.668929350273629e-08\n",
      "step: 28306, loss: 0.0\n",
      "step: 28307, loss: 7.15255676908555e-09\n",
      "step: 28308, loss: 9.53674117454284e-09\n",
      "step: 28309, loss: 0.0\n",
      "step: 28310, loss: 2.38418573772492e-09\n",
      "step: 28311, loss: 2.38418573772492e-09\n",
      "step: 28312, loss: 9.53674206272126e-09\n",
      "step: 28313, loss: 4.76837103136063e-09\n",
      "step: 28314, loss: 1.192092646817855e-08\n",
      "step: 28315, loss: 5.245142347121146e-07\n",
      "step: 28316, loss: 0.0\n",
      "step: 28317, loss: 2.1457660537294032e-08\n",
      "step: 28318, loss: 0.00021347640722524375\n",
      "step: 28319, loss: 2.38418573772492e-09\n",
      "step: 28320, loss: 2.6729434466687962e-05\n",
      "step: 28321, loss: 1.192092558000013e-08\n",
      "step: 28322, loss: 0.0\n",
      "step: 28323, loss: 0.0\n",
      "step: 28324, loss: 1.668929527909313e-08\n",
      "step: 28325, loss: 0.0\n",
      "step: 28326, loss: 0.0\n",
      "step: 28327, loss: 1.2397727289226168e-07\n",
      "step: 28328, loss: 1.192092646817855e-08\n",
      "step: 28329, loss: 0.0\n",
      "step: 28330, loss: 0.0\n",
      "step: 28331, loss: 0.0\n",
      "step: 28332, loss: 1.192092824453539e-08\n",
      "step: 28333, loss: 0.0\n",
      "step: 28334, loss: 3.33785834527589e-08\n",
      "step: 28335, loss: 2.38418573772492e-09\n",
      "step: 28336, loss: 1.1205641214928619e-07\n",
      "step: 28337, loss: 0.0\n",
      "step: 28338, loss: 9.53674206272126e-09\n",
      "step: 28339, loss: 4.76837147544984e-09\n",
      "step: 28340, loss: 2.38418573772492e-09\n",
      "step: 28341, loss: 4.76837103136063e-09\n",
      "step: 28342, loss: 0.0\n",
      "step: 28343, loss: 5.006783965200157e-08\n",
      "step: 28344, loss: 2.145766586636455e-08\n",
      "step: 28345, loss: 0.0\n",
      "step: 28346, loss: 9.53674117454284e-09\n",
      "step: 28347, loss: 1.907348234908568e-08\n",
      "step: 28348, loss: 2.622603290092229e-08\n",
      "step: 28349, loss: 2.38418573772492e-09\n",
      "step: 28350, loss: 0.0\n",
      "step: 28351, loss: 2.3841844054572903e-08\n",
      "step: 28352, loss: 0.0\n",
      "step: 28353, loss: 0.0\n",
      "step: 28354, loss: 2.0980725423669355e-07\n",
      "step: 28355, loss: 2.38418573772492e-09\n",
      "step: 28356, loss: 0.13542868196964264\n",
      "step: 28357, loss: 0.0\n",
      "step: 28358, loss: 2.38418573772492e-09\n",
      "step: 28359, loss: 2.38418573772492e-09\n",
      "step: 28360, loss: 0.13488107919692993\n",
      "step: 28361, loss: 2.145766231365087e-08\n",
      "step: 28362, loss: 0.0\n",
      "step: 28363, loss: 0.07223079353570938\n",
      "step: 28364, loss: 1.430511176181426e-08\n",
      "step: 28365, loss: 0.0\n",
      "step: 28366, loss: 4.76837147544984e-09\n",
      "step: 28367, loss: 0.0\n",
      "step: 28368, loss: 0.0\n",
      "step: 28369, loss: 2.3841844054572903e-08\n",
      "step: 28370, loss: 7.15255632499634e-09\n",
      "step: 28371, loss: 0.0\n",
      "step: 28372, loss: 1.192092646817855e-08\n",
      "step: 28373, loss: 0.07694810628890991\n",
      "step: 28374, loss: 0.07917093485593796\n",
      "step: 28375, loss: 4.76837103136063e-09\n",
      "step: 28376, loss: 2.1457660537294032e-08\n",
      "step: 28377, loss: 1.9788645033713692e-07\n",
      "step: 28378, loss: 0.0\n",
      "step: 28379, loss: 7.15255676908555e-09\n",
      "step: 28380, loss: 0.0\n",
      "step: 28381, loss: 2.38418573772492e-09\n",
      "step: 28382, loss: 4.76837103136063e-09\n",
      "step: 28383, loss: 0.0\n",
      "step: 28384, loss: 1.668929527909313e-08\n",
      "step: 28385, loss: 0.0\n",
      "step: 28386, loss: 0.04251066595315933\n",
      "step: 28387, loss: 1.907347702001516e-08\n",
      "step: 28388, loss: 2.38418573772492e-09\n",
      "step: 28389, loss: 0.0\n",
      "step: 28390, loss: 9.53674117454284e-09\n",
      "step: 28391, loss: 0.0\n",
      "step: 28392, loss: 4.76837103136063e-09\n",
      "step: 28393, loss: 3.3378576347331546e-08\n",
      "step: 28394, loss: 0.0\n",
      "step: 28395, loss: 0.0\n",
      "step: 28396, loss: 0.0605354942381382\n",
      "step: 28397, loss: 1.621242091687236e-07\n",
      "step: 28398, loss: 4.76837103136063e-09\n",
      "step: 28399, loss: 5.483622800284138e-08\n",
      "step: 28400, loss: 3.1947863021741796e-07\n",
      "step: 28401, loss: 7.867804185934801e-08\n",
      "step: 28402, loss: 0.0\n",
      "step: 28403, loss: 9.53674117454284e-09\n",
      "step: 28404, loss: 0.0\n",
      "step: 28405, loss: 9.53674206272126e-09\n",
      "step: 28406, loss: 2.622603645363597e-08\n",
      "step: 28407, loss: 5.483623510826874e-08\n",
      "step: 28408, loss: 2.145766764272139e-08\n",
      "step: 28409, loss: 2.38418573772492e-09\n",
      "step: 28410, loss: 2.38418573772492e-09\n",
      "step: 28411, loss: 9.77513892053139e-08\n",
      "step: 28412, loss: 6.985542881920992e-07\n",
      "step: 28413, loss: 5.006785741556996e-08\n",
      "step: 28414, loss: 1.1682478628927129e-07\n",
      "step: 28415, loss: 0.0\n",
      "step: 28416, loss: 0.0\n",
      "step: 28417, loss: 2.3841845830929742e-08\n",
      "step: 28418, loss: 0.0\n",
      "step: 28419, loss: 5.468039307743311e-05\n",
      "step: 28420, loss: 2.38418573772492e-09\n",
      "step: 28421, loss: 4.4102580432081595e-06\n",
      "step: 28422, loss: 0.0\n",
      "step: 28423, loss: 0.1521894484758377\n",
      "step: 28424, loss: 0.0\n",
      "step: 28425, loss: 3.313993488518463e-07\n",
      "step: 28426, loss: 1.668929527909313e-08\n",
      "step: 28427, loss: 7.15255632499634e-09\n",
      "step: 28428, loss: 0.0\n",
      "step: 28429, loss: 2.38418573772492e-09\n",
      "step: 28430, loss: 2.38418573772492e-09\n",
      "step: 28431, loss: 4.76837147544984e-09\n",
      "step: 28432, loss: 0.0\n",
      "step: 28433, loss: 0.08246898651123047\n",
      "step: 28434, loss: 4.76837103136063e-09\n",
      "step: 28435, loss: 2.5033799033735704e-07\n",
      "step: 28436, loss: 1.430510998545742e-08\n",
      "step: 28437, loss: 1.2636157009637827e-07\n",
      "step: 28438, loss: 0.0\n",
      "step: 28439, loss: 1.5735570002561872e-07\n",
      "step: 28440, loss: 3.814693982917561e-08\n",
      "step: 28441, loss: 2.38418573772492e-09\n",
      "step: 28442, loss: 2.38418573772492e-09\n",
      "step: 28443, loss: 3.8146946934602965e-08\n",
      "step: 28444, loss: 2.622604000634965e-08\n",
      "step: 28445, loss: 0.007523532025516033\n",
      "step: 28446, loss: 0.0\n",
      "step: 28447, loss: 4.76837103136063e-09\n",
      "step: 28448, loss: 7.15255632499634e-09\n",
      "step: 28449, loss: 2.38418573772492e-09\n",
      "step: 28450, loss: 1.907347702001516e-08\n",
      "step: 28451, loss: 2.38418573772492e-09\n",
      "step: 28452, loss: 0.0\n",
      "step: 28453, loss: 3.576275631189674e-08\n",
      "step: 28454, loss: 2.38418573772492e-09\n",
      "step: 28455, loss: 4.76837103136063e-09\n",
      "step: 28456, loss: 0.0\n",
      "step: 28457, loss: 1.7166101429211267e-07\n",
      "step: 28458, loss: 0.006469539366662502\n",
      "step: 28459, loss: 7.629387965835122e-08\n",
      "step: 28460, loss: 0.17919637262821198\n",
      "step: 28461, loss: 4.76837103136063e-09\n",
      "step: 28462, loss: 1.2397727289226168e-07\n",
      "step: 28463, loss: 2.38418573772492e-09\n",
      "step: 28464, loss: 0.0\n",
      "step: 28465, loss: 2.38418573772492e-09\n",
      "step: 28466, loss: 4.76837147544984e-09\n",
      "step: 28467, loss: 7.15255676908555e-09\n",
      "step: 28468, loss: 1.192092558000013e-08\n",
      "step: 28469, loss: 7.15255676908555e-09\n",
      "step: 28470, loss: 4.76837147544984e-09\n",
      "step: 28471, loss: 2.38418573772492e-09\n",
      "step: 28472, loss: 4.053114111002287e-08\n",
      "step: 28473, loss: 9.53674117454284e-09\n",
      "step: 28474, loss: 2.38418573772492e-09\n",
      "step: 28475, loss: 2.38418573772492e-09\n",
      "step: 28476, loss: 2.9988305527695047e-07\n",
      "step: 28477, loss: 1.430510998545742e-08\n",
      "step: 28478, loss: 2.38418573772492e-09\n",
      "step: 28479, loss: 0.0\n",
      "step: 28480, loss: 0.0\n",
      "step: 28481, loss: 0.0\n",
      "step: 28482, loss: 3.69517306353373e-06\n",
      "step: 28483, loss: 0.0\n",
      "step: 28484, loss: 2.38418573772492e-09\n",
      "step: 28485, loss: 3.337859055818626e-08\n",
      "step: 28486, loss: 3.0994396382766354e-08\n",
      "step: 28487, loss: 1.3589820468951075e-07\n",
      "step: 28488, loss: 0.0\n",
      "step: 28489, loss: 7.15255632499634e-09\n",
      "step: 28490, loss: 1.907348234908568e-08\n",
      "step: 28491, loss: 4.76837103136063e-09\n",
      "step: 28492, loss: 0.0\n",
      "step: 28493, loss: 0.0\n",
      "step: 28494, loss: 2.38418573772492e-09\n",
      "step: 28495, loss: 7.15255676908555e-09\n",
      "step: 28496, loss: 0.0\n",
      "step: 28497, loss: 1.907347702001516e-08\n",
      "step: 28498, loss: 2.6226029348208613e-08\n",
      "step: 28499, loss: 4.76837147544984e-09\n",
      "step: 28500, loss: 1.645082221557459e-07\n",
      "step: 28501, loss: 2.38418573772492e-09\n",
      "step: 28502, loss: 3.337857279461787e-08\n",
      "step: 28503, loss: 2.38418573772492e-09\n",
      "step: 28504, loss: 0.0\n",
      "step: 28505, loss: 4.76837147544984e-09\n",
      "step: 28506, loss: 1.0013562956601163e-07\n",
      "step: 28507, loss: 9.53674117454284e-09\n",
      "step: 28508, loss: 1.192092824453539e-08\n",
      "step: 28509, loss: 3.6000881209474755e-07\n",
      "step: 28510, loss: 2.3841844054572903e-08\n",
      "step: 28511, loss: 5.245204093284883e-08\n",
      "step: 28512, loss: 0.0\n",
      "step: 28513, loss: 0.0\n",
      "step: 28514, loss: 0.0\n",
      "step: 28515, loss: 9.53674117454284e-09\n",
      "step: 28516, loss: 0.0002503468422219157\n",
      "step: 28517, loss: 2.38418573772492e-09\n",
      "step: 28518, loss: 4.76837147544984e-09\n",
      "step: 28519, loss: 2.38418573772492e-09\n",
      "step: 28520, loss: 0.0\n",
      "step: 28521, loss: 7.057115567477013e-07\n",
      "step: 28522, loss: 9.53674206272126e-09\n",
      "step: 28523, loss: 1.1205643346556826e-07\n",
      "step: 28524, loss: 0.0\n",
      "step: 28525, loss: 2.145766764272139e-08\n",
      "step: 28526, loss: 2.38418529363571e-08\n",
      "step: 28527, loss: 5.006785741556996e-08\n",
      "step: 28528, loss: 0.0\n",
      "step: 28529, loss: 1.668929350273629e-08\n",
      "step: 28530, loss: 2.622603645363597e-08\n",
      "step: 28531, loss: 9.53674117454284e-09\n",
      "step: 28532, loss: 4.982886707693979e-07\n",
      "step: 28533, loss: 1.192092646817855e-08\n",
      "step: 28534, loss: 1.5735565739305457e-07\n",
      "step: 28535, loss: 0.09040847420692444\n",
      "step: 28536, loss: 4.76837103136063e-09\n",
      "step: 28537, loss: 0.0\n",
      "step: 28538, loss: 2.38418573772492e-09\n",
      "step: 28539, loss: 2.38418573772492e-09\n",
      "step: 28540, loss: 0.06100809946656227\n",
      "step: 28541, loss: 7.15255632499634e-09\n",
      "step: 28542, loss: 2.38418573772492e-09\n",
      "step: 28543, loss: 0.001489457325078547\n",
      "step: 28544, loss: 0.040527138859033585\n",
      "step: 28545, loss: 4.76837103136063e-09\n",
      "step: 28546, loss: 0.16963788866996765\n",
      "step: 28547, loss: 0.0\n",
      "step: 28548, loss: 1.192092558000013e-08\n",
      "step: 28549, loss: 0.0\n",
      "step: 28550, loss: 7.15255632499634e-09\n",
      "step: 28551, loss: 0.0\n",
      "step: 28552, loss: 1.2636147062039527e-07\n",
      "step: 28553, loss: 2.38418573772492e-09\n",
      "step: 28554, loss: 0.0\n",
      "step: 28555, loss: 0.0\n",
      "step: 28556, loss: 0.0\n",
      "step: 28557, loss: 3.814693982917561e-08\n",
      "step: 28558, loss: 0.0741935670375824\n",
      "step: 28559, loss: 0.0\n",
      "step: 28560, loss: 0.0\n",
      "step: 28561, loss: 0.0\n",
      "step: 28562, loss: 4.291529975830599e-08\n",
      "step: 28563, loss: 0.0\n",
      "step: 28564, loss: 4.76837103136063e-09\n",
      "step: 28565, loss: 3.194785733739991e-07\n",
      "step: 28566, loss: 4.7683659687436375e-08\n",
      "step: 28567, loss: 0.0\n",
      "step: 28568, loss: 7.15255676908555e-09\n",
      "step: 28569, loss: 0.0\n",
      "step: 28570, loss: 8.10621898494901e-08\n",
      "step: 28571, loss: 0.08869398385286331\n",
      "step: 28572, loss: 0.06842175126075745\n",
      "step: 28573, loss: 0.0\n",
      "step: 28574, loss: 4.76837103136063e-09\n",
      "step: 28575, loss: 2.38418573772492e-09\n",
      "step: 28576, loss: 2.6226025795494934e-08\n",
      "step: 28577, loss: 3.337859055818626e-08\n",
      "step: 28578, loss: 0.0\n",
      "step: 28579, loss: 2.074230849302694e-07\n",
      "step: 28580, loss: 0.0014440498780459166\n",
      "step: 28581, loss: 0.0\n",
      "step: 28582, loss: 0.0\n",
      "step: 28583, loss: 3.2186395060307404e-07\n",
      "step: 28584, loss: 0.0\n",
      "step: 28585, loss: 0.0\n",
      "step: 28586, loss: 7.15255676908555e-09\n",
      "step: 28587, loss: 1.907347702001516e-08\n",
      "step: 28588, loss: 0.0\n",
      "step: 28589, loss: 1.0108694823429687e-06\n",
      "step: 28590, loss: 2.38418573772492e-09\n",
      "step: 28591, loss: 9.53674117454284e-09\n",
      "step: 28592, loss: 0.0\n",
      "step: 28593, loss: 1.430511264999268e-08\n",
      "step: 28594, loss: 0.0\n",
      "step: 28595, loss: 4.76837103136063e-09\n",
      "step: 28596, loss: 7.15255676908555e-09\n",
      "step: 28597, loss: 1.907348234908568e-08\n",
      "step: 28598, loss: 2.38418573772492e-09\n",
      "step: 28599, loss: 6.914127226309574e-08\n",
      "step: 28600, loss: 1.192092558000013e-08\n",
      "step: 28601, loss: 9.53674117454284e-09\n",
      "step: 28602, loss: 2.145766586636455e-08\n",
      "step: 28603, loss: 0.0\n",
      "step: 28604, loss: 0.0\n",
      "step: 28605, loss: 1.9073397083957389e-07\n",
      "step: 28606, loss: 2.38418573772492e-09\n",
      "step: 28607, loss: 0.04763408005237579\n",
      "step: 28608, loss: 0.0\n",
      "step: 28609, loss: 1.192092646817855e-08\n",
      "step: 28610, loss: 0.0\n",
      "step: 28611, loss: 0.0\n",
      "step: 28612, loss: 0.0\n",
      "step: 28613, loss: 2.145766764272139e-08\n",
      "step: 28614, loss: 0.0\n",
      "step: 28615, loss: 7.15255676908555e-09\n",
      "step: 28616, loss: 2.38418573772492e-09\n",
      "step: 28617, loss: 4.76837103136063e-09\n",
      "step: 28618, loss: 3.789172842516564e-05\n",
      "step: 28619, loss: 1.0728817301242088e-07\n",
      "step: 28620, loss: 0.0\n",
      "step: 28621, loss: 0.0\n",
      "step: 28622, loss: 4.76837103136063e-09\n",
      "step: 28623, loss: 2.38418573772492e-09\n",
      "step: 28624, loss: 0.0\n",
      "step: 28625, loss: 2.3126467851852794e-07\n",
      "step: 28626, loss: 0.16521649062633514\n",
      "step: 28627, loss: 4.76837103136063e-09\n",
      "step: 28628, loss: 2.1934388882982603e-07\n",
      "step: 28629, loss: 8.106215432235331e-08\n",
      "step: 28630, loss: 2.38418573772492e-09\n",
      "step: 28631, loss: 8.344633073420482e-08\n",
      "step: 28632, loss: 7.15255632499634e-09\n",
      "step: 28633, loss: 0.0\n",
      "step: 28634, loss: 4.76837103136063e-09\n",
      "step: 28635, loss: 0.14331574738025665\n",
      "step: 28636, loss: 8.344635915591425e-08\n",
      "step: 28637, loss: 1.192092646817855e-08\n",
      "step: 28638, loss: 2.0265477473913052e-07\n",
      "step: 28639, loss: 4.76837103136063e-09\n",
      "step: 28640, loss: 2.3841844054572903e-08\n",
      "step: 28641, loss: 1.907348234908568e-08\n",
      "step: 28642, loss: 2.3841844054572903e-08\n",
      "step: 28643, loss: 0.0\n",
      "step: 28644, loss: 0.0\n",
      "step: 28645, loss: 1.24927464639768e-06\n",
      "step: 28646, loss: 1.907347702001516e-08\n",
      "step: 28647, loss: 0.0\n",
      "step: 28648, loss: 0.0\n",
      "step: 28649, loss: 5.251818038232159e-06\n",
      "step: 28650, loss: 2.38418573772492e-09\n",
      "step: 28651, loss: 5.4836217344700344e-08\n",
      "step: 28652, loss: 2.38418573772492e-09\n",
      "step: 28653, loss: 1.3589817626780132e-07\n",
      "step: 28654, loss: 3.957709111546137e-07\n",
      "step: 28655, loss: 4.76837103136063e-09\n",
      "step: 28656, loss: 2.38418573772492e-09\n",
      "step: 28657, loss: 7.15255676908555e-09\n",
      "step: 28658, loss: 1.668929527909313e-08\n",
      "step: 28659, loss: 4.76837103136063e-09\n",
      "step: 28660, loss: 4.76837103136063e-09\n",
      "step: 28661, loss: 5.102092472952791e-07\n",
      "step: 28662, loss: 1.549714738757757e-07\n",
      "step: 28663, loss: 1.192092646817855e-08\n",
      "step: 28664, loss: 0.0\n",
      "step: 28665, loss: 2.38418573772492e-09\n",
      "step: 28666, loss: 4.76837103136063e-09\n",
      "step: 28667, loss: 4.76837103136063e-09\n",
      "step: 28668, loss: 0.0\n",
      "step: 28669, loss: 2.8610211089130644e-08\n",
      "step: 28670, loss: 1.907347702001516e-08\n",
      "step: 28671, loss: 1.192092558000013e-08\n",
      "step: 28672, loss: 4.76837103136063e-09\n",
      "step: 28673, loss: 8.821468355790785e-08\n",
      "step: 28674, loss: 2.38418573772492e-09\n",
      "step: 28675, loss: 4.76837147544984e-09\n",
      "step: 28676, loss: 2.38418573772492e-09\n",
      "step: 28677, loss: 2.145766586636455e-08\n",
      "step: 28678, loss: 1.430510998545742e-08\n",
      "step: 28679, loss: 0.0\n",
      "step: 28680, loss: 0.0\n",
      "step: 28681, loss: 0.0\n",
      "step: 28682, loss: 0.0\n",
      "step: 28683, loss: 0.0\n",
      "step: 28684, loss: 0.013135621324181557\n",
      "step: 28685, loss: 2.3841844054572903e-08\n",
      "step: 28686, loss: 1.668929705544997e-08\n",
      "step: 28687, loss: 2.384184938364342e-08\n",
      "step: 28688, loss: 0.0\n",
      "step: 28689, loss: 0.01587623544037342\n",
      "step: 28690, loss: 0.0\n",
      "step: 28691, loss: 4.529947972287118e-08\n",
      "step: 28692, loss: 0.0\n",
      "step: 28693, loss: 2.38418573772492e-09\n",
      "step: 28694, loss: 2.38418573772492e-09\n",
      "step: 28695, loss: 4.76837103136063e-09\n",
      "step: 28696, loss: 2.38418573772492e-09\n",
      "step: 28697, loss: 9.53674117454284e-09\n",
      "step: 28698, loss: 0.0\n",
      "step: 28699, loss: 4.76837103136063e-09\n",
      "step: 28700, loss: 1.263579065380327e-06\n",
      "step: 28701, loss: 0.12922486662864685\n",
      "step: 28702, loss: 5.587935003603661e-09\n",
      "step: 28703, loss: 2.38418573772492e-09\n",
      "step: 28704, loss: 0.0\n",
      "step: 28705, loss: 3.33785834527589e-08\n",
      "step: 28706, loss: 2.145766764272139e-08\n",
      "step: 28707, loss: 0.0\n",
      "step: 28708, loss: 2.38418573772492e-09\n",
      "step: 28709, loss: 0.0\n",
      "step: 28710, loss: 0.0\n",
      "step: 28711, loss: 0.0\n",
      "step: 28712, loss: 0.0\n",
      "step: 28713, loss: 1.192092558000013e-08\n",
      "step: 28714, loss: 0.0\n",
      "step: 28715, loss: 0.0\n",
      "step: 28716, loss: 4.76837103136063e-09\n",
      "step: 28717, loss: 0.0\n",
      "step: 28718, loss: 0.0\n",
      "step: 28719, loss: 4.76837103136063e-09\n",
      "step: 28720, loss: 7.15255676908555e-09\n",
      "step: 28721, loss: 4.76837147544984e-09\n",
      "step: 28722, loss: 2.38418573772492e-09\n",
      "step: 28723, loss: 3.0994389277338996e-08\n",
      "step: 28724, loss: 0.0\n",
      "step: 28725, loss: 3.814693982917561e-08\n",
      "step: 28726, loss: 1.192092558000013e-08\n",
      "step: 28727, loss: 7.15255632499634e-09\n",
      "step: 28728, loss: 0.0\n",
      "step: 28729, loss: 0.0\n",
      "step: 28730, loss: 0.04445882886648178\n",
      "step: 28731, loss: 0.0\n",
      "step: 28732, loss: 9.53674295089968e-09\n",
      "step: 28733, loss: 0.0\n",
      "step: 28734, loss: 0.0\n",
      "step: 28735, loss: 1.668929350273629e-08\n",
      "step: 28736, loss: 0.0\n",
      "step: 28737, loss: 4.768366679286373e-08\n",
      "step: 28738, loss: 4.64385857412708e-06\n",
      "step: 28739, loss: 4.76837103136063e-09\n",
      "step: 28740, loss: 2.38418573772492e-09\n",
      "step: 28741, loss: 3.576277052275145e-08\n",
      "step: 28742, loss: 0.0\n",
      "step: 28743, loss: 1.1205643346556826e-07\n",
      "step: 28744, loss: 0.0\n",
      "step: 28745, loss: 2.38418573772492e-09\n",
      "step: 28746, loss: 2.38418573772492e-09\n",
      "step: 28747, loss: 0.0\n",
      "step: 28748, loss: 2.8610211089130644e-08\n",
      "step: 28749, loss: 0.0\n",
      "step: 28750, loss: 1.668929527909313e-08\n",
      "step: 28751, loss: 2.145766764272139e-08\n",
      "step: 28752, loss: 0.0\n",
      "step: 28753, loss: 0.0\n",
      "step: 28754, loss: 2.6226025795494934e-08\n",
      "step: 28755, loss: 0.0\n",
      "step: 28756, loss: 0.0\n",
      "step: 28757, loss: 1.192092824453539e-08\n",
      "step: 28758, loss: 2.38418573772492e-09\n",
      "step: 28759, loss: 0.0741889551281929\n",
      "step: 28760, loss: 2.694111742584937e-07\n",
      "step: 28761, loss: 4.6252668539636943e-07\n",
      "step: 28762, loss: 0.0\n",
      "step: 28763, loss: 4.76837147544984e-09\n",
      "step: 28764, loss: 0.0\n",
      "step: 28765, loss: 1.0967225705371675e-07\n",
      "step: 28766, loss: 2.38418573772492e-09\n",
      "step: 28767, loss: 0.0\n",
      "step: 28768, loss: 0.0\n",
      "step: 28769, loss: 0.0\n",
      "step: 28770, loss: 2.7656366796691145e-07\n",
      "step: 28771, loss: 4.76837103136063e-09\n",
      "step: 28772, loss: 2.38418573772492e-09\n",
      "step: 28773, loss: 1.0375669262430165e-05\n",
      "step: 28774, loss: 2.38418573772492e-09\n",
      "step: 28775, loss: 1.668929883180681e-08\n",
      "step: 28776, loss: 2.38418573772492e-09\n",
      "step: 28777, loss: 2.145766764272139e-08\n",
      "step: 28778, loss: 0.0\n",
      "step: 28779, loss: 0.0\n",
      "step: 28780, loss: 2.38418573772492e-09\n",
      "step: 28781, loss: 0.029984690248966217\n",
      "step: 28782, loss: 8.106215432235331e-08\n",
      "step: 28783, loss: 2.3841844054572903e-08\n",
      "step: 28784, loss: 7.15255632499634e-09\n",
      "step: 28785, loss: 0.0\n",
      "step: 28786, loss: 2.38418573772492e-09\n",
      "step: 28787, loss: 1.668929350273629e-08\n",
      "step: 28788, loss: 9.53674206272126e-09\n",
      "step: 28789, loss: 0.0\n",
      "step: 28790, loss: 1.430510998545742e-08\n",
      "step: 28791, loss: 3.576275631189674e-08\n",
      "step: 28792, loss: 0.002046006266027689\n",
      "step: 28793, loss: 7.15255632499634e-09\n",
      "step: 28794, loss: 2.0265478894998523e-07\n",
      "step: 28795, loss: 0.06077837944030762\n",
      "step: 28796, loss: 1.4543485349349794e-07\n",
      "step: 28797, loss: 2.38418573772492e-09\n",
      "step: 28798, loss: 0.0\n",
      "step: 28799, loss: 0.0\n",
      "step: 28800, loss: 0.0\n",
      "step: 28801, loss: 3.313991783215897e-07\n",
      "step: 28802, loss: 0.0\n",
      "step: 28803, loss: 3.337857279461787e-08\n",
      "step: 28804, loss: 2.38418573772492e-09\n",
      "step: 28805, loss: 0.0\n",
      "step: 28806, loss: 2.622603467727913e-08\n",
      "step: 28807, loss: 0.0\n",
      "step: 28808, loss: 0.0\n",
      "step: 28809, loss: 9.53674206272126e-09\n",
      "step: 28810, loss: 2.38418573772492e-09\n",
      "step: 28811, loss: 6.198877855467799e-08\n",
      "step: 28812, loss: 0.0\n",
      "step: 28813, loss: 0.06799567490816116\n",
      "step: 28814, loss: 1.192092646817855e-08\n",
      "step: 28815, loss: 0.07900065183639526\n",
      "step: 28816, loss: 0.0\n",
      "step: 28817, loss: 0.0\n",
      "step: 28818, loss: 0.0\n",
      "step: 28819, loss: 4.76837103136063e-09\n",
      "step: 28820, loss: 4.76837147544984e-09\n",
      "step: 28821, loss: 2.8610209312773804e-08\n",
      "step: 28822, loss: 0.0\n",
      "step: 28823, loss: 0.11941199004650116\n",
      "step: 28824, loss: 5.245204093284883e-08\n",
      "step: 28825, loss: 4.76837147544984e-09\n",
      "step: 28826, loss: 0.0\n",
      "step: 28827, loss: 4.76837147544984e-09\n",
      "step: 28828, loss: 4.291529975830599e-08\n",
      "step: 28829, loss: 7.15255632499634e-09\n",
      "step: 28830, loss: 2.38418573772492e-09\n",
      "step: 28831, loss: 8.797453574516112e-07\n",
      "step: 28832, loss: 0.09188421070575714\n",
      "step: 28833, loss: 1.1896830756086274e-06\n",
      "step: 28834, loss: 0.0\n",
      "step: 28835, loss: 5.604464149655541e-06\n",
      "step: 28836, loss: 0.0031742346473038197\n",
      "step: 28837, loss: 2.38418573772492e-09\n",
      "step: 28838, loss: 2.0980728265840298e-07\n",
      "step: 28839, loss: 1.2778882592101581e-06\n",
      "step: 28840, loss: 1.192092646817855e-08\n",
      "step: 28841, loss: 3.8146946934602965e-08\n",
      "step: 28842, loss: 0.03362047299742699\n",
      "step: 28843, loss: 4.2915317521874385e-08\n",
      "step: 28844, loss: 0.0\n",
      "step: 28845, loss: 2.6226029348208613e-08\n",
      "step: 28846, loss: 2.38418573772492e-09\n",
      "step: 28847, loss: 0.14741718769073486\n",
      "step: 28848, loss: 0.02713940665125847\n",
      "step: 28849, loss: 1.192092646817855e-08\n",
      "step: 28850, loss: 2.145766764272139e-08\n",
      "step: 28851, loss: 3.337858700547258e-08\n",
      "step: 28852, loss: 3.337859055818626e-08\n",
      "step: 28853, loss: 8.106226090376367e-08\n",
      "step: 28854, loss: 0.04589555412530899\n",
      "step: 28855, loss: 4.76837147544984e-09\n",
      "step: 28856, loss: 2.38418573772492e-09\n",
      "step: 28857, loss: 0.0\n",
      "step: 28858, loss: 1.192092824453539e-08\n",
      "step: 28859, loss: 0.0\n",
      "step: 28860, loss: 3.576277052275145e-08\n",
      "step: 28861, loss: 0.09194406867027283\n",
      "step: 28862, loss: 9.53674117454284e-09\n",
      "step: 28863, loss: 0.0\n",
      "step: 28864, loss: 5.722037599298346e-08\n",
      "step: 28865, loss: 1.668929350273629e-08\n",
      "step: 28866, loss: 1.1205643346556826e-07\n",
      "step: 28867, loss: 7.15255632499634e-09\n",
      "step: 28868, loss: 3.814695404003032e-08\n",
      "step: 28869, loss: 0.0\n",
      "step: 28870, loss: 0.0\n",
      "step: 28871, loss: 0.0740748941898346\n",
      "step: 28872, loss: 0.0\n",
      "step: 28873, loss: 0.0\n",
      "step: 28874, loss: 1.0728817301242088e-07\n",
      "step: 28875, loss: 2.38418573772492e-09\n",
      "step: 28876, loss: 0.0\n",
      "step: 28877, loss: 7.15255676908555e-09\n",
      "step: 28878, loss: 2.8610209312773804e-08\n",
      "step: 28879, loss: 9.465001653552463e-07\n",
      "step: 28880, loss: 2.38418573772492e-09\n",
      "step: 28881, loss: 3.5047224855588865e-07\n",
      "step: 28882, loss: 0.0\n",
      "step: 28883, loss: 0.0\n",
      "step: 28884, loss: 0.0\n",
      "step: 28885, loss: 2.384184938364342e-08\n",
      "step: 28886, loss: 9.53674117454284e-09\n",
      "step: 28887, loss: 4.76837103136063e-09\n",
      "step: 28888, loss: 1.192092646817855e-08\n",
      "step: 28889, loss: 9.53674206272126e-09\n",
      "step: 28890, loss: 4.76837103136063e-09\n",
      "step: 28891, loss: 0.0\n",
      "step: 28892, loss: 9.53674206272126e-09\n",
      "step: 28893, loss: 2.932528957444447e-07\n",
      "step: 28894, loss: 4.76837103136063e-09\n",
      "step: 28895, loss: 8.821471908504463e-08\n",
      "step: 28896, loss: 2.1457660537294032e-08\n",
      "step: 28897, loss: 4.291529975830599e-08\n",
      "step: 28898, loss: 4.053112689916816e-08\n",
      "step: 28899, loss: 1.668929527909313e-08\n",
      "step: 28900, loss: 7.15255632499634e-09\n",
      "step: 28901, loss: 9.775139631074126e-08\n",
      "step: 28902, loss: 0.0\n",
      "step: 28903, loss: 1.668929883180681e-08\n",
      "step: 28904, loss: 0.0\n",
      "step: 28905, loss: 4.76837147544984e-09\n",
      "step: 28906, loss: 0.11338241398334503\n",
      "step: 28907, loss: 0.0\n",
      "step: 28908, loss: 0.06884405016899109\n",
      "step: 28909, loss: 0.0\n",
      "step: 28910, loss: 7.86779779105018e-08\n",
      "step: 28911, loss: 0.0\n",
      "step: 28912, loss: 2.861021997091484e-08\n",
      "step: 28913, loss: 1.907348412544252e-08\n",
      "step: 28914, loss: 2.38418573772492e-09\n",
      "step: 28915, loss: 0.0\n",
      "step: 28916, loss: 8.583050714605633e-08\n",
      "step: 28917, loss: 1.192092646817855e-08\n",
      "step: 28918, loss: 0.0\n",
      "step: 28919, loss: 0.0\n",
      "step: 28920, loss: 7.15255676908555e-09\n",
      "step: 28921, loss: 1.430511176181426e-08\n",
      "step: 28922, loss: 3.8146950487316644e-08\n",
      "step: 28923, loss: 2.38418573772492e-09\n",
      "step: 28924, loss: 1.5329734424085473e-06\n",
      "step: 28925, loss: 2.38418573772492e-09\n",
      "step: 28926, loss: 0.0\n",
      "step: 28927, loss: 1.4066647224808548e-07\n",
      "step: 28928, loss: 2.6077010772951326e-08\n",
      "step: 28929, loss: 1.430511264999268e-08\n",
      "step: 28930, loss: 1.549715733517587e-07\n",
      "step: 28931, loss: 1.192092646817855e-08\n",
      "step: 28932, loss: 1.430510998545742e-08\n",
      "step: 28933, loss: 4.480587813304737e-05\n",
      "step: 28934, loss: 0.0\n",
      "step: 28935, loss: 2.145766764272139e-08\n",
      "step: 28936, loss: 7.15255632499634e-09\n",
      "step: 28937, loss: 4.76837147544984e-09\n",
      "step: 28938, loss: 4.76837147544984e-09\n",
      "step: 28939, loss: 8.106216142778067e-08\n",
      "step: 28940, loss: 2.38418573772492e-09\n",
      "step: 28941, loss: 4.768314738612389e-07\n",
      "step: 28942, loss: 2.3841844054572903e-08\n",
      "step: 28943, loss: 2.503379334939382e-07\n",
      "step: 28944, loss: 0.0\n",
      "step: 28945, loss: 0.0\n",
      "step: 28946, loss: 0.0\n",
      "step: 28947, loss: 4.76837103136063e-09\n",
      "step: 28948, loss: 2.38418573772492e-09\n",
      "step: 28949, loss: 2.145766764272139e-08\n",
      "step: 28950, loss: 1.8119767730695457e-07\n",
      "step: 28951, loss: 0.0\n",
      "step: 28952, loss: 1.6212405284932174e-07\n",
      "step: 28953, loss: 2.38418573772492e-09\n",
      "step: 28954, loss: 2.38418573772492e-09\n",
      "step: 28955, loss: 0.0\n",
      "step: 28956, loss: 7.15255632499634e-09\n",
      "step: 28957, loss: 9.53674117454284e-09\n",
      "step: 28958, loss: 4.529949748643958e-08\n",
      "step: 28959, loss: 0.0\n",
      "step: 28960, loss: 2.38418573772492e-09\n",
      "step: 28961, loss: 7.15255676908555e-09\n",
      "step: 28962, loss: 5.7220425730974966e-08\n",
      "step: 28963, loss: 2.8848444344475865e-07\n",
      "step: 28964, loss: 2.2600804641115246e-06\n",
      "step: 28965, loss: 1.430510998545742e-08\n",
      "step: 28966, loss: 2.38418573772492e-09\n",
      "step: 28967, loss: 1.668929883180681e-08\n",
      "step: 28968, loss: 3.9815751051719417e-07\n",
      "step: 28969, loss: 8.045708091231063e-05\n",
      "step: 28970, loss: 4.76837103136063e-09\n",
      "step: 28971, loss: 1.0490398238971466e-07\n",
      "step: 28972, loss: 9.53674206272126e-09\n",
      "step: 28973, loss: 2.622603467727913e-08\n",
      "step: 28974, loss: 0.0\n",
      "step: 28975, loss: 0.0\n",
      "step: 28976, loss: 2.527220885895076e-07\n",
      "step: 28977, loss: 2.38418573772492e-09\n",
      "step: 28978, loss: 0.0\n",
      "step: 28979, loss: 0.0\n",
      "step: 28980, loss: 1.5020316368463682e-07\n",
      "step: 28981, loss: 1.668929350273629e-08\n",
      "step: 28982, loss: 5.722039020383818e-08\n",
      "step: 28983, loss: 4.053114111002287e-08\n",
      "step: 28984, loss: 0.0\n",
      "step: 28985, loss: 2.6226025795494934e-08\n",
      "step: 28986, loss: 2.38418573772492e-09\n",
      "step: 28987, loss: 3.5762759864610416e-08\n",
      "step: 28988, loss: 2.38418573772492e-09\n",
      "step: 28989, loss: 7.15255676908555e-09\n",
      "step: 28990, loss: 2.38418573772492e-09\n",
      "step: 28991, loss: 7.15255632499634e-09\n",
      "step: 28992, loss: 7.15255676908555e-09\n",
      "step: 28993, loss: 0.0\n",
      "step: 28994, loss: 1.7356121588818496e-06\n",
      "step: 28995, loss: 2.145766231365087e-08\n",
      "step: 28996, loss: 0.0\n",
      "step: 28997, loss: 4.76837103136063e-09\n",
      "step: 28998, loss: 0.0\n",
      "step: 28999, loss: 0.0\n",
      "step: 29000, loss: 9.53674206272126e-09\n",
      "step: 29001, loss: 7.15255676908555e-09\n",
      "step: 29002, loss: 1.668929350273629e-08\n",
      "step: 29003, loss: 0.0\n",
      "step: 29004, loss: 4.76837147544984e-09\n",
      "step: 29005, loss: 0.0\n",
      "step: 29006, loss: 2.0503892983469996e-07\n",
      "step: 29007, loss: 0.0\n",
      "step: 29008, loss: 0.11123993992805481\n",
      "step: 29009, loss: 0.0\n",
      "step: 29010, loss: 2.38418573772492e-09\n",
      "step: 29011, loss: 1.668929350273629e-08\n",
      "step: 29012, loss: 7.15255632499634e-09\n",
      "step: 29013, loss: 2.38418573772492e-09\n",
      "step: 29014, loss: 0.0003545664658304304\n",
      "step: 29015, loss: 1.0490393975715051e-07\n",
      "step: 29016, loss: 2.38418573772492e-09\n",
      "step: 29017, loss: 2.38418573772492e-09\n",
      "step: 29018, loss: 1.85965660648435e-07\n",
      "step: 29019, loss: 4.291529975830599e-08\n",
      "step: 29020, loss: 7.15255676908555e-09\n",
      "step: 29021, loss: 0.0\n",
      "step: 29022, loss: 0.0\n",
      "step: 29023, loss: 4.76837147544984e-09\n",
      "step: 29024, loss: 1.0490411028740709e-07\n",
      "step: 29025, loss: 0.0\n",
      "step: 29026, loss: 1.43051135381711e-08\n",
      "step: 29027, loss: 0.0\n",
      "step: 29028, loss: 2.6226025795494934e-08\n",
      "step: 29029, loss: 2.38418573772492e-09\n",
      "step: 29030, loss: 0.0\n",
      "step: 29031, loss: 1.192092824453539e-08\n",
      "step: 29032, loss: 0.0\n",
      "step: 29033, loss: 0.0\n",
      "step: 29034, loss: 7.39096535085082e-08\n",
      "step: 29035, loss: 0.0\n",
      "step: 29036, loss: 0.0\n",
      "step: 29037, loss: 3.099441059362107e-08\n",
      "step: 29038, loss: 4.76837147544984e-09\n",
      "step: 29039, loss: 5.292825449032534e-07\n",
      "step: 29040, loss: 2.38418573772492e-09\n",
      "step: 29041, loss: 4.76837103136063e-09\n",
      "step: 29042, loss: 0.0\n",
      "step: 29043, loss: 1.3146854143997189e-05\n",
      "step: 29044, loss: 0.0\n",
      "step: 29045, loss: 1.430511176181426e-08\n",
      "step: 29046, loss: 4.1460916690994054e-05\n",
      "step: 29047, loss: 0.0\n",
      "step: 29048, loss: 0.0014071729965507984\n",
      "step: 29049, loss: 0.08443085849285126\n",
      "step: 29050, loss: 0.0\n",
      "step: 29051, loss: 2.38418573772492e-09\n",
      "step: 29052, loss: 2.0265480316083995e-07\n",
      "step: 29053, loss: 2.38418573772492e-09\n",
      "step: 29054, loss: 0.0\n",
      "step: 29055, loss: 0.0\n",
      "step: 29056, loss: 1.668929705544997e-08\n",
      "step: 29057, loss: 1.430510998545742e-08\n",
      "step: 29058, loss: 7.15255632499634e-09\n",
      "step: 29059, loss: 1.668929705544997e-08\n",
      "step: 29060, loss: 2.145766586636455e-08\n",
      "step: 29061, loss: 5.245201961656676e-08\n",
      "step: 29062, loss: 0.0\n",
      "step: 29063, loss: 2.622603467727913e-08\n",
      "step: 29064, loss: 4.76837147544984e-09\n",
      "step: 29065, loss: 0.0\n",
      "step: 29066, loss: 4.2915317521874385e-08\n",
      "step: 29067, loss: 2.38418573772492e-09\n",
      "step: 29068, loss: 9.53674117454284e-09\n",
      "step: 29069, loss: 3.6477712228588643e-07\n",
      "step: 29070, loss: 3.5762759864610416e-08\n",
      "step: 29071, loss: 0.24997954070568085\n",
      "step: 29072, loss: 0.0\n",
      "step: 29073, loss: 0.06992851942777634\n",
      "step: 29074, loss: 0.059117116034030914\n",
      "step: 29075, loss: 1.907348234908568e-08\n",
      "step: 29076, loss: 4.76837147544984e-09\n",
      "step: 29077, loss: 1.668929527909313e-08\n",
      "step: 29078, loss: 7.15255676908555e-09\n",
      "step: 29079, loss: 7.867805607020273e-08\n",
      "step: 29080, loss: 4.76837147544984e-09\n",
      "step: 29081, loss: 1.907348234908568e-08\n",
      "step: 29082, loss: 4.100771775483736e-07\n",
      "step: 29083, loss: 0.0\n",
      "step: 29084, loss: 0.0\n",
      "step: 29085, loss: 4.76837103136063e-09\n",
      "step: 29086, loss: 1.668929350273629e-08\n",
      "step: 29087, loss: 1.192092558000013e-08\n",
      "step: 29088, loss: 0.0\n",
      "step: 29089, loss: 0.0\n",
      "step: 29090, loss: 0.08860331773757935\n",
      "step: 29091, loss: 0.0\n",
      "step: 29092, loss: 2.38418573772492e-09\n",
      "step: 29093, loss: 9.536720568803503e-08\n",
      "step: 29094, loss: 2.38418573772492e-09\n",
      "step: 29095, loss: 0.0\n",
      "step: 29096, loss: 2.2100182377471356e-06\n",
      "step: 29097, loss: 2.38418573772492e-09\n",
      "step: 29098, loss: 0.0\n",
      "step: 29099, loss: 1.907347702001516e-08\n",
      "step: 29100, loss: 0.0664118304848671\n",
      "step: 29101, loss: 6.198876434382328e-08\n",
      "step: 29102, loss: 4.76837103136063e-09\n",
      "step: 29103, loss: 2.38418573772492e-09\n",
      "step: 29104, loss: 0.0\n",
      "step: 29105, loss: 2.38418573772492e-09\n",
      "step: 29106, loss: 0.0\n",
      "step: 29107, loss: 0.0\n",
      "step: 29108, loss: 0.0\n",
      "step: 29109, loss: 1.9073478796372e-08\n",
      "step: 29110, loss: 3.5762763417324095e-08\n",
      "step: 29111, loss: 9.53674117454284e-09\n",
      "step: 29112, loss: 9.53674117454284e-09\n",
      "step: 29113, loss: 4.768367389829109e-08\n",
      "step: 29114, loss: 0.0\n",
      "step: 29115, loss: 0.0\n",
      "step: 29116, loss: 0.0\n",
      "step: 29117, loss: 0.0\n",
      "step: 29118, loss: 1.0728808774729259e-07\n",
      "step: 29119, loss: 6.914133621194196e-08\n",
      "step: 29120, loss: 2.38418573772492e-09\n",
      "step: 29121, loss: 2.38418573772492e-09\n",
      "step: 29122, loss: 2.38418573772492e-09\n",
      "step: 29123, loss: 0.0\n",
      "step: 29124, loss: 0.0\n",
      "step: 29125, loss: 1.192092558000013e-08\n",
      "step: 29126, loss: 1.192092558000013e-08\n",
      "step: 29127, loss: 4.76837103136063e-09\n",
      "step: 29128, loss: 1.668929527909313e-08\n",
      "step: 29129, loss: 0.0\n",
      "step: 29130, loss: 0.0\n",
      "step: 29131, loss: 0.0\n",
      "step: 29132, loss: 4.291529975830599e-08\n",
      "step: 29133, loss: 1.907347702001516e-08\n",
      "step: 29134, loss: 0.1902173012495041\n",
      "step: 29135, loss: 7.15255632499634e-09\n",
      "step: 29136, loss: 4.76837103136063e-09\n",
      "step: 29137, loss: 0.0\n",
      "step: 29138, loss: 0.0\n",
      "step: 29139, loss: 1.907347702001516e-08\n",
      "step: 29140, loss: 0.08281812816858292\n",
      "step: 29141, loss: 4.76837103136063e-09\n",
      "step: 29142, loss: 0.08426082879304886\n",
      "step: 29143, loss: 0.0\n",
      "step: 29144, loss: 4.76837103136063e-09\n",
      "step: 29145, loss: 0.0\n",
      "step: 29146, loss: 4.76837147544984e-09\n",
      "step: 29147, loss: 9.53674117454284e-09\n",
      "step: 29148, loss: 1.192092824453539e-08\n",
      "step: 29149, loss: 0.0\n",
      "step: 29150, loss: 4.76837147544984e-09\n",
      "step: 29151, loss: 2.38418573772492e-09\n",
      "step: 29152, loss: 1.668929705544997e-08\n",
      "step: 29153, loss: 7.15255632499634e-09\n",
      "step: 29154, loss: 5.587934559514451e-09\n",
      "step: 29155, loss: 1.192092558000013e-08\n",
      "step: 29156, loss: 7.15255676908555e-09\n",
      "step: 29157, loss: 0.0\n",
      "step: 29158, loss: 9.53674206272126e-09\n",
      "step: 29159, loss: 8.94049833277677e-07\n",
      "step: 29160, loss: 9.53674117454284e-09\n",
      "step: 29161, loss: 3.8623446130259254e-07\n",
      "step: 29162, loss: 0.0\n",
      "step: 29163, loss: 0.0\n",
      "step: 29164, loss: 6.437291233396536e-08\n",
      "step: 29165, loss: 0.0\n",
      "step: 29166, loss: 0.0\n",
      "step: 29167, loss: 1.41139230436238e-06\n",
      "step: 29168, loss: 2.3841844054572903e-08\n",
      "step: 29169, loss: 4.76837147544984e-09\n",
      "step: 29170, loss: 1.907348234908568e-08\n",
      "step: 29171, loss: 0.16415657103061676\n",
      "step: 29172, loss: 4.76837147544984e-09\n",
      "step: 29173, loss: 0.0\n",
      "step: 29174, loss: 2.38418573772492e-09\n",
      "step: 29175, loss: 0.0\n",
      "step: 29176, loss: 4.76837147544984e-09\n",
      "step: 29177, loss: 2.38418573772492e-09\n",
      "step: 29178, loss: 0.0\n",
      "step: 29179, loss: 6.437291233396536e-08\n",
      "step: 29180, loss: 0.0\n",
      "step: 29181, loss: 9.53674206272126e-09\n",
      "step: 29182, loss: 4.76837147544984e-09\n",
      "step: 29183, loss: 1.430511264999268e-08\n",
      "step: 29184, loss: 2.38418573772492e-09\n",
      "step: 29185, loss: 2.38418573772492e-09\n",
      "step: 29186, loss: 0.0\n",
      "step: 29187, loss: 2.3841844054572903e-08\n",
      "step: 29188, loss: 2.38418573772492e-09\n",
      "step: 29189, loss: 0.0\n",
      "step: 29190, loss: 2.38418573772492e-09\n",
      "step: 29191, loss: 0.0\n",
      "step: 29192, loss: 4.76837147544984e-09\n",
      "step: 29193, loss: 2.38418573772492e-09\n",
      "step: 29194, loss: 4.76837103136063e-09\n",
      "step: 29195, loss: 2.38418573772492e-09\n",
      "step: 29196, loss: 0.0\n",
      "step: 29197, loss: 7.15255676908555e-09\n",
      "step: 29198, loss: 4.5299508144580614e-08\n",
      "step: 29199, loss: 4.76837103136063e-09\n",
      "step: 29200, loss: 0.0\n",
      "step: 29201, loss: 2.38418573772492e-09\n",
      "step: 29202, loss: 2.145766764272139e-08\n",
      "step: 29203, loss: 0.0\n",
      "step: 29204, loss: 2.8610216418201162e-08\n",
      "step: 29205, loss: 2.38418573772492e-09\n",
      "step: 29206, loss: 2.38418573772492e-09\n",
      "step: 29207, loss: 1.668929350273629e-08\n",
      "step: 29208, loss: 7.15255632499634e-09\n",
      "step: 29209, loss: 0.0\n",
      "step: 29210, loss: 2.38418573772492e-09\n",
      "step: 29211, loss: 2.38418573772492e-09\n",
      "step: 29212, loss: 2.241122842860932e-07\n",
      "step: 29213, loss: 9.53674206272126e-09\n",
      "step: 29214, loss: 9.53674117454284e-09\n",
      "step: 29215, loss: 0.0\n",
      "step: 29216, loss: 0.0\n",
      "step: 29217, loss: 0.0\n",
      "step: 29218, loss: 0.0\n",
      "step: 29219, loss: 0.0\n",
      "step: 29220, loss: 0.0022215626668184996\n",
      "step: 29221, loss: 2.3841844054572903e-08\n",
      "step: 29222, loss: 4.76837103136063e-09\n",
      "step: 29223, loss: 0.1277501881122589\n",
      "step: 29224, loss: 3.6716133422487474e-07\n",
      "step: 29225, loss: 1.192092824453539e-08\n",
      "step: 29226, loss: 0.0\n",
      "step: 29227, loss: 4.76837103136063e-09\n",
      "step: 29228, loss: 2.38418573772492e-09\n",
      "step: 29229, loss: 2.38418573772492e-09\n",
      "step: 29230, loss: 1.43051135381711e-08\n",
      "step: 29231, loss: 0.0\n",
      "step: 29232, loss: 0.0\n",
      "step: 29233, loss: 0.0014678139705210924\n",
      "step: 29234, loss: 2.38418573772492e-09\n",
      "step: 29235, loss: 0.0\n",
      "step: 29236, loss: 1.4085576367506292e-05\n",
      "step: 29237, loss: 1.9550226681985805e-07\n",
      "step: 29238, loss: 4.2915317521874385e-08\n",
      "step: 29239, loss: 0.0\n",
      "step: 29240, loss: 2.38418573772492e-09\n",
      "step: 29241, loss: 0.03541085124015808\n",
      "step: 29242, loss: 2.3841844054572903e-08\n",
      "step: 29243, loss: 0.0\n",
      "step: 29244, loss: 0.062098558992147446\n",
      "step: 29245, loss: 1.668929527909313e-08\n",
      "step: 29246, loss: 2.38418573772492e-09\n",
      "step: 29247, loss: 0.0\n",
      "step: 29248, loss: 6.746600411133841e-05\n",
      "step: 29249, loss: 0.0\n",
      "step: 29250, loss: 0.06295058876276016\n",
      "step: 29251, loss: 7.15255632499634e-09\n",
      "step: 29252, loss: 0.0\n",
      "step: 29253, loss: 1.430510998545742e-08\n",
      "step: 29254, loss: 0.0\n",
      "step: 29255, loss: 0.0\n",
      "step: 29256, loss: 0.0\n",
      "step: 29257, loss: 0.0\n",
      "step: 29258, loss: 0.03978268802165985\n",
      "step: 29259, loss: 9.53674206272126e-09\n",
      "step: 29260, loss: 0.0\n",
      "step: 29261, loss: 7.15255632499634e-09\n",
      "step: 29262, loss: 0.0\n",
      "step: 29263, loss: 0.0\n",
      "step: 29264, loss: 5.722038309841082e-08\n",
      "step: 29265, loss: 9.53674117454284e-09\n",
      "step: 29266, loss: 2.38418573772492e-09\n",
      "step: 29267, loss: 0.0\n",
      "step: 29268, loss: 0.0\n",
      "step: 29269, loss: 3.3378321973032143e-07\n",
      "step: 29270, loss: 0.0\n",
      "step: 29271, loss: 0.0\n",
      "step: 29272, loss: 2.1457660537294032e-08\n",
      "step: 29273, loss: 4.768319286085898e-07\n",
      "step: 29274, loss: 9.53674117454284e-09\n",
      "step: 29275, loss: 2.646435177666717e-07\n",
      "step: 29276, loss: 2.38418573772492e-09\n",
      "step: 29277, loss: 2.38418573772492e-09\n",
      "step: 29278, loss: 4.5299501039153256e-08\n",
      "step: 29279, loss: 9.53674117454284e-09\n",
      "step: 29280, loss: 5.555082793762267e-07\n",
      "step: 29281, loss: 3.623933366725396e-07\n",
      "step: 29282, loss: 2.38418573772492e-09\n",
      "step: 29283, loss: 3.337858700547258e-08\n",
      "step: 29284, loss: 7.15255632499634e-09\n",
      "step: 29285, loss: 9.775142473245069e-08\n",
      "step: 29286, loss: 2.145766231365087e-08\n",
      "step: 29287, loss: 1.430511264999268e-08\n",
      "step: 29288, loss: 4.76837103136063e-09\n",
      "step: 29289, loss: 1.907348234908568e-08\n",
      "step: 29290, loss: 0.0\n",
      "step: 29291, loss: 1.430511264999268e-08\n",
      "step: 29292, loss: 0.0\n",
      "step: 29293, loss: 1.192092824453539e-08\n",
      "step: 29294, loss: 2.6226025795494934e-08\n",
      "step: 29295, loss: 2.145766764272139e-08\n",
      "step: 29296, loss: 0.0\n",
      "step: 29297, loss: 2.861021997091484e-08\n",
      "step: 29298, loss: 0.0\n",
      "step: 29299, loss: 2.622587089717854e-07\n",
      "step: 29300, loss: 4.315337776006345e-07\n",
      "step: 29301, loss: 1.668929527909313e-08\n",
      "step: 29302, loss: 7.15255632499634e-09\n",
      "step: 29303, loss: 0.08485668897628784\n",
      "step: 29304, loss: 3.3855286574180354e-07\n",
      "step: 29305, loss: 0.0\n",
      "step: 29306, loss: 2.38418573772492e-09\n",
      "step: 29307, loss: 0.0\n",
      "step: 29308, loss: 2.38418573772492e-09\n",
      "step: 29309, loss: 0.0\n",
      "step: 29310, loss: 0.0\n",
      "step: 29311, loss: 9.53674206272126e-09\n",
      "step: 29312, loss: 4.76837147544984e-09\n",
      "step: 29313, loss: 5.483619602841827e-08\n",
      "step: 29314, loss: 3.0994389277338996e-08\n",
      "step: 29315, loss: 1.192092558000013e-08\n",
      "step: 29316, loss: 6.437291943939272e-08\n",
      "step: 29317, loss: 4.768314738612389e-07\n",
      "step: 29318, loss: 7.15255632499634e-09\n",
      "step: 29319, loss: 0.0\n",
      "step: 29320, loss: 2.551062721067865e-07\n",
      "step: 29321, loss: 2.751161900960142e-06\n",
      "step: 29322, loss: 0.0\n",
      "step: 29323, loss: 1.192092646817855e-08\n",
      "step: 29324, loss: 0.0\n",
      "step: 29325, loss: 1.668929883180681e-08\n",
      "step: 29326, loss: 2.38418573772492e-09\n",
      "step: 29327, loss: 5.2452058696417225e-08\n",
      "step: 29328, loss: 2.38418573772492e-09\n",
      "step: 29329, loss: 4.76837103136063e-09\n",
      "step: 29330, loss: 0.0\n",
      "step: 29331, loss: 3.5762763417324095e-08\n",
      "step: 29332, loss: 4.76837147544984e-09\n",
      "step: 29333, loss: 0.0005633049295283854\n",
      "step: 29334, loss: 2.38418573772492e-09\n",
      "step: 29335, loss: 0.0\n",
      "step: 29336, loss: 3.337857279461787e-08\n",
      "step: 29337, loss: 1.1205646188727769e-07\n",
      "step: 29338, loss: 1.668929705544997e-08\n",
      "step: 29339, loss: 4.76837147544984e-09\n",
      "step: 29340, loss: 4.0531130451881836e-08\n",
      "step: 29341, loss: 0.03893366456031799\n",
      "step: 29342, loss: 7.15255676908555e-09\n",
      "step: 29343, loss: 1.2874561150511e-07\n",
      "step: 29344, loss: 2.3841844054572903e-08\n",
      "step: 29345, loss: 4.76837103136063e-09\n",
      "step: 29346, loss: 0.0015031566144898534\n",
      "step: 29347, loss: 0.12930482625961304\n",
      "step: 29348, loss: 2.38418573772492e-09\n",
      "step: 29349, loss: 0.0\n",
      "step: 29350, loss: 3.5047298752033385e-07\n",
      "step: 29351, loss: 4.76837103136063e-09\n",
      "step: 29352, loss: 1.192092646817855e-08\n",
      "step: 29353, loss: 0.0\n",
      "step: 29354, loss: 7.867804896477537e-08\n",
      "step: 29355, loss: 0.0\n",
      "step: 29356, loss: 2.384184938364342e-08\n",
      "step: 29357, loss: 1.9073397083957389e-07\n",
      "step: 29358, loss: 9.536731226944539e-08\n",
      "step: 29359, loss: 4.76837103136063e-09\n",
      "step: 29360, loss: 1.430511264999268e-08\n",
      "step: 29361, loss: 0.06490285694599152\n",
      "step: 29362, loss: 0.0\n",
      "step: 29363, loss: 1.192092558000013e-08\n",
      "step: 29364, loss: 6.914130779023253e-08\n",
      "step: 29365, loss: 0.0\n",
      "step: 29366, loss: 2.38418573772492e-09\n",
      "step: 29367, loss: 0.0\n",
      "step: 29368, loss: 0.0\n",
      "step: 29369, loss: 8.821468355790785e-08\n",
      "step: 29370, loss: 2.622603290092229e-08\n",
      "step: 29371, loss: 0.21294797956943512\n",
      "step: 29372, loss: 2.38418573772492e-09\n",
      "step: 29373, loss: 4.3153306705789873e-07\n",
      "step: 29374, loss: 0.0\n",
      "step: 29375, loss: 2.38418573772492e-09\n",
      "step: 29376, loss: 3.8146946934602965e-08\n",
      "step: 29377, loss: 0.0\n",
      "step: 29378, loss: 7.15255632499634e-09\n",
      "step: 29379, loss: 3.8146950487316644e-08\n",
      "step: 29380, loss: 0.00044785873615182936\n",
      "step: 29381, loss: 2.1457660537294032e-08\n",
      "step: 29382, loss: 8.583058530575727e-08\n",
      "step: 29383, loss: 0.0\n",
      "step: 29384, loss: 2.8610209312773804e-08\n",
      "step: 29385, loss: 2.1457660537294032e-08\n",
      "step: 29386, loss: 0.0\n",
      "step: 29387, loss: 0.0\n",
      "step: 29388, loss: 0.0\n",
      "step: 29389, loss: 0.0\n",
      "step: 29390, loss: 4.291530331101967e-08\n",
      "step: 29391, loss: 0.0\n",
      "step: 29392, loss: 2.38418573772492e-09\n",
      "step: 29393, loss: 2.8610218194558e-08\n",
      "step: 29394, loss: 1.430510998545742e-08\n",
      "step: 29395, loss: 0.09329970180988312\n",
      "step: 29396, loss: 4.053111624102712e-08\n",
      "step: 29397, loss: 2.38418573772492e-09\n",
      "step: 29398, loss: 9.53674117454284e-09\n",
      "step: 29399, loss: 0.0\n",
      "step: 29400, loss: 4.76837103136063e-09\n",
      "step: 29401, loss: 0.0\n",
      "step: 29402, loss: 1.2397735815738997e-07\n",
      "step: 29403, loss: 9.53674295089968e-09\n",
      "step: 29404, loss: 2.38418573772492e-09\n",
      "step: 29405, loss: 2.38418573772492e-09\n",
      "step: 29406, loss: 0.0\n",
      "step: 29407, loss: 4.76837103136063e-09\n",
      "step: 29408, loss: 0.0\n",
      "step: 29409, loss: 0.07236457616090775\n",
      "step: 29410, loss: 0.0\n",
      "step: 29411, loss: 1.5735564318219986e-07\n",
      "step: 29412, loss: 0.0002659353194758296\n",
      "step: 29413, loss: 0.0\n",
      "step: 29414, loss: 2.38418573772492e-09\n",
      "step: 29415, loss: 7.15255676908555e-09\n",
      "step: 29416, loss: 0.0\n",
      "step: 29417, loss: 0.0\n",
      "step: 29418, loss: 2.3841844054572903e-08\n",
      "step: 29419, loss: 0.0\n",
      "step: 29420, loss: 2.38418573772492e-09\n",
      "step: 29421, loss: 1.192092646817855e-08\n",
      "step: 29422, loss: 2.38418573772492e-09\n",
      "step: 29423, loss: 2.38418573772492e-09\n",
      "step: 29424, loss: 2.38418529363571e-08\n",
      "step: 29425, loss: 9.53674206272126e-09\n",
      "step: 29426, loss: 1.192092646817855e-08\n",
      "step: 29427, loss: 3.3378576347331546e-08\n",
      "step: 29428, loss: 5.865011871719616e-07\n",
      "step: 29429, loss: 0.0\n",
      "step: 29430, loss: 0.0\n",
      "step: 29431, loss: 1.668929350273629e-08\n",
      "step: 29432, loss: 9.53674206272126e-09\n",
      "step: 29433, loss: 2.38418573772492e-09\n",
      "step: 29434, loss: 2.38418573772492e-09\n",
      "step: 29435, loss: 2.38418573772492e-09\n",
      "step: 29436, loss: 3.099440704090739e-08\n",
      "step: 29437, loss: 0.0029929836746305227\n",
      "step: 29438, loss: 1.1205660399582484e-07\n",
      "step: 29439, loss: 0.0\n",
      "step: 29440, loss: 0.0\n",
      "step: 29441, loss: 1.9073478796372e-08\n",
      "step: 29442, loss: 0.11725863814353943\n",
      "step: 29443, loss: 0.0\n",
      "step: 29444, loss: 5.245147463028843e-07\n",
      "step: 29445, loss: 2.38418573772492e-09\n",
      "step: 29446, loss: 0.0\n",
      "step: 29447, loss: 7.390967482479027e-08\n",
      "step: 29448, loss: 4.76837103136063e-09\n",
      "step: 29449, loss: 9.059708077074902e-07\n",
      "step: 29450, loss: 7.15255632499634e-09\n",
      "step: 29451, loss: 4.76837147544984e-09\n",
      "step: 29452, loss: 7.15255632499634e-09\n",
      "step: 29453, loss: 1.668929527909313e-08\n",
      "step: 29454, loss: 9.53674206272126e-09\n",
      "step: 29455, loss: 1.192092646817855e-08\n",
      "step: 29456, loss: 2.38418573772492e-09\n",
      "step: 29457, loss: 0.1114550530910492\n",
      "step: 29458, loss: 2.38418573772492e-09\n",
      "step: 29459, loss: 2.38418573772492e-09\n",
      "step: 29460, loss: 7.15255632499634e-09\n",
      "step: 29461, loss: 0.0\n",
      "step: 29462, loss: 0.0\n",
      "step: 29463, loss: 4.76837147544984e-09\n",
      "step: 29464, loss: 9.53674206272126e-09\n",
      "step: 29465, loss: 0.03445933014154434\n",
      "step: 29466, loss: 0.0\n",
      "step: 29467, loss: 2.384184938364342e-08\n",
      "step: 29468, loss: 0.0\n",
      "step: 29469, loss: 2.38418573772492e-09\n",
      "step: 29470, loss: 2.76563639545202e-07\n",
      "step: 29471, loss: 0.0\n",
      "step: 29472, loss: 0.03530461713671684\n",
      "step: 29473, loss: 0.0\n",
      "step: 29474, loss: 0.07567504048347473\n",
      "step: 29475, loss: 4.76837147544984e-09\n",
      "step: 29476, loss: 1.3112979502238886e-07\n",
      "step: 29477, loss: 4.76837103136063e-09\n",
      "step: 29478, loss: 0.0\n",
      "step: 29479, loss: 0.0\n",
      "step: 29480, loss: 2.8610209312773804e-08\n",
      "step: 29481, loss: 0.06235406920313835\n",
      "step: 29482, loss: 2.38418573772492e-09\n",
      "step: 29483, loss: 1.192092558000013e-08\n",
      "step: 29484, loss: 2.38418573772492e-09\n",
      "step: 29485, loss: 0.0\n",
      "step: 29486, loss: 2.38418573772492e-09\n",
      "step: 29487, loss: 1.43051135381711e-08\n",
      "step: 29488, loss: 5.435869638859003e-07\n",
      "step: 29489, loss: 0.0\n",
      "step: 29490, loss: 0.0\n",
      "step: 29491, loss: 4.76837103136063e-09\n",
      "step: 29492, loss: 2.622603645363597e-08\n",
      "step: 29493, loss: 1.668929705544997e-08\n",
      "step: 29494, loss: 4.76837103136063e-09\n",
      "step: 29495, loss: 7.15255632499634e-09\n",
      "step: 29496, loss: 2.38418573772492e-09\n",
      "step: 29497, loss: 2.3841844054572903e-08\n",
      "step: 29498, loss: 2.6226025795494934e-08\n",
      "step: 29499, loss: 2.38418573772492e-09\n",
      "step: 29500, loss: 7.15255632499634e-09\n",
      "step: 29501, loss: 3.3378576347331546e-08\n",
      "step: 29502, loss: 0.0\n",
      "step: 29503, loss: 0.0\n",
      "step: 29504, loss: 0.0\n",
      "step: 29505, loss: 0.0\n",
      "step: 29506, loss: 0.0\n",
      "step: 29507, loss: 2.38418573772492e-09\n",
      "step: 29508, loss: 0.0\n",
      "step: 29509, loss: 1.668929527909313e-08\n",
      "step: 29510, loss: 0.0\n",
      "step: 29511, loss: 2.38418573772492e-09\n",
      "step: 29512, loss: 1.668929527909313e-08\n",
      "step: 29513, loss: 1.9073404189384746e-07\n",
      "step: 29514, loss: 3.3378579900045224e-08\n",
      "step: 29515, loss: 1.9073478796372e-08\n",
      "step: 29516, loss: 8.344634494505954e-08\n",
      "step: 29517, loss: 2.38418573772492e-09\n",
      "step: 29518, loss: 2.38418573772492e-09\n",
      "step: 29519, loss: 0.0018484690226614475\n",
      "step: 29520, loss: 7.15255632499634e-09\n",
      "step: 29521, loss: 8.106216853320802e-08\n",
      "step: 29522, loss: 2.38418573772492e-09\n",
      "step: 29523, loss: 0.08253530412912369\n",
      "step: 29524, loss: 2.38418573772492e-09\n",
      "step: 29525, loss: 9.53674295089968e-09\n",
      "step: 29526, loss: 0.0\n",
      "step: 29527, loss: 4.76837103136063e-09\n",
      "step: 29528, loss: 1.192092646817855e-08\n",
      "step: 29529, loss: 1.430511264999268e-08\n",
      "step: 29530, loss: 1.668929350273629e-08\n",
      "step: 29531, loss: 4.76837147544984e-09\n",
      "step: 29532, loss: 4.76837147544984e-09\n",
      "step: 29533, loss: 0.0\n",
      "step: 29534, loss: 0.0\n",
      "step: 29535, loss: 0.0\n",
      "step: 29536, loss: 4.76837103136063e-09\n",
      "step: 29537, loss: 0.0\n",
      "step: 29538, loss: 0.0\n",
      "step: 29539, loss: 4.76837103136063e-09\n",
      "step: 29540, loss: 0.09631408751010895\n",
      "step: 29541, loss: 2.38418573772492e-09\n",
      "step: 29542, loss: 0.0\n",
      "step: 29543, loss: 2.384184938364342e-08\n",
      "step: 29544, loss: 0.0\n",
      "step: 29545, loss: 0.0\n",
      "step: 29546, loss: 0.0\n",
      "step: 29547, loss: 2.38418573772492e-09\n",
      "step: 29548, loss: 1.0466302455824916e-06\n",
      "step: 29549, loss: 0.14854684472084045\n",
      "step: 29550, loss: 2.8610216418201162e-08\n",
      "step: 29551, loss: 2.38418573772492e-09\n",
      "step: 29552, loss: 2.38418573772492e-09\n",
      "step: 29553, loss: 0.0\n",
      "step: 29554, loss: 3.1471114425585256e-07\n",
      "step: 29555, loss: 2.38418573772492e-09\n",
      "step: 29556, loss: 4.76837103136063e-09\n",
      "step: 29557, loss: 1.192092646817855e-08\n",
      "step: 29558, loss: 0.0\n",
      "step: 29559, loss: 2.38418573772492e-09\n",
      "step: 29560, loss: 1.430511176181426e-08\n",
      "step: 29561, loss: 7.390963219222613e-08\n",
      "step: 29562, loss: 0.0\n",
      "step: 29563, loss: 0.05246914550662041\n",
      "step: 29564, loss: 7.15255632499634e-09\n",
      "step: 29565, loss: 0.0\n",
      "step: 29566, loss: 4.291533528544278e-08\n",
      "step: 29567, loss: 7.15255632499634e-09\n",
      "step: 29568, loss: 0.0\n",
      "step: 29569, loss: 4.76837147544984e-09\n",
      "step: 29570, loss: 0.0\n",
      "step: 29571, loss: 3.6570083921105834e-06\n",
      "step: 29572, loss: 2.38418573772492e-09\n",
      "step: 29573, loss: 9.53674117454284e-09\n",
      "step: 29574, loss: 0.07107225060462952\n",
      "step: 29575, loss: 1.192092646817855e-08\n",
      "step: 29576, loss: 2.38418573772492e-09\n",
      "step: 29577, loss: 0.0\n",
      "step: 29578, loss: 7.15255676908555e-09\n",
      "step: 29579, loss: 2.38418573772492e-09\n",
      "step: 29580, loss: 0.0\n",
      "step: 29581, loss: 0.0\n",
      "step: 29582, loss: 0.0\n",
      "step: 29583, loss: 4.76837103136063e-09\n",
      "step: 29584, loss: 6.198877855467799e-08\n",
      "step: 29585, loss: 0.0\n",
      "step: 29586, loss: 4.76837147544984e-09\n",
      "step: 29587, loss: 4.76837147544984e-09\n",
      "step: 29588, loss: 0.0\n",
      "step: 29589, loss: 1.9311812593514333e-07\n",
      "step: 29590, loss: 2.1457660537294032e-08\n",
      "step: 29591, loss: 0.0\n",
      "step: 29592, loss: 0.0\n",
      "step: 29593, loss: 0.0\n",
      "step: 29594, loss: 2.38418573772492e-09\n",
      "step: 29595, loss: 0.0\n",
      "step: 29596, loss: 2.1457660537294032e-08\n",
      "step: 29597, loss: 0.0\n",
      "step: 29598, loss: 7.15255676908555e-09\n",
      "step: 29599, loss: 2.145766764272139e-08\n",
      "step: 29600, loss: 0.0\n",
      "step: 29601, loss: 3.3378576347331546e-08\n",
      "step: 29602, loss: 0.0\n",
      "step: 29603, loss: 0.0\n",
      "step: 29604, loss: 7.15255676908555e-09\n",
      "step: 29605, loss: 2.1457660537294032e-08\n",
      "step: 29606, loss: 1.8626450382086546e-09\n",
      "step: 29607, loss: 2.38418573772492e-09\n",
      "step: 29608, loss: 1.192092646817855e-08\n",
      "step: 29609, loss: 2.38418573772492e-09\n",
      "step: 29610, loss: 4.76837147544984e-09\n",
      "step: 29611, loss: 1.192092558000013e-08\n",
      "step: 29612, loss: 4.76837103136063e-09\n",
      "step: 29613, loss: 9.055489499587566e-06\n",
      "step: 29614, loss: 2.38418573772492e-09\n",
      "step: 29615, loss: 7.15255632499634e-09\n",
      "step: 29616, loss: 9.53674295089968e-09\n",
      "step: 29617, loss: 4.76837147544984e-09\n",
      "step: 29618, loss: 1.382822887308066e-07\n",
      "step: 29619, loss: 2.38418573772492e-09\n",
      "step: 29620, loss: 4.5299508144580614e-08\n",
      "step: 29621, loss: 0.14878179132938385\n",
      "step: 29622, loss: 0.0\n",
      "step: 29623, loss: 2.38418573772492e-09\n",
      "step: 29624, loss: 0.0\n",
      "step: 29625, loss: 2.38418573772492e-09\n",
      "step: 29626, loss: 5.722041507283393e-08\n",
      "step: 29627, loss: 2.38418573772492e-09\n",
      "step: 29628, loss: 0.0\n",
      "step: 29629, loss: 0.0\n",
      "step: 29630, loss: 1.430510998545742e-08\n",
      "step: 29631, loss: 0.0004352082614786923\n",
      "step: 29632, loss: 0.0\n",
      "step: 29633, loss: 0.057215623557567596\n",
      "step: 29634, loss: 2.38418573772492e-09\n",
      "step: 29635, loss: 1.9073478796372e-08\n",
      "step: 29636, loss: 0.0\n",
      "step: 29637, loss: 1.192092824453539e-08\n",
      "step: 29638, loss: 3.099439993548003e-08\n",
      "step: 29639, loss: 2.38418573772492e-09\n",
      "step: 29640, loss: 2.38418573772492e-09\n",
      "step: 29641, loss: 0.0\n",
      "step: 29642, loss: 2.384184938364342e-08\n",
      "step: 29643, loss: 4.76837103136063e-09\n",
      "step: 29644, loss: 0.0013639454264193773\n",
      "step: 29645, loss: 1.0251982729414522e-07\n",
      "step: 29646, loss: 0.0\n",
      "step: 29647, loss: 0.0005507910391315818\n",
      "step: 29648, loss: 0.14817951619625092\n",
      "step: 29649, loss: 0.0\n",
      "step: 29650, loss: 0.0\n",
      "step: 29651, loss: 1.907347702001516e-08\n",
      "step: 29652, loss: 0.0\n",
      "step: 29653, loss: 1.192092558000013e-08\n",
      "step: 29654, loss: 4.76837147544984e-09\n",
      "step: 29655, loss: 2.38418573772492e-09\n",
      "step: 29656, loss: 0.0\n",
      "step: 29657, loss: 0.0\n",
      "step: 29658, loss: 0.0\n",
      "step: 29659, loss: 0.0\n",
      "step: 29660, loss: 2.6226025795494934e-08\n",
      "step: 29661, loss: 0.0\n",
      "step: 29662, loss: 0.0\n",
      "step: 29663, loss: 2.38418573772492e-09\n",
      "step: 29664, loss: 1.668929527909313e-08\n",
      "step: 29665, loss: 7.15255676908555e-09\n",
      "step: 29666, loss: 2.145766764272139e-08\n",
      "step: 29667, loss: 1.668929350273629e-08\n",
      "step: 29668, loss: 0.0\n",
      "step: 29669, loss: 0.0\n",
      "step: 29670, loss: 4.76837147544984e-09\n",
      "step: 29671, loss: 2.38418573772492e-09\n",
      "step: 29672, loss: 1.907347702001516e-08\n",
      "step: 29673, loss: 9.53674206272126e-09\n",
      "step: 29674, loss: 1.430510998545742e-08\n",
      "step: 29675, loss: 4.76837103136063e-09\n",
      "step: 29676, loss: 2.8610209312773804e-08\n",
      "step: 29677, loss: 7.15255676908555e-09\n",
      "step: 29678, loss: 0.0\n",
      "step: 29679, loss: 7.15255632499634e-09\n",
      "step: 29680, loss: 0.0\n",
      "step: 29681, loss: 0.0\n",
      "step: 29682, loss: 4.76837147544984e-09\n",
      "step: 29683, loss: 8.27295366434555e-07\n",
      "step: 29684, loss: 1.430510998545742e-08\n",
      "step: 29685, loss: 0.0\n",
      "step: 29686, loss: 9.53674117454284e-09\n",
      "step: 29687, loss: 1.668929883180681e-08\n",
      "step: 29688, loss: 1.668929883180681e-08\n",
      "step: 29689, loss: 0.0\n",
      "step: 29690, loss: 1.6879323538887547e-06\n",
      "step: 29691, loss: 0.0\n",
      "step: 29692, loss: 0.0\n",
      "step: 29693, loss: 0.0\n",
      "step: 29694, loss: 7.15255676908555e-09\n",
      "step: 29695, loss: 4.1246062210120726e-07\n",
      "step: 29696, loss: 4.2915317521874385e-08\n",
      "step: 29697, loss: 2.38418573772492e-09\n",
      "step: 29698, loss: 0.0\n",
      "step: 29699, loss: 0.0\n",
      "step: 29700, loss: 1.192092824453539e-08\n",
      "step: 29701, loss: 1.668929527909313e-08\n",
      "step: 29702, loss: 4.52994939337259e-08\n",
      "step: 29703, loss: 9.53674117454284e-09\n",
      "step: 29704, loss: 0.0\n",
      "step: 29705, loss: 0.0\n",
      "step: 29706, loss: 6.914133621194196e-08\n",
      "step: 29707, loss: 1.668929350273629e-08\n",
      "step: 29708, loss: 0.07871221750974655\n",
      "step: 29709, loss: 4.76837147544984e-09\n",
      "step: 29710, loss: 1.907347702001516e-08\n",
      "step: 29711, loss: 2.38418573772492e-09\n",
      "step: 29712, loss: 0.0\n",
      "step: 29713, loss: 0.05623192340135574\n",
      "step: 29714, loss: 0.0\n",
      "step: 29715, loss: 4.76837147544984e-09\n",
      "step: 29716, loss: 0.0\n",
      "step: 29717, loss: 1.430511264999268e-08\n",
      "step: 29718, loss: 0.0\n",
      "step: 29719, loss: 6.198873592211385e-08\n",
      "step: 29720, loss: 2.38418573772492e-09\n",
      "step: 29721, loss: 0.0\n",
      "step: 29722, loss: 2.38418573772492e-09\n",
      "step: 29723, loss: 4.76837103136063e-09\n",
      "step: 29724, loss: 2.38418573772492e-09\n",
      "step: 29725, loss: 0.0\n",
      "step: 29726, loss: 0.0\n",
      "step: 29727, loss: 7.15255676908555e-09\n",
      "step: 29728, loss: 0.0\n",
      "step: 29729, loss: 0.0\n",
      "step: 29730, loss: 2.1457660537294032e-08\n",
      "step: 29731, loss: 1.8596601591980289e-07\n",
      "step: 29732, loss: 3.576275631189674e-08\n",
      "step: 29733, loss: 1.907347702001516e-08\n",
      "step: 29734, loss: 7.15255676908555e-09\n",
      "step: 29735, loss: 2.38418573772492e-09\n",
      "step: 29736, loss: 4.529947972287118e-08\n",
      "step: 29737, loss: 7.15255676908555e-09\n",
      "step: 29738, loss: 0.0\n",
      "step: 29739, loss: 0.0\n",
      "step: 29740, loss: 2.38418573772492e-09\n",
      "step: 29741, loss: 0.0\n",
      "step: 29742, loss: 1.6656145817250945e-05\n",
      "step: 29743, loss: 2.38418573772492e-09\n",
      "step: 29744, loss: 2.38418573772492e-09\n",
      "step: 29745, loss: 0.0\n",
      "step: 29746, loss: 1.192092646817855e-08\n",
      "step: 29747, loss: 7.15255676908555e-09\n",
      "step: 29748, loss: 2.38418573772492e-09\n",
      "step: 29749, loss: 1.0013567930400313e-07\n",
      "step: 29750, loss: 4.76837147544984e-09\n",
      "step: 29751, loss: 0.0\n",
      "step: 29752, loss: 3.447236167630763e-06\n",
      "step: 29753, loss: 4.529947972287118e-08\n",
      "step: 29754, loss: 1.192092558000013e-08\n",
      "step: 29755, loss: 0.08379421383142471\n",
      "step: 29756, loss: 2.38418573772492e-09\n",
      "step: 29757, loss: 0.0\n",
      "step: 29758, loss: 0.0\n",
      "step: 29759, loss: 7.15255676908555e-09\n",
      "step: 29760, loss: 0.0\n",
      "step: 29761, loss: 2.38418573772492e-09\n",
      "step: 29762, loss: 0.06876115500926971\n",
      "step: 29763, loss: 2.38418573772492e-09\n",
      "step: 29764, loss: 0.0\n",
      "step: 29765, loss: 1.668929350273629e-08\n",
      "step: 29766, loss: 2.050389582564094e-07\n",
      "step: 29767, loss: 1.0967228547542618e-07\n",
      "step: 29768, loss: 0.0\n",
      "step: 29769, loss: 0.0\n",
      "step: 29770, loss: 4.76837103136063e-09\n",
      "step: 29771, loss: 0.0002208346559200436\n",
      "step: 29772, loss: 0.0\n",
      "step: 29773, loss: 0.0\n",
      "step: 29774, loss: 0.0\n",
      "step: 29775, loss: 0.0\n",
      "step: 29776, loss: 2.38418573772492e-09\n",
      "step: 29777, loss: 0.0\n",
      "step: 29778, loss: 2.38418573772492e-09\n",
      "step: 29779, loss: 3.5762759864610416e-08\n",
      "step: 29780, loss: 7.15255676908555e-09\n",
      "step: 29781, loss: 2.384185116000026e-08\n",
      "step: 29782, loss: 2.14575635482106e-07\n",
      "step: 29783, loss: 2.38418573772492e-09\n",
      "step: 29784, loss: 3.26630811287032e-07\n",
      "step: 29785, loss: 1.192092558000013e-08\n",
      "step: 29786, loss: 3.3378579900045224e-08\n",
      "step: 29787, loss: 0.0\n",
      "step: 29788, loss: 2.3841844054572903e-08\n",
      "step: 29789, loss: 3.8146957592744e-08\n",
      "step: 29790, loss: 9.298302927618352e-08\n",
      "step: 29791, loss: 0.0\n",
      "step: 29792, loss: 0.0\n",
      "step: 29793, loss: 0.18189691007137299\n",
      "step: 29794, loss: 2.8610211089130644e-08\n",
      "step: 29795, loss: 0.0\n",
      "step: 29796, loss: 9.53674117454284e-09\n",
      "step: 29797, loss: 2.38418573772492e-09\n",
      "step: 29798, loss: 2.38418573772492e-09\n",
      "step: 29799, loss: 4.76837147544984e-09\n",
      "step: 29800, loss: 0.06390196084976196\n",
      "step: 29801, loss: 0.08956986665725708\n",
      "step: 29802, loss: 0.0\n",
      "step: 29803, loss: 2.38418573772492e-09\n",
      "step: 29804, loss: 8.583063504374877e-08\n",
      "step: 29805, loss: 2.38418573772492e-09\n",
      "step: 29806, loss: 0.0\n",
      "step: 29807, loss: 0.0\n",
      "step: 29808, loss: 6.914129357937782e-08\n",
      "step: 29809, loss: 0.0\n",
      "step: 29810, loss: 0.0\n",
      "step: 29811, loss: 2.145766231365087e-08\n",
      "step: 29812, loss: 3.5762759864610416e-08\n",
      "step: 29813, loss: 4.76837103136063e-09\n",
      "step: 29814, loss: 2.193439456732449e-07\n",
      "step: 29815, loss: 6.437291233396536e-08\n",
      "step: 29816, loss: 2.38418573772492e-09\n",
      "step: 29817, loss: 9.53674117454284e-09\n",
      "step: 29818, loss: 6.437291943939272e-08\n",
      "step: 29819, loss: 1.955023236632769e-07\n",
      "step: 29820, loss: 1.0013558693344748e-07\n",
      "step: 29821, loss: 2.6226029348208613e-08\n",
      "step: 29822, loss: 2.2411288114199124e-07\n",
      "step: 29823, loss: 2.38418573772492e-09\n",
      "step: 29824, loss: 1.37562790314405e-06\n",
      "step: 29825, loss: 0.08958116173744202\n",
      "step: 29826, loss: 4.291530331101967e-08\n",
      "step: 29827, loss: 0.0\n",
      "step: 29828, loss: 0.0\n",
      "step: 29829, loss: 4.76837103136063e-09\n",
      "step: 29830, loss: 0.0\n",
      "step: 29831, loss: 2.145766586636455e-08\n",
      "step: 29832, loss: 3.725290076417309e-09\n",
      "step: 29833, loss: 0.07785556465387344\n",
      "step: 29834, loss: 0.0\n",
      "step: 29835, loss: 4.76837147544984e-09\n",
      "step: 29836, loss: 3.3378579900045224e-08\n",
      "step: 29837, loss: 7.15255676908555e-09\n",
      "step: 29838, loss: 3.0250588679336943e-05\n",
      "step: 29839, loss: 1.668929350273629e-08\n",
      "step: 29840, loss: 0.0\n",
      "step: 29841, loss: 1.668929883180681e-08\n",
      "step: 29842, loss: 3.337859055818626e-08\n",
      "step: 29843, loss: 0.0\n",
      "step: 29844, loss: 6.57688769933884e-06\n",
      "step: 29845, loss: 2.622603290092229e-08\n",
      "step: 29846, loss: 7.390966061393556e-08\n",
      "step: 29847, loss: 7.15255632499634e-09\n",
      "step: 29848, loss: 7.15255676908555e-09\n",
      "step: 29849, loss: 1.430511264999268e-08\n",
      "step: 29850, loss: 1.668929527909313e-08\n",
      "step: 29851, loss: 2.145766231365087e-08\n",
      "step: 29852, loss: 0.0\n",
      "step: 29853, loss: 0.0\n",
      "step: 29854, loss: 0.005144515074789524\n",
      "step: 29855, loss: 0.0\n",
      "step: 29856, loss: 4.76837147544984e-09\n",
      "step: 29857, loss: 5.793517061647435e-07\n",
      "step: 29858, loss: 2.3841845830929742e-08\n",
      "step: 29859, loss: 3.0994396382766354e-08\n",
      "step: 29860, loss: 7.15255676908555e-09\n",
      "step: 29861, loss: 0.0\n",
      "step: 29862, loss: 9.53674295089968e-09\n",
      "step: 29863, loss: 1.668929527909313e-08\n",
      "step: 29864, loss: 0.0\n",
      "step: 29865, loss: 3.814693627646193e-08\n",
      "step: 29866, loss: 0.0\n",
      "step: 29867, loss: 0.0\n",
      "step: 29868, loss: 6.198879276553271e-08\n",
      "step: 29869, loss: 2.551062721067865e-07\n",
      "step: 29870, loss: 2.38418573772492e-09\n",
      "step: 29871, loss: 2.38418573772492e-09\n",
      "step: 29872, loss: 2.38418573772492e-09\n",
      "step: 29873, loss: 2.38418573772492e-09\n",
      "step: 29874, loss: 2.6225887950204196e-07\n",
      "step: 29875, loss: 3.337858700547258e-08\n",
      "step: 29876, loss: 4.76837103136063e-09\n",
      "step: 29877, loss: 7.15255676908555e-09\n",
      "step: 29878, loss: 0.0\n",
      "step: 29879, loss: 0.0\n",
      "step: 29880, loss: 0.0\n",
      "step: 29881, loss: 4.76837103136063e-09\n",
      "step: 29882, loss: 1.0967228547542618e-07\n",
      "step: 29883, loss: 2.38418573772492e-09\n",
      "step: 29884, loss: 0.0\n",
      "step: 29885, loss: 4.76837147544984e-09\n",
      "step: 29886, loss: 0.0\n",
      "step: 29887, loss: 0.0\n",
      "step: 29888, loss: 0.0\n",
      "step: 29889, loss: 7.15255632499634e-09\n",
      "step: 29890, loss: 1.430511176181426e-08\n",
      "step: 29891, loss: 3.88822945751599e-06\n",
      "step: 29892, loss: 3.337834755257063e-07\n",
      "step: 29893, loss: 4.76837103136063e-09\n",
      "step: 29894, loss: 7.462362532351108e-07\n",
      "step: 29895, loss: 7.15255632499634e-09\n",
      "step: 29896, loss: 0.0\n",
      "step: 29897, loss: 9.53674117454284e-09\n",
      "step: 29898, loss: 0.05936133861541748\n",
      "step: 29899, loss: 0.0\n",
      "step: 29900, loss: 0.0002153609093511477\n",
      "step: 29901, loss: 2.6226029348208613e-08\n",
      "step: 29902, loss: 0.0\n",
      "step: 29903, loss: 7.15255676908555e-09\n",
      "step: 29904, loss: 1.192092558000013e-08\n",
      "step: 29905, loss: 7.15255632499634e-09\n",
      "step: 29906, loss: 2.145766764272139e-08\n",
      "step: 29907, loss: 4.76837147544984e-09\n",
      "step: 29908, loss: 0.0\n",
      "step: 29909, loss: 0.0\n",
      "step: 29910, loss: 2.38418573772492e-09\n",
      "step: 29911, loss: 2.38418573772492e-09\n",
      "step: 29912, loss: 2.38418573772492e-09\n",
      "step: 29913, loss: 0.0\n",
      "step: 29914, loss: 3.3378579900045224e-08\n",
      "step: 29915, loss: 1.668929350273629e-08\n",
      "step: 29916, loss: 4.76837147544984e-09\n",
      "step: 29917, loss: 0.0\n",
      "step: 29918, loss: 4.839839107262378e-07\n",
      "step: 29919, loss: 9.53674295089968e-09\n",
      "step: 29920, loss: 4.76837103136063e-09\n",
      "step: 29921, loss: 0.0\n",
      "step: 29922, loss: 2.38418573772492e-09\n",
      "step: 29923, loss: 1.430511264999268e-08\n",
      "step: 29924, loss: 0.032913561910390854\n",
      "step: 29925, loss: 7.15255676908555e-09\n",
      "step: 29926, loss: 5.826119831908727e-06\n",
      "step: 29927, loss: 1.907347702001516e-08\n",
      "step: 29928, loss: 4.768367034557741e-08\n",
      "step: 29929, loss: 2.145766586636455e-08\n",
      "step: 29930, loss: 1.7166068744245422e-07\n",
      "step: 29931, loss: 1.668929705544997e-08\n",
      "step: 29932, loss: 2.38418573772492e-09\n",
      "step: 29933, loss: 2.3932438125484623e-05\n",
      "step: 29934, loss: 4.52994939337259e-08\n",
      "step: 29935, loss: 1.3828241662849905e-07\n",
      "step: 29936, loss: 9.059888128604143e-08\n",
      "step: 29937, loss: 4.76837147544984e-09\n",
      "step: 29938, loss: 0.0\n",
      "step: 29939, loss: 6.198877855467799e-08\n",
      "step: 29940, loss: 0.0\n",
      "step: 29941, loss: 0.0\n",
      "step: 29942, loss: 1.8834978732229501e-07\n",
      "step: 29943, loss: 2.145766764272139e-08\n",
      "step: 29944, loss: 9.53674206272126e-09\n",
      "step: 29945, loss: 3.814696469817136e-08\n",
      "step: 29946, loss: 2.38418573772492e-09\n",
      "step: 29947, loss: 3.075581958000839e-07\n",
      "step: 29948, loss: 0.0\n",
      "step: 29949, loss: 7.152549841293876e-08\n",
      "step: 29950, loss: 4.76837147544984e-09\n",
      "step: 29951, loss: 2.145766764272139e-08\n",
      "step: 29952, loss: 0.0\n",
      "step: 29953, loss: 4.2915317521874385e-08\n",
      "step: 29954, loss: 0.0955510213971138\n",
      "step: 29955, loss: 0.06609319895505905\n",
      "step: 29956, loss: 2.38418573772492e-09\n",
      "step: 29957, loss: 1.430510998545742e-08\n",
      "step: 29958, loss: 4.76837103136063e-09\n",
      "step: 29959, loss: 2.38418573772492e-09\n",
      "step: 29960, loss: 1.430511264999268e-08\n",
      "step: 29961, loss: 0.14144915342330933\n",
      "step: 29962, loss: 0.0\n",
      "step: 29963, loss: 2.38418573772492e-09\n",
      "step: 29964, loss: 8.106216142778067e-08\n",
      "step: 29965, loss: 0.07506243139505386\n",
      "step: 29966, loss: 0.0\n",
      "step: 29967, loss: 2.38418573772492e-09\n",
      "step: 29968, loss: 0.0\n",
      "step: 29969, loss: 0.0\n",
      "step: 29970, loss: 7.15255632499634e-09\n",
      "step: 29971, loss: 9.372446584166028e-06\n",
      "step: 29972, loss: 2.38418573772492e-09\n",
      "step: 29973, loss: 4.529947972287118e-08\n",
      "step: 29974, loss: 0.0\n",
      "step: 29975, loss: 5.006783965200157e-08\n",
      "step: 29976, loss: 2.38418573772492e-09\n",
      "step: 29977, loss: 0.0\n",
      "step: 29978, loss: 4.76837147544984e-09\n",
      "step: 29979, loss: 0.0679750144481659\n",
      "step: 29980, loss: 2.3841844054572903e-08\n",
      "step: 29981, loss: 0.0\n",
      "step: 29982, loss: 2.38418573772492e-09\n",
      "step: 29983, loss: 1.192092646817855e-08\n",
      "step: 29984, loss: 0.0\n",
      "step: 29985, loss: 2.38418573772492e-09\n",
      "step: 29986, loss: 0.146047443151474\n",
      "step: 29987, loss: 1.430510998545742e-08\n",
      "step: 29988, loss: 0.0\n",
      "step: 29989, loss: 0.0\n",
      "step: 29990, loss: 2.074230849302694e-07\n",
      "step: 29991, loss: 2.38418573772492e-09\n",
      "step: 29992, loss: 6.675714558923573e-08\n",
      "step: 29993, loss: 1.192092558000013e-08\n",
      "step: 29994, loss: 0.0\n",
      "step: 29995, loss: 0.0\n",
      "step: 29996, loss: 1.430511176181426e-08\n",
      "step: 29997, loss: 0.0\n",
      "step: 29998, loss: 1.907348234908568e-08\n",
      "step: 29999, loss: 1.430510998545742e-08\n",
      "step: 30000, loss: 1.192092558000013e-08\n",
      "step: 30001, loss: 0.0\n",
      "step: 30002, loss: 0.0\n",
      "step: 30003, loss: 0.0\n",
      "step: 30004, loss: 4.76837147544984e-09\n",
      "step: 30005, loss: 0.0\n",
      "step: 30006, loss: 0.05243559181690216\n",
      "step: 30007, loss: 4.76837103136063e-09\n",
      "step: 30008, loss: 2.38418573772492e-09\n",
      "step: 30009, loss: 2.38418573772492e-09\n",
      "step: 30010, loss: 0.0\n",
      "step: 30011, loss: 6.437291233396536e-08\n",
      "step: 30012, loss: 0.03825938329100609\n",
      "step: 30013, loss: 5.722039020383818e-08\n",
      "step: 30014, loss: 0.0\n",
      "step: 30015, loss: 2.38418573772492e-09\n",
      "step: 30016, loss: 0.0\n",
      "step: 30017, loss: 1.5020316368463682e-07\n",
      "step: 30018, loss: 0.03294268995523453\n",
      "step: 30019, loss: 6.437291233396536e-08\n",
      "step: 30020, loss: 7.15255632499634e-09\n",
      "step: 30021, loss: 4.76837147544984e-09\n",
      "step: 30022, loss: 4.2915321074588064e-08\n",
      "step: 30023, loss: 4.291530686373335e-08\n",
      "step: 30024, loss: 1.192092558000013e-08\n",
      "step: 30025, loss: 1.907347702001516e-08\n",
      "step: 30026, loss: 7.748453754174989e-07\n",
      "step: 30027, loss: 0.0\n",
      "step: 30028, loss: 0.0\n",
      "step: 30029, loss: 9.53674117454284e-09\n",
      "step: 30030, loss: 2.38418573772492e-09\n",
      "step: 30031, loss: 0.0\n",
      "step: 30032, loss: 5.245203738013515e-08\n",
      "step: 30033, loss: 4.529947972287118e-08\n",
      "step: 30034, loss: 4.76837147544984e-09\n",
      "step: 30035, loss: 4.76837103136063e-09\n",
      "step: 30036, loss: 4.76837147544984e-09\n",
      "step: 30037, loss: 1.0728809485271995e-07\n",
      "step: 30038, loss: 1.192092646817855e-08\n",
      "step: 30039, loss: 0.05772330239415169\n",
      "step: 30040, loss: 0.0\n",
      "step: 30041, loss: 1.729939140204806e-05\n",
      "step: 30042, loss: 2.1457660537294032e-08\n",
      "step: 30043, loss: 4.76837103136063e-09\n",
      "step: 30044, loss: 2.38418573772492e-09\n",
      "step: 30045, loss: 0.0\n",
      "step: 30046, loss: 0.0\n",
      "step: 30047, loss: 2.384184938364342e-08\n",
      "step: 30048, loss: 0.0\n",
      "step: 30049, loss: 1.2159320306182053e-07\n",
      "step: 30050, loss: 9.53674206272126e-09\n",
      "step: 30051, loss: 1.668929527909313e-08\n",
      "step: 30052, loss: 2.1457660537294032e-08\n",
      "step: 30053, loss: 0.0215340256690979\n",
      "step: 30054, loss: 2.38418573772492e-09\n",
      "step: 30055, loss: 3.337839302730572e-07\n",
      "step: 30056, loss: 0.0\n",
      "step: 30057, loss: 0.0\n",
      "step: 30058, loss: 2.887073549118213e-07\n",
      "step: 30059, loss: 3.923985332221491e-06\n",
      "step: 30060, loss: 7.15255632499634e-09\n",
      "step: 30061, loss: 0.0\n",
      "step: 30062, loss: 4.76837103136063e-09\n",
      "step: 30063, loss: 0.0005253009730949998\n",
      "step: 30064, loss: 3.5934510378865525e-05\n",
      "step: 30065, loss: 2.38418573772492e-09\n",
      "step: 30066, loss: 4.053077589105669e-07\n",
      "step: 30067, loss: 0.0\n",
      "step: 30068, loss: 0.0\n",
      "step: 30069, loss: 1.2041281479469035e-05\n",
      "step: 30070, loss: 5.245204803827619e-08\n",
      "step: 30071, loss: 0.0\n",
      "step: 30072, loss: 5.245203738013515e-08\n",
      "step: 30073, loss: 2.1457570653637958e-07\n",
      "step: 30074, loss: 4.76837147544984e-09\n",
      "step: 30075, loss: 9.05989878674518e-08\n",
      "step: 30076, loss: 1.382822887308066e-07\n",
      "step: 30077, loss: 3.0205601433408447e-06\n",
      "step: 30078, loss: 2.38418573772492e-09\n",
      "step: 30079, loss: 1.8347427612752654e-05\n",
      "step: 30080, loss: 1.5401249129354255e-06\n",
      "step: 30081, loss: 0.15534304082393646\n",
      "step: 30082, loss: 0.08391260355710983\n",
      "step: 30083, loss: 2.6820414404937765e-06\n",
      "step: 30084, loss: 0.0\n",
      "step: 30085, loss: 0.0031927525997161865\n",
      "step: 30086, loss: 4.5795109144819435e-06\n",
      "step: 30087, loss: 5.102095883557922e-07\n",
      "step: 30088, loss: 4.887531872554973e-07\n",
      "step: 30089, loss: 2.38418573772492e-09\n",
      "step: 30090, loss: 8.183234967873432e-06\n",
      "step: 30091, loss: 3.9669288526056334e-05\n",
      "step: 30092, loss: 0.03086913749575615\n",
      "step: 30093, loss: 0.00035161376581527293\n",
      "step: 30094, loss: 1.0307698175893165e-05\n",
      "step: 30095, loss: 2.8610209312773804e-08\n",
      "step: 30096, loss: 6.079582703932829e-07\n",
      "step: 30097, loss: 1.6497884871569113e-06\n",
      "step: 30098, loss: 0.0\n",
      "step: 30099, loss: 4.291530686373335e-08\n",
      "step: 30100, loss: 0.0005137749831192195\n",
      "step: 30101, loss: 0.0615495927631855\n",
      "step: 30102, loss: 2.76563639545202e-07\n",
      "step: 30103, loss: 2.38418573772492e-09\n",
      "step: 30104, loss: 1.192092558000013e-08\n",
      "step: 30105, loss: 0.0015005826717242599\n",
      "step: 30106, loss: 3.4019924441963667e-06\n",
      "step: 30107, loss: 4.76837103136063e-09\n",
      "step: 30108, loss: 0.0\n",
      "step: 30109, loss: 3.0994389277338996e-08\n",
      "step: 30110, loss: 3.1006187782622874e-05\n",
      "step: 30111, loss: 8.988178024083027e-07\n",
      "step: 30112, loss: 2.38418573772492e-09\n",
      "step: 30113, loss: 2.5700126116134925e-06\n",
      "step: 30114, loss: 2.38418573772492e-09\n",
      "step: 30115, loss: 9.860948921414092e-06\n",
      "step: 30116, loss: 5.722041507283393e-08\n",
      "step: 30117, loss: 0.0\n",
      "step: 30118, loss: 2.145766764272139e-08\n",
      "step: 30119, loss: 4.768368100371845e-08\n",
      "step: 30120, loss: 9.059893102403294e-08\n",
      "step: 30121, loss: 8.344633073420482e-08\n",
      "step: 30122, loss: 4.76837103136063e-09\n",
      "step: 30123, loss: 4.76837147544984e-09\n",
      "step: 30124, loss: 5.5789473663026e-07\n",
      "step: 30125, loss: 5.721973934669222e-07\n",
      "step: 30126, loss: 5.139644599694293e-06\n",
      "step: 30127, loss: 7.319382575587952e-07\n",
      "step: 30128, loss: 1.78091704583494e-06\n",
      "step: 30129, loss: 2.38418573772492e-09\n",
      "step: 30130, loss: 2.6371066269348375e-05\n",
      "step: 30131, loss: 1.7404482832716894e-07\n",
      "step: 30132, loss: 1.192092558000013e-08\n",
      "step: 30133, loss: 0.0\n",
      "step: 30134, loss: 8.583051425148369e-08\n",
      "step: 30135, loss: 2.38418573772492e-09\n",
      "step: 30136, loss: 1.192092646817855e-08\n",
      "step: 30137, loss: 0.0506582111120224\n",
      "step: 30138, loss: 1.7189335039802245e-06\n",
      "step: 30139, loss: 6.692880560876802e-05\n",
      "step: 30140, loss: 2.38418573772492e-09\n",
      "step: 30141, loss: 0.07962948083877563\n",
      "step: 30142, loss: 2.38418573772492e-09\n",
      "step: 30143, loss: 1.935865157065564e-06\n",
      "step: 30144, loss: 0.0\n",
      "step: 30145, loss: 7.629380860407764e-08\n",
      "step: 30146, loss: 7.15255676908555e-09\n",
      "step: 30147, loss: 7.15255632499634e-09\n",
      "step: 30148, loss: 3.7193080970610026e-07\n",
      "step: 30149, loss: 1.430511264999268e-08\n",
      "step: 30150, loss: 1.668929705544997e-08\n",
      "step: 30151, loss: 7.867805607020273e-08\n",
      "step: 30152, loss: 2.612917796795955e-06\n",
      "step: 30153, loss: 2.8610211089130644e-08\n",
      "step: 30154, loss: 7.867802054306594e-08\n",
      "step: 30155, loss: 2.38418573772492e-09\n",
      "step: 30156, loss: 7.15255632499634e-09\n",
      "step: 30157, loss: 0.0\n",
      "step: 30158, loss: 0.0\n",
      "step: 30159, loss: 0.0\n",
      "step: 30160, loss: 4.768368100371845e-08\n",
      "step: 30161, loss: 4.29148826697201e-07\n",
      "step: 30162, loss: 5.245201961656676e-08\n",
      "step: 30163, loss: 4.76837103136063e-09\n",
      "step: 30164, loss: 4.053112334645448e-08\n",
      "step: 30165, loss: 1.668929527909313e-08\n",
      "step: 30166, loss: 6.723291221533145e-07\n",
      "step: 30167, loss: 2.574903987806465e-07\n",
      "step: 30168, loss: 0.08138822764158249\n",
      "step: 30169, loss: 0.0\n",
      "step: 30170, loss: 4.76837147544984e-09\n",
      "step: 30171, loss: 9.99921394395642e-06\n",
      "step: 30172, loss: 0.0\n",
      "step: 30173, loss: 0.0\n",
      "step: 30174, loss: 2.145766764272139e-08\n",
      "step: 30175, loss: 2.8371744065225357e-07\n",
      "step: 30176, loss: 0.0\n",
      "step: 30177, loss: 1.668929527909313e-08\n",
      "step: 30178, loss: 1.430511176181426e-08\n",
      "step: 30179, loss: 0.0\n",
      "step: 30180, loss: 0.0\n",
      "step: 30181, loss: 1.1682475786756186e-07\n",
      "step: 30182, loss: 2.3459092517441604e-06\n",
      "step: 30183, loss: 0.00038948291330598295\n",
      "step: 30184, loss: 0.00011844314576592296\n",
      "step: 30185, loss: 4.76837103136063e-09\n",
      "step: 30186, loss: 1.0132658871953026e-06\n",
      "step: 30187, loss: 3.33785834527589e-08\n",
      "step: 30188, loss: 1.907347702001516e-08\n",
      "step: 30189, loss: 0.20406435430049896\n",
      "step: 30190, loss: 7.15255676908555e-09\n",
      "step: 30191, loss: 2.38418573772492e-09\n",
      "step: 30192, loss: 7.295475370483473e-07\n",
      "step: 30193, loss: 7.15255632499634e-09\n",
      "step: 30194, loss: 0.0\n",
      "step: 30195, loss: 0.0006157290772534907\n",
      "step: 30196, loss: 4.5299508144580614e-08\n",
      "step: 30197, loss: 2.694115437407163e-07\n",
      "step: 30198, loss: 2.670275023319846e-07\n",
      "step: 30199, loss: 7.86779779105018e-08\n",
      "step: 30200, loss: 5.006786452099732e-08\n",
      "step: 30201, loss: 0.08786306530237198\n",
      "step: 30202, loss: 5.0067846757428924e-08\n",
      "step: 30203, loss: 2.622587089717854e-07\n",
      "step: 30204, loss: 0.0\n",
      "step: 30205, loss: 2.38418573772492e-09\n",
      "step: 30206, loss: 9.703400110083749e-07\n",
      "step: 30207, loss: 1.3970853842693032e-06\n",
      "step: 30208, loss: 2.3014046746538952e-05\n",
      "step: 30209, loss: 2.38418573772492e-09\n",
      "step: 30210, loss: 6.198877855467799e-08\n",
      "step: 30211, loss: 2.38418573772492e-09\n",
      "step: 30212, loss: 3.983618626079988e-06\n",
      "step: 30213, loss: 6.457721610786393e-06\n",
      "step: 30214, loss: 0.0005230343085713685\n",
      "step: 30215, loss: 2.1266346266202163e-06\n",
      "step: 30216, loss: 8.583057109490255e-08\n",
      "step: 30217, loss: 0.06497080624103546\n",
      "step: 30218, loss: 0.0\n",
      "step: 30219, loss: 9.059888128604143e-08\n",
      "step: 30220, loss: 0.07689306139945984\n",
      "step: 30221, loss: 1.430510998545742e-08\n",
      "step: 30222, loss: 9.775139631074126e-08\n",
      "step: 30223, loss: 7.15255676908555e-09\n",
      "step: 30224, loss: 3.337857279461787e-08\n",
      "step: 30225, loss: 2.38418573772492e-09\n",
      "step: 30226, loss: 1.0223178833257407e-05\n",
      "step: 30227, loss: 5.48362244501277e-08\n",
      "step: 30228, loss: 0.0\n",
      "step: 30229, loss: 3.814695404003032e-08\n",
      "step: 30230, loss: 0.0\n",
      "step: 30231, loss: 4.7683659687436375e-08\n",
      "step: 30232, loss: 8.939691724663135e-06\n",
      "step: 30233, loss: 4.76837103136063e-09\n",
      "step: 30234, loss: 5.006783965200157e-08\n",
      "step: 30235, loss: 3.814693627646193e-08\n",
      "step: 30236, loss: 2.1457660537294032e-08\n",
      "step: 30237, loss: 6.437291943939272e-08\n",
      "step: 30238, loss: 4.743967110698577e-06\n",
      "step: 30239, loss: 0.0\n",
      "step: 30240, loss: 1.1682492839781844e-07\n",
      "step: 30241, loss: 4.76837147544984e-09\n",
      "step: 30242, loss: 1.430511264999268e-08\n",
      "step: 30243, loss: 2.3126531800699013e-07\n",
      "step: 30244, loss: 0.0\n",
      "step: 30245, loss: 0.0\n",
      "step: 30246, loss: 0.0\n",
      "step: 30247, loss: 8.152434747898951e-05\n",
      "step: 30248, loss: 4.76837103136063e-09\n",
      "step: 30249, loss: 2.9563798875642533e-07\n",
      "step: 30250, loss: 4.76837103136063e-09\n",
      "step: 30251, loss: 2.38418573772492e-09\n",
      "step: 30252, loss: 2.5987463914134423e-07\n",
      "step: 30253, loss: 9.53674206272126e-09\n",
      "step: 30254, loss: 0.0023056792560964823\n",
      "step: 30255, loss: 3.3378576347331546e-08\n",
      "step: 30256, loss: 0.0\n",
      "step: 30257, loss: 4.3868541865776933e-07\n",
      "step: 30258, loss: 4.76837103136063e-09\n",
      "step: 30259, loss: 2.302037137269508e-05\n",
      "step: 30260, loss: 2.38418573772492e-09\n",
      "step: 30261, loss: 7.15255676908555e-09\n",
      "step: 30262, loss: 6.348582701320993e-06\n",
      "step: 30263, loss: 1.6259541553154122e-06\n",
      "step: 30264, loss: 3.814693982917561e-08\n",
      "step: 30265, loss: 0.0005507733440026641\n",
      "step: 30266, loss: 9.53674295089968e-09\n",
      "step: 30267, loss: 6.222638830877258e-07\n",
      "step: 30268, loss: 2.2411222744267434e-07\n",
      "step: 30269, loss: 0.0\n",
      "step: 30270, loss: 1.9073402768299275e-07\n",
      "step: 30271, loss: 2.5887880838126875e-05\n",
      "step: 30272, loss: 9.298313585759388e-08\n",
      "step: 30273, loss: 8.749793778406456e-07\n",
      "step: 30274, loss: 8.326815441250801e-05\n",
      "step: 30275, loss: 0.0\n",
      "step: 30276, loss: 0.07879069447517395\n",
      "step: 30277, loss: 6.43729549665295e-08\n",
      "step: 30278, loss: 2.471351581334602e-05\n",
      "step: 30279, loss: 0.0\n",
      "step: 30280, loss: 2.408013983767887e-07\n",
      "step: 30281, loss: 0.0\n",
      "step: 30282, loss: 2.38418573772492e-09\n",
      "step: 30283, loss: 0.17440994083881378\n",
      "step: 30284, loss: 7.4505797087454084e-09\n",
      "step: 30285, loss: 0.0\n",
      "step: 30286, loss: 6.675714558923573e-08\n",
      "step: 30287, loss: 5.793528430331207e-07\n",
      "step: 30288, loss: 4.76837103136063e-09\n",
      "step: 30289, loss: 1.430511176181426e-08\n",
      "step: 30290, loss: 4.76837103136063e-09\n",
      "step: 30291, loss: 2.3841845830929742e-08\n",
      "step: 30292, loss: 4.2915321074588064e-08\n",
      "step: 30293, loss: 0.0\n",
      "step: 30294, loss: 4.053111624102712e-08\n",
      "step: 30295, loss: 2.38418573772492e-09\n",
      "step: 30296, loss: 2.467277954565361e-05\n",
      "step: 30297, loss: 0.0004666919994633645\n",
      "step: 30298, loss: 3.5762759864610416e-08\n",
      "step: 30299, loss: 4.073767922818661e-05\n",
      "step: 30300, loss: 7.15255632499634e-09\n",
      "step: 30301, loss: 6.91412793685231e-08\n",
      "step: 30302, loss: 1.5997246691767941e-06\n",
      "step: 30303, loss: 3.3616734640418144e-07\n",
      "step: 30304, loss: 0.0\n",
      "step: 30305, loss: 8.535204756299208e-07\n",
      "step: 30306, loss: 0.0\n",
      "step: 30307, loss: 2.0193033378745895e-06\n",
      "step: 30308, loss: 1.192092646817855e-08\n",
      "step: 30309, loss: 1.740447999054595e-07\n",
      "step: 30310, loss: 0.0007632734486833215\n",
      "step: 30311, loss: 9.501046588411555e-05\n",
      "step: 30312, loss: 0.07727231830358505\n",
      "step: 30313, loss: 0.0002747371909208596\n",
      "step: 30314, loss: 0.0\n",
      "step: 30315, loss: 1.4378622836375143e-05\n",
      "step: 30316, loss: 3.2186304110837227e-07\n",
      "step: 30317, loss: 0.0\n",
      "step: 30318, loss: 0.0\n",
      "step: 30319, loss: 5.960456661568969e-08\n",
      "step: 30320, loss: 5.96045985901128e-08\n",
      "step: 30321, loss: 0.0164549108594656\n",
      "step: 30322, loss: 0.0\n",
      "step: 30323, loss: 1.430510998545742e-08\n",
      "step: 30324, loss: 2.5510632895020535e-07\n",
      "step: 30325, loss: 0.1906542330980301\n",
      "step: 30326, loss: 3.337857279461787e-08\n",
      "step: 30327, loss: 1.7761625485945842e-06\n",
      "step: 30328, loss: 6.246480666050047e-07\n",
      "step: 30329, loss: 2.38418573772492e-09\n",
      "step: 30330, loss: 9.53674206272126e-09\n",
      "step: 30331, loss: 4.76837103136063e-09\n",
      "step: 30332, loss: 2.38418529363571e-08\n",
      "step: 30333, loss: 1.7642911132043082e-07\n",
      "step: 30334, loss: 5.495681762113236e-05\n",
      "step: 30335, loss: 1.4781932122787111e-07\n",
      "step: 30336, loss: 3.5762763417324095e-08\n",
      "step: 30337, loss: 0.0008666248759254813\n",
      "step: 30338, loss: 1.152432832896011e-05\n",
      "step: 30339, loss: 1.9073395662871917e-07\n",
      "step: 30340, loss: 0.0016869624378159642\n",
      "step: 30341, loss: 2.1508094505406916e-05\n",
      "step: 30342, loss: 5.054438361185021e-07\n",
      "step: 30343, loss: 7.676960649405373e-07\n",
      "step: 30344, loss: 1.0800069958349923e-06\n",
      "step: 30345, loss: 7.748465122858761e-07\n",
      "step: 30346, loss: 9.75906823441619e-06\n",
      "step: 30347, loss: 6.174990403451375e-07\n",
      "step: 30348, loss: 1.0146949534828309e-05\n",
      "step: 30349, loss: 1.0012286111305002e-05\n",
      "step: 30350, loss: 2.3841722907036456e-07\n",
      "step: 30351, loss: 2.145766586636455e-08\n",
      "step: 30352, loss: 2.477029738656711e-06\n",
      "step: 30353, loss: 1.7642915395299497e-07\n",
      "step: 30354, loss: 0.006520207971334457\n",
      "step: 30355, loss: 1.9335277556820074e-06\n",
      "step: 30356, loss: 0.00017062886036001146\n",
      "step: 30357, loss: 0.10575525462627411\n",
      "step: 30358, loss: 0.011320344172418118\n",
      "step: 30359, loss: 0.0\n",
      "step: 30360, loss: 0.0\n",
      "step: 30361, loss: 1.3351402117223188e-07\n",
      "step: 30362, loss: 0.00018440447456669062\n",
      "step: 30363, loss: 1.192092558000013e-08\n",
      "step: 30364, loss: 4.76837103136063e-09\n",
      "step: 30365, loss: 3.0278931717475643e-07\n",
      "step: 30366, loss: 2.3126470694023737e-07\n",
      "step: 30367, loss: 0.0009565472137182951\n",
      "step: 30368, loss: 1.2636144219868584e-07\n",
      "step: 30369, loss: 0.014458447694778442\n",
      "step: 30370, loss: 3.005182406923268e-05\n",
      "step: 30371, loss: 0.0\n",
      "step: 30372, loss: 8.821470487418992e-08\n",
      "step: 30373, loss: 0.08746233582496643\n",
      "step: 30374, loss: 0.000499321089591831\n",
      "step: 30375, loss: 2.7179535777577257e-07\n",
      "step: 30376, loss: 0.20710906386375427\n",
      "step: 30377, loss: 2.6226025795494934e-08\n",
      "step: 30378, loss: 2.38418573772492e-09\n",
      "step: 30379, loss: 1.430510998545742e-08\n",
      "step: 30380, loss: 0.00010658385144779459\n",
      "step: 30381, loss: 8.095074008451775e-06\n",
      "step: 30382, loss: 2.9086857011861866e-07\n",
      "step: 30383, loss: 0.0\n",
      "step: 30384, loss: 2.38418573772492e-09\n",
      "step: 30385, loss: 2.455700212067313e-07\n",
      "step: 30386, loss: 4.419792276166845e-06\n",
      "step: 30387, loss: 8.106215432235331e-08\n",
      "step: 30388, loss: 7.15255632499634e-09\n",
      "step: 30389, loss: 3.337858700547258e-08\n",
      "step: 30390, loss: 0.0\n",
      "step: 30391, loss: 0.0\n",
      "step: 30392, loss: 0.0\n",
      "step: 30393, loss: 2.5573110178811476e-05\n",
      "step: 30394, loss: 1.4877025478199357e-06\n",
      "step: 30395, loss: 1.2725118722300977e-05\n",
      "step: 30396, loss: 0.0\n",
      "step: 30397, loss: 0.00045655862777493894\n",
      "step: 30398, loss: 1.0169574125029612e-05\n",
      "step: 30399, loss: 1.222713490278693e-05\n",
      "step: 30400, loss: 0.0\n",
      "step: 30401, loss: 0.003427092917263508\n",
      "step: 30402, loss: 8.344633783963218e-08\n",
      "step: 30403, loss: 3.814693627646193e-08\n",
      "step: 30404, loss: 1.430510998545742e-08\n",
      "step: 30405, loss: 1.0490398949514201e-07\n",
      "step: 30406, loss: 6.031904717929137e-07\n",
      "step: 30407, loss: 0.0\n",
      "step: 30408, loss: 1.617731322767213e-05\n",
      "step: 30409, loss: 0.0\n",
      "step: 30410, loss: 6.914127226309574e-08\n",
      "step: 30411, loss: 0.08014529198408127\n",
      "step: 30412, loss: 2.38418573772492e-09\n",
      "step: 30413, loss: 0.014069084078073502\n",
      "step: 30414, loss: 0.0\n",
      "step: 30415, loss: 0.0\n",
      "step: 30416, loss: 1.2087471077393275e-06\n",
      "step: 30417, loss: 0.0\n",
      "step: 30418, loss: 9.138880159298424e-06\n",
      "step: 30419, loss: 1.2834914741688408e-05\n",
      "step: 30420, loss: 9.059895234031501e-08\n",
      "step: 30421, loss: 4.6135908633004874e-05\n",
      "step: 30422, loss: 0.0\n",
      "step: 30423, loss: 3.150856719003059e-05\n",
      "step: 30424, loss: 9.677562047727406e-05\n",
      "step: 30425, loss: 0.0009345843573100865\n",
      "step: 30426, loss: 1.192092646817855e-08\n",
      "step: 30427, loss: 0.0\n",
      "step: 30428, loss: 3.978908353019506e-06\n",
      "step: 30429, loss: 0.019366977736353874\n",
      "step: 30430, loss: 0.04826904833316803\n",
      "step: 30431, loss: 0.008269619196653366\n",
      "step: 30432, loss: 2.005010628636228e-06\n",
      "step: 30433, loss: 0.005017981398850679\n",
      "step: 30434, loss: 6.1695859585597645e-06\n",
      "step: 30435, loss: 0.00035165753797627985\n",
      "step: 30436, loss: 0.0\n",
      "step: 30437, loss: 9.53674117454284e-09\n",
      "step: 30438, loss: 0.0\n",
      "step: 30439, loss: 0.0010723917512223125\n",
      "step: 30440, loss: 1.4378942069015466e-05\n",
      "step: 30441, loss: 0.050289515405893326\n",
      "step: 30442, loss: 1.671244945100625e-06\n",
      "step: 30443, loss: 0.0\n",
      "step: 30444, loss: 2.6820700895768823e-06\n",
      "step: 30445, loss: 0.00014400732470676303\n",
      "step: 30446, loss: 1.0967225705371675e-07\n",
      "step: 30447, loss: 0.0\n",
      "step: 30448, loss: 0.0029893487226217985\n",
      "step: 30449, loss: 0.020628560334444046\n",
      "step: 30450, loss: 0.0001582969562150538\n",
      "step: 30451, loss: 0.16508157551288605\n",
      "step: 30452, loss: 6.174948907755606e-07\n",
      "step: 30453, loss: 3.0994389277338996e-08\n",
      "step: 30454, loss: 3.5762759864610416e-08\n",
      "step: 30455, loss: 4.6729729774597217e-07\n",
      "step: 30456, loss: 0.0\n",
      "step: 30457, loss: 0.0\n",
      "step: 30458, loss: 0.033725883811712265\n",
      "step: 30459, loss: 0.8134294748306274\n",
      "step: 30460, loss: 0.00023162976140156388\n",
      "step: 30461, loss: 0.2275579869747162\n",
      "step: 30462, loss: 0.017812011763453484\n",
      "step: 30463, loss: 3.33785834527589e-08\n",
      "step: 30464, loss: 0.0007039945339784026\n",
      "step: 30465, loss: 3.00405162079187e-07\n",
      "step: 30466, loss: 0.0\n",
      "step: 30467, loss: 5.722038309841082e-08\n",
      "step: 30468, loss: 7.152545578037461e-08\n",
      "step: 30469, loss: 0.001431010663509369\n",
      "step: 30470, loss: 1.835819034567976e-07\n",
      "step: 30471, loss: 1.192092824453539e-08\n",
      "step: 30472, loss: 0.08171848952770233\n",
      "step: 30473, loss: 0.006251736078411341\n",
      "step: 30474, loss: 0.012892667204141617\n",
      "step: 30475, loss: 7.200151799224841e-07\n",
      "step: 30476, loss: 0.00011069604806834832\n",
      "step: 30477, loss: 2.2411221323181962e-07\n",
      "step: 30478, loss: 1.5497158756261342e-07\n",
      "step: 30479, loss: 0.07708689570426941\n",
      "step: 30480, loss: 5.483619602841827e-08\n",
      "step: 30481, loss: 0.015530295670032501\n",
      "step: 30482, loss: 0.15617986023426056\n",
      "step: 30483, loss: 0.1359710544347763\n",
      "step: 30484, loss: 1.192092824453539e-08\n",
      "step: 30485, loss: 9.45008359849453e-05\n",
      "step: 30486, loss: 0.014970212243497372\n",
      "step: 30487, loss: 0.0012401173589751124\n",
      "step: 30488, loss: 0.0\n",
      "step: 30489, loss: 0.09011656045913696\n",
      "step: 30490, loss: 2.38418573772492e-09\n",
      "step: 30491, loss: 3.611752163124038e-06\n",
      "step: 30492, loss: 0.05856615677475929\n",
      "step: 30493, loss: 1.430511176181426e-08\n",
      "step: 30494, loss: 1.192092558000013e-08\n",
      "step: 30495, loss: 0.00028451866819523275\n",
      "step: 30496, loss: 9.083647682928131e-07\n",
      "step: 30497, loss: 9.77513892053139e-08\n",
      "step: 30498, loss: 2.89097642962588e-05\n",
      "step: 30499, loss: 0.20128639042377472\n",
      "step: 30500, loss: 0.0002166416379623115\n",
      "step: 30501, loss: 4.172285059667047e-07\n",
      "step: 30502, loss: 6.91412793685231e-08\n",
      "step: 30503, loss: 7.408028614008799e-05\n",
      "step: 30504, loss: 1.668929527909313e-08\n",
      "step: 30505, loss: 3.123259375570342e-07\n",
      "step: 30506, loss: 1.0967224284286203e-07\n",
      "step: 30507, loss: 0.0\n",
      "step: 30508, loss: 0.03468030318617821\n",
      "step: 30509, loss: 0.03981003910303116\n",
      "step: 30510, loss: 4.538549546850845e-05\n",
      "step: 30511, loss: 5.1777810767816845e-06\n",
      "step: 30512, loss: 3.873926743835909e-06\n",
      "step: 30513, loss: 0.0\n",
      "step: 30514, loss: 0.0\n",
      "step: 30515, loss: 0.0002031283947871998\n",
      "step: 30516, loss: 0.057646702975034714\n",
      "step: 30517, loss: 2.6743831767817028e-05\n",
      "step: 30518, loss: 7.15255676908555e-09\n",
      "step: 30519, loss: 0.003939732443541288\n",
      "step: 30520, loss: 0.00015997354057617486\n",
      "step: 30521, loss: 0.0\n",
      "step: 30522, loss: 1.9073478796372e-08\n",
      "step: 30523, loss: 2.553302692831494e-06\n",
      "step: 30524, loss: 1.974008682736894e-06\n",
      "step: 30525, loss: 0.0010439169127494097\n",
      "step: 30526, loss: 4.76837103136063e-09\n",
      "step: 30527, loss: 2.7439953555585817e-05\n",
      "step: 30528, loss: 2.1720061340602115e-05\n",
      "step: 30529, loss: 0.27104100584983826\n",
      "step: 30530, loss: 5.03056867273699e-07\n",
      "step: 30531, loss: 1.5091602563188644e-06\n",
      "step: 30532, loss: 0.09795135259628296\n",
      "step: 30533, loss: 0.0024545681662857533\n",
      "step: 30534, loss: 7.15255632499634e-09\n",
      "step: 30535, loss: 4.76837103136063e-09\n",
      "step: 30536, loss: 4.76837103136063e-09\n",
      "step: 30537, loss: 0.0005986380856484175\n",
      "step: 30538, loss: 0.0\n",
      "step: 30539, loss: 4.6525001380359754e-05\n",
      "step: 30540, loss: 1.263614990421047e-07\n",
      "step: 30541, loss: 0.27447277307510376\n",
      "step: 30542, loss: 1.5139006563913426e-06\n",
      "step: 30543, loss: 6.277739157667384e-05\n",
      "step: 30544, loss: 2.908495616793516e-06\n",
      "step: 30545, loss: 3.29014937960892e-07\n",
      "step: 30546, loss: 5.746921669924632e-05\n",
      "step: 30547, loss: 9.775142473245069e-08\n",
      "step: 30548, loss: 3.5762766970037774e-08\n",
      "step: 30549, loss: 0.0004955872427672148\n",
      "step: 30550, loss: 1.4168969755701255e-05\n",
      "step: 30551, loss: 0.21857821941375732\n",
      "step: 30552, loss: 1.938249170052586e-06\n",
      "step: 30553, loss: 2.5510726686661656e-07\n",
      "step: 30554, loss: 3.814693982917561e-08\n",
      "step: 30555, loss: 0.00038184926961548626\n",
      "step: 30556, loss: 8.058416938183655e-07\n",
      "step: 30557, loss: 9.580379992257804e-05\n",
      "step: 30558, loss: 2.38418573772492e-09\n",
      "step: 30559, loss: 0.1362335979938507\n",
      "step: 30560, loss: 3.073102789130644e-06\n",
      "step: 30561, loss: 1.3828244505020848e-07\n",
      "step: 30562, loss: 8.399294893024489e-05\n",
      "step: 30563, loss: 0.11280753463506699\n",
      "step: 30564, loss: 3.0994396382766354e-08\n",
      "step: 30565, loss: 0.0004188648599665612\n",
      "step: 30566, loss: 2.798839886963833e-06\n",
      "step: 30567, loss: 4.76837147544984e-09\n",
      "step: 30568, loss: 0.0\n",
      "step: 30569, loss: 9.53674206272126e-09\n",
      "step: 30570, loss: 1.7380036752001615e-06\n",
      "step: 30571, loss: 0.027553830295801163\n",
      "step: 30572, loss: 3.004929021699354e-05\n",
      "step: 30573, loss: 0.0\n",
      "step: 30574, loss: 2.3911077732918784e-05\n",
      "step: 30575, loss: 0.14929576218128204\n",
      "step: 30576, loss: 2.38418573772492e-09\n",
      "step: 30577, loss: 0.00018142889894079417\n",
      "step: 30578, loss: 4.5199517444416415e-06\n",
      "step: 30579, loss: 1.7880711311590858e-06\n",
      "step: 30580, loss: 5.188160503166728e-05\n",
      "step: 30581, loss: 7.939225952213746e-07\n",
      "step: 30582, loss: 0.00011215183621970937\n",
      "step: 30583, loss: 0.0\n",
      "step: 30584, loss: 4.028868261229945e-06\n",
      "step: 30585, loss: 3.3616910855016613e-07\n",
      "step: 30586, loss: 8.344633783963218e-08\n",
      "step: 30587, loss: 2.1457660537294032e-08\n",
      "step: 30588, loss: 0.0072700041346251965\n",
      "step: 30589, loss: 0.141044482588768\n",
      "step: 30590, loss: 5.149806270310364e-07\n",
      "step: 30591, loss: 7.15255632499634e-09\n",
      "step: 30592, loss: 2.6439867724548094e-05\n",
      "step: 30593, loss: 6.675709585124423e-08\n",
      "step: 30594, loss: 0.0009317297954112291\n",
      "step: 30595, loss: 0.04854340851306915\n",
      "step: 30596, loss: 0.0009384206496179104\n",
      "step: 30597, loss: 4.0872189856600016e-05\n",
      "step: 30598, loss: 0.03583608567714691\n",
      "step: 30599, loss: 0.0\n",
      "step: 30600, loss: 7.595295755891129e-06\n",
      "step: 30601, loss: 0.0024523327592760324\n",
      "step: 30602, loss: 0.20256172120571136\n",
      "step: 30603, loss: 6.007502452121116e-05\n",
      "step: 30604, loss: 4.76837147544984e-09\n",
      "step: 30605, loss: 6.33378158454434e-06\n",
      "step: 30606, loss: 0.0006636021425947547\n",
      "step: 30607, loss: 0.07097279280424118\n",
      "step: 30608, loss: 1.0187418411078397e-05\n",
      "step: 30609, loss: 8.106215432235331e-08\n",
      "step: 30610, loss: 1.835815481854297e-07\n",
      "step: 30611, loss: 5.184931978874374e-06\n",
      "step: 30612, loss: 7.15255676908555e-09\n",
      "step: 30613, loss: 0.033471472561359406\n",
      "step: 30614, loss: 0.10882317274808884\n",
      "step: 30615, loss: 0.042100414633750916\n",
      "step: 30616, loss: 4.7683659687436375e-08\n",
      "step: 30617, loss: 9.975073771784082e-05\n",
      "step: 30618, loss: 0.22445747256278992\n",
      "step: 30619, loss: 2.288805234229585e-07\n",
      "step: 30620, loss: 0.0\n",
      "step: 30621, loss: 4.4244396121939644e-05\n",
      "step: 30622, loss: 3.471073341643205e-06\n",
      "step: 30623, loss: 1.4018750107425149e-06\n",
      "step: 30624, loss: 1.668929350273629e-08\n",
      "step: 30625, loss: 0.0\n",
      "step: 30626, loss: 0.0016548230778425932\n",
      "step: 30627, loss: 0.014702762477099895\n",
      "step: 30628, loss: 9.03585942069185e-07\n",
      "step: 30629, loss: 2.7125593987875618e-05\n",
      "step: 30630, loss: 6.356379890348762e-05\n",
      "step: 30631, loss: 0.10200081020593643\n",
      "step: 30632, loss: 1.192092646817855e-08\n",
      "step: 30633, loss: 0.5282390713691711\n",
      "step: 30634, loss: 0.04981774836778641\n",
      "step: 30635, loss: 0.00044189675827510655\n",
      "step: 30636, loss: 6.151104798846063e-07\n",
      "step: 30637, loss: 0.0005492715281434357\n",
      "step: 30638, loss: 4.529948327558486e-08\n",
      "step: 30639, loss: 0.0\n",
      "step: 30640, loss: 3.4570439311210066e-07\n",
      "step: 30641, loss: 1.276088096346939e-05\n",
      "step: 30642, loss: 0.24270540475845337\n",
      "step: 30643, loss: 5.277107629808597e-05\n",
      "step: 30644, loss: 6.877742271171883e-05\n",
      "step: 30645, loss: 1.2663169400184415e-05\n",
      "step: 30646, loss: 5.404257535701618e-06\n",
      "step: 30647, loss: 0.026074282824993134\n",
      "step: 30648, loss: 0.274215430021286\n",
      "step: 30649, loss: 2.947532084363047e-05\n",
      "step: 30650, loss: 0.09866484999656677\n",
      "step: 30651, loss: 0.016138505190610886\n",
      "step: 30652, loss: 0.13198232650756836\n",
      "step: 30653, loss: 0.08005669713020325\n",
      "step: 30654, loss: 6.097818186390214e-06\n",
      "step: 30655, loss: 0.054626092314720154\n",
      "step: 30656, loss: 1.2397744342251826e-07\n",
      "step: 30657, loss: 0.22004657983779907\n",
      "step: 30658, loss: 6.172717257868499e-05\n",
      "step: 30659, loss: 0.1363150179386139\n",
      "step: 30660, loss: 0.005729891825467348\n",
      "step: 30661, loss: 4.76837103136063e-09\n",
      "step: 30662, loss: 0.19606012105941772\n",
      "step: 30663, loss: 0.045226678252220154\n",
      "step: 30664, loss: 3.7477188925549854e-06\n",
      "step: 30665, loss: 0.1357489675283432\n",
      "step: 30666, loss: 0.031869884580373764\n",
      "step: 30667, loss: 0.006746423430740833\n",
      "step: 30668, loss: 1.737003003654536e-05\n",
      "step: 30669, loss: 0.13412469625473022\n",
      "step: 30670, loss: 2.38418573772492e-09\n",
      "step: 30671, loss: 0.024259569123387337\n",
      "step: 30672, loss: 0.001946285250596702\n",
      "step: 30673, loss: 9.298303638161087e-08\n",
      "step: 30674, loss: 1.239773865790994e-07\n",
      "step: 30675, loss: 0.0905386433005333\n",
      "step: 30676, loss: 0.1121773049235344\n",
      "step: 30677, loss: 3.843218655674718e-05\n",
      "step: 30678, loss: 4.996629286324605e-06\n",
      "step: 30679, loss: 1.068123401637422e-05\n",
      "step: 30680, loss: 3.480880934603192e-07\n",
      "step: 30681, loss: 0.0003205144021194428\n",
      "step: 30682, loss: 0.0009102164767682552\n",
      "step: 30683, loss: 0.052239276468753815\n",
      "step: 30684, loss: 0.12539000809192657\n",
      "step: 30685, loss: 7.15255676908555e-09\n",
      "step: 30686, loss: 0.1616804301738739\n",
      "step: 30687, loss: 0.018530987203121185\n",
      "step: 30688, loss: 5.265974778012605e-06\n",
      "step: 30689, loss: 0.14607137441635132\n",
      "step: 30690, loss: 0.012570816092193127\n",
      "step: 30691, loss: 0.24514228105545044\n",
      "step: 30692, loss: 0.07450585067272186\n",
      "step: 30693, loss: 0.00021135831775609404\n",
      "step: 30694, loss: 9.399681403010618e-06\n",
      "step: 30695, loss: 1.120566111012522e-07\n",
      "step: 30696, loss: 2.38418573772492e-09\n",
      "step: 30697, loss: 0.07633298635482788\n",
      "step: 30698, loss: 2.38418573772492e-09\n",
      "step: 30699, loss: 0.0\n",
      "step: 30700, loss: 1.3589811942438246e-07\n",
      "step: 30701, loss: 2.38418573772492e-09\n",
      "step: 30702, loss: 0.05111929029226303\n",
      "step: 30703, loss: 0.11242730915546417\n",
      "step: 30704, loss: 0.19453418254852295\n",
      "step: 30705, loss: 1.0251972781816221e-07\n",
      "step: 30706, loss: 0.0002446394646540284\n",
      "step: 30707, loss: 0.0003030845837201923\n",
      "step: 30708, loss: 0.0\n",
      "step: 30709, loss: 4.2915321074588064e-08\n",
      "step: 30710, loss: 3.902561729773879e-06\n",
      "step: 30711, loss: 1.0132556553799077e-06\n",
      "step: 30712, loss: 0.000823561626020819\n",
      "step: 30713, loss: 0.19789935648441315\n",
      "step: 30714, loss: 4.887521640739578e-07\n",
      "step: 30715, loss: 0.0026375555898994207\n",
      "step: 30716, loss: 0.04700249060988426\n",
      "step: 30717, loss: 0.20527033507823944\n",
      "step: 30718, loss: 4.5207874791231006e-05\n",
      "step: 30719, loss: 0.0\n",
      "step: 30720, loss: 0.10056587308645248\n",
      "step: 30721, loss: 0.01336937677115202\n",
      "step: 30722, loss: 2.219879752374254e-05\n",
      "step: 30723, loss: 0.044724296778440475\n",
      "step: 30724, loss: 1.668929883180681e-08\n",
      "step: 30725, loss: 0.6166430115699768\n",
      "step: 30726, loss: 4.76837103136063e-09\n",
      "step: 30727, loss: 2.38418573772492e-09\n",
      "step: 30728, loss: 0.08342992514371872\n",
      "step: 30729, loss: 0.1673319786787033\n",
      "step: 30730, loss: 4.072807860211469e-05\n",
      "step: 30731, loss: 0.21677134931087494\n",
      "step: 30732, loss: 0.0\n",
      "step: 30733, loss: 1.8142839053325588e-06\n",
      "step: 30734, loss: 7.128588208615838e-07\n",
      "step: 30735, loss: 6.437294075567479e-08\n",
      "step: 30736, loss: 3.9115498395858594e-08\n",
      "step: 30737, loss: 0.012633340433239937\n",
      "step: 30738, loss: 0.060784712433815\n",
      "step: 30739, loss: 0.0\n",
      "step: 30740, loss: 0.0\n",
      "step: 30741, loss: 4.76837103136063e-09\n",
      "step: 30742, loss: 0.0\n",
      "step: 30743, loss: 2.38418573772492e-09\n",
      "step: 30744, loss: 1.6545844800930354e-06\n",
      "step: 30745, loss: 0.0007842548657208681\n",
      "step: 30746, loss: 0.009391623549163342\n",
      "step: 30747, loss: 5.722037599298346e-08\n",
      "step: 30748, loss: 1.0967224284286203e-07\n",
      "step: 30749, loss: 0.0059906067326664925\n",
      "step: 30750, loss: 0.04437199607491493\n",
      "step: 30751, loss: 0.23556537926197052\n",
      "step: 30752, loss: 0.0\n",
      "step: 30753, loss: 0.0019397936994209886\n",
      "step: 30754, loss: 2.398352535237791e-06\n",
      "step: 30755, loss: 0.00010502393706701696\n",
      "step: 30756, loss: 0.0\n",
      "step: 30757, loss: 1.406665717240685e-07\n",
      "step: 30758, loss: 1.6832480469020084e-05\n",
      "step: 30759, loss: 6.761818076483905e-05\n",
      "step: 30760, loss: 0.2607845366001129\n",
      "step: 30761, loss: 0.023954126983880997\n",
      "step: 30762, loss: 7.55772703087132e-07\n",
      "step: 30763, loss: 0.1526850312948227\n",
      "step: 30764, loss: 0.0\n",
      "step: 30765, loss: 4.76837103136063e-09\n",
      "step: 30766, loss: 1.668929527909313e-08\n",
      "step: 30767, loss: 0.0030296409968286753\n",
      "step: 30768, loss: 2.0980725423669355e-07\n",
      "step: 30769, loss: 0.024028588086366653\n",
      "step: 30770, loss: 0.0\n",
      "step: 30771, loss: 1.0967230679170825e-07\n",
      "step: 30772, loss: 1.001355514063107e-07\n",
      "step: 30773, loss: 0.0\n",
      "step: 30774, loss: 0.06431998312473297\n",
      "step: 30775, loss: 2.74179512871342e-07\n",
      "step: 30776, loss: 1.0728679171734257e-06\n",
      "step: 30777, loss: 5.793488071503816e-07\n",
      "step: 30778, loss: 5.006783965200157e-08\n",
      "step: 30779, loss: 8.392162840209494e-07\n",
      "step: 30780, loss: 0.0\n",
      "step: 30781, loss: 6.937864895917301e-07\n",
      "step: 30782, loss: 0.009289348497986794\n",
      "step: 30783, loss: 0.0010678673861548305\n",
      "step: 30784, loss: 1.4407211892830674e-05\n",
      "step: 30785, loss: 0.2077084183692932\n",
      "step: 30786, loss: 5.984220479149371e-07\n",
      "step: 30787, loss: 0.004433490801602602\n",
      "step: 30788, loss: 4.524678388406755e-06\n",
      "step: 30789, loss: 4.76837103136063e-09\n",
      "step: 30790, loss: 0.0\n",
      "step: 30791, loss: 0.06257626414299011\n",
      "step: 30792, loss: 9.870286703517195e-07\n",
      "step: 30793, loss: 0.31773215532302856\n",
      "step: 30794, loss: 6.419760211429093e-06\n",
      "step: 30795, loss: 0.0567324198782444\n",
      "step: 30796, loss: 2.38418573772492e-09\n",
      "step: 30797, loss: 2.8536683203128632e-06\n",
      "step: 30798, loss: 4.0054038663583924e-07\n",
      "step: 30799, loss: 0.007093921769410372\n",
      "step: 30800, loss: 0.07963038235902786\n",
      "step: 30801, loss: 2.2649635411653435e-07\n",
      "step: 30802, loss: 9.01154417078942e-05\n",
      "step: 30803, loss: 5.245203738013515e-08\n",
      "step: 30804, loss: 1.2203315236547496e-05\n",
      "step: 30805, loss: 7.390963219222613e-08\n",
      "step: 30806, loss: 0.00018822017591446638\n",
      "step: 30807, loss: 1.9787853489106055e-06\n",
      "step: 30808, loss: 0.15196767449378967\n",
      "step: 30809, loss: 0.19602082669734955\n",
      "step: 30810, loss: 1.907348234908568e-08\n",
      "step: 30811, loss: 7.15255632499634e-09\n",
      "step: 30812, loss: 0.05919284000992775\n",
      "step: 30813, loss: 3.4610344300745055e-05\n",
      "step: 30814, loss: 8.619539585197344e-06\n",
      "step: 30815, loss: 1.6003403288777918e-05\n",
      "step: 30816, loss: 0.0037272553890943527\n",
      "step: 30817, loss: 2.38418573772492e-09\n",
      "step: 30818, loss: 0.0024020448327064514\n",
      "step: 30819, loss: 0.0549936406314373\n",
      "step: 30820, loss: 0.00013213919010013342\n",
      "step: 30821, loss: 0.0005294063012115657\n",
      "step: 30822, loss: 0.0004334996920078993\n",
      "step: 30823, loss: 1.0481392564543057e-05\n",
      "step: 30824, loss: 3.303748235339299e-05\n",
      "step: 30825, loss: 4.7680132411187515e-06\n",
      "step: 30826, loss: 0.00032408908009529114\n",
      "step: 30827, loss: 0.05039885640144348\n",
      "step: 30828, loss: 0.08930008858442307\n",
      "step: 30829, loss: 3.6477797493716935e-07\n",
      "step: 30830, loss: 2.6226025795494934e-08\n",
      "step: 30831, loss: 0.4379567801952362\n",
      "step: 30832, loss: 2.3101440547179664e-06\n",
      "step: 30833, loss: 9.427867917111143e-05\n",
      "step: 30834, loss: 0.4133528172969818\n",
      "step: 30835, loss: 0.23100203275680542\n",
      "step: 30836, loss: 0.0007861030171625316\n",
      "step: 30837, loss: 4.529947972287118e-08\n",
      "step: 30838, loss: 9.370040970679838e-06\n",
      "step: 30839, loss: 4.660736522055231e-06\n",
      "step: 30840, loss: 4.148459993302822e-07\n",
      "step: 30841, loss: 6.497546564787626e-05\n",
      "step: 30842, loss: 4.860765784542309e-06\n",
      "step: 30843, loss: 0.0036783101968467236\n",
      "step: 30844, loss: 2.598593709990382e-06\n",
      "step: 30845, loss: 0.0010488841217011213\n",
      "step: 30846, loss: 7.15255632499634e-09\n",
      "step: 30847, loss: 0.03680785372853279\n",
      "step: 30848, loss: 1.0728554116212763e-06\n",
      "step: 30849, loss: 9.560357057125657e-07\n",
      "step: 30850, loss: 1.1634489283096627e-06\n",
      "step: 30851, loss: 0.10908812284469604\n",
      "step: 30852, loss: 0.0005714402068406343\n",
      "step: 30853, loss: 0.12018205970525742\n",
      "step: 30854, loss: 8.256195724243298e-05\n",
      "step: 30855, loss: 0.0\n",
      "step: 30856, loss: 2.5873132472042926e-05\n",
      "step: 30857, loss: 0.0\n",
      "step: 30858, loss: 0.03753392770886421\n",
      "step: 30859, loss: 1.4066651488064963e-07\n",
      "step: 30860, loss: 1.8786502096190816e-06\n",
      "step: 30861, loss: 3.814693627646193e-08\n",
      "step: 30862, loss: 1.668923346187512e-07\n",
      "step: 30863, loss: 0.00014239053416531533\n",
      "step: 30864, loss: 2.145766231365087e-08\n",
      "step: 30865, loss: 0.00041020274511538446\n",
      "step: 30866, loss: 0.22168172895908356\n",
      "step: 30867, loss: 2.38418573772492e-09\n",
      "step: 30868, loss: 2.630216295074206e-05\n",
      "step: 30869, loss: 0.05567064881324768\n",
      "step: 30870, loss: 5.0015099986921996e-05\n",
      "step: 30871, loss: 0.0003786329471040517\n",
      "step: 30872, loss: 8.797451869213546e-07\n",
      "step: 30873, loss: 0.0\n",
      "step: 30874, loss: 3.5762766970037774e-08\n",
      "step: 30875, loss: 0.0006014318205416203\n",
      "step: 30876, loss: 0.0\n",
      "step: 30877, loss: 0.014320854097604752\n",
      "step: 30878, loss: 1.192092558000013e-08\n",
      "step: 30879, loss: 0.00037860494921915233\n",
      "step: 30880, loss: 2.38418573772492e-09\n",
      "step: 30881, loss: 1.7881322378343611e-07\n",
      "step: 30882, loss: 9.321954621555051e-07\n",
      "step: 30883, loss: 3.1506071536568925e-05\n",
      "step: 30884, loss: 0.4254516363143921\n",
      "step: 30885, loss: 3.5762766970037774e-08\n",
      "step: 30886, loss: 0.027447901666164398\n",
      "step: 30887, loss: 0.23001541197299957\n",
      "step: 30888, loss: 5.197487666919187e-07\n",
      "step: 30889, loss: 4.0292331959790317e-07\n",
      "step: 30890, loss: 0.0\n",
      "step: 30891, loss: 0.0\n",
      "step: 30892, loss: 0.0016355834668502212\n",
      "step: 30893, loss: 1.4352546713780612e-05\n",
      "step: 30894, loss: 1.6980113286990672e-05\n",
      "step: 30895, loss: 0.0\n",
      "step: 30896, loss: 2.38418573772492e-09\n",
      "step: 30897, loss: 2.701101266211481e-06\n",
      "step: 30898, loss: 1.0021738489740528e-05\n",
      "step: 30899, loss: 0.0005896099028177559\n",
      "step: 30900, loss: 5.157015402801335e-05\n",
      "step: 30901, loss: 0.0\n",
      "step: 30902, loss: 5.664446871378459e-05\n",
      "step: 30903, loss: 8.600292858318426e-06\n",
      "step: 30904, loss: 0.04360384866595268\n",
      "step: 30905, loss: 2.38418573772492e-09\n",
      "step: 30906, loss: 2.8848444344475865e-07\n",
      "step: 30907, loss: 1.1682486444897222e-07\n",
      "step: 30908, loss: 0.0013026524102315307\n",
      "step: 30909, loss: 2.4410150217590854e-05\n",
      "step: 30910, loss: 4.7683659687436375e-08\n",
      "step: 30911, loss: 0.0008020074455998838\n",
      "step: 30912, loss: 0.07067511975765228\n",
      "step: 30913, loss: 1.1738668945326935e-05\n",
      "step: 30914, loss: 0.0\n",
      "step: 30915, loss: 9.584463987266645e-05\n",
      "step: 30916, loss: 7.15255632499634e-09\n",
      "step: 30917, loss: 9.363087156089023e-05\n",
      "step: 30918, loss: 4.76837103136063e-09\n",
      "step: 30919, loss: 0.0\n",
      "step: 30920, loss: 0.09476421028375626\n",
      "step: 30921, loss: 0.000495032174512744\n",
      "step: 30922, loss: 0.08870755136013031\n",
      "step: 30923, loss: 0.054649051278829575\n",
      "step: 30924, loss: 0.057786550372838974\n",
      "step: 30925, loss: 1.0880899026233237e-05\n",
      "step: 30926, loss: 1.0490401081142409e-07\n",
      "step: 30927, loss: 0.19263453781604767\n",
      "step: 30928, loss: 7.867805607020273e-08\n",
      "step: 30929, loss: 3.409361966077995e-07\n",
      "step: 30930, loss: 0.0\n",
      "step: 30931, loss: 7.629380149865028e-08\n",
      "step: 30932, loss: 0.0005034327623434365\n",
      "step: 30933, loss: 0.0\n",
      "step: 30934, loss: 0.0\n",
      "step: 30935, loss: 0.18740831315517426\n",
      "step: 30936, loss: 2.503379334939382e-07\n",
      "step: 30937, loss: 0.0\n",
      "step: 30938, loss: 5.722038309841082e-08\n",
      "step: 30939, loss: 1.6689264725755493e-07\n",
      "step: 30940, loss: 7.032404937490355e-06\n",
      "step: 30941, loss: 1.1205646899270505e-07\n",
      "step: 30942, loss: 3.886973354383372e-05\n",
      "step: 30943, loss: 0.00038820155896246433\n",
      "step: 30944, loss: 0.002306575421243906\n",
      "step: 30945, loss: 8.344476327692973e-07\n",
      "step: 30946, loss: 0.0008319626213051379\n",
      "step: 30947, loss: 0.03801777958869934\n",
      "step: 30948, loss: 1.080017682397738e-06\n",
      "step: 30949, loss: 4.005393918760092e-07\n",
      "step: 30950, loss: 7.629261631336703e-07\n",
      "step: 30951, loss: 3.9338681290246313e-07\n",
      "step: 30952, loss: 2.2172852709445579e-07\n",
      "step: 30953, loss: 0.0\n",
      "step: 30954, loss: 1.3112979502238886e-07\n",
      "step: 30955, loss: 1.2159316042925639e-07\n",
      "step: 30956, loss: 7.778091458021663e-06\n",
      "step: 30957, loss: 2.38418573772492e-09\n",
      "step: 30958, loss: 7.152429475354438e-07\n",
      "step: 30959, loss: 0.009780505672097206\n",
      "step: 30960, loss: 0.0020468714646995068\n",
      "step: 30961, loss: 0.0\n",
      "step: 30962, loss: 1.8626442610525373e-08\n",
      "step: 30963, loss: 9.53674117454284e-09\n",
      "step: 30964, loss: 0.0004953148309141397\n",
      "step: 30965, loss: 4.12460735788045e-07\n",
      "step: 30966, loss: 2.38418573772492e-09\n",
      "step: 30967, loss: 0.0\n",
      "step: 30968, loss: 2.622603467727913e-08\n",
      "step: 30969, loss: 0.0\n",
      "step: 30970, loss: 0.0\n",
      "step: 30971, loss: 0.0\n",
      "step: 30972, loss: 4.76837103136063e-09\n",
      "step: 30973, loss: 8.116506251099054e-06\n",
      "step: 30974, loss: 5.0067868073711e-08\n",
      "step: 30975, loss: 0.0\n",
      "step: 30976, loss: 0.0\n",
      "step: 30977, loss: 5.166161372471834e-06\n",
      "step: 30978, loss: 4.24831750933663e-06\n",
      "step: 30979, loss: 2.38418573772492e-09\n",
      "step: 30980, loss: 0.0\n",
      "step: 30981, loss: 0.0\n",
      "step: 30982, loss: 0.0\n",
      "step: 30983, loss: 1.560079363116529e-05\n",
      "step: 30984, loss: 2.6226029348208613e-08\n",
      "step: 30985, loss: 2.38418573772492e-09\n",
      "step: 30986, loss: 0.0\n",
      "step: 30987, loss: 0.0\n",
      "step: 30988, loss: 1.430510998545742e-08\n",
      "step: 30989, loss: 0.0002145118487533182\n",
      "step: 30990, loss: 0.0008704577339813113\n",
      "step: 30991, loss: 0.0\n",
      "step: 30992, loss: 1.192092558000013e-08\n",
      "step: 30993, loss: 1.740451693876821e-07\n",
      "step: 30994, loss: 2.38418573772492e-09\n",
      "step: 30995, loss: 2.903583663282916e-05\n",
      "step: 30996, loss: 0.004726376850157976\n",
      "step: 30997, loss: 7.15255632499634e-09\n",
      "step: 30998, loss: 0.0\n",
      "step: 30999, loss: 4.76837103136063e-09\n",
      "step: 31000, loss: 6.372034022206208e-06\n",
      "step: 31001, loss: 2.38418573772492e-09\n",
      "step: 31002, loss: 5.798984784632921e-05\n",
      "step: 31003, loss: 3.981552083587303e-07\n",
      "step: 31004, loss: 3.380490170457051e-06\n",
      "step: 31005, loss: 0.006591083947569132\n",
      "step: 31006, loss: 4.76837147544984e-09\n",
      "step: 31007, loss: 0.0004208297759760171\n",
      "step: 31008, loss: 1.2874561150511e-07\n",
      "step: 31009, loss: 0.5554350018501282\n",
      "step: 31010, loss: 1.2138982128817588e-05\n",
      "step: 31011, loss: 0.0\n",
      "step: 31012, loss: 9.894129107124172e-07\n",
      "step: 31013, loss: 0.007319465279579163\n",
      "step: 31014, loss: 2.0503892983469996e-07\n",
      "step: 31015, loss: 1.668929350273629e-08\n",
      "step: 31016, loss: 2.76563639545202e-07\n",
      "step: 31017, loss: 7.015428309387062e-06\n",
      "step: 31018, loss: 0.029615694656968117\n",
      "step: 31019, loss: 2.145766764272139e-08\n",
      "step: 31020, loss: 0.0\n",
      "step: 31021, loss: 0.39197540283203125\n",
      "step: 31022, loss: 9.397022949997336e-06\n",
      "step: 31023, loss: 3.518748599162791e-06\n",
      "step: 31024, loss: 0.0\n",
      "step: 31025, loss: 4.76837103136063e-09\n",
      "step: 31026, loss: 0.0\n",
      "step: 31027, loss: 2.600985226308694e-06\n",
      "step: 31028, loss: 0.0\n",
      "step: 31029, loss: 0.0\n",
      "step: 31030, loss: 0.15662412345409393\n",
      "step: 31031, loss: 0.009141494520008564\n",
      "step: 31032, loss: 0.21732819080352783\n",
      "step: 31033, loss: 2.38418573772492e-09\n",
      "step: 31034, loss: 0.0015589984832331538\n",
      "step: 31035, loss: 0.0\n",
      "step: 31036, loss: 0.029267067089676857\n",
      "step: 31037, loss: 2.38418573772492e-09\n",
      "step: 31038, loss: 0.0\n",
      "step: 31039, loss: 3.814693627646193e-08\n",
      "step: 31040, loss: 0.06104570999741554\n",
      "step: 31041, loss: 1.8262034018334816e-06\n",
      "step: 31042, loss: 2.38418573772492e-09\n",
      "step: 31043, loss: 7.295475370483473e-07\n",
      "step: 31044, loss: 0.0\n",
      "step: 31045, loss: 1.4519182514050044e-06\n",
      "step: 31046, loss: 9.703405794425635e-07\n",
      "step: 31047, loss: 0.0\n",
      "step: 31048, loss: 0.0\n",
      "step: 31049, loss: 0.00013507647963706404\n",
      "step: 31050, loss: 1.192092558000013e-08\n",
      "step: 31051, loss: 0.12864361703395844\n",
      "step: 31052, loss: 0.0\n",
      "step: 31053, loss: 0.0003505536587908864\n",
      "step: 31054, loss: 0.0\n",
      "step: 31055, loss: 1.8147342416341417e-05\n",
      "step: 31056, loss: 6.86633711666218e-07\n",
      "step: 31057, loss: 9.0598852864332e-08\n",
      "step: 31058, loss: 0.00010970018774969503\n",
      "step: 31059, loss: 4.52994939337259e-08\n",
      "step: 31060, loss: 2.861021997091484e-08\n",
      "step: 31061, loss: 9.42102269618772e-05\n",
      "step: 31062, loss: 0.009135573171079159\n",
      "step: 31063, loss: 4.76837103136063e-09\n",
      "step: 31064, loss: 0.0\n",
      "step: 31065, loss: 0.00029468286084011197\n",
      "step: 31066, loss: 4.8462134145665914e-05\n",
      "step: 31067, loss: 3.480880934603192e-07\n",
      "step: 31068, loss: 9.53674206272126e-09\n",
      "step: 31069, loss: 0.03917348384857178\n",
      "step: 31070, loss: 4.52994939337259e-08\n",
      "step: 31071, loss: 5.8285495470045134e-06\n",
      "step: 31072, loss: 9.536733358572747e-08\n",
      "step: 31073, loss: 2.38418573772492e-09\n",
      "step: 31074, loss: 5.9604552404834976e-08\n",
      "step: 31075, loss: 9.417414617018949e-07\n",
      "step: 31076, loss: 2.38418573772492e-09\n",
      "step: 31077, loss: 3.1413190299645066e-05\n",
      "step: 31078, loss: 0.0\n",
      "step: 31079, loss: 0.0\n",
      "step: 31080, loss: 2.622603290092229e-08\n",
      "step: 31081, loss: 2.38418573772492e-09\n",
      "step: 31082, loss: 0.23212990164756775\n",
      "step: 31083, loss: 0.0\n",
      "step: 31084, loss: 0.0\n",
      "step: 31085, loss: 1.430511264999268e-08\n",
      "step: 31086, loss: 3.7714269183197757e-06\n",
      "step: 31087, loss: 0.0\n",
      "step: 31088, loss: 5.0067846757428924e-08\n",
      "step: 31089, loss: 2.8775075406883843e-06\n",
      "step: 31090, loss: 1.2636147062039527e-07\n",
      "step: 31091, loss: 1.3279474160299287e-06\n",
      "step: 31092, loss: 9.53674117454284e-09\n",
      "step: 31093, loss: 0.0\n",
      "step: 31094, loss: 4.76837103136063e-09\n",
      "step: 31095, loss: 0.0\n",
      "step: 31096, loss: 0.03780852258205414\n",
      "step: 31097, loss: 1.7761459503162769e-06\n",
      "step: 31098, loss: 0.04857032001018524\n",
      "step: 31099, loss: 3.814693627646193e-08\n",
      "step: 31100, loss: 7.271634103744873e-07\n",
      "step: 31101, loss: 0.0572851337492466\n",
      "step: 31102, loss: 0.0\n",
      "step: 31103, loss: 0.0018058867426589131\n",
      "step: 31104, loss: 0.0\n",
      "step: 31105, loss: 3.814693627646193e-08\n",
      "step: 31106, loss: 0.0\n",
      "step: 31107, loss: 3.0994389277338996e-08\n",
      "step: 31108, loss: 0.0\n",
      "step: 31109, loss: 5.006783965200157e-08\n",
      "step: 31110, loss: 0.0\n",
      "step: 31111, loss: 0.13516287505626678\n",
      "step: 31112, loss: 0.0\n",
      "step: 31113, loss: 6.798895810788963e-06\n",
      "step: 31114, loss: 7.761413144180551e-06\n",
      "step: 31115, loss: 4.3719242967199534e-05\n",
      "step: 31116, loss: 0.0002564069873187691\n",
      "step: 31117, loss: 0.0\n",
      "step: 31118, loss: 0.00014035924687050283\n",
      "step: 31119, loss: 2.3841844054572903e-08\n",
      "step: 31120, loss: 9.53674117454284e-09\n",
      "step: 31121, loss: 0.02850482054054737\n",
      "step: 31122, loss: 6.675709585124423e-08\n",
      "step: 31123, loss: 4.3630123514049046e-07\n",
      "step: 31124, loss: 8.495384099660441e-05\n",
      "step: 31125, loss: 1.1476560757728294e-05\n",
      "step: 31126, loss: 1.430510998545742e-08\n",
      "step: 31127, loss: 1.234971136909735e-06\n",
      "step: 31128, loss: 2.0860634322161786e-06\n",
      "step: 31129, loss: 2.38418573772492e-09\n",
      "step: 31130, loss: 3.576275631189674e-08\n",
      "step: 31131, loss: 1.192092558000013e-08\n",
      "step: 31132, loss: 0.0\n",
      "step: 31133, loss: 9.334544301964343e-05\n",
      "step: 31134, loss: 5.793488639938005e-07\n",
      "step: 31135, loss: 0.12182653695344925\n",
      "step: 31136, loss: 2.38418573772492e-09\n",
      "step: 31137, loss: 1.6689239146217005e-07\n",
      "step: 31138, loss: 0.0\n",
      "step: 31139, loss: 0.01919769123196602\n",
      "step: 31140, loss: 2.1457660537294032e-08\n",
      "step: 31141, loss: 4.7683659687436375e-08\n",
      "step: 31142, loss: 0.06747665256261826\n",
      "step: 31143, loss: 2.38418573772492e-09\n",
      "step: 31144, loss: 0.013086224906146526\n",
      "step: 31145, loss: 7.15255632499634e-09\n",
      "step: 31146, loss: 2.38418573772492e-09\n",
      "step: 31147, loss: 3.328046659589745e-06\n",
      "step: 31148, loss: 0.1143430545926094\n",
      "step: 31149, loss: 9.20799357118085e-06\n",
      "step: 31150, loss: 0.0\n",
      "step: 31151, loss: 2.2148931748233736e-05\n",
      "step: 31152, loss: 0.27540966868400574\n",
      "step: 31153, loss: 4.193674249108881e-05\n",
      "step: 31154, loss: 0.0\n",
      "step: 31155, loss: 0.0\n",
      "step: 31156, loss: 0.0\n",
      "step: 31157, loss: 2.38418573772492e-09\n",
      "step: 31158, loss: 4.446012098924257e-06\n",
      "step: 31159, loss: 7.510091108997585e-07\n",
      "step: 31160, loss: 0.0\n",
      "step: 31161, loss: 0.0\n",
      "step: 31162, loss: 1.623210118850693e-05\n",
      "step: 31163, loss: 0.0\n",
      "step: 31164, loss: 1.828618451327202e-06\n",
      "step: 31165, loss: 2.38418573772492e-09\n",
      "step: 31166, loss: 0.0\n",
      "step: 31167, loss: 7.724623856120161e-07\n",
      "step: 31168, loss: 0.0\n",
      "step: 31169, loss: 2.145766586636455e-08\n",
      "step: 31170, loss: 0.0008351127617061138\n",
      "step: 31171, loss: 0.0\n",
      "step: 31172, loss: 4.727281520899851e-06\n",
      "step: 31173, loss: 1.430511264999268e-08\n",
      "step: 31174, loss: 2.861002599274798e-07\n",
      "step: 31175, loss: 7.390962508679877e-08\n",
      "step: 31176, loss: 0.0\n",
      "step: 31177, loss: 2.6702775812736945e-07\n",
      "step: 31178, loss: 2.1457660537294032e-08\n",
      "step: 31179, loss: 1.192092558000013e-08\n",
      "step: 31180, loss: 0.0\n",
      "step: 31181, loss: 2.138500349246897e-06\n",
      "step: 31182, loss: 0.025543468073010445\n",
      "step: 31183, loss: 3.337857279461787e-08\n",
      "step: 31184, loss: 0.0\n",
      "step: 31185, loss: 4.053111624102712e-08\n",
      "step: 31186, loss: 4.768368100371845e-08\n",
      "step: 31187, loss: 2.6820291623153025e-06\n",
      "step: 31188, loss: 1.8626450382086546e-09\n",
      "step: 31189, loss: 5.722038309841082e-08\n",
      "step: 31190, loss: 2.2888053763381322e-07\n",
      "step: 31191, loss: 3.602187462092843e-06\n",
      "step: 31192, loss: 0.0\n",
      "step: 31193, loss: 2.38418573772492e-09\n",
      "step: 31194, loss: 0.000846315233502537\n",
      "step: 31195, loss: 7.15255676908555e-09\n",
      "step: 31196, loss: 0.0\n",
      "step: 31197, loss: 2.3175158275989816e-05\n",
      "step: 31198, loss: 2.38418573772492e-09\n",
      "step: 31199, loss: 4.232456922181882e-05\n",
      "step: 31200, loss: 1.0490390423001372e-07\n",
      "step: 31201, loss: 0.00014460374950431287\n",
      "step: 31202, loss: 1.3061276149528567e-05\n",
      "step: 31203, loss: 1.9073478796372e-08\n",
      "step: 31204, loss: 2.38418573772492e-09\n",
      "step: 31205, loss: 9.53674117454284e-09\n",
      "step: 31206, loss: 2.38418573772492e-09\n",
      "step: 31207, loss: 2.3126533221784484e-07\n",
      "step: 31208, loss: 0.00031864794436842203\n",
      "step: 31209, loss: 0.0\n",
      "step: 31210, loss: 0.05457392334938049\n",
      "step: 31211, loss: 0.0\n",
      "step: 31212, loss: 6.770974323444534e-07\n",
      "step: 31213, loss: 4.76837147544984e-09\n",
      "step: 31214, loss: 0.0\n",
      "step: 31215, loss: 0.0\n",
      "step: 31216, loss: 0.0\n",
      "step: 31217, loss: 0.0\n",
      "step: 31218, loss: 0.0\n",
      "step: 31219, loss: 2.38418573772492e-09\n",
      "step: 31220, loss: 0.0\n",
      "step: 31221, loss: 0.050661906599998474\n",
      "step: 31222, loss: 0.0\n",
      "step: 31223, loss: 2.38418573772492e-09\n",
      "step: 31224, loss: 1.015637622003851e-06\n",
      "step: 31225, loss: 1.2683466366070206e-06\n",
      "step: 31226, loss: 4.76837103136063e-09\n",
      "step: 31227, loss: 0.0\n",
      "step: 31228, loss: 0.0\n",
      "step: 31229, loss: 2.38418573772492e-09\n",
      "step: 31230, loss: 0.0\n",
      "step: 31231, loss: 0.0\n",
      "step: 31232, loss: 0.0003863387682940811\n",
      "step: 31233, loss: 0.0\n",
      "step: 31234, loss: 0.0\n",
      "step: 31235, loss: 1.668929350273629e-08\n",
      "step: 31236, loss: 0.0\n",
      "step: 31237, loss: 2.38418573772492e-09\n",
      "step: 31238, loss: 0.0\n",
      "step: 31239, loss: 0.0\n",
      "step: 31240, loss: 0.0\n",
      "step: 31241, loss: 0.0\n",
      "step: 31242, loss: 0.0\n",
      "step: 31243, loss: 2.622603467727913e-08\n",
      "step: 31244, loss: 0.0\n",
      "step: 31245, loss: 0.004256952088326216\n",
      "step: 31246, loss: 2.1457660537294032e-08\n",
      "step: 31247, loss: 4.76837147544984e-09\n",
      "step: 31248, loss: 2.6226025795494934e-08\n",
      "step: 31249, loss: 1.668929350273629e-08\n",
      "step: 31250, loss: 3.635557050074567e-06\n",
      "step: 31251, loss: 0.0\n",
      "step: 31252, loss: 0.0\n",
      "step: 31253, loss: 0.0\n",
      "step: 31254, loss: 0.11182413250207901\n",
      "step: 31255, loss: 0.0\n",
      "step: 31256, loss: 4.768367389829109e-08\n",
      "step: 31257, loss: 0.0\n",
      "step: 31258, loss: 0.0\n",
      "step: 31259, loss: 0.0\n",
      "step: 31260, loss: 1.0251972781816221e-07\n",
      "step: 31261, loss: 0.0\n",
      "step: 31262, loss: 0.0\n",
      "step: 31263, loss: 0.0\n",
      "step: 31264, loss: 2.5033844508470793e-07\n",
      "step: 31265, loss: 0.0\n",
      "step: 31266, loss: 0.0\n",
      "step: 31267, loss: 1.0728809485271995e-07\n",
      "step: 31268, loss: 0.0\n",
      "step: 31269, loss: 0.0\n",
      "step: 31270, loss: 0.0\n",
      "step: 31271, loss: 1.907348234908568e-08\n",
      "step: 31272, loss: 2.38418573772492e-09\n",
      "step: 31273, loss: 8.94049833277677e-07\n",
      "step: 31274, loss: 0.0\n",
      "step: 31275, loss: 0.0\n",
      "step: 31276, loss: 0.0\n",
      "step: 31277, loss: 0.0\n",
      "step: 31278, loss: 2.38418573772492e-09\n",
      "step: 31279, loss: 5.119450361235067e-05\n",
      "step: 31280, loss: 1.4890847523929551e-05\n",
      "step: 31281, loss: 0.0\n",
      "step: 31282, loss: 0.029563050717115402\n",
      "step: 31283, loss: 0.0\n",
      "step: 31284, loss: 0.0\n",
      "step: 31285, loss: 0.0\n",
      "step: 31286, loss: 1.192092558000013e-08\n",
      "step: 31287, loss: 0.0\n",
      "step: 31288, loss: 0.0\n",
      "step: 31289, loss: 0.0011707920348271728\n",
      "step: 31290, loss: 0.0\n",
      "step: 31291, loss: 0.0\n",
      "step: 31292, loss: 0.0\n",
      "step: 31293, loss: 7.15255632499634e-09\n",
      "step: 31294, loss: 1.192092646817855e-08\n",
      "step: 31295, loss: 0.0\n",
      "step: 31296, loss: 5.9604552404834976e-08\n",
      "step: 31297, loss: 0.0\n",
      "step: 31298, loss: 0.0\n",
      "step: 31299, loss: 7.15255632499634e-09\n",
      "step: 31300, loss: 0.0\n",
      "step: 31301, loss: 1.668929350273629e-08\n",
      "step: 31302, loss: 1.0967224284286203e-07\n",
      "step: 31303, loss: 3.3638032164162723e-06\n",
      "step: 31304, loss: 3.4330589642195264e-06\n",
      "step: 31305, loss: 4.76837103136063e-09\n",
      "step: 31306, loss: 0.03906528651714325\n",
      "step: 31307, loss: 0.0\n",
      "step: 31308, loss: 3.290150232260203e-07\n",
      "step: 31309, loss: 9.53674117454284e-09\n",
      "step: 31310, loss: 2.38418573772492e-09\n",
      "step: 31311, loss: 2.38418573772492e-09\n",
      "step: 31312, loss: 1.0490401081142409e-07\n",
      "step: 31313, loss: 5.245205159098987e-08\n",
      "step: 31314, loss: 4.76837147544984e-09\n",
      "step: 31315, loss: 0.04991923272609711\n",
      "step: 31316, loss: 0.0\n",
      "step: 31317, loss: 1.430511264999268e-08\n",
      "step: 31318, loss: 0.0\n",
      "step: 31319, loss: 9.703400110083749e-07\n",
      "step: 31320, loss: 6.198873592211385e-08\n",
      "step: 31321, loss: 9.53674206272126e-09\n",
      "step: 31322, loss: 0.0\n",
      "step: 31323, loss: 3.0517344384861644e-07\n",
      "step: 31324, loss: 1.0704846999942674e-06\n",
      "step: 31325, loss: 9.53674206272126e-09\n",
      "step: 31326, loss: 0.0037023895420134068\n",
      "step: 31327, loss: 2.4737695639487356e-05\n",
      "step: 31328, loss: 4.0292331959790317e-07\n",
      "step: 31329, loss: 0.0\n",
      "step: 31330, loss: 0.0\n",
      "step: 31331, loss: 1.0227895472780801e-06\n",
      "step: 31332, loss: 0.008039411157369614\n",
      "step: 31333, loss: 2.38418573772492e-09\n",
      "step: 31334, loss: 9.53674117454284e-09\n",
      "step: 31335, loss: 0.0\n",
      "step: 31336, loss: 0.0\n",
      "step: 31337, loss: 0.09753952920436859\n",
      "step: 31338, loss: 6.437291233396536e-08\n",
      "step: 31339, loss: 8.725929205866123e-07\n",
      "step: 31340, loss: 1.668929350273629e-08\n",
      "step: 31341, loss: 0.08240506798028946\n",
      "step: 31342, loss: 0.0\n",
      "step: 31343, loss: 0.0\n",
      "step: 31344, loss: 0.0\n",
      "step: 31345, loss: 0.0\n",
      "step: 31346, loss: 0.0\n",
      "step: 31347, loss: 0.09179207682609558\n",
      "step: 31348, loss: 5.722037599298346e-08\n",
      "step: 31349, loss: 1.6927647550346592e-07\n",
      "step: 31350, loss: 1.430511176181426e-08\n",
      "step: 31351, loss: 0.0\n",
      "step: 31352, loss: 7.07024537405232e-06\n",
      "step: 31353, loss: 0.0006080891471356153\n",
      "step: 31354, loss: 1.430511264999268e-08\n",
      "step: 31355, loss: 7.15255632499634e-09\n",
      "step: 31356, loss: 2.2411221323181962e-07\n",
      "step: 31357, loss: 7.15255676908555e-09\n",
      "step: 31358, loss: 3.5073979233857244e-05\n",
      "step: 31359, loss: 1.018027091959084e-06\n",
      "step: 31360, loss: 0.0\n",
      "step: 31361, loss: 0.0\n",
      "step: 31362, loss: 0.0\n",
      "step: 31363, loss: 2.145766231365087e-08\n",
      "step: 31364, loss: 0.0\n",
      "step: 31365, loss: 2.38418573772492e-09\n",
      "step: 31366, loss: 0.0\n",
      "step: 31367, loss: 6.340928848658223e-06\n",
      "step: 31368, loss: 0.15101484954357147\n",
      "step: 31369, loss: 0.00010318793647456914\n",
      "step: 31370, loss: 2.6226025795494934e-08\n",
      "step: 31371, loss: 0.10626988112926483\n",
      "step: 31372, loss: 0.0\n",
      "step: 31373, loss: 0.03523308411240578\n",
      "step: 31374, loss: 0.0001726709888316691\n",
      "step: 31375, loss: 0.0\n",
      "step: 31376, loss: 0.0\n",
      "step: 31377, loss: 4.76837147544984e-09\n",
      "step: 31378, loss: 4.76837103136063e-09\n",
      "step: 31379, loss: 4.76837103136063e-09\n",
      "step: 31380, loss: 0.0\n",
      "step: 31381, loss: 0.0\n",
      "step: 31382, loss: 7.15255632499634e-09\n",
      "step: 31383, loss: 6.675709585124423e-08\n",
      "step: 31384, loss: 0.0\n",
      "step: 31385, loss: 0.0\n",
      "step: 31386, loss: 2.38418573772492e-09\n",
      "step: 31387, loss: 0.0\n",
      "step: 31388, loss: 0.0\n",
      "step: 31389, loss: 1.2874561150511e-07\n",
      "step: 31390, loss: 0.0006550585967488587\n",
      "step: 31391, loss: 0.0\n",
      "step: 31392, loss: 2.38418573772492e-09\n",
      "step: 31393, loss: 1.1669754712784197e-05\n",
      "step: 31394, loss: 1.6092606074380456e-06\n",
      "step: 31395, loss: 0.0\n",
      "step: 31396, loss: 1.0636267688823864e-05\n",
      "step: 31397, loss: 1.1444066672083864e-07\n",
      "step: 31398, loss: 2.38418573772492e-09\n",
      "step: 31399, loss: 7.15255632499634e-09\n",
      "step: 31400, loss: 4.053114466273655e-08\n",
      "step: 31401, loss: 0.0\n",
      "step: 31402, loss: 0.0\n",
      "step: 31403, loss: 0.0\n",
      "step: 31404, loss: 0.0\n",
      "step: 31405, loss: 3.785877424888895e-06\n",
      "step: 31406, loss: 0.0\n",
      "step: 31407, loss: 0.17681190371513367\n",
      "step: 31408, loss: 0.027494654059410095\n",
      "step: 31409, loss: 0.0\n",
      "step: 31410, loss: 0.0\n",
      "step: 31411, loss: 9.059895234031501e-08\n",
      "step: 31412, loss: 0.0\n",
      "step: 31413, loss: 0.0\n",
      "step: 31414, loss: 0.0\n",
      "step: 31415, loss: 2.38418573772492e-09\n",
      "step: 31416, loss: 0.0\n",
      "step: 31417, loss: 0.06636722385883331\n",
      "step: 31418, loss: 2.38418573772492e-09\n",
      "step: 31419, loss: 0.0\n",
      "step: 31420, loss: 0.0\n",
      "step: 31421, loss: 1.192092558000013e-08\n",
      "step: 31422, loss: 0.0\n",
      "step: 31423, loss: 0.0\n",
      "step: 31424, loss: 0.0\n",
      "step: 31425, loss: 8.87282585608773e-05\n",
      "step: 31426, loss: 0.0\n",
      "step: 31427, loss: 1.668929350273629e-08\n",
      "step: 31428, loss: 0.0\n",
      "step: 31429, loss: 2.3841844054572903e-08\n",
      "step: 31430, loss: 9.53674117454284e-09\n",
      "step: 31431, loss: 1.668929350273629e-08\n",
      "step: 31432, loss: 4.5299508144580614e-08\n",
      "step: 31433, loss: 0.0\n",
      "step: 31434, loss: 0.0\n",
      "step: 31435, loss: 0.14307717978954315\n",
      "step: 31436, loss: 0.0\n",
      "step: 31437, loss: 0.0\n",
      "step: 31438, loss: 2.38418573772492e-09\n",
      "step: 31439, loss: 3.814693627646193e-08\n",
      "step: 31440, loss: 0.10952171683311462\n",
      "step: 31441, loss: 0.0\n",
      "step: 31442, loss: 0.0\n",
      "step: 31443, loss: 0.0\n",
      "step: 31444, loss: 0.0\n",
      "step: 31445, loss: 0.0\n",
      "step: 31446, loss: 0.0\n",
      "step: 31447, loss: 0.0\n",
      "step: 31448, loss: 0.0\n",
      "step: 31449, loss: 2.6226025795494934e-08\n",
      "step: 31450, loss: 9.53674117454284e-09\n",
      "step: 31451, loss: 0.0\n",
      "step: 31452, loss: 2.38418573772492e-09\n",
      "step: 31453, loss: 0.0\n",
      "step: 31454, loss: 2.38418573772492e-09\n",
      "step: 31455, loss: 0.0\n",
      "step: 31456, loss: 0.035415057092905045\n",
      "step: 31457, loss: 0.0\n",
      "step: 31458, loss: 0.0\n",
      "step: 31459, loss: 0.0\n",
      "step: 31460, loss: 0.0\n",
      "step: 31461, loss: 2.38418573772492e-09\n",
      "step: 31462, loss: 0.0001605723227839917\n",
      "step: 31463, loss: 0.0\n",
      "step: 31464, loss: 0.0\n",
      "step: 31465, loss: 7.15255632499634e-09\n",
      "step: 31466, loss: 0.0\n",
      "step: 31467, loss: 4.76837103136063e-09\n",
      "step: 31468, loss: 0.0\n",
      "step: 31469, loss: 0.0\n",
      "step: 31470, loss: 7.15255632499634e-09\n",
      "step: 31471, loss: 0.0\n",
      "step: 31472, loss: 4.76837147544984e-09\n",
      "step: 31473, loss: 0.0\n",
      "step: 31474, loss: 0.0\n",
      "step: 31475, loss: 0.0\n",
      "step: 31476, loss: 0.0\n",
      "step: 31477, loss: 0.11572891473770142\n",
      "step: 31478, loss: 5.0067868073711e-08\n",
      "step: 31479, loss: 4.76837103136063e-09\n",
      "step: 31480, loss: 4.76837103136063e-09\n",
      "step: 31481, loss: 2.38418573772492e-09\n",
      "step: 31482, loss: 0.0\n",
      "step: 31483, loss: 0.0\n",
      "step: 31484, loss: 0.0\n",
      "step: 31485, loss: 0.0\n",
      "step: 31486, loss: 0.0\n",
      "step: 31487, loss: 0.0\n",
      "step: 31488, loss: 0.0\n",
      "step: 31489, loss: 0.0\n",
      "step: 31490, loss: 0.0\n",
      "step: 31491, loss: 7.15255632499634e-09\n",
      "step: 31492, loss: 0.0\n",
      "step: 31493, loss: 0.0\n",
      "step: 31494, loss: 4.76837103136063e-09\n",
      "step: 31495, loss: 9.53674117454284e-09\n",
      "step: 31496, loss: 5.793488639938005e-07\n",
      "step: 31497, loss: 0.0\n",
      "step: 31498, loss: 7.15255632499634e-09\n",
      "step: 31499, loss: 0.0\n",
      "step: 31500, loss: 0.0\n",
      "step: 31501, loss: 1.430510998545742e-08\n",
      "step: 31502, loss: 7.15255632499634e-09\n",
      "step: 31503, loss: 0.0011797469342127442\n",
      "step: 31504, loss: 0.05918954312801361\n",
      "step: 31505, loss: 0.0\n",
      "step: 31506, loss: 0.0\n",
      "step: 31507, loss: 0.0\n",
      "step: 31508, loss: 0.0\n",
      "step: 31509, loss: 0.0\n",
      "step: 31510, loss: 0.0\n",
      "step: 31511, loss: 0.0\n",
      "step: 31512, loss: 0.0\n",
      "step: 31513, loss: 2.2649635411653435e-07\n",
      "step: 31514, loss: 4.76837147544984e-09\n",
      "step: 31515, loss: 0.0\n",
      "step: 31516, loss: 0.0\n",
      "step: 31517, loss: 0.0\n",
      "step: 31518, loss: 7.390962508679877e-08\n",
      "step: 31519, loss: 7.15255632499634e-09\n",
      "step: 31520, loss: 0.007979809306561947\n",
      "step: 31521, loss: 0.0\n",
      "step: 31522, loss: 0.0\n",
      "step: 31523, loss: 3.576275631189674e-08\n",
      "step: 31524, loss: 0.15621459484100342\n",
      "step: 31525, loss: 2.6226025795494934e-08\n",
      "step: 31526, loss: 1.9311814014599804e-07\n",
      "step: 31527, loss: 1.5258730456935155e-07\n",
      "step: 31528, loss: 4.291529975830599e-08\n",
      "step: 31529, loss: 0.0\n",
      "step: 31530, loss: 1.907347702001516e-08\n",
      "step: 31531, loss: 2.190947043345659e-06\n",
      "step: 31532, loss: 0.0\n",
      "step: 31533, loss: 4.76837103136063e-09\n",
      "step: 31534, loss: 0.0\n",
      "step: 31535, loss: 0.0\n",
      "step: 31536, loss: 0.0\n",
      "step: 31537, loss: 0.0\n",
      "step: 31538, loss: 7.15255676908555e-09\n",
      "step: 31539, loss: 1.192092646817855e-08\n",
      "step: 31540, loss: 0.0\n",
      "step: 31541, loss: 0.0\n",
      "step: 31542, loss: 9.298312164673916e-08\n",
      "step: 31543, loss: 0.0\n",
      "step: 31544, loss: 2.38418573772492e-09\n",
      "step: 31545, loss: 1.907347702001516e-08\n",
      "step: 31546, loss: 0.042688775807619095\n",
      "step: 31547, loss: 9.240942017640918e-05\n",
      "step: 31548, loss: 0.0\n",
      "step: 31549, loss: 0.0\n",
      "step: 31550, loss: 1.3351396432881302e-07\n",
      "step: 31551, loss: 0.0\n",
      "step: 31552, loss: 3.814693627646193e-08\n",
      "step: 31553, loss: 0.0\n",
      "step: 31554, loss: 0.0\n",
      "step: 31555, loss: 1.192092558000013e-08\n",
      "step: 31556, loss: 0.0\n",
      "step: 31557, loss: 4.76837103136063e-09\n",
      "step: 31558, loss: 2.3101426904759137e-06\n",
      "step: 31559, loss: 1.1444061698284713e-07\n",
      "step: 31560, loss: 0.0\n",
      "step: 31561, loss: 8.821468355790785e-08\n",
      "step: 31562, loss: 0.0\n",
      "step: 31563, loss: 3.659713183878921e-05\n",
      "step: 31564, loss: 0.0\n",
      "step: 31565, loss: 0.0\n",
      "step: 31566, loss: 3.576275631189674e-08\n",
      "step: 31567, loss: 4.76837103136063e-09\n",
      "step: 31568, loss: 0.0\n",
      "step: 31569, loss: 0.0\n",
      "step: 31570, loss: 1.6927650392517535e-07\n",
      "step: 31571, loss: 2.38418573772492e-09\n",
      "step: 31572, loss: 0.0\n",
      "step: 31573, loss: 2.741794844496326e-07\n",
      "step: 31574, loss: 2.38418573772492e-09\n",
      "step: 31575, loss: 2.1457660537294032e-08\n",
      "step: 31576, loss: 5.245205159098987e-08\n",
      "step: 31577, loss: 7.15255676908555e-09\n",
      "step: 31578, loss: 0.0\n",
      "step: 31579, loss: 2.38418573772492e-09\n",
      "step: 31580, loss: 4.053111624102712e-08\n",
      "step: 31581, loss: 9.53674206272126e-09\n",
      "step: 31582, loss: 2.38418573772492e-09\n",
      "step: 31583, loss: 0.0\n",
      "step: 31584, loss: 0.0\n",
      "step: 31585, loss: 7.15255632499634e-09\n",
      "step: 31586, loss: 0.0\n",
      "step: 31587, loss: 1.4781898016735795e-07\n",
      "step: 31588, loss: 2.38418573772492e-09\n",
      "step: 31589, loss: 0.032040487974882126\n",
      "step: 31590, loss: 0.0\n",
      "step: 31591, loss: 0.0\n",
      "step: 31592, loss: 0.0\n",
      "step: 31593, loss: 0.0\n",
      "step: 31594, loss: 0.0\n",
      "step: 31595, loss: 0.0\n",
      "step: 31596, loss: 0.0\n",
      "step: 31597, loss: 6.094934360589832e-05\n",
      "step: 31598, loss: 0.0\n",
      "step: 31599, loss: 2.38418573772492e-09\n",
      "step: 31600, loss: 0.0\n",
      "step: 31601, loss: 0.0\n",
      "step: 31602, loss: 0.0681232213973999\n",
      "step: 31603, loss: 0.0\n",
      "step: 31604, loss: 0.0\n",
      "step: 31605, loss: 4.76837103136063e-09\n",
      "step: 31606, loss: 0.0\n",
      "step: 31607, loss: 4.76837147544984e-09\n",
      "step: 31608, loss: 0.1555495411157608\n",
      "step: 31609, loss: 1.192092646817855e-08\n",
      "step: 31610, loss: 0.0\n",
      "step: 31611, loss: 8.018796506803483e-06\n",
      "step: 31612, loss: 0.0\n",
      "step: 31613, loss: 7.15255632499634e-09\n",
      "step: 31614, loss: 0.0\n",
      "step: 31615, loss: 0.0\n",
      "step: 31616, loss: 2.38418573772492e-09\n",
      "step: 31617, loss: 2.38418573772492e-09\n",
      "step: 31618, loss: 4.76837103136063e-09\n",
      "step: 31619, loss: 0.0\n",
      "step: 31620, loss: 4.76837103136063e-09\n",
      "step: 31621, loss: 9.53674206272126e-09\n",
      "step: 31622, loss: 0.0\n",
      "step: 31623, loss: 3.409356565953203e-07\n",
      "step: 31624, loss: 0.0\n",
      "step: 31625, loss: 0.0\n",
      "step: 31626, loss: 0.1066296175122261\n",
      "step: 31627, loss: 1.192092558000013e-08\n",
      "step: 31628, loss: 1.192092558000013e-08\n",
      "step: 31629, loss: 0.0\n",
      "step: 31630, loss: 4.76837103136063e-09\n",
      "step: 31631, loss: 2.7179609674021776e-07\n",
      "step: 31632, loss: 2.38418573772492e-09\n",
      "step: 31633, loss: 2.38418573772492e-09\n",
      "step: 31634, loss: 0.0\n",
      "step: 31635, loss: 4.76837103136063e-09\n",
      "step: 31636, loss: 0.0\n",
      "step: 31637, loss: 0.0\n",
      "step: 31638, loss: 6.198877855467799e-08\n",
      "step: 31639, loss: 2.38418573772492e-09\n",
      "step: 31640, loss: 0.0\n",
      "step: 31641, loss: 4.76837103136063e-09\n",
      "step: 31642, loss: 0.0\n",
      "step: 31643, loss: 0.0\n",
      "step: 31644, loss: 0.0\n",
      "step: 31645, loss: 0.0\n",
      "step: 31646, loss: 8.106216142778067e-08\n",
      "step: 31647, loss: 0.011688627302646637\n",
      "step: 31648, loss: 2.38418573772492e-09\n",
      "step: 31649, loss: 0.0\n",
      "step: 31650, loss: 0.0\n",
      "step: 31651, loss: 0.0\n",
      "step: 31652, loss: 2.38418573772492e-09\n",
      "step: 31653, loss: 0.0\n",
      "step: 31654, loss: 2.4795380682007817e-07\n",
      "step: 31655, loss: 0.0\n",
      "step: 31656, loss: 0.08167202025651932\n",
      "step: 31657, loss: 0.0\n",
      "step: 31658, loss: 7.15255632499634e-09\n",
      "step: 31659, loss: 0.0\n",
      "step: 31660, loss: 0.0\n",
      "step: 31661, loss: 0.0\n",
      "step: 31662, loss: 2.38418573772492e-09\n",
      "step: 31663, loss: 0.0\n",
      "step: 31664, loss: 0.0\n",
      "step: 31665, loss: 0.0\n",
      "step: 31666, loss: 0.0\n",
      "step: 31667, loss: 0.0\n",
      "step: 31668, loss: 1.192092558000013e-08\n",
      "step: 31669, loss: 2.38418573772492e-09\n",
      "step: 31670, loss: 4.76837103136063e-09\n",
      "step: 31671, loss: 1.430510998545742e-08\n",
      "step: 31672, loss: 0.0\n",
      "step: 31673, loss: 0.0\n",
      "step: 31674, loss: 0.0\n",
      "step: 31675, loss: 1.668929350273629e-08\n",
      "step: 31676, loss: 4.76837103136063e-09\n",
      "step: 31677, loss: 7.15255632499634e-09\n",
      "step: 31678, loss: 0.0\n",
      "step: 31679, loss: 2.38418573772492e-09\n",
      "step: 31680, loss: 6.437198294406699e-07\n",
      "step: 31681, loss: 0.0\n",
      "step: 31682, loss: 0.0\n",
      "step: 31683, loss: 0.0\n",
      "step: 31684, loss: 2.3841844054572903e-08\n",
      "step: 31685, loss: 0.0\n",
      "step: 31686, loss: 2.8610209312773804e-08\n",
      "step: 31687, loss: 1.192092558000013e-08\n",
      "step: 31688, loss: 4.76837103136063e-09\n",
      "step: 31689, loss: 0.0\n",
      "step: 31690, loss: 0.0\n",
      "step: 31691, loss: 0.0\n",
      "step: 31692, loss: 0.0\n",
      "step: 31693, loss: 5.340512529983243e-07\n",
      "step: 31694, loss: 0.0\n",
      "step: 31695, loss: 9.059889549689615e-08\n",
      "step: 31696, loss: 2.2649635411653435e-07\n",
      "step: 31697, loss: 0.00014718362945131958\n",
      "step: 31698, loss: 1.907347702001516e-08\n",
      "step: 31699, loss: 0.0\n",
      "step: 31700, loss: 0.0\n",
      "step: 31701, loss: 0.0\n",
      "step: 31702, loss: 9.53674295089968e-09\n",
      "step: 31703, loss: 5.245205159098987e-08\n",
      "step: 31704, loss: 0.0\n",
      "step: 31705, loss: 0.0\n",
      "step: 31706, loss: 1.430510998545742e-08\n",
      "step: 31707, loss: 0.0\n",
      "step: 31708, loss: 0.0\n",
      "step: 31709, loss: 4.76837103136063e-09\n",
      "step: 31710, loss: 0.0\n",
      "step: 31711, loss: 0.08500948548316956\n",
      "step: 31712, loss: 0.004030028358101845\n",
      "step: 31713, loss: 0.0\n",
      "step: 31714, loss: 0.0\n",
      "step: 31715, loss: 0.0\n",
      "step: 31716, loss: 0.0\n",
      "step: 31717, loss: 6.333778856060235e-06\n",
      "step: 31718, loss: 0.0\n",
      "step: 31719, loss: 0.0\n",
      "step: 31720, loss: 0.0\n",
      "step: 31721, loss: 2.1934415883606562e-07\n",
      "step: 31722, loss: 0.0\n",
      "step: 31723, loss: 3.0994389277338996e-08\n",
      "step: 31724, loss: 0.0\n",
      "step: 31725, loss: 1.168248857652543e-07\n",
      "step: 31726, loss: 2.38418573772492e-09\n",
      "step: 31727, loss: 0.0\n",
      "step: 31728, loss: 7.15255632499634e-09\n",
      "step: 31729, loss: 4.76837103136063e-09\n",
      "step: 31730, loss: 1.430510998545742e-08\n",
      "step: 31731, loss: 2.6226025795494934e-08\n",
      "step: 31732, loss: 1.430510998545742e-08\n",
      "step: 31733, loss: 0.0\n",
      "step: 31734, loss: 2.38418573772492e-09\n",
      "step: 31735, loss: 2.38418573772492e-09\n",
      "step: 31736, loss: 2.6226029348208613e-08\n",
      "step: 31737, loss: 0.0\n",
      "step: 31738, loss: 7.15255676908555e-09\n",
      "step: 31739, loss: 0.0\n",
      "step: 31740, loss: 0.0\n",
      "step: 31741, loss: 0.0\n",
      "step: 31742, loss: 0.0\n",
      "step: 31743, loss: 4.76837103136063e-09\n",
      "step: 31744, loss: 0.0\n",
      "step: 31745, loss: 6.2932604123489e-06\n",
      "step: 31746, loss: 0.0\n",
      "step: 31747, loss: 0.0\n",
      "step: 31748, loss: 0.0\n",
      "step: 31749, loss: 0.0\n",
      "step: 31750, loss: 0.010020459070801735\n",
      "step: 31751, loss: 4.529948327558486e-08\n",
      "step: 31752, loss: 2.38418573772492e-09\n",
      "step: 31753, loss: 1.430511264999268e-08\n",
      "step: 31754, loss: 9.53674117454284e-09\n",
      "step: 31755, loss: 0.0\n",
      "step: 31756, loss: 2.38418573772492e-09\n",
      "step: 31757, loss: 2.38418573772492e-09\n",
      "step: 31758, loss: 0.0\n",
      "step: 31759, loss: 1.8596570328099915e-07\n",
      "step: 31760, loss: 9.53674117454284e-09\n",
      "step: 31761, loss: 0.0\n",
      "step: 31762, loss: 0.0\n",
      "step: 31763, loss: 0.0\n",
      "step: 31764, loss: 1.192092646817855e-08\n",
      "step: 31765, loss: 1.668929350273629e-08\n",
      "step: 31766, loss: 0.0\n",
      "step: 31767, loss: 0.0065849535167217255\n",
      "step: 31768, loss: 0.11655009537935257\n",
      "step: 31769, loss: 0.03137015178799629\n",
      "step: 31770, loss: 0.0\n",
      "step: 31771, loss: 0.0\n",
      "step: 31772, loss: 1.3112979502238886e-07\n",
      "step: 31773, loss: 4.529947972287118e-08\n",
      "step: 31774, loss: 0.0\n",
      "step: 31775, loss: 9.53674117454284e-09\n",
      "step: 31776, loss: 1.5020326316061983e-07\n",
      "step: 31777, loss: 0.0\n",
      "step: 31778, loss: 0.0\n",
      "step: 31779, loss: 0.05103559046983719\n",
      "step: 31780, loss: 3.3378576347331546e-08\n",
      "step: 31781, loss: 0.0\n",
      "step: 31782, loss: 0.0\n",
      "step: 31783, loss: 0.0\n",
      "step: 31784, loss: 4.76837147544984e-09\n",
      "step: 31785, loss: 0.0\n",
      "step: 31786, loss: 1.192092558000013e-08\n",
      "step: 31787, loss: 0.0\n",
      "step: 31788, loss: 0.0\n",
      "step: 31789, loss: 0.05494409427046776\n",
      "step: 31790, loss: 2.38418573772492e-09\n",
      "step: 31791, loss: 3.3616734640418144e-07\n",
      "step: 31792, loss: 2.38418573772492e-09\n",
      "step: 31793, loss: 2.8610211089130644e-08\n",
      "step: 31794, loss: 2.38418573772492e-09\n",
      "step: 31795, loss: 4.76837103136063e-09\n",
      "step: 31796, loss: 0.0\n",
      "step: 31797, loss: 0.0\n",
      "step: 31798, loss: 2.38418573772492e-09\n",
      "step: 31799, loss: 1.668929350273629e-08\n",
      "step: 31800, loss: 0.0\n",
      "step: 31801, loss: 0.0\n",
      "step: 31802, loss: 0.0\n",
      "step: 31803, loss: 2.3841844054572903e-08\n",
      "step: 31804, loss: 7.15255632499634e-09\n",
      "step: 31805, loss: 2.38418573772492e-09\n",
      "step: 31806, loss: 4.76837147544984e-09\n",
      "step: 31807, loss: 0.0\n",
      "step: 31808, loss: 8.106224669290896e-08\n",
      "step: 31809, loss: 2.145766764272139e-08\n",
      "step: 31810, loss: 0.0\n",
      "step: 31811, loss: 3.0994389277338996e-08\n",
      "step: 31812, loss: 2.38418573772492e-09\n",
      "step: 31813, loss: 0.0\n",
      "step: 31814, loss: 0.0\n",
      "step: 31815, loss: 2.38418573772492e-09\n",
      "step: 31816, loss: 0.0\n",
      "step: 31817, loss: 0.0\n",
      "step: 31818, loss: 0.0\n",
      "step: 31819, loss: 0.0\n",
      "step: 31820, loss: 0.0\n",
      "step: 31821, loss: 0.0\n",
      "step: 31822, loss: 0.0\n",
      "step: 31823, loss: 0.0\n",
      "step: 31824, loss: 4.76837103136063e-09\n",
      "step: 31825, loss: 0.0\n",
      "step: 31826, loss: 2.38418573772492e-09\n",
      "step: 31827, loss: 0.0\n",
      "step: 31828, loss: 0.0\n",
      "step: 31829, loss: 0.1336386352777481\n",
      "step: 31830, loss: 1.4185403642841266e-06\n",
      "step: 31831, loss: 0.0\n",
      "step: 31832, loss: 2.6226025795494934e-08\n",
      "step: 31833, loss: 0.0\n",
      "step: 31834, loss: 7.15255632499634e-09\n",
      "step: 31835, loss: 0.0\n",
      "step: 31836, loss: 2.38418573772492e-09\n",
      "step: 31837, loss: 0.0\n",
      "step: 31838, loss: 0.0\n",
      "step: 31839, loss: 0.0\n",
      "step: 31840, loss: 0.0\n",
      "step: 31841, loss: 1.430511176181426e-08\n",
      "step: 31842, loss: 0.04352466017007828\n",
      "step: 31843, loss: 0.008868948556482792\n",
      "step: 31844, loss: 2.1457660537294032e-08\n",
      "step: 31845, loss: 0.00329639227129519\n",
      "step: 31846, loss: 0.0\n",
      "step: 31847, loss: 0.0\n",
      "step: 31848, loss: 4.76837103136063e-09\n",
      "step: 31849, loss: 0.0\n",
      "step: 31850, loss: 0.042330577969551086\n",
      "step: 31851, loss: 7.390962508679877e-08\n",
      "step: 31852, loss: 0.0\n",
      "step: 31853, loss: 2.956368518880481e-07\n",
      "step: 31854, loss: 0.0\n",
      "step: 31855, loss: 5.722039020383818e-08\n",
      "step: 31856, loss: 0.04083448275923729\n",
      "step: 31857, loss: 0.0\n",
      "step: 31858, loss: 0.1103425920009613\n",
      "step: 31859, loss: 4.172281364844821e-07\n",
      "step: 31860, loss: 0.0\n",
      "step: 31861, loss: 0.0\n",
      "step: 31862, loss: 2.3841844054572903e-08\n",
      "step: 31863, loss: 1.8881883079302497e-06\n",
      "step: 31864, loss: 0.0\n",
      "step: 31865, loss: 1.2874573940280243e-07\n",
      "step: 31866, loss: 9.313223081619526e-09\n",
      "step: 31867, loss: 4.76837147544984e-09\n",
      "step: 31868, loss: 0.0\n",
      "step: 31869, loss: 2.38418573772492e-09\n",
      "step: 31870, loss: 0.0\n",
      "step: 31871, loss: 0.0\n",
      "step: 31872, loss: 3.5762759864610416e-08\n",
      "step: 31873, loss: 0.0\n",
      "step: 31874, loss: 1.6879325812624302e-06\n",
      "step: 31875, loss: 0.0\n",
      "step: 31876, loss: 0.0011782299261540174\n",
      "step: 31877, loss: 1.1682484313269015e-07\n",
      "step: 31878, loss: 2.145766586636455e-08\n",
      "step: 31879, loss: 2.38418573772492e-09\n",
      "step: 31880, loss: 4.76837103136063e-09\n",
      "step: 31881, loss: 0.0\n",
      "step: 31882, loss: 3.0994389277338996e-08\n",
      "step: 31883, loss: 0.0\n",
      "step: 31884, loss: 0.0\n",
      "step: 31885, loss: 2.050389582564094e-07\n",
      "step: 31886, loss: 0.18692216277122498\n",
      "step: 31887, loss: 0.0\n",
      "step: 31888, loss: 3.0994389277338996e-08\n",
      "step: 31889, loss: 1.192092558000013e-08\n",
      "step: 31890, loss: 0.0\n",
      "step: 31891, loss: 2.38418573772492e-09\n",
      "step: 31892, loss: 0.0\n",
      "step: 31893, loss: 0.0\n",
      "step: 31894, loss: 2.38418573772492e-09\n",
      "step: 31895, loss: 9.53674117454284e-09\n",
      "step: 31896, loss: 0.0\n",
      "step: 31897, loss: 0.0\n",
      "step: 31898, loss: 0.0\n",
      "step: 31899, loss: 0.0\n",
      "step: 31900, loss: 0.0\n",
      "step: 31901, loss: 2.38418573772492e-09\n",
      "step: 31902, loss: 7.15255676908555e-09\n",
      "step: 31903, loss: 8.583050714605633e-08\n",
      "step: 31904, loss: 2.38418573772492e-09\n",
      "step: 31905, loss: 0.0\n",
      "step: 31906, loss: 2.38418573772492e-09\n",
      "step: 31907, loss: 0.0\n",
      "step: 31908, loss: 0.0\n",
      "step: 31909, loss: 0.0\n",
      "step: 31910, loss: 2.38418573772492e-09\n",
      "step: 31911, loss: 1.907347702001516e-08\n",
      "step: 31912, loss: 0.0\n",
      "step: 31913, loss: 2.38418573772492e-09\n",
      "step: 31914, loss: 4.76837103136063e-09\n",
      "step: 31915, loss: 0.0\n",
      "step: 31916, loss: 0.0\n",
      "step: 31917, loss: 0.0016438185703009367\n",
      "step: 31918, loss: 0.0\n",
      "step: 31919, loss: 3.576275631189674e-08\n",
      "step: 31920, loss: 4.291530686373335e-08\n",
      "step: 31921, loss: 9.53674117454284e-09\n",
      "step: 31922, loss: 4.76837103136063e-09\n",
      "step: 31923, loss: 0.0\n",
      "step: 31924, loss: 0.0\n",
      "step: 31925, loss: 6.19887430275412e-08\n",
      "step: 31926, loss: 0.0\n",
      "step: 31927, loss: 7.15255632499634e-09\n",
      "step: 31928, loss: 2.38418573772492e-09\n",
      "step: 31929, loss: 0.0\n",
      "step: 31930, loss: 0.0\n",
      "step: 31931, loss: 0.0\n",
      "step: 31932, loss: 2.38418573772492e-09\n",
      "step: 31933, loss: 0.0\n",
      "step: 31934, loss: 0.0\n",
      "step: 31935, loss: 0.0\n",
      "step: 31936, loss: 0.0\n",
      "step: 31937, loss: 0.0\n",
      "step: 31938, loss: 0.0\n",
      "step: 31939, loss: 4.76837103136063e-09\n",
      "step: 31940, loss: 0.0\n",
      "step: 31941, loss: 9.53674117454284e-09\n",
      "step: 31942, loss: 7.611256933159893e-06\n",
      "step: 31943, loss: 0.0\n",
      "step: 31944, loss: 0.0\n",
      "step: 31945, loss: 0.0\n",
      "step: 31946, loss: 0.0\n",
      "step: 31947, loss: 0.0\n",
      "step: 31948, loss: 2.38418573772492e-09\n",
      "step: 31949, loss: 3.0994389277338996e-08\n",
      "step: 31950, loss: 0.0\n",
      "step: 31951, loss: 0.0\n",
      "step: 31952, loss: 0.0\n",
      "step: 31953, loss: 0.0\n",
      "step: 31954, loss: 0.0\n",
      "step: 31955, loss: 2.38418573772492e-09\n",
      "step: 31956, loss: 0.0\n",
      "step: 31957, loss: 0.0\n",
      "step: 31958, loss: 1.192092558000013e-08\n",
      "step: 31959, loss: 0.0\n",
      "step: 31960, loss: 9.53674206272126e-09\n",
      "step: 31961, loss: 0.0\n",
      "step: 31962, loss: 2.38418573772492e-09\n",
      "step: 31963, loss: 3.337858700547258e-08\n",
      "step: 31964, loss: 0.0\n",
      "step: 31965, loss: 2.38418573772492e-09\n",
      "step: 31966, loss: 0.0\n",
      "step: 31967, loss: 9.040217264555395e-05\n",
      "step: 31968, loss: 0.0\n",
      "step: 31969, loss: 0.0\n",
      "step: 31970, loss: 0.0\n",
      "step: 31971, loss: 0.0\n",
      "step: 31972, loss: 0.0\n",
      "step: 31973, loss: 4.76837103136063e-09\n",
      "step: 31974, loss: 3.135130100417882e-05\n",
      "step: 31975, loss: 2.145766764272139e-08\n",
      "step: 31976, loss: 0.0\n",
      "step: 31977, loss: 0.06245299428701401\n",
      "step: 31978, loss: 2.38418573772492e-09\n",
      "step: 31979, loss: 2.38418573772492e-09\n",
      "step: 31980, loss: 4.76837103136063e-09\n",
      "step: 31981, loss: 0.0\n",
      "step: 31982, loss: 0.0\n",
      "step: 31983, loss: 0.0\n",
      "step: 31984, loss: 0.0\n",
      "step: 31985, loss: 2.8610216418201162e-08\n",
      "step: 31986, loss: 1.192092558000013e-08\n",
      "step: 31987, loss: 1.192092558000013e-08\n",
      "step: 31988, loss: 4.76837103136063e-09\n",
      "step: 31989, loss: 0.0\n",
      "step: 31990, loss: 0.0\n",
      "step: 31991, loss: 0.0\n",
      "step: 31992, loss: 1.192092558000013e-08\n",
      "step: 31993, loss: 2.38418573772492e-09\n",
      "step: 31994, loss: 3.2279285733238794e-06\n",
      "step: 31995, loss: 0.0\n",
      "step: 31996, loss: 2.38418573772492e-09\n",
      "step: 31997, loss: 0.0\n",
      "step: 31998, loss: 9.53674117454284e-09\n",
      "step: 31999, loss: 2.38418573772492e-09\n",
      "step: 32000, loss: 0.0\n",
      "step: 32001, loss: 4.76837103136063e-09\n",
      "step: 32002, loss: 0.0\n",
      "step: 32003, loss: 0.0\n",
      "step: 32004, loss: 0.001866971724666655\n",
      "step: 32005, loss: 0.0\n",
      "step: 32006, loss: 0.13080213963985443\n",
      "step: 32007, loss: 4.76837147544984e-09\n",
      "step: 32008, loss: 0.0\n",
      "step: 32009, loss: 0.0\n",
      "step: 32010, loss: 3.5762759864610416e-08\n",
      "step: 32011, loss: 0.020589666441082954\n",
      "step: 32012, loss: 0.0\n",
      "step: 32013, loss: 0.008461245335638523\n",
      "step: 32014, loss: 0.00045896574738435447\n",
      "step: 32015, loss: 0.0\n",
      "step: 32016, loss: 2.38418573772492e-09\n",
      "step: 32017, loss: 0.0\n",
      "step: 32018, loss: 0.005173246376216412\n",
      "step: 32019, loss: 0.08896690607070923\n",
      "step: 32020, loss: 0.0\n",
      "step: 32021, loss: 9.53674117454284e-09\n",
      "step: 32022, loss: 1.430510998545742e-08\n",
      "step: 32023, loss: 0.0\n",
      "step: 32024, loss: 0.0\n",
      "step: 32025, loss: 2.38418573772492e-09\n",
      "step: 32026, loss: 0.0\n",
      "step: 32027, loss: 0.0\n",
      "step: 32028, loss: 0.0\n",
      "step: 32029, loss: 0.080957792699337\n",
      "step: 32030, loss: 0.0\n",
      "step: 32031, loss: 0.0\n",
      "step: 32032, loss: 0.0\n",
      "step: 32033, loss: 0.0\n",
      "step: 32034, loss: 0.09725165367126465\n",
      "step: 32035, loss: 4.76837103136063e-09\n",
      "step: 32036, loss: 0.0\n",
      "step: 32037, loss: 0.0\n",
      "step: 32038, loss: 0.0\n",
      "step: 32039, loss: 0.0\n",
      "step: 32040, loss: 7.15255676908555e-09\n",
      "step: 32041, loss: 0.0\n",
      "step: 32042, loss: 2.38418573772492e-09\n",
      "step: 32043, loss: 0.0\n",
      "step: 32044, loss: 4.76837103136063e-09\n",
      "step: 32045, loss: 0.0\n",
      "step: 32046, loss: 0.0\n",
      "step: 32047, loss: 0.0\n",
      "step: 32048, loss: 1.430510998545742e-08\n",
      "step: 32049, loss: 1.668929350273629e-08\n",
      "step: 32050, loss: 3.576275631189674e-08\n",
      "step: 32051, loss: 0.043684929609298706\n",
      "step: 32052, loss: 0.0\n",
      "step: 32053, loss: 0.03497372940182686\n",
      "step: 32054, loss: 0.0\n",
      "step: 32055, loss: 0.0\n",
      "step: 32056, loss: 0.0\n",
      "step: 32057, loss: 2.38418573772492e-09\n",
      "step: 32058, loss: 0.1311257779598236\n",
      "step: 32059, loss: 0.0\n",
      "step: 32060, loss: 0.0\n",
      "step: 32061, loss: 6.651767989751534e-07\n",
      "step: 32062, loss: 7.390967482479027e-08\n",
      "step: 32063, loss: 0.0\n",
      "step: 32064, loss: 0.0\n",
      "step: 32065, loss: 0.0\n",
      "step: 32066, loss: 0.0\n",
      "step: 32067, loss: 0.0\n",
      "step: 32068, loss: 4.172281364844821e-07\n",
      "step: 32069, loss: 2.8610209312773804e-08\n",
      "step: 32070, loss: 4.76837103136063e-09\n",
      "step: 32071, loss: 1.907347702001516e-08\n",
      "step: 32072, loss: 0.0\n",
      "step: 32073, loss: 0.0\n",
      "step: 32074, loss: 3.337857279461787e-08\n",
      "step: 32075, loss: 0.0\n",
      "step: 32076, loss: 0.0\n",
      "step: 32077, loss: 0.0\n",
      "step: 32078, loss: 0.0\n",
      "step: 32079, loss: 7.15255632499634e-09\n",
      "step: 32080, loss: 0.0\n",
      "step: 32081, loss: 0.0\n",
      "step: 32082, loss: 0.08989919722080231\n",
      "step: 32083, loss: 0.0\n",
      "step: 32084, loss: 0.0\n",
      "step: 32085, loss: 0.0\n",
      "step: 32086, loss: 2.38418573772492e-09\n",
      "step: 32087, loss: 1.430510998545742e-08\n",
      "step: 32088, loss: 2.38418573772492e-09\n",
      "step: 32089, loss: 0.0\n",
      "step: 32090, loss: 0.0\n",
      "step: 32091, loss: 0.0\n",
      "step: 32092, loss: 2.2351731132630448e-08\n",
      "step: 32093, loss: 0.0\n",
      "step: 32094, loss: 0.0\n",
      "step: 32095, loss: 0.0\n",
      "step: 32096, loss: 0.0\n",
      "step: 32097, loss: 0.0\n",
      "step: 32098, loss: 0.0\n",
      "step: 32099, loss: 0.0\n",
      "step: 32100, loss: 4.76837103136063e-09\n",
      "step: 32101, loss: 0.01673903502523899\n",
      "step: 32102, loss: 0.12612123787403107\n",
      "step: 32103, loss: 4.76837103136063e-09\n",
      "step: 32104, loss: 0.0\n",
      "step: 32105, loss: 0.0\n",
      "step: 32106, loss: 0.0\n",
      "step: 32107, loss: 0.0\n",
      "step: 32108, loss: 0.0\n",
      "step: 32109, loss: 0.0\n",
      "step: 32110, loss: 0.0\n",
      "step: 32111, loss: 0.0\n",
      "step: 32112, loss: 4.76837103136063e-09\n",
      "step: 32113, loss: 4.76837103136063e-09\n",
      "step: 32114, loss: 2.38418573772492e-09\n",
      "step: 32115, loss: 0.0\n",
      "step: 32116, loss: 0.0\n",
      "step: 32117, loss: 5.006783965200157e-08\n",
      "step: 32118, loss: 2.7179532935406314e-07\n",
      "step: 32119, loss: 4.529947972287118e-08\n",
      "step: 32120, loss: 0.0\n",
      "step: 32121, loss: 0.0\n",
      "step: 32122, loss: 1.454348108609338e-07\n",
      "step: 32123, loss: 0.0\n",
      "step: 32124, loss: 1.263614990421047e-07\n",
      "step: 32125, loss: 0.0\n",
      "step: 32126, loss: 0.0\n",
      "step: 32127, loss: 2.1457660537294032e-08\n",
      "step: 32128, loss: 1.1920893427941337e-07\n",
      "step: 32129, loss: 7.15255676908555e-09\n",
      "step: 32130, loss: 4.529947972287118e-08\n",
      "step: 32131, loss: 0.0\n",
      "step: 32132, loss: 0.0\n",
      "step: 32133, loss: 0.0\n",
      "step: 32134, loss: 0.0\n",
      "step: 32135, loss: 0.0\n",
      "step: 32136, loss: 2.38418573772492e-09\n",
      "step: 32137, loss: 2.38418573772492e-09\n",
      "step: 32138, loss: 0.07545478641986847\n",
      "step: 32139, loss: 0.09464535117149353\n",
      "step: 32140, loss: 0.0\n",
      "step: 32141, loss: 0.003348932834342122\n",
      "step: 32142, loss: 0.0\n",
      "step: 32143, loss: 0.0\n",
      "step: 32144, loss: 1.192092646817855e-08\n",
      "step: 32145, loss: 9.53674117454284e-09\n",
      "step: 32146, loss: 0.0\n",
      "step: 32147, loss: 0.07779687643051147\n",
      "step: 32148, loss: 0.0\n",
      "step: 32149, loss: 1.430510998545742e-08\n",
      "step: 32150, loss: 0.0\n",
      "step: 32151, loss: 0.0\n",
      "step: 32152, loss: 4.76837103136063e-09\n",
      "step: 32153, loss: 2.8610216418201162e-08\n",
      "step: 32154, loss: 4.005392213457526e-07\n",
      "step: 32155, loss: 9.53674295089968e-09\n",
      "step: 32156, loss: 0.0\n",
      "step: 32157, loss: 0.0\n",
      "step: 32158, loss: 0.0\n",
      "step: 32159, loss: 3.0994389277338996e-08\n",
      "step: 32160, loss: 1.192092558000013e-08\n",
      "step: 32161, loss: 0.00015898235142230988\n",
      "step: 32162, loss: 0.0\n",
      "step: 32163, loss: 0.0\n",
      "step: 32164, loss: 6.675709585124423e-08\n",
      "step: 32165, loss: 2.455698222547653e-07\n",
      "step: 32166, loss: 0.0\n",
      "step: 32167, loss: 4.291529975830599e-08\n",
      "step: 32168, loss: 2.38418573772492e-09\n",
      "step: 32169, loss: 0.0\n",
      "step: 32170, loss: 1.668929350273629e-08\n",
      "step: 32171, loss: 9.53674117454284e-09\n",
      "step: 32172, loss: 0.0\n",
      "step: 32173, loss: 2.3292157038667938e-06\n",
      "step: 32174, loss: 5.722040796740657e-08\n",
      "step: 32175, loss: 4.76837103136063e-09\n",
      "step: 32176, loss: 0.0\n",
      "step: 32177, loss: 3.814693627646193e-08\n",
      "step: 32178, loss: 5.722037599298346e-08\n",
      "step: 32179, loss: 1.9311812593514333e-07\n",
      "step: 32180, loss: 0.0\n",
      "step: 32181, loss: 0.0\n",
      "step: 32182, loss: 0.0\n",
      "step: 32183, loss: 0.0\n",
      "step: 32184, loss: 3.850089797197143e-06\n",
      "step: 32185, loss: 2.38418573772492e-09\n",
      "step: 32186, loss: 0.0\n",
      "step: 32187, loss: 0.0\n",
      "step: 32188, loss: 0.0\n",
      "step: 32189, loss: 0.0\n",
      "step: 32190, loss: 0.0\n",
      "step: 32191, loss: 9.53674117454284e-09\n",
      "step: 32192, loss: 0.0\n",
      "step: 32193, loss: 0.0\n",
      "step: 32194, loss: 0.0\n",
      "step: 32195, loss: 0.0\n",
      "step: 32196, loss: 1.2874561150511e-07\n",
      "step: 32197, loss: 3.0897097076376667e-06\n",
      "step: 32198, loss: 0.0\n",
      "step: 32199, loss: 0.0\n",
      "step: 32200, loss: 0.0\n",
      "step: 32201, loss: 0.0\n",
      "step: 32202, loss: 2.38418573772492e-09\n",
      "step: 32203, loss: 0.1167866438627243\n",
      "step: 32204, loss: 0.0\n",
      "step: 32205, loss: 7.152550551836612e-08\n",
      "step: 32206, loss: 0.0\n",
      "step: 32207, loss: 2.38418573772492e-09\n",
      "step: 32208, loss: 0.0\n",
      "step: 32209, loss: 0.0\n",
      "step: 32210, loss: 4.76837103136063e-09\n",
      "step: 32211, loss: 9.939584742824081e-06\n",
      "step: 32212, loss: 0.0\n",
      "step: 32213, loss: 7.15255676908555e-09\n",
      "step: 32214, loss: 4.7683659687436375e-08\n",
      "step: 32215, loss: 0.0\n",
      "step: 32216, loss: 0.0\n",
      "step: 32217, loss: 0.0\n",
      "step: 32218, loss: 3.1022871553432196e-05\n",
      "step: 32219, loss: 0.0\n",
      "step: 32220, loss: 0.0\n",
      "step: 32221, loss: 0.0988679975271225\n",
      "step: 32222, loss: 0.0\n",
      "step: 32223, loss: 0.0\n",
      "step: 32224, loss: 3.337857279461787e-08\n",
      "step: 32225, loss: 0.0\n",
      "step: 32226, loss: 0.0\n",
      "step: 32227, loss: 0.05408085882663727\n",
      "step: 32228, loss: 0.0\n",
      "step: 32229, loss: 2.197012145188637e-05\n",
      "step: 32230, loss: 5.078255185253511e-07\n",
      "step: 32231, loss: 4.76837103136063e-09\n",
      "step: 32232, loss: 0.0\n",
      "step: 32233, loss: 0.17408159375190735\n",
      "step: 32234, loss: 0.0\n",
      "step: 32235, loss: 1.430511264999268e-08\n",
      "step: 32236, loss: 7.15255632499634e-09\n",
      "step: 32237, loss: 7.15255676908555e-09\n",
      "step: 32238, loss: 2.38418573772492e-09\n",
      "step: 32239, loss: 0.0\n",
      "step: 32240, loss: 0.0\n",
      "step: 32241, loss: 1.192092558000013e-08\n",
      "step: 32242, loss: 0.0\n",
      "step: 32243, loss: 2.6226025795494934e-08\n",
      "step: 32244, loss: 0.0\n",
      "step: 32245, loss: 0.0\n",
      "step: 32246, loss: 0.0\n",
      "step: 32247, loss: 9.53674117454284e-09\n",
      "step: 32248, loss: 0.0\n",
      "step: 32249, loss: 2.38418573772492e-09\n",
      "step: 32250, loss: 2.38418573772492e-09\n",
      "step: 32251, loss: 3.814693627646193e-08\n",
      "step: 32252, loss: 9.53674117454284e-09\n",
      "step: 32253, loss: 4.76837147544984e-09\n",
      "step: 32254, loss: 0.0\n",
      "step: 32255, loss: 0.0\n",
      "step: 32256, loss: 0.0\n",
      "step: 32257, loss: 1.9550233787413163e-07\n",
      "step: 32258, loss: 0.0\n",
      "step: 32259, loss: 2.38418573772492e-09\n",
      "step: 32260, loss: 4.053111624102712e-08\n",
      "step: 32261, loss: 0.0\n",
      "step: 32262, loss: 2.3841845830929742e-08\n",
      "step: 32263, loss: 0.0\n",
      "step: 32264, loss: 0.01390036940574646\n",
      "step: 32265, loss: 0.0\n",
      "step: 32266, loss: 4.291529975830599e-08\n",
      "step: 32267, loss: 0.0\n",
      "step: 32268, loss: 0.0\n",
      "step: 32269, loss: 0.0\n",
      "step: 32270, loss: 4.76837103136063e-09\n",
      "step: 32271, loss: 2.38418573772492e-09\n",
      "step: 32272, loss: 7.915340347608435e-07\n",
      "step: 32273, loss: 0.0\n",
      "step: 32274, loss: 2.38418573772492e-09\n",
      "step: 32275, loss: 2.1457660537294032e-08\n",
      "step: 32276, loss: 0.0635642260313034\n",
      "step: 32277, loss: 2.38418573772492e-09\n",
      "step: 32278, loss: 0.0\n",
      "step: 32279, loss: 7.15255676908555e-09\n",
      "step: 32280, loss: 0.0\n",
      "step: 32281, loss: 0.0\n",
      "step: 32282, loss: 0.0\n",
      "step: 32283, loss: 2.38418573772492e-09\n",
      "step: 32284, loss: 4.76837147544984e-09\n",
      "step: 32285, loss: 1.192092646817855e-08\n",
      "step: 32286, loss: 0.0\n",
      "step: 32287, loss: 2.1457660537294032e-08\n",
      "step: 32288, loss: 9.53674206272126e-09\n",
      "step: 32289, loss: 0.15214906632900238\n",
      "step: 32290, loss: 0.0\n",
      "step: 32291, loss: 9.53674206272126e-09\n",
      "step: 32292, loss: 0.0\n",
      "step: 32293, loss: 0.0\n",
      "step: 32294, loss: 0.0\n",
      "step: 32295, loss: 0.0\n",
      "step: 32296, loss: 0.0\n",
      "step: 32297, loss: 0.0\n",
      "step: 32298, loss: 0.0\n",
      "step: 32299, loss: 0.033140718936920166\n",
      "step: 32300, loss: 0.0\n",
      "step: 32301, loss: 9.53674117454284e-09\n",
      "step: 32302, loss: 8.106216142778067e-08\n",
      "step: 32303, loss: 9.298302927618352e-08\n",
      "step: 32304, loss: 2.38418573772492e-09\n",
      "step: 32305, loss: 1.3589811942438246e-07\n",
      "step: 32306, loss: 1.192092558000013e-08\n",
      "step: 32307, loss: 2.336488478249521e-07\n",
      "step: 32308, loss: 0.0\n",
      "step: 32309, loss: 0.0\n",
      "step: 32310, loss: 0.0\n",
      "step: 32311, loss: 0.0\n",
      "step: 32312, loss: 7.15255676908555e-09\n",
      "step: 32313, loss: 0.13741998374462128\n",
      "step: 32314, loss: 0.0\n",
      "step: 32315, loss: 4.76837103136063e-09\n",
      "step: 32316, loss: 0.0\n",
      "step: 32317, loss: 0.0\n",
      "step: 32318, loss: 0.0002909062022808939\n",
      "step: 32319, loss: 0.0\n",
      "step: 32320, loss: 3.5762763417324095e-08\n",
      "step: 32321, loss: 0.0\n",
      "step: 32322, loss: 0.0\n",
      "step: 32323, loss: 0.06492731720209122\n",
      "step: 32324, loss: 2.38418573772492e-09\n",
      "step: 32325, loss: 4.76837103136063e-09\n",
      "step: 32326, loss: 0.0\n",
      "step: 32327, loss: 2.6226025795494934e-08\n",
      "step: 32328, loss: 1.8834978732229501e-07\n",
      "step: 32329, loss: 0.0\n",
      "step: 32330, loss: 1.907347702001516e-08\n",
      "step: 32331, loss: 1.192092558000013e-08\n",
      "step: 32332, loss: 2.38418573772492e-09\n",
      "step: 32333, loss: 0.0\n",
      "step: 32334, loss: 0.0\n",
      "step: 32335, loss: 0.0\n",
      "step: 32336, loss: 0.0\n",
      "step: 32337, loss: 7.15255632499634e-09\n",
      "step: 32338, loss: 0.0\n",
      "step: 32339, loss: 0.0\n",
      "step: 32340, loss: 0.0\n",
      "step: 32341, loss: 1.6927650392517535e-07\n",
      "step: 32342, loss: 2.38418573772492e-09\n",
      "step: 32343, loss: 1.192092558000013e-08\n",
      "step: 32344, loss: 0.0\n",
      "step: 32345, loss: 2.38418573772492e-09\n",
      "step: 32346, loss: 0.0\n",
      "step: 32347, loss: 4.76837147544984e-09\n",
      "step: 32348, loss: 2.1695976215596602e-07\n",
      "step: 32349, loss: 0.0\n",
      "step: 32350, loss: 2.38418573772492e-09\n",
      "step: 32351, loss: 0.0\n",
      "step: 32352, loss: 4.76837103136063e-09\n",
      "step: 32353, loss: 0.0\n",
      "step: 32354, loss: 6.198873592211385e-08\n",
      "step: 32355, loss: 2.3841844054572903e-08\n",
      "step: 32356, loss: 0.05259140580892563\n",
      "step: 32357, loss: 0.0\n",
      "step: 32358, loss: 0.0\n",
      "step: 32359, loss: 0.0\n",
      "step: 32360, loss: 0.0\n",
      "step: 32361, loss: 0.0\n",
      "step: 32362, loss: 2.38418573772492e-09\n",
      "step: 32363, loss: 0.0\n",
      "step: 32364, loss: 0.0\n",
      "step: 32365, loss: 0.0\n",
      "step: 32366, loss: 4.76837103136063e-09\n",
      "step: 32367, loss: 0.0\n",
      "step: 32368, loss: 0.0\n",
      "step: 32369, loss: 4.76837103136063e-09\n",
      "step: 32370, loss: 2.38418573772492e-09\n",
      "step: 32371, loss: 2.38418573772492e-09\n",
      "step: 32372, loss: 2.551066700107185e-07\n",
      "step: 32373, loss: 0.09926643967628479\n",
      "step: 32374, loss: 0.0\n",
      "step: 32375, loss: 0.0\n",
      "step: 32376, loss: 0.0\n",
      "step: 32377, loss: 0.0\n",
      "step: 32378, loss: 0.0\n",
      "step: 32379, loss: 0.0\n",
      "step: 32380, loss: 0.008425405248999596\n",
      "step: 32381, loss: 4.76837103136063e-09\n",
      "step: 32382, loss: 0.0\n",
      "step: 32383, loss: 4.0531130451881836e-08\n",
      "step: 32384, loss: 2.38418573772492e-09\n",
      "step: 32385, loss: 0.0\n",
      "step: 32386, loss: 0.0\n",
      "step: 32387, loss: 7.390963219222613e-08\n",
      "step: 32388, loss: 2.38418573772492e-09\n",
      "step: 32389, loss: 0.0\n",
      "step: 32390, loss: 0.0\n",
      "step: 32391, loss: 0.0\n",
      "step: 32392, loss: 0.0\n",
      "step: 32393, loss: 0.0\n",
      "step: 32394, loss: 0.0\n",
      "step: 32395, loss: 1.430511176181426e-08\n",
      "step: 32396, loss: 0.0\n",
      "step: 32397, loss: 0.0\n",
      "step: 32398, loss: 0.0\n",
      "step: 32399, loss: 0.0\n",
      "step: 32400, loss: 0.0\n",
      "step: 32401, loss: 2.38418573772492e-09\n",
      "step: 32402, loss: 0.0\n",
      "step: 32403, loss: 0.0\n",
      "step: 32404, loss: 7.15255632499634e-09\n",
      "step: 32405, loss: 0.0\n",
      "step: 32406, loss: 0.0\n",
      "step: 32407, loss: 1.907347702001516e-08\n",
      "step: 32408, loss: 0.053164634853601456\n",
      "step: 32409, loss: 1.6259486983472016e-06\n",
      "step: 32410, loss: 0.0\n",
      "step: 32411, loss: 0.0\n",
      "step: 32412, loss: 2.38418573772492e-09\n",
      "step: 32413, loss: 0.0\n",
      "step: 32414, loss: 0.0\n",
      "step: 32415, loss: 4.0336381061933935e-06\n",
      "step: 32416, loss: 9.53674206272126e-09\n",
      "step: 32417, loss: 1.6617126448181807e-06\n",
      "step: 32418, loss: 2.38418573772492e-09\n",
      "step: 32419, loss: 0.0\n",
      "step: 32420, loss: 0.0\n",
      "step: 32421, loss: 0.007873144000768661\n",
      "step: 32422, loss: 0.1356753706932068\n",
      "step: 32423, loss: 0.0\n",
      "step: 32424, loss: 2.38418573772492e-09\n",
      "step: 32425, loss: 0.0\n",
      "step: 32426, loss: 2.38418573772492e-09\n",
      "step: 32427, loss: 0.0\n",
      "step: 32428, loss: 0.0\n",
      "step: 32429, loss: 0.000668256136123091\n",
      "step: 32430, loss: 0.0\n",
      "step: 32431, loss: 0.0\n",
      "step: 32432, loss: 0.04259338974952698\n",
      "step: 32433, loss: 1.430510998545742e-08\n",
      "step: 32434, loss: 0.0\n",
      "step: 32435, loss: 2.38418573772492e-09\n",
      "step: 32436, loss: 0.0\n",
      "step: 32437, loss: 0.15967920422554016\n",
      "step: 32438, loss: 5.483619602841827e-08\n",
      "step: 32439, loss: 0.0\n",
      "step: 32440, loss: 0.0\n",
      "step: 32441, loss: 0.0\n",
      "step: 32442, loss: 2.38418573772492e-09\n",
      "step: 32443, loss: 2.38418573772492e-09\n",
      "step: 32444, loss: 0.0\n",
      "step: 32445, loss: 0.0\n",
      "step: 32446, loss: 7.15255632499634e-09\n",
      "step: 32447, loss: 0.0\n",
      "step: 32448, loss: 0.0\n",
      "step: 32449, loss: 7.15255632499634e-09\n",
      "step: 32450, loss: 7.152544867494726e-08\n",
      "step: 32451, loss: 0.0\n",
      "step: 32452, loss: 0.0\n",
      "step: 32453, loss: 5.960456661568969e-08\n",
      "step: 32454, loss: 3.7908210970272194e-07\n",
      "step: 32455, loss: 4.76837103136063e-09\n",
      "step: 32456, loss: 2.384185116000026e-08\n",
      "step: 32457, loss: 0.0\n",
      "step: 32458, loss: 2.38418573772492e-09\n",
      "step: 32459, loss: 0.0\n",
      "step: 32460, loss: 0.0\n",
      "step: 32461, loss: 0.0\n",
      "step: 32462, loss: 0.0\n",
      "step: 32463, loss: 0.0\n",
      "step: 32464, loss: 2.38418573772492e-09\n",
      "step: 32465, loss: 0.0\n",
      "step: 32466, loss: 2.145766231365087e-08\n",
      "step: 32467, loss: 2.384185116000026e-08\n",
      "step: 32468, loss: 4.76837147544984e-09\n",
      "step: 32469, loss: 0.0\n",
      "step: 32470, loss: 9.53674117454284e-09\n",
      "step: 32471, loss: 2.38418573772492e-09\n",
      "step: 32472, loss: 0.0\n",
      "step: 32473, loss: 1.1205654715240598e-07\n",
      "step: 32474, loss: 0.0\n",
      "step: 32475, loss: 2.38418573772492e-09\n",
      "step: 32476, loss: 7.15255632499634e-09\n",
      "step: 32477, loss: 8.106222537662688e-08\n",
      "step: 32478, loss: 2.38418573772492e-09\n",
      "step: 32479, loss: 0.0\n",
      "step: 32480, loss: 0.0\n",
      "step: 32481, loss: 0.0\n",
      "step: 32482, loss: 0.0\n",
      "step: 32483, loss: 7.15255676908555e-09\n",
      "step: 32484, loss: 0.0\n",
      "step: 32485, loss: 0.06128263846039772\n",
      "step: 32486, loss: 0.0\n",
      "step: 32487, loss: 0.0\n",
      "step: 32488, loss: 0.0\n",
      "step: 32489, loss: 0.0\n",
      "step: 32490, loss: 0.0\n",
      "step: 32491, loss: 0.0\n",
      "step: 32492, loss: 0.0\n",
      "step: 32493, loss: 0.0\n",
      "step: 32494, loss: 0.0673871859908104\n",
      "step: 32495, loss: 4.529947972287118e-08\n",
      "step: 32496, loss: 5.5233958846656606e-06\n",
      "step: 32497, loss: 9.53674117454284e-09\n",
      "step: 32498, loss: 2.38418573772492e-09\n",
      "step: 32499, loss: 0.0\n",
      "step: 32500, loss: 4.2915313969160707e-08\n",
      "step: 32501, loss: 0.0\n",
      "step: 32502, loss: 0.0\n",
      "step: 32503, loss: 4.76837103136063e-09\n",
      "step: 32504, loss: 0.0\n",
      "step: 32505, loss: 2.38418573772492e-09\n",
      "step: 32506, loss: 1.2439196325431112e-05\n",
      "step: 32507, loss: 0.0\n",
      "step: 32508, loss: 0.0\n",
      "step: 32509, loss: 0.0\n",
      "step: 32510, loss: 0.0\n",
      "step: 32511, loss: 0.0\n",
      "step: 32512, loss: 0.0\n",
      "step: 32513, loss: 0.0\n",
      "step: 32514, loss: 0.0\n",
      "step: 32515, loss: 0.0\n",
      "step: 32516, loss: 0.0\n",
      "step: 32517, loss: 0.0\n",
      "step: 32518, loss: 0.0\n",
      "step: 32519, loss: 0.0\n",
      "step: 32520, loss: 0.0\n",
      "step: 32521, loss: 2.275840779475402e-05\n",
      "step: 32522, loss: 2.38418573772492e-09\n",
      "step: 32523, loss: 0.0\n",
      "step: 32524, loss: 4.76837147544984e-09\n",
      "step: 32525, loss: 0.0\n",
      "step: 32526, loss: 0.0\n",
      "step: 32527, loss: 0.08325386792421341\n",
      "step: 32528, loss: 0.0\n",
      "step: 32529, loss: 0.0\n",
      "step: 32530, loss: 0.0\n",
      "step: 32531, loss: 2.38418573772492e-09\n",
      "step: 32532, loss: 0.0\n",
      "step: 32533, loss: 3.814693627646193e-08\n",
      "step: 32534, loss: 0.0\n",
      "step: 32535, loss: 4.7683659687436375e-08\n",
      "step: 32536, loss: 4.76837147544984e-09\n",
      "step: 32537, loss: 0.0\n",
      "step: 32538, loss: 0.0\n",
      "step: 32539, loss: 0.0\n",
      "step: 32540, loss: 0.0\n",
      "step: 32541, loss: 9.53674206272126e-09\n",
      "step: 32542, loss: 0.009533338248729706\n",
      "step: 32543, loss: 0.0\n",
      "step: 32544, loss: 0.0\n",
      "step: 32545, loss: 0.0\n",
      "step: 32546, loss: 0.0\n",
      "step: 32547, loss: 3.3089827411458828e-06\n",
      "step: 32548, loss: 0.0\n",
      "step: 32549, loss: 0.0\n",
      "step: 32550, loss: 0.0\n",
      "step: 32551, loss: 0.0\n",
      "step: 32552, loss: 0.0\n",
      "step: 32553, loss: 4.76837103136063e-09\n",
      "step: 32554, loss: 2.38418573772492e-09\n",
      "step: 32555, loss: 0.058275237679481506\n",
      "step: 32556, loss: 0.0\n",
      "step: 32557, loss: 1.4781898016735795e-07\n",
      "step: 32558, loss: 0.0\n",
      "step: 32559, loss: 0.0\n",
      "step: 32560, loss: 0.0\n",
      "step: 32561, loss: 6.914127226309574e-08\n",
      "step: 32562, loss: 0.0\n",
      "step: 32563, loss: 1.192092558000013e-08\n",
      "step: 32564, loss: 3.0994396382766354e-08\n",
      "step: 32565, loss: 0.0\n",
      "step: 32566, loss: 0.0\n",
      "step: 32567, loss: 0.0\n",
      "step: 32568, loss: 1.668929350273629e-08\n",
      "step: 32569, loss: 2.38418573772492e-09\n",
      "step: 32570, loss: 0.0\n",
      "step: 32571, loss: 2.38418573772492e-09\n",
      "step: 32572, loss: 1.192092558000013e-08\n",
      "step: 32573, loss: 1.1205641214928619e-07\n",
      "step: 32574, loss: 0.0\n",
      "step: 32575, loss: 0.0\n",
      "step: 32576, loss: 0.0\n",
      "step: 32577, loss: 0.0\n",
      "step: 32578, loss: 0.0\n",
      "step: 32579, loss: 0.0\n",
      "step: 32580, loss: 0.0\n",
      "step: 32581, loss: 2.38418573772492e-09\n",
      "step: 32582, loss: 0.0\n",
      "step: 32583, loss: 0.0\n",
      "step: 32584, loss: 3.8861850271132425e-07\n",
      "step: 32585, loss: 2.38418573772492e-09\n",
      "step: 32586, loss: 4.6252668539636943e-07\n",
      "step: 32587, loss: 0.0\n",
      "step: 32588, loss: 1.430511264999268e-08\n",
      "step: 32589, loss: 0.0\n",
      "step: 32590, loss: 0.0\n",
      "step: 32591, loss: 2.38418573772492e-09\n",
      "step: 32592, loss: 0.0\n",
      "step: 32593, loss: 0.0\n",
      "step: 32594, loss: 0.0\n",
      "step: 32595, loss: 0.0\n",
      "step: 32596, loss: 1.668929350273629e-08\n",
      "step: 32597, loss: 4.76837103136063e-09\n",
      "step: 32598, loss: 7.432509846694302e-06\n",
      "step: 32599, loss: 0.00018024342716671526\n",
      "step: 32600, loss: 2.6226025795494934e-08\n",
      "step: 32601, loss: 0.0\n",
      "step: 32602, loss: 0.0\n",
      "step: 32603, loss: 2.38418573772492e-09\n",
      "step: 32604, loss: 0.000862880377098918\n",
      "step: 32605, loss: 0.0\n",
      "step: 32606, loss: 2.38418573772492e-09\n",
      "step: 32607, loss: 2.38418573772492e-09\n",
      "step: 32608, loss: 5.722037599298346e-08\n",
      "step: 32609, loss: 0.0\n",
      "step: 32610, loss: 0.0\n",
      "step: 32611, loss: 2.0265490263682295e-07\n",
      "step: 32612, loss: 2.38418573772492e-09\n",
      "step: 32613, loss: 0.0\n",
      "step: 32614, loss: 2.38418573772492e-09\n",
      "step: 32615, loss: 0.0\n",
      "step: 32616, loss: 0.0\n",
      "step: 32617, loss: 0.0\n",
      "step: 32618, loss: 0.06974466145038605\n",
      "step: 32619, loss: 0.0\n",
      "step: 32620, loss: 0.014930975623428822\n",
      "step: 32621, loss: 0.0\n",
      "step: 32622, loss: 0.0\n",
      "step: 32623, loss: 0.0\n",
      "step: 32624, loss: 2.6226029348208613e-08\n",
      "step: 32625, loss: 9.53674117454284e-09\n",
      "step: 32626, loss: 0.0\n",
      "step: 32627, loss: 0.00029858146444894373\n",
      "step: 32628, loss: 0.0\n",
      "step: 32629, loss: 0.0\n",
      "step: 32630, loss: 0.0\n",
      "step: 32631, loss: 4.00539249767462e-07\n",
      "step: 32632, loss: 0.0\n",
      "step: 32633, loss: 0.0\n",
      "step: 32634, loss: 0.0\n",
      "step: 32635, loss: 0.0\n",
      "step: 32636, loss: 0.0\n",
      "step: 32637, loss: 7.15255676908555e-09\n",
      "step: 32638, loss: 1.668929350273629e-08\n",
      "step: 32639, loss: 9.53674117454284e-09\n",
      "step: 32640, loss: 3.337857279461787e-08\n",
      "step: 32641, loss: 2.38418573772492e-09\n",
      "step: 32642, loss: 2.38418573772492e-09\n",
      "step: 32643, loss: 8.344633073420482e-08\n",
      "step: 32644, loss: 0.0\n",
      "step: 32645, loss: 0.0\n",
      "step: 32646, loss: 0.0\n",
      "step: 32647, loss: 0.0\n",
      "step: 32648, loss: 5.342246822692687e-06\n",
      "step: 32649, loss: 0.0\n",
      "step: 32650, loss: 0.0\n",
      "step: 32651, loss: 7.15255632499634e-09\n",
      "step: 32652, loss: 0.0\n",
      "step: 32653, loss: 0.0\n",
      "step: 32654, loss: 4.76837103136063e-09\n",
      "step: 32655, loss: 2.38418573772492e-09\n",
      "step: 32656, loss: 0.0\n",
      "step: 32657, loss: 2.38418573772492e-09\n",
      "step: 32658, loss: 1.0585506515781162e-06\n",
      "step: 32659, loss: 7.152551972922083e-08\n",
      "step: 32660, loss: 9.53674206272126e-09\n",
      "step: 32661, loss: 0.0\n",
      "step: 32662, loss: 0.0\n",
      "step: 32663, loss: 2.38418573772492e-09\n",
      "step: 32664, loss: 0.0\n",
      "step: 32665, loss: 0.0\n",
      "step: 32666, loss: 3.576275631189674e-08\n",
      "step: 32667, loss: 7.15255632499634e-09\n",
      "step: 32668, loss: 0.0\n",
      "step: 32669, loss: 0.0\n",
      "step: 32670, loss: 0.0\n",
      "step: 32671, loss: 6.989211669861106e-06\n",
      "step: 32672, loss: 1.907348234908568e-08\n",
      "step: 32673, loss: 1.881042749118933e-06\n",
      "step: 32674, loss: 0.0\n",
      "step: 32675, loss: 2.8610209312773804e-08\n",
      "step: 32676, loss: 0.0\n",
      "step: 32677, loss: 0.0\n",
      "step: 32678, loss: 3.0515250273310812e-06\n",
      "step: 32679, loss: 0.0\n",
      "step: 32680, loss: 0.0\n",
      "step: 32681, loss: 2.38418573772492e-09\n",
      "step: 32682, loss: 0.0\n",
      "step: 32683, loss: 0.0\n",
      "step: 32684, loss: 2.38418573772492e-09\n",
      "step: 32685, loss: 0.07058626413345337\n",
      "step: 32686, loss: 0.0\n",
      "step: 32687, loss: 0.0\n",
      "step: 32688, loss: 0.0\n",
      "step: 32689, loss: 7.15255632499634e-09\n",
      "step: 32690, loss: 2.6226025795494934e-08\n",
      "step: 32691, loss: 0.0\n",
      "step: 32692, loss: 0.03169920668005943\n",
      "step: 32693, loss: 0.0\n",
      "step: 32694, loss: 0.0\n",
      "step: 32695, loss: 0.0\n",
      "step: 32696, loss: 0.0\n",
      "step: 32697, loss: 0.0\n",
      "step: 32698, loss: 0.0\n",
      "step: 32699, loss: 0.0\n",
      "step: 32700, loss: 0.0\n",
      "step: 32701, loss: 0.0\n",
      "step: 32702, loss: 0.0\n",
      "step: 32703, loss: 0.0\n",
      "step: 32704, loss: 0.0\n",
      "step: 32705, loss: 6.198873592211385e-08\n",
      "step: 32706, loss: 0.0\n",
      "step: 32707, loss: 2.3603298870966682e-07\n",
      "step: 32708, loss: 0.0\n",
      "step: 32709, loss: 2.38418573772492e-09\n",
      "step: 32710, loss: 3.0994389277338996e-08\n",
      "step: 32711, loss: 7.15255676908555e-09\n",
      "step: 32712, loss: 2.38418573772492e-09\n",
      "step: 32713, loss: 2.38418573772492e-09\n",
      "step: 32714, loss: 4.76837147544984e-09\n",
      "step: 32715, loss: 1.907347702001516e-08\n",
      "step: 32716, loss: 0.0\n",
      "step: 32717, loss: 9.53674117454284e-09\n",
      "step: 32718, loss: 0.0\n",
      "step: 32719, loss: 5.722038309841082e-08\n",
      "step: 32720, loss: 2.38418573772492e-09\n",
      "step: 32721, loss: 0.0\n",
      "step: 32722, loss: 0.0\n",
      "step: 32723, loss: 2.38418573772492e-09\n",
      "step: 32724, loss: 0.0\n",
      "step: 32725, loss: 0.0\n",
      "step: 32726, loss: 0.0015931484522297978\n",
      "step: 32727, loss: 1.192092824453539e-08\n",
      "step: 32728, loss: 4.26764700023341e-07\n",
      "step: 32729, loss: 0.0\n",
      "step: 32730, loss: 7.15255676908555e-09\n",
      "step: 32731, loss: 0.0\n",
      "step: 32732, loss: 0.0\n",
      "step: 32733, loss: 0.1747441440820694\n",
      "step: 32734, loss: 0.13484688103199005\n",
      "step: 32735, loss: 9.775141052159597e-08\n",
      "step: 32736, loss: 0.0\n",
      "step: 32737, loss: 0.0\n",
      "step: 32738, loss: 5.006783965200157e-08\n",
      "step: 32739, loss: 2.38418573772492e-09\n",
      "step: 32740, loss: 0.05571423098444939\n",
      "step: 32741, loss: 0.0\n",
      "step: 32742, loss: 0.0\n",
      "step: 32743, loss: 2.38418573772492e-09\n",
      "step: 32744, loss: 2.38418573772492e-09\n",
      "step: 32745, loss: 2.38418573772492e-09\n",
      "step: 32746, loss: 7.15255632499634e-09\n",
      "step: 32747, loss: 2.38418573772492e-09\n",
      "step: 32748, loss: 1.430510998545742e-08\n",
      "step: 32749, loss: 0.0\n",
      "step: 32750, loss: 0.0\n",
      "step: 32751, loss: 0.008058801293373108\n",
      "step: 32752, loss: 2.38418573772492e-09\n",
      "step: 32753, loss: 0.0\n",
      "step: 32754, loss: 2.8610218194558e-08\n",
      "step: 32755, loss: 0.07279817759990692\n",
      "step: 32756, loss: 0.0\n",
      "step: 32757, loss: 0.0\n",
      "step: 32758, loss: 2.3841844054572903e-08\n",
      "step: 32759, loss: 1.430510998545742e-08\n",
      "step: 32760, loss: 0.0\n",
      "step: 32761, loss: 0.0\n",
      "step: 32762, loss: 4.76837103136063e-09\n",
      "step: 32763, loss: 0.0\n",
      "step: 32764, loss: 0.0\n",
      "step: 32765, loss: 0.0\n",
      "step: 32766, loss: 0.0\n",
      "step: 32767, loss: 2.38418573772492e-09\n",
      "step: 32768, loss: 0.0\n",
      "step: 32769, loss: 1.192092558000013e-08\n",
      "step: 32770, loss: 5.587934559514451e-09\n",
      "step: 32771, loss: 0.0\n",
      "step: 32772, loss: 4.76837147544984e-09\n",
      "step: 32773, loss: 9.53674117454284e-09\n",
      "step: 32774, loss: 0.0\n",
      "step: 32775, loss: 0.0\n",
      "step: 32776, loss: 2.38418573772492e-09\n",
      "step: 32777, loss: 0.0\n",
      "step: 32778, loss: 2.38418573772492e-09\n",
      "step: 32779, loss: 0.0\n",
      "step: 32780, loss: 0.0\n",
      "step: 32781, loss: 4.76837103136063e-09\n",
      "step: 32782, loss: 0.0\n",
      "step: 32783, loss: 0.0\n",
      "step: 32784, loss: 0.0\n",
      "step: 32785, loss: 0.0\n",
      "step: 32786, loss: 2.408013131116604e-07\n",
      "step: 32787, loss: 0.0\n",
      "step: 32788, loss: 0.04809587821364403\n",
      "step: 32789, loss: 9.0598852864332e-08\n",
      "step: 32790, loss: 0.0\n",
      "step: 32791, loss: 0.0\n",
      "step: 32792, loss: 3.814693627646193e-08\n",
      "step: 32793, loss: 2.38418573772492e-09\n",
      "step: 32794, loss: 0.0\n",
      "step: 32795, loss: 0.0\n",
      "step: 32796, loss: 0.0\n",
      "step: 32797, loss: 2.38418573772492e-09\n",
      "step: 32798, loss: 3.8146946934602965e-08\n",
      "step: 32799, loss: 0.0\n",
      "step: 32800, loss: 0.21967849135398865\n",
      "step: 32801, loss: 0.0\n",
      "step: 32802, loss: 0.07063516974449158\n",
      "step: 32803, loss: 0.0\n",
      "step: 32804, loss: 0.0\n",
      "step: 32805, loss: 0.0\n",
      "step: 32806, loss: 0.0\n",
      "step: 32807, loss: 0.0\n",
      "step: 32808, loss: 2.38418573772492e-09\n",
      "step: 32809, loss: 0.0\n",
      "step: 32810, loss: 0.0\n",
      "step: 32811, loss: 0.0\n",
      "step: 32812, loss: 0.0\n",
      "step: 32813, loss: 0.0\n",
      "step: 32814, loss: 0.0\n",
      "step: 32815, loss: 1.430510998545742e-08\n",
      "step: 32816, loss: 0.0\n",
      "step: 32817, loss: 2.38418573772492e-09\n",
      "step: 32818, loss: 2.1457660537294032e-08\n",
      "step: 32819, loss: 0.0010807376820594072\n",
      "step: 32820, loss: 0.0\n",
      "step: 32821, loss: 0.0\n",
      "step: 32822, loss: 0.0\n",
      "step: 32823, loss: 0.0\n",
      "step: 32824, loss: 0.0\n",
      "step: 32825, loss: 2.551062721067865e-07\n",
      "step: 32826, loss: 0.0\n",
      "step: 32827, loss: 0.0\n",
      "step: 32828, loss: 2.8610209312773804e-08\n",
      "step: 32829, loss: 0.0\n",
      "step: 32830, loss: 0.0\n",
      "step: 32831, loss: 3.576275631189674e-08\n",
      "step: 32832, loss: 0.0\n",
      "step: 32833, loss: 0.0\n",
      "step: 32834, loss: 0.0\n",
      "step: 32835, loss: 2.38418573772492e-09\n",
      "step: 32836, loss: 0.0009269145084545016\n",
      "step: 32837, loss: 0.0\n",
      "step: 32838, loss: 1.192092558000013e-08\n",
      "step: 32839, loss: 9.53674295089968e-09\n",
      "step: 32840, loss: 1.192092558000013e-08\n",
      "step: 32841, loss: 0.0007024171063676476\n",
      "step: 32842, loss: 0.0\n",
      "step: 32843, loss: 2.3841845830929742e-08\n",
      "step: 32844, loss: 0.0\n",
      "step: 32845, loss: 0.0\n",
      "step: 32846, loss: 0.0\n",
      "step: 32847, loss: 0.0\n",
      "step: 32848, loss: 2.38418573772492e-09\n",
      "step: 32849, loss: 0.05129827931523323\n",
      "step: 32850, loss: 0.0\n",
      "step: 32851, loss: 9.53674206272126e-09\n",
      "step: 32852, loss: 0.0\n",
      "step: 32853, loss: 0.0\n",
      "step: 32854, loss: 2.6226029348208613e-08\n",
      "step: 32855, loss: 0.0\n",
      "step: 32856, loss: 2.38418573772492e-09\n",
      "step: 32857, loss: 0.0\n",
      "step: 32858, loss: 2.3126467851852794e-07\n",
      "step: 32859, loss: 0.0\n",
      "step: 32860, loss: 4.76837103136063e-09\n",
      "step: 32861, loss: 0.0\n",
      "step: 32862, loss: 9.53674117454284e-09\n",
      "step: 32863, loss: 2.38418573772492e-09\n",
      "step: 32864, loss: 0.0\n",
      "step: 32865, loss: 0.12662507593631744\n",
      "step: 32866, loss: 1.090475961973425e-05\n",
      "step: 32867, loss: 2.38418573772492e-09\n",
      "step: 32868, loss: 2.38418573772492e-09\n",
      "step: 32869, loss: 0.0\n",
      "step: 32870, loss: 0.0\n",
      "step: 32871, loss: 3.0994389277338996e-08\n",
      "step: 32872, loss: 4.76837103136063e-09\n",
      "step: 32873, loss: 0.03567986935377121\n",
      "step: 32874, loss: 2.38418573772492e-09\n",
      "step: 32875, loss: 0.0\n",
      "step: 32876, loss: 0.0\n",
      "step: 32877, loss: 0.0\n",
      "step: 32878, loss: 2.38418573772492e-09\n",
      "step: 32879, loss: 0.0\n",
      "step: 32880, loss: 0.0\n",
      "step: 32881, loss: 4.76837147544984e-09\n",
      "step: 32882, loss: 0.0\n",
      "step: 32883, loss: 2.38418573772492e-09\n",
      "step: 32884, loss: 0.0\n",
      "step: 32885, loss: 0.0\n",
      "step: 32886, loss: 0.0\n",
      "step: 32887, loss: 0.0\n",
      "step: 32888, loss: 2.38418573772492e-09\n",
      "step: 32889, loss: 0.0\n",
      "step: 32890, loss: 0.0\n",
      "step: 32891, loss: 9.77514389433054e-08\n",
      "step: 32892, loss: 0.0\n",
      "step: 32893, loss: 2.38418573772492e-09\n",
      "step: 32894, loss: 0.1514013409614563\n",
      "step: 32895, loss: 0.0\n",
      "step: 32896, loss: 0.0\n",
      "step: 32897, loss: 0.0\n",
      "step: 32898, loss: 0.13823452591896057\n",
      "step: 32899, loss: 0.0\n",
      "step: 32900, loss: 0.0\n",
      "step: 32901, loss: 0.0\n",
      "step: 32902, loss: 7.390966061393556e-08\n",
      "step: 32903, loss: 0.0\n",
      "step: 32904, loss: 2.8610209312773804e-08\n",
      "step: 32905, loss: 1.192092558000013e-08\n",
      "step: 32906, loss: 0.0\n",
      "step: 32907, loss: 2.8610209312773804e-08\n",
      "step: 32908, loss: 2.38418573772492e-09\n",
      "step: 32909, loss: 4.76837103136063e-09\n",
      "step: 32910, loss: 0.0\n",
      "step: 32911, loss: 0.0\n",
      "step: 32912, loss: 0.0\n",
      "step: 32913, loss: 0.0\n",
      "step: 32914, loss: 0.0\n",
      "step: 32915, loss: 9.53674117454284e-09\n",
      "step: 32916, loss: 0.0\n",
      "step: 32917, loss: 0.0\n",
      "step: 32918, loss: 9.53674117454284e-09\n",
      "step: 32919, loss: 0.0\n",
      "step: 32920, loss: 0.0\n",
      "step: 32921, loss: 0.0\n",
      "step: 32922, loss: 0.0\n",
      "step: 32923, loss: 0.0\n",
      "step: 32924, loss: 3.926368663087487e-06\n",
      "step: 32925, loss: 4.529947972287118e-08\n",
      "step: 32926, loss: 0.0\n",
      "step: 32927, loss: 0.0\n",
      "step: 32928, loss: 0.0\n",
      "step: 32929, loss: 4.76837103136063e-09\n",
      "step: 32930, loss: 0.0\n",
      "step: 32931, loss: 0.0\n",
      "step: 32932, loss: 1.668929350273629e-08\n",
      "step: 32933, loss: 2.38418573772492e-09\n",
      "step: 32934, loss: 0.0\n",
      "step: 32935, loss: 0.0\n",
      "step: 32936, loss: 0.0\n",
      "step: 32937, loss: 0.0\n",
      "step: 32938, loss: 1.192092558000013e-08\n",
      "step: 32939, loss: 0.0\n",
      "step: 32940, loss: 0.0\n",
      "step: 32941, loss: 7.15255676908555e-09\n",
      "step: 32942, loss: 0.0\n",
      "step: 32943, loss: 0.06797266751527786\n",
      "step: 32944, loss: 0.0\n",
      "step: 32945, loss: 0.0\n",
      "step: 32946, loss: 4.76837103136063e-09\n",
      "step: 32947, loss: 0.0\n",
      "step: 32948, loss: 0.0\n",
      "step: 32949, loss: 0.0\n",
      "step: 32950, loss: 9.53674117454284e-09\n",
      "step: 32951, loss: 2.38418573772492e-09\n",
      "step: 32952, loss: 0.0\n",
      "step: 32953, loss: 0.0\n",
      "step: 32954, loss: 0.0\n",
      "step: 32955, loss: 2.38418573772492e-09\n",
      "step: 32956, loss: 0.5769484639167786\n",
      "step: 32957, loss: 0.0\n",
      "step: 32958, loss: 7.15255632499634e-09\n",
      "step: 32959, loss: 0.0\n",
      "step: 32960, loss: 0.0\n",
      "step: 32961, loss: 0.0\n",
      "step: 32962, loss: 7.15255632499634e-09\n",
      "step: 32963, loss: 0.10699639469385147\n",
      "step: 32964, loss: 7.152544867494726e-08\n",
      "step: 32965, loss: 0.0\n",
      "step: 32966, loss: 0.0\n",
      "step: 32967, loss: 3.337857279461787e-08\n",
      "step: 32968, loss: 2.38418573772492e-09\n",
      "step: 32969, loss: 2.38418573772492e-09\n",
      "step: 32970, loss: 4.76837103136063e-09\n",
      "step: 32971, loss: 1.668929527909313e-08\n",
      "step: 32972, loss: 2.38418573772492e-09\n",
      "step: 32973, loss: 2.38418573772492e-09\n",
      "step: 32974, loss: 0.0\n",
      "step: 32975, loss: 0.0\n",
      "step: 32976, loss: 7.15255632499634e-09\n",
      "step: 32977, loss: 0.0\n",
      "step: 32978, loss: 0.0\n",
      "step: 32979, loss: 7.15255632499634e-09\n",
      "step: 32980, loss: 0.0\n",
      "step: 32981, loss: 4.76837103136063e-09\n",
      "step: 32982, loss: 7.15255632499634e-09\n",
      "step: 32983, loss: 2.38418573772492e-09\n",
      "step: 32984, loss: 4.76837103136063e-09\n",
      "step: 32985, loss: 0.0\n",
      "step: 32986, loss: 5.960456306297601e-08\n",
      "step: 32987, loss: 0.07447009533643723\n",
      "step: 32988, loss: 0.0\n",
      "step: 32989, loss: 2.38418573772492e-09\n",
      "step: 32990, loss: 0.0\n",
      "step: 32991, loss: 0.0\n",
      "step: 32992, loss: 2.38418573772492e-09\n",
      "step: 32993, loss: 0.0\n",
      "step: 32994, loss: 0.0\n",
      "step: 32995, loss: 4.76837103136063e-09\n",
      "step: 32996, loss: 0.0\n",
      "step: 32997, loss: 0.0\n",
      "step: 32998, loss: 4.76837103136063e-09\n",
      "step: 32999, loss: 0.0\n",
      "step: 33000, loss: 2.38418573772492e-09\n",
      "step: 33001, loss: 2.38418573772492e-09\n",
      "step: 33002, loss: 2.717962104270555e-07\n",
      "step: 33003, loss: 0.0\n",
      "step: 33004, loss: 0.0\n",
      "step: 33005, loss: 1.192092558000013e-08\n",
      "step: 33006, loss: 0.0\n",
      "step: 33007, loss: 0.0\n",
      "step: 33008, loss: 4.76837103136063e-09\n",
      "step: 33009, loss: 1.1205641214928619e-07\n",
      "step: 33010, loss: 0.0\n",
      "step: 33011, loss: 9.53674206272126e-09\n",
      "step: 33012, loss: 2.503379334939382e-07\n",
      "step: 33013, loss: 0.0\n",
      "step: 33014, loss: 3.3378579900045224e-08\n",
      "step: 33015, loss: 0.0\n",
      "step: 33016, loss: 0.0\n",
      "step: 33017, loss: 0.0\n",
      "step: 33018, loss: 0.0\n",
      "step: 33019, loss: 0.0\n",
      "step: 33020, loss: 5.483620668655931e-08\n",
      "step: 33021, loss: 0.0\n",
      "step: 33022, loss: 1.907347702001516e-08\n",
      "step: 33023, loss: 0.0\n",
      "step: 33024, loss: 4.76837103136063e-09\n",
      "step: 33025, loss: 0.03749131038784981\n",
      "step: 33026, loss: 3.444851927270065e-06\n",
      "step: 33027, loss: 4.76837147544984e-09\n",
      "step: 33028, loss: 0.0\n",
      "step: 33029, loss: 2.38418573772492e-09\n",
      "step: 33030, loss: 2.38418573772492e-09\n",
      "step: 33031, loss: 0.0\n",
      "step: 33032, loss: 0.0\n",
      "step: 33033, loss: 0.0\n",
      "step: 33034, loss: 0.0006637937622144818\n",
      "step: 33035, loss: 2.6226029348208613e-08\n",
      "step: 33036, loss: 0.0\n",
      "step: 33037, loss: 1.668929350273629e-08\n",
      "step: 33038, loss: 0.0\n",
      "step: 33039, loss: 2.38418573772492e-09\n",
      "step: 33040, loss: 0.0\n",
      "step: 33041, loss: 0.040548719465732574\n",
      "step: 33042, loss: 0.0\n",
      "step: 33043, loss: 0.0\n",
      "step: 33044, loss: 1.668929350273629e-08\n",
      "step: 33045, loss: 0.0\n",
      "step: 33046, loss: 0.0\n",
      "step: 33047, loss: 1.0251973492358957e-07\n",
      "step: 33048, loss: 2.38418573772492e-09\n",
      "step: 33049, loss: 0.0\n",
      "step: 33050, loss: 2.3841844054572903e-08\n",
      "step: 33051, loss: 0.0\n",
      "step: 33052, loss: 0.0\n",
      "step: 33053, loss: 0.0\n",
      "step: 33054, loss: 0.0\n",
      "step: 33055, loss: 3.0994389277338996e-08\n",
      "step: 33056, loss: 6.437198294406699e-07\n",
      "step: 33057, loss: 2.38418573772492e-09\n",
      "step: 33058, loss: 0.0\n",
      "step: 33059, loss: 0.0\n",
      "step: 33060, loss: 2.38418573772492e-09\n",
      "step: 33061, loss: 3.576277052275145e-08\n",
      "step: 33062, loss: 0.0\n",
      "step: 33063, loss: 0.000909626018255949\n",
      "step: 33064, loss: 2.8610209312773804e-08\n",
      "step: 33065, loss: 0.0\n",
      "step: 33066, loss: 3.1471105899072427e-07\n",
      "step: 33067, loss: 0.0\n",
      "step: 33068, loss: 4.841664485866204e-05\n",
      "step: 33069, loss: 0.0\n",
      "step: 33070, loss: 0.0\n",
      "step: 33071, loss: 0.0\n",
      "step: 33072, loss: 0.07787513732910156\n",
      "step: 33073, loss: 0.0\n",
      "step: 33074, loss: 0.0\n",
      "step: 33075, loss: 0.0\n",
      "step: 33076, loss: 9.53674117454284e-09\n",
      "step: 33077, loss: 0.0\n",
      "step: 33078, loss: 0.0\n",
      "step: 33079, loss: 0.0\n",
      "step: 33080, loss: 1.4738372556166723e-05\n",
      "step: 33081, loss: 4.291529975830599e-08\n",
      "step: 33082, loss: 0.0\n",
      "step: 33083, loss: 0.0\n",
      "step: 33084, loss: 0.0\n",
      "step: 33085, loss: 0.0\n",
      "step: 33086, loss: 4.76837147544984e-09\n",
      "step: 33087, loss: 0.0\n",
      "step: 33088, loss: 0.0\n",
      "step: 33089, loss: 7.319316637222073e-07\n",
      "step: 33090, loss: 4.76837103136063e-09\n",
      "step: 33091, loss: 3.728519459400559e-06\n",
      "step: 33092, loss: 0.0\n",
      "step: 33093, loss: 1.192092558000013e-08\n",
      "step: 33094, loss: 0.0\n",
      "step: 33095, loss: 0.0\n",
      "step: 33096, loss: 0.0\n",
      "step: 33097, loss: 0.0\n",
      "step: 33098, loss: 2.38418573772492e-09\n",
      "step: 33099, loss: 0.0\n",
      "step: 33100, loss: 0.0\n",
      "step: 33101, loss: 3.3378576347331546e-08\n",
      "step: 33102, loss: 0.0\n",
      "step: 33103, loss: 1.8654704035725445e-05\n",
      "step: 33104, loss: 0.0\n",
      "step: 33105, loss: 0.0\n",
      "step: 33106, loss: 7.15255632499634e-09\n",
      "step: 33107, loss: 1.430511176181426e-08\n",
      "step: 33108, loss: 2.38418573772492e-09\n",
      "step: 33109, loss: 0.0\n",
      "step: 33110, loss: 0.0\n",
      "step: 33111, loss: 3.814693627646193e-08\n",
      "step: 33112, loss: 4.76837103136063e-09\n",
      "step: 33113, loss: 0.0\n",
      "step: 33114, loss: 1.192092824453539e-08\n",
      "step: 33115, loss: 1.8500426222090027e-06\n",
      "step: 33116, loss: 0.0\n",
      "step: 33117, loss: 2.460328460074379e-06\n",
      "step: 33118, loss: 0.0\n",
      "step: 33119, loss: 0.0\n",
      "step: 33120, loss: 0.0\n",
      "step: 33121, loss: 0.0\n",
      "step: 33122, loss: 0.0\n",
      "step: 33123, loss: 0.0017808808479458094\n",
      "step: 33124, loss: 0.0\n",
      "step: 33125, loss: 0.0\n",
      "step: 33126, loss: 0.0\n",
      "step: 33127, loss: 0.0\n",
      "step: 33128, loss: 0.0\n",
      "step: 33129, loss: 2.38418573772492e-09\n",
      "step: 33130, loss: 6.055776111679734e-07\n",
      "step: 33131, loss: 9.53674117454284e-09\n",
      "step: 33132, loss: 0.16722097992897034\n",
      "step: 33133, loss: 1.192092558000013e-08\n",
      "step: 33134, loss: 0.0\n",
      "step: 33135, loss: 0.0\n",
      "step: 33136, loss: 0.0\n",
      "step: 33137, loss: 4.4583777025763993e-07\n",
      "step: 33138, loss: 0.05624856427311897\n",
      "step: 33139, loss: 0.0\n",
      "step: 33140, loss: 0.0\n",
      "step: 33141, loss: 3.576275631189674e-08\n",
      "step: 33142, loss: 0.0\n",
      "step: 33143, loss: 0.0\n",
      "step: 33144, loss: 0.01543000340461731\n",
      "step: 33145, loss: 0.0\n",
      "step: 33146, loss: 0.0\n",
      "step: 33147, loss: 0.0\n",
      "step: 33148, loss: 4.76837103136063e-09\n",
      "step: 33149, loss: 0.0\n",
      "step: 33150, loss: 2.38418573772492e-09\n",
      "step: 33151, loss: 0.0\n",
      "step: 33152, loss: 0.0\n",
      "step: 33153, loss: 7.15255632499634e-09\n",
      "step: 33154, loss: 0.0\n",
      "step: 33155, loss: 0.0\n",
      "step: 33156, loss: 2.38418573772492e-09\n",
      "step: 33157, loss: 4.76837103136063e-09\n",
      "step: 33158, loss: 0.0\n",
      "step: 33159, loss: 0.0\n",
      "step: 33160, loss: 0.07949861884117126\n",
      "step: 33161, loss: 2.38418573772492e-09\n",
      "step: 33162, loss: 0.0\n",
      "step: 33163, loss: 0.0\n",
      "step: 33164, loss: 0.0\n",
      "step: 33165, loss: 0.0\n",
      "step: 33166, loss: 0.0\n",
      "step: 33167, loss: 0.0\n",
      "step: 33168, loss: 0.12116888165473938\n",
      "step: 33169, loss: 0.0\n",
      "step: 33170, loss: 0.0\n",
      "step: 33171, loss: 1.907347702001516e-08\n",
      "step: 33172, loss: 0.0\n",
      "step: 33173, loss: 0.0\n",
      "step: 33174, loss: 0.0\n",
      "step: 33175, loss: 7.15255632499634e-09\n",
      "step: 33176, loss: 2.38418573772492e-09\n",
      "step: 33177, loss: 0.0\n",
      "step: 33178, loss: 0.0\n",
      "step: 33179, loss: 9.059888128604143e-08\n",
      "step: 33180, loss: 1.0013559403887484e-07\n",
      "step: 33181, loss: 0.0\n",
      "step: 33182, loss: 0.0\n",
      "step: 33183, loss: 0.0\n",
      "step: 33184, loss: 0.0\n",
      "step: 33185, loss: 0.0\n",
      "step: 33186, loss: 2.38418573772492e-09\n",
      "step: 33187, loss: 4.76837103136063e-09\n",
      "step: 33188, loss: 0.07377699762582779\n",
      "step: 33189, loss: 5.245202316928044e-08\n",
      "step: 33190, loss: 2.38418573772492e-09\n",
      "step: 33191, loss: 0.0\n",
      "step: 33192, loss: 2.38418573772492e-09\n",
      "step: 33193, loss: 0.0\n",
      "step: 33194, loss: 2.38418573772492e-09\n",
      "step: 33195, loss: 0.0\n",
      "step: 33196, loss: 2.38418573772492e-09\n",
      "step: 33197, loss: 0.0\n",
      "step: 33198, loss: 4.76837103136063e-09\n",
      "step: 33199, loss: 0.0\n",
      "step: 33200, loss: 0.0\n",
      "step: 33201, loss: 7.15255632499634e-09\n",
      "step: 33202, loss: 1.478190227999221e-07\n",
      "step: 33203, loss: 0.0\n",
      "step: 33204, loss: 0.0\n",
      "step: 33205, loss: 0.0\n",
      "step: 33206, loss: 0.0\n",
      "step: 33207, loss: 0.0\n",
      "step: 33208, loss: 0.0\n",
      "step: 33209, loss: 1.430510998545742e-08\n",
      "step: 33210, loss: 0.0\n",
      "step: 33211, loss: 1.0728806643101052e-07\n",
      "step: 33212, loss: 0.0\n",
      "step: 33213, loss: 0.0\n",
      "step: 33214, loss: 2.38418573772492e-09\n",
      "step: 33215, loss: 0.0\n",
      "step: 33216, loss: 1.1822066880995408e-05\n",
      "step: 33217, loss: 0.11186050623655319\n",
      "step: 33218, loss: 4.76837103136063e-09\n",
      "step: 33219, loss: 0.0\n",
      "step: 33220, loss: 3.314002015031292e-07\n",
      "step: 33221, loss: 5.483619602841827e-08\n",
      "step: 33222, loss: 1.8626450382086546e-09\n",
      "step: 33223, loss: 2.38418573772492e-09\n",
      "step: 33224, loss: 0.0\n",
      "step: 33225, loss: 7.15255632499634e-09\n",
      "step: 33226, loss: 7.15255676908555e-09\n",
      "step: 33227, loss: 0.0\n",
      "step: 33228, loss: 0.14027315378189087\n",
      "step: 33229, loss: 1.430511176181426e-08\n",
      "step: 33230, loss: 3.0994389277338996e-08\n",
      "step: 33231, loss: 0.0\n",
      "step: 33232, loss: 0.0\n",
      "step: 33233, loss: 0.0\n",
      "step: 33234, loss: 2.3841845830929742e-08\n",
      "step: 33235, loss: 0.0\n",
      "step: 33236, loss: 0.0\n",
      "step: 33237, loss: 0.02626287378370762\n",
      "step: 33238, loss: 0.0\n",
      "step: 33239, loss: 0.0\n",
      "step: 33240, loss: 2.38418573772492e-09\n",
      "step: 33241, loss: 4.76837103136063e-09\n",
      "step: 33242, loss: 0.0\n",
      "step: 33243, loss: 4.76837103136063e-09\n",
      "step: 33244, loss: 7.152544867494726e-08\n",
      "step: 33245, loss: 4.76837103136063e-09\n",
      "step: 33246, loss: 0.0\n",
      "step: 33247, loss: 1.907347702001516e-08\n",
      "step: 33248, loss: 2.3841844054572903e-08\n",
      "step: 33249, loss: 0.0\n",
      "step: 33250, loss: 0.0\n",
      "step: 33251, loss: 2.38418573772492e-09\n",
      "step: 33252, loss: 0.0\n",
      "step: 33253, loss: 0.0\n",
      "step: 33254, loss: 0.06087403744459152\n",
      "step: 33255, loss: 4.76837103136063e-09\n",
      "step: 33256, loss: 4.76837103136063e-09\n",
      "step: 33257, loss: 0.06019221618771553\n",
      "step: 33258, loss: 2.38418573772492e-09\n",
      "step: 33259, loss: 0.0\n",
      "step: 33260, loss: 0.0\n",
      "step: 33261, loss: 0.0\n",
      "step: 33262, loss: 0.06082026660442352\n",
      "step: 33263, loss: 0.0\n",
      "step: 33264, loss: 2.38418573772492e-09\n",
      "step: 33265, loss: 0.0\n",
      "step: 33266, loss: 4.76837103136063e-09\n",
      "step: 33267, loss: 0.0\n",
      "step: 33268, loss: 4.76837103136063e-09\n",
      "step: 33269, loss: 0.0\n",
      "step: 33270, loss: 0.0\n",
      "step: 33271, loss: 2.38418573772492e-09\n",
      "step: 33272, loss: 4.291529975830599e-08\n",
      "step: 33273, loss: 0.0\n",
      "step: 33274, loss: 0.0\n",
      "step: 33275, loss: 2.6226025795494934e-08\n",
      "step: 33276, loss: 0.0\n",
      "step: 33277, loss: 0.0\n",
      "step: 33278, loss: 4.76837103136063e-09\n",
      "step: 33279, loss: 0.0\n",
      "step: 33280, loss: 1.192092824453539e-08\n",
      "step: 33281, loss: 0.0\n",
      "step: 33282, loss: 0.0\n",
      "step: 33283, loss: 0.0\n",
      "step: 33284, loss: 0.0\n",
      "step: 33285, loss: 0.07166445255279541\n",
      "step: 33286, loss: 0.00019963691011071205\n",
      "step: 33287, loss: 0.0\n",
      "step: 33288, loss: 0.0\n",
      "step: 33289, loss: 0.06658906489610672\n",
      "step: 33290, loss: 0.0\n",
      "step: 33291, loss: 0.0\n",
      "step: 33292, loss: 1.668929350273629e-08\n",
      "step: 33293, loss: 0.0\n",
      "step: 33294, loss: 0.0\n",
      "step: 33295, loss: 2.3841844054572903e-08\n",
      "step: 33296, loss: 0.0\n",
      "step: 33297, loss: 0.0\n",
      "step: 33298, loss: 0.0\n",
      "step: 33299, loss: 2.38418573772492e-09\n",
      "step: 33300, loss: 0.0\n",
      "step: 33301, loss: 0.0\n",
      "step: 33302, loss: 7.390963929765348e-08\n",
      "step: 33303, loss: 2.38418573772492e-09\n",
      "step: 33304, loss: 0.0\n",
      "step: 33305, loss: 0.0\n",
      "step: 33306, loss: 0.0\n",
      "step: 33307, loss: 0.0\n",
      "step: 33308, loss: 0.0\n",
      "step: 33309, loss: 0.0\n",
      "step: 33310, loss: 2.884844150230492e-07\n",
      "step: 33311, loss: 0.0\n",
      "step: 33312, loss: 2.8610209312773804e-08\n",
      "step: 33313, loss: 0.0\n",
      "step: 33314, loss: 0.0\n",
      "step: 33315, loss: 0.0\n",
      "step: 33316, loss: 0.0\n",
      "step: 33317, loss: 0.0\n",
      "step: 33318, loss: 7.581572845083429e-07\n",
      "step: 33319, loss: 3.099440348819371e-08\n",
      "step: 33320, loss: 0.0\n",
      "step: 33321, loss: 0.0\n",
      "step: 33322, loss: 0.0\n",
      "step: 33323, loss: 0.0\n",
      "step: 33324, loss: 0.0\n",
      "step: 33325, loss: 0.0\n",
      "step: 33326, loss: 0.0\n",
      "step: 33327, loss: 0.0\n",
      "step: 33328, loss: 1.907347702001516e-08\n",
      "step: 33329, loss: 7.15255676908555e-09\n",
      "step: 33330, loss: 0.0\n",
      "step: 33331, loss: 0.0\n",
      "step: 33332, loss: 0.0\n",
      "step: 33333, loss: 0.0\n",
      "step: 33334, loss: 0.08030501008033752\n",
      "step: 33335, loss: 2.38418573772492e-09\n",
      "step: 33336, loss: 3.814693982917561e-08\n",
      "step: 33337, loss: 0.0\n",
      "step: 33338, loss: 1.430510998545742e-08\n",
      "step: 33339, loss: 0.0\n",
      "step: 33340, loss: 0.0\n",
      "step: 33341, loss: 0.0\n",
      "step: 33342, loss: 3.576275631189674e-08\n",
      "step: 33343, loss: 0.0\n",
      "step: 33344, loss: 0.0\n",
      "step: 33345, loss: 0.0\n",
      "step: 33346, loss: 1.9239457742514787e-06\n",
      "step: 33347, loss: 0.0\n",
      "step: 33348, loss: 0.0\n",
      "step: 33349, loss: 0.46788397431373596\n",
      "step: 33350, loss: 4.76837103136063e-09\n",
      "step: 33351, loss: 4.76837103136063e-09\n",
      "step: 33352, loss: 0.0\n",
      "step: 33353, loss: 0.0\n",
      "step: 33354, loss: 2.38418573772492e-09\n",
      "step: 33355, loss: 0.0\n",
      "step: 33356, loss: 0.0\n",
      "step: 33357, loss: 4.76837147544984e-09\n",
      "step: 33358, loss: 0.0\n",
      "step: 33359, loss: 0.053600724786520004\n",
      "step: 33360, loss: 0.0\n",
      "step: 33361, loss: 7.15255632499634e-09\n",
      "step: 33362, loss: 0.0\n",
      "step: 33363, loss: 0.0\n",
      "step: 33364, loss: 0.0\n",
      "step: 33365, loss: 0.0\n",
      "step: 33366, loss: 0.0\n",
      "step: 33367, loss: 7.15255632499634e-09\n",
      "step: 33368, loss: 0.0\n",
      "step: 33369, loss: 4.505086690187454e-05\n",
      "step: 33370, loss: 0.0\n",
      "step: 33371, loss: 0.0\n",
      "step: 33372, loss: 0.0\n",
      "step: 33373, loss: 0.0\n",
      "step: 33374, loss: 0.0\n",
      "step: 33375, loss: 0.0\n",
      "step: 33376, loss: 0.1084996908903122\n",
      "step: 33377, loss: 2.38418573772492e-09\n",
      "step: 33378, loss: 0.07509000599384308\n",
      "step: 33379, loss: 4.76837103136063e-09\n",
      "step: 33380, loss: 2.38418573772492e-09\n",
      "step: 33381, loss: 4.76837103136063e-09\n",
      "step: 33382, loss: 2.38418573772492e-09\n",
      "step: 33383, loss: 2.38418573772492e-09\n",
      "step: 33384, loss: 0.0\n",
      "step: 33385, loss: 0.0\n",
      "step: 33386, loss: 0.0\n",
      "step: 33387, loss: 4.76837103136063e-09\n",
      "step: 33388, loss: 0.0\n",
      "step: 33389, loss: 0.0\n",
      "step: 33390, loss: 0.0\n",
      "step: 33391, loss: 0.0\n",
      "step: 33392, loss: 0.0\n",
      "step: 33393, loss: 0.0\n",
      "step: 33394, loss: 2.38418573772492e-09\n",
      "step: 33395, loss: 2.38418573772492e-09\n",
      "step: 33396, loss: 1.907347702001516e-08\n",
      "step: 33397, loss: 0.0\n",
      "step: 33398, loss: 1.7404494201400666e-07\n",
      "step: 33399, loss: 0.0\n",
      "step: 33400, loss: 9.53674117454284e-09\n",
      "step: 33401, loss: 4.8846000026969705e-06\n",
      "step: 33402, loss: 0.016551941633224487\n",
      "step: 33403, loss: 0.0\n",
      "step: 33404, loss: 0.0\n",
      "step: 33405, loss: 0.0\n",
      "step: 33406, loss: 0.0\n",
      "step: 33407, loss: 6.437291233396536e-08\n",
      "step: 33408, loss: 0.0\n",
      "step: 33409, loss: 6.437291233396536e-08\n",
      "step: 33410, loss: 0.0\n",
      "step: 33411, loss: 4.76837103136063e-09\n",
      "step: 33412, loss: 2.6941131636704085e-07\n",
      "step: 33413, loss: 3.4210017474833876e-05\n",
      "step: 33414, loss: 0.0\n",
      "step: 33415, loss: 0.0\n",
      "step: 33416, loss: 0.0\n",
      "step: 33417, loss: 7.319316637222073e-07\n",
      "step: 33418, loss: 2.38418573772492e-09\n",
      "step: 33419, loss: 0.22145797312259674\n",
      "step: 33420, loss: 0.0\n",
      "step: 33421, loss: 4.76837103136063e-09\n",
      "step: 33422, loss: 1.907347702001516e-08\n",
      "step: 33423, loss: 0.0\n",
      "step: 33424, loss: 0.0\n",
      "step: 33425, loss: 2.38418573772492e-09\n",
      "step: 33426, loss: 2.38418573772492e-09\n",
      "step: 33427, loss: 0.0\n",
      "step: 33428, loss: 0.0\n",
      "step: 33429, loss: 2.622603467727913e-08\n",
      "step: 33430, loss: 2.336488478249521e-07\n",
      "step: 33431, loss: 1.488846646680031e-05\n",
      "step: 33432, loss: 0.041463132947683334\n",
      "step: 33433, loss: 1.192092558000013e-08\n",
      "step: 33434, loss: 0.0\n",
      "step: 33435, loss: 0.0\n",
      "step: 33436, loss: 0.0\n",
      "step: 33437, loss: 2.38418573772492e-09\n",
      "step: 33438, loss: 0.0\n",
      "step: 33439, loss: 0.0\n",
      "step: 33440, loss: 4.76837147544984e-09\n",
      "step: 33441, loss: 0.0\n",
      "step: 33442, loss: 0.0\n",
      "step: 33443, loss: 0.0\n",
      "step: 33444, loss: 5.483619602841827e-08\n",
      "step: 33445, loss: 0.0\n",
      "step: 33446, loss: 0.0\n",
      "step: 33447, loss: 0.0\n",
      "step: 33448, loss: 0.0\n",
      "step: 33449, loss: 0.0\n",
      "step: 33450, loss: 0.0\n",
      "step: 33451, loss: 2.8610209312773804e-08\n",
      "step: 33452, loss: 0.0\n",
      "step: 33453, loss: 2.8610088520508725e-07\n",
      "step: 33454, loss: 0.0\n",
      "step: 33455, loss: 0.0\n",
      "step: 33456, loss: 0.0\n",
      "step: 33457, loss: 2.38418573772492e-09\n",
      "step: 33458, loss: 0.0\n",
      "step: 33459, loss: 0.0\n",
      "step: 33460, loss: 0.0\n",
      "step: 33461, loss: 2.38418573772492e-09\n",
      "step: 33462, loss: 0.0\n",
      "step: 33463, loss: 7.15255676908555e-09\n",
      "step: 33464, loss: 0.0\n",
      "step: 33465, loss: 0.0\n",
      "step: 33466, loss: 0.0\n",
      "step: 33467, loss: 4.76837103136063e-09\n",
      "step: 33468, loss: 2.38418573772492e-09\n",
      "step: 33469, loss: 4.768367034557741e-08\n",
      "step: 33470, loss: 4.291529975830599e-08\n",
      "step: 33471, loss: 2.38418573772492e-09\n",
      "step: 33472, loss: 2.1457660537294032e-08\n",
      "step: 33473, loss: 0.0\n",
      "step: 33474, loss: 0.031871214509010315\n",
      "step: 33475, loss: 0.0\n",
      "step: 33476, loss: 1.430510998545742e-08\n",
      "step: 33477, loss: 0.0\n",
      "step: 33478, loss: 3.62392967190317e-07\n",
      "step: 33479, loss: 0.0\n",
      "step: 33480, loss: 4.458379407878965e-07\n",
      "step: 33481, loss: 0.07420356571674347\n",
      "step: 33482, loss: 0.0\n",
      "step: 33483, loss: 0.0\n",
      "step: 33484, loss: 4.529947972287118e-08\n",
      "step: 33485, loss: 7.15255632499634e-09\n",
      "step: 33486, loss: 4.7683659687436375e-08\n",
      "step: 33487, loss: 0.0\n",
      "step: 33488, loss: 7.15255632499634e-09\n",
      "step: 33489, loss: 0.0\n",
      "step: 33490, loss: 2.38418573772492e-09\n",
      "step: 33491, loss: 0.0\n",
      "step: 33492, loss: 4.76837103136063e-09\n",
      "step: 33493, loss: 0.0\n",
      "step: 33494, loss: 0.0\n",
      "step: 33495, loss: 0.0\n",
      "step: 33496, loss: 3.3378576347331546e-08\n",
      "step: 33497, loss: 6.437291233396536e-08\n",
      "step: 33498, loss: 0.0\n",
      "step: 33499, loss: 0.0\n",
      "step: 33500, loss: 0.0\n",
      "step: 33501, loss: 0.0\n",
      "step: 33502, loss: 3.0994389277338996e-08\n",
      "step: 33503, loss: 0.0\n",
      "step: 33504, loss: 2.38418573772492e-09\n",
      "step: 33505, loss: 0.0\n",
      "step: 33506, loss: 4.76837147544984e-09\n",
      "step: 33507, loss: 0.0\n",
      "step: 33508, loss: 2.38418573772492e-09\n",
      "step: 33509, loss: 0.0\n",
      "step: 33510, loss: 2.38418573772492e-09\n",
      "step: 33511, loss: 7.15255676908555e-09\n",
      "step: 33512, loss: 5.722042217826129e-08\n",
      "step: 33513, loss: 2.38418573772492e-09\n",
      "step: 33514, loss: 0.0\n",
      "step: 33515, loss: 2.6225944793623057e-07\n",
      "step: 33516, loss: 0.0\n",
      "step: 33517, loss: 0.0\n",
      "step: 33518, loss: 2.6226025795494934e-08\n",
      "step: 33519, loss: 0.09972036629915237\n",
      "step: 33520, loss: 1.3112979502238886e-07\n",
      "step: 33521, loss: 0.0\n",
      "step: 33522, loss: 2.38418573772492e-09\n",
      "step: 33523, loss: 0.0\n",
      "step: 33524, loss: 2.38418573772492e-09\n",
      "step: 33525, loss: 6.914130068480517e-08\n",
      "step: 33526, loss: 0.0\n",
      "step: 33527, loss: 0.0\n",
      "step: 33528, loss: 3.576275631189674e-08\n",
      "step: 33529, loss: 2.38418573772492e-09\n",
      "step: 33530, loss: 0.0\n",
      "step: 33531, loss: 1.287456257159647e-07\n",
      "step: 33532, loss: 2.8610209312773804e-08\n",
      "step: 33533, loss: 0.0\n",
      "step: 33534, loss: 0.0\n",
      "step: 33535, loss: 4.76837103136063e-09\n",
      "step: 33536, loss: 0.0009451819933019578\n",
      "step: 33537, loss: 0.0\n",
      "step: 33538, loss: 2.38418573772492e-09\n",
      "step: 33539, loss: 7.15255632499634e-09\n",
      "step: 33540, loss: 0.0\n",
      "step: 33541, loss: 0.0\n",
      "step: 33542, loss: 1.430510998545742e-08\n",
      "step: 33543, loss: 0.0\n",
      "step: 33544, loss: 0.0\n",
      "step: 33545, loss: 1.192092558000013e-08\n",
      "step: 33546, loss: 0.0\n",
      "step: 33547, loss: 0.0\n",
      "step: 33548, loss: 0.0\n",
      "step: 33549, loss: 0.0\n",
      "step: 33550, loss: 0.0\n",
      "step: 33551, loss: 0.0\n",
      "step: 33552, loss: 0.0\n",
      "step: 33553, loss: 2.38418573772492e-09\n",
      "step: 33554, loss: 0.0\n",
      "step: 33555, loss: 0.0\n",
      "step: 33556, loss: 0.0\n",
      "step: 33557, loss: 9.53674117454284e-09\n",
      "step: 33558, loss: 0.0\n",
      "step: 33559, loss: 0.048272695392370224\n",
      "step: 33560, loss: 0.0\n",
      "step: 33561, loss: 0.0\n",
      "step: 33562, loss: 9.53674117454284e-09\n",
      "step: 33563, loss: 0.0\n",
      "step: 33564, loss: 2.38418573772492e-09\n",
      "step: 33565, loss: 0.0\n",
      "step: 33566, loss: 0.0\n",
      "step: 33567, loss: 0.21856939792633057\n",
      "step: 33568, loss: 0.0\n",
      "step: 33569, loss: 2.38418573772492e-09\n",
      "step: 33570, loss: 0.0\n",
      "step: 33571, loss: 0.0\n",
      "step: 33572, loss: 2.2649635411653435e-07\n",
      "step: 33573, loss: 2.38418573772492e-09\n",
      "step: 33574, loss: 0.0\n",
      "step: 33575, loss: 7.15255676908555e-09\n",
      "step: 33576, loss: 0.0\n",
      "step: 33577, loss: 0.0\n",
      "step: 33578, loss: 0.0\n",
      "step: 33579, loss: 0.0\n",
      "step: 33580, loss: 0.007034918759018183\n",
      "step: 33581, loss: 0.0\n",
      "step: 33582, loss: 0.0\n",
      "step: 33583, loss: 4.76837103136063e-09\n",
      "step: 33584, loss: 0.0\n",
      "step: 33585, loss: 0.0\n",
      "step: 33586, loss: 0.0\n",
      "step: 33587, loss: 2.38418573772492e-09\n",
      "step: 33588, loss: 0.0\n",
      "step: 33589, loss: 0.0\n",
      "step: 33590, loss: 0.0\n",
      "step: 33591, loss: 0.0\n",
      "step: 33592, loss: 2.38418573772492e-09\n",
      "step: 33593, loss: 9.53674117454284e-09\n",
      "step: 33594, loss: 0.0\n",
      "step: 33595, loss: 7.473026016668882e-06\n",
      "step: 33596, loss: 2.38418573772492e-09\n",
      "step: 33597, loss: 0.0\n",
      "step: 33598, loss: 0.11122472584247589\n",
      "step: 33599, loss: 0.0\n",
      "step: 33600, loss: 0.0\n",
      "step: 33601, loss: 0.0\n",
      "step: 33602, loss: 0.0\n",
      "step: 33603, loss: 0.0\n",
      "step: 33604, loss: 0.010384528897702694\n",
      "step: 33605, loss: 1.1920893427941337e-07\n",
      "step: 33606, loss: 2.622603467727913e-08\n",
      "step: 33607, loss: 9.53674206272126e-09\n",
      "step: 33608, loss: 0.0\n",
      "step: 33609, loss: 2.38418573772492e-09\n",
      "step: 33610, loss: 0.0\n",
      "step: 33611, loss: 2.38418573772492e-09\n",
      "step: 33612, loss: 2.3126489168134867e-07\n",
      "step: 33613, loss: 4.76837103136063e-09\n",
      "step: 33614, loss: 2.8610211089130644e-08\n",
      "step: 33615, loss: 0.0\n",
      "step: 33616, loss: 0.0\n",
      "step: 33617, loss: 0.0\n",
      "step: 33618, loss: 0.0\n",
      "step: 33619, loss: 0.0\n",
      "step: 33620, loss: 1.5782688933541067e-06\n",
      "step: 33621, loss: 0.0\n",
      "step: 33622, loss: 0.0\n",
      "step: 33623, loss: 2.145766231365087e-08\n",
      "step: 33624, loss: 2.467301965225488e-05\n",
      "step: 33625, loss: 0.0\n",
      "step: 33626, loss: 0.0006743792328052223\n",
      "step: 33627, loss: 0.0\n",
      "step: 33628, loss: 2.38418573772492e-09\n",
      "step: 33629, loss: 0.0\n",
      "step: 33630, loss: 0.0\n",
      "step: 33631, loss: 2.38418573772492e-09\n",
      "step: 33632, loss: 0.0\n",
      "step: 33633, loss: 4.76837103136063e-09\n",
      "step: 33634, loss: 4.291530331101967e-08\n",
      "step: 33635, loss: 0.0870901569724083\n",
      "step: 33636, loss: 0.07426908612251282\n",
      "step: 33637, loss: 0.0\n",
      "step: 33638, loss: 0.0\n",
      "step: 33639, loss: 0.0\n",
      "step: 33640, loss: 0.0\n",
      "step: 33641, loss: 4.76837103136063e-09\n",
      "step: 33642, loss: 9.53674117454284e-09\n",
      "step: 33643, loss: 4.291529975830599e-08\n",
      "step: 33644, loss: 7.414681135742285e-07\n",
      "step: 33645, loss: 3.576275631189674e-08\n",
      "step: 33646, loss: 0.0\n",
      "step: 33647, loss: 4.76837103136063e-09\n",
      "step: 33648, loss: 0.0\n",
      "step: 33649, loss: 0.0\n",
      "step: 33650, loss: 4.76837103136063e-09\n",
      "step: 33651, loss: 4.6967906541794946e-07\n",
      "step: 33652, loss: 2.38418573772492e-09\n",
      "step: 33653, loss: 0.0\n",
      "step: 33654, loss: 0.0\n",
      "step: 33655, loss: 0.0\n",
      "step: 33656, loss: 0.0\n",
      "step: 33657, loss: 0.048713188618421555\n",
      "step: 33658, loss: 1.668929350273629e-08\n",
      "step: 33659, loss: 0.07791003584861755\n",
      "step: 33660, loss: 2.38418573772492e-09\n",
      "step: 33661, loss: 2.38418573772492e-09\n",
      "step: 33662, loss: 4.76837103136063e-09\n",
      "step: 33663, loss: 2.002706480652705e-07\n",
      "step: 33664, loss: 0.0\n",
      "step: 33665, loss: 0.04851680248975754\n",
      "step: 33666, loss: 0.0\n",
      "step: 33667, loss: 0.0\n",
      "step: 33668, loss: 0.0\n",
      "step: 33669, loss: 0.0\n",
      "step: 33670, loss: 2.18248569581192e-05\n",
      "step: 33671, loss: 4.76837103136063e-09\n",
      "step: 33672, loss: 0.0\n",
      "step: 33673, loss: 2.38418573772492e-09\n",
      "step: 33674, loss: 0.0\n",
      "step: 33675, loss: 3.24247082517104e-07\n",
      "step: 33676, loss: 0.0030466571915894747\n",
      "step: 33677, loss: 2.38418573772492e-09\n",
      "step: 33678, loss: 0.0\n",
      "step: 33679, loss: 2.38418573772492e-09\n",
      "step: 33680, loss: 0.0\n",
      "step: 33681, loss: 7.390962508679877e-08\n",
      "step: 33682, loss: 4.76837103136063e-09\n",
      "step: 33683, loss: 4.76837103136063e-09\n",
      "step: 33684, loss: 0.0\n",
      "step: 33685, loss: 2.38418573772492e-09\n",
      "step: 33686, loss: 0.0\n",
      "step: 33687, loss: 0.0\n",
      "step: 33688, loss: 0.0\n",
      "step: 33689, loss: 0.0\n",
      "step: 33690, loss: 2.38418573772492e-09\n",
      "step: 33691, loss: 0.0\n",
      "step: 33692, loss: 0.0\n",
      "step: 33693, loss: 0.0\n",
      "step: 33694, loss: 7.676932227695943e-07\n",
      "step: 33695, loss: 0.0\n",
      "step: 33696, loss: 7.438521265612508e-07\n",
      "step: 33697, loss: 2.38418573772492e-09\n",
      "step: 33698, loss: 9.298302927618352e-08\n",
      "step: 33699, loss: 0.0\n",
      "step: 33700, loss: 2.38418573772492e-09\n",
      "step: 33701, loss: 0.0002251308033009991\n",
      "step: 33702, loss: 0.0\n",
      "step: 33703, loss: 4.76837103136063e-09\n",
      "step: 33704, loss: 4.76837147544984e-09\n",
      "step: 33705, loss: 0.00012501911260187626\n",
      "step: 33706, loss: 0.0\n",
      "step: 33707, loss: 0.0\n",
      "step: 33708, loss: 2.38418573772492e-09\n",
      "step: 33709, loss: 0.0\n",
      "step: 33710, loss: 2.38418573772492e-09\n",
      "step: 33711, loss: 2.38418573772492e-09\n",
      "step: 33712, loss: 0.0\n",
      "step: 33713, loss: 0.0\n",
      "step: 33714, loss: 0.0\n",
      "step: 33715, loss: 1.430511176181426e-08\n",
      "step: 33716, loss: 0.0\n",
      "step: 33717, loss: 0.0006606336100958288\n",
      "step: 33718, loss: 2.38418573772492e-09\n",
      "step: 33719, loss: 2.1457660537294032e-08\n",
      "step: 33720, loss: 0.08843536674976349\n",
      "step: 33721, loss: 0.0\n",
      "step: 33722, loss: 0.0\n",
      "step: 33723, loss: 4.76837103136063e-09\n",
      "step: 33724, loss: 0.0\n",
      "step: 33725, loss: 9.53674206272126e-09\n",
      "step: 33726, loss: 2.38418573772492e-09\n",
      "step: 33727, loss: 2.38418573772492e-09\n",
      "step: 33728, loss: 0.0\n",
      "step: 33729, loss: 4.76837103136063e-09\n",
      "step: 33730, loss: 1.430510998545742e-08\n",
      "step: 33731, loss: 0.0\n",
      "step: 33732, loss: 6.800925348215969e-06\n",
      "step: 33733, loss: 2.38418573772492e-09\n",
      "step: 33734, loss: 0.0\n",
      "step: 33735, loss: 0.0\n",
      "step: 33736, loss: 0.0\n",
      "step: 33737, loss: 0.0\n",
      "step: 33738, loss: 0.0\n",
      "step: 33739, loss: 0.0\n",
      "step: 33740, loss: 0.0\n",
      "step: 33741, loss: 0.0\n",
      "step: 33742, loss: 0.0\n",
      "step: 33743, loss: 0.0\n",
      "step: 33744, loss: 0.0\n",
      "step: 33745, loss: 0.0\n",
      "step: 33746, loss: 0.0\n",
      "step: 33747, loss: 0.0\n",
      "step: 33748, loss: 1.192092558000013e-08\n",
      "step: 33749, loss: 2.38418573772492e-09\n",
      "step: 33750, loss: 0.0\n",
      "step: 33751, loss: 1.8834980153314973e-07\n",
      "step: 33752, loss: 2.38418573772492e-09\n",
      "step: 33753, loss: 0.0\n",
      "step: 33754, loss: 0.04056081175804138\n",
      "step: 33755, loss: 0.0\n",
      "step: 33756, loss: 4.76837103136063e-09\n",
      "step: 33757, loss: 0.0008731487905606627\n",
      "step: 33758, loss: 0.0029067369177937508\n",
      "step: 33759, loss: 0.0\n",
      "step: 33760, loss: 0.0\n",
      "step: 33761, loss: 0.0\n",
      "step: 33762, loss: 0.0\n",
      "step: 33763, loss: 0.0\n",
      "step: 33764, loss: 2.38418573772492e-09\n",
      "step: 33765, loss: 2.38418573772492e-09\n",
      "step: 33766, loss: 0.0\n",
      "step: 33767, loss: 0.0\n",
      "step: 33768, loss: 0.053200557827949524\n",
      "step: 33769, loss: 0.0\n",
      "step: 33770, loss: 0.0\n",
      "step: 33771, loss: 0.0\n",
      "step: 33772, loss: 4.76837147544984e-09\n",
      "step: 33773, loss: 0.0\n",
      "step: 33774, loss: 0.0\n",
      "step: 33775, loss: 0.0\n",
      "step: 33776, loss: 0.12190263718366623\n",
      "step: 33777, loss: 0.0\n",
      "step: 33778, loss: 0.0\n",
      "step: 33779, loss: 4.672949955875083e-07\n",
      "step: 33780, loss: 0.0\n",
      "step: 33781, loss: 0.0\n",
      "step: 33782, loss: 9.481094457441941e-05\n",
      "step: 33783, loss: 4.5299501039153256e-08\n",
      "step: 33784, loss: 0.3723013699054718\n",
      "step: 33785, loss: 0.0\n",
      "step: 33786, loss: 0.0\n",
      "step: 33787, loss: 1.0473379916220438e-05\n",
      "step: 33788, loss: 0.0\n",
      "step: 33789, loss: 0.0\n",
      "step: 33790, loss: 0.0\n",
      "step: 33791, loss: 0.0\n",
      "step: 33792, loss: 0.0\n",
      "step: 33793, loss: 0.0\n",
      "step: 33794, loss: 0.0\n",
      "step: 33795, loss: 2.38418573772492e-09\n",
      "step: 33796, loss: 0.0\n",
      "step: 33797, loss: 2.38418573772492e-09\n",
      "step: 33798, loss: 1.5735570002561872e-07\n",
      "step: 33799, loss: 0.0\n",
      "step: 33800, loss: 0.0\n",
      "step: 33801, loss: 1.668929527909313e-08\n",
      "step: 33802, loss: 2.38418573772492e-09\n",
      "step: 33803, loss: 4.76837147544984e-09\n",
      "step: 33804, loss: 2.38418573772492e-09\n",
      "step: 33805, loss: 0.0\n",
      "step: 33806, loss: 1.192092646817855e-08\n",
      "step: 33807, loss: 0.0\n",
      "step: 33808, loss: 1.430510998545742e-08\n",
      "step: 33809, loss: 0.0\n",
      "step: 33810, loss: 0.0\n",
      "step: 33811, loss: 0.0\n",
      "step: 33812, loss: 7.15255632499634e-09\n",
      "step: 33813, loss: 0.0\n",
      "step: 33814, loss: 0.0\n",
      "step: 33815, loss: 0.0\n",
      "step: 33816, loss: 2.38418573772492e-09\n",
      "step: 33817, loss: 3.054907210753299e-05\n",
      "step: 33818, loss: 0.0\n",
      "step: 33819, loss: 2.6939485451293876e-06\n",
      "step: 33820, loss: 2.38418573772492e-09\n",
      "step: 33821, loss: 0.0\n",
      "step: 33822, loss: 0.0\n",
      "step: 33823, loss: 1.2874561150511e-07\n",
      "step: 33824, loss: 2.38418573772492e-09\n",
      "step: 33825, loss: 1.907347702001516e-08\n",
      "step: 33826, loss: 4.76837103136063e-09\n",
      "step: 33827, loss: 2.38418573772492e-09\n",
      "step: 33828, loss: 0.0\n",
      "step: 33829, loss: 0.0\n",
      "step: 33830, loss: 0.0\n",
      "step: 33831, loss: 0.0\n",
      "step: 33832, loss: 0.0166804026812315\n",
      "step: 33833, loss: 0.06845627725124359\n",
      "step: 33834, loss: 0.0\n",
      "step: 33835, loss: 0.0\n",
      "step: 33836, loss: 0.0\n",
      "step: 33837, loss: 0.0\n",
      "step: 33838, loss: 7.15255632499634e-09\n",
      "step: 33839, loss: 1.907347702001516e-08\n",
      "step: 33840, loss: 0.0\n",
      "step: 33841, loss: 0.17244639992713928\n",
      "step: 33842, loss: 0.0\n",
      "step: 33843, loss: 6.517302153952187e-06\n",
      "step: 33844, loss: 6.914127226309574e-08\n",
      "step: 33845, loss: 0.0\n",
      "step: 33846, loss: 0.0\n",
      "step: 33847, loss: 0.08024313300848007\n",
      "step: 33848, loss: 0.1019567996263504\n",
      "step: 33849, loss: 2.3841844054572903e-08\n",
      "step: 33850, loss: 2.38418573772492e-09\n",
      "step: 33851, loss: 4.76837103136063e-09\n",
      "step: 33852, loss: 4.76837147544984e-09\n",
      "step: 33853, loss: 2.38418573772492e-09\n",
      "step: 33854, loss: 0.022112760692834854\n",
      "step: 33855, loss: 1.668929883180681e-08\n",
      "step: 33856, loss: 4.76837103136063e-09\n",
      "step: 33857, loss: 2.6226029348208613e-08\n",
      "step: 33858, loss: 4.76837103136063e-09\n",
      "step: 33859, loss: 0.0\n",
      "step: 33860, loss: 0.0\n",
      "step: 33861, loss: 0.0\n",
      "step: 33862, loss: 0.0\n",
      "step: 33863, loss: 2.38418573772492e-09\n",
      "step: 33864, loss: 2.38418573772492e-09\n",
      "step: 33865, loss: 0.0\n",
      "step: 33866, loss: 9.53674117454284e-09\n",
      "step: 33867, loss: 0.0\n",
      "step: 33868, loss: 0.0\n",
      "step: 33869, loss: 1.907347702001516e-08\n",
      "step: 33870, loss: 0.0\n",
      "step: 33871, loss: 0.0\n",
      "step: 33872, loss: 2.38418573772492e-09\n",
      "step: 33873, loss: 2.38418573772492e-09\n",
      "step: 33874, loss: 5.00678503101426e-08\n",
      "step: 33875, loss: 4.76837103136063e-09\n",
      "step: 33876, loss: 1.2636147062039527e-07\n",
      "step: 33877, loss: 0.0\n",
      "step: 33878, loss: 0.0\n",
      "step: 33879, loss: 0.0\n",
      "step: 33880, loss: 0.0\n",
      "step: 33881, loss: 2.38418573772492e-09\n",
      "step: 33882, loss: 0.0\n",
      "step: 33883, loss: 1.5735564318219986e-07\n",
      "step: 33884, loss: 4.291529975830599e-08\n",
      "step: 33885, loss: 7.15255676908555e-09\n",
      "step: 33886, loss: 0.0\n",
      "step: 33887, loss: 2.38418573772492e-09\n",
      "step: 33888, loss: 4.7683659687436375e-08\n",
      "step: 33889, loss: 1.430511176181426e-08\n",
      "step: 33890, loss: 0.0\n",
      "step: 33891, loss: 0.0\n",
      "step: 33892, loss: 2.38418573772492e-09\n",
      "step: 33893, loss: 2.38418573772492e-09\n",
      "step: 33894, loss: 1.192092824453539e-08\n",
      "step: 33895, loss: 0.0\n",
      "step: 33896, loss: 0.0\n",
      "step: 33897, loss: 1.430510998545742e-08\n",
      "step: 33898, loss: 0.0\n",
      "step: 33899, loss: 0.0\n",
      "step: 33900, loss: 0.0\n",
      "step: 33901, loss: 0.0\n",
      "step: 33902, loss: 9.53674117454284e-09\n",
      "step: 33903, loss: 0.05211470648646355\n",
      "step: 33904, loss: 0.030639415606856346\n",
      "step: 33905, loss: 0.0\n",
      "step: 33906, loss: 4.76837103136063e-09\n",
      "step: 33907, loss: 4.76837103136063e-09\n",
      "step: 33908, loss: 1.192092646817855e-08\n",
      "step: 33909, loss: 4.76837147544984e-09\n",
      "step: 33910, loss: 2.38418573772492e-09\n",
      "step: 33911, loss: 0.0\n",
      "step: 33912, loss: 4.76837103136063e-09\n",
      "step: 33913, loss: 0.0006553253042511642\n",
      "step: 33914, loss: 0.0\n",
      "step: 33915, loss: 2.38418573772492e-09\n",
      "step: 33916, loss: 2.38418573772492e-09\n",
      "step: 33917, loss: 2.38418573772492e-09\n",
      "step: 33918, loss: 0.0\n",
      "step: 33919, loss: 0.0\n",
      "step: 33920, loss: 0.0\n",
      "step: 33921, loss: 0.0\n",
      "step: 33922, loss: 0.0\n",
      "step: 33923, loss: 0.04284211993217468\n",
      "step: 33924, loss: 0.0\n",
      "step: 33925, loss: 0.13030867278575897\n",
      "step: 33926, loss: 0.0\n",
      "step: 33927, loss: 2.38418573772492e-09\n",
      "step: 33928, loss: 0.059145741164684296\n",
      "step: 33929, loss: 2.1457660537294032e-08\n",
      "step: 33930, loss: 0.0\n",
      "step: 33931, loss: 1.0180214076171978e-06\n",
      "step: 33932, loss: 0.0\n",
      "step: 33933, loss: 0.0\n",
      "step: 33934, loss: 0.0\n",
      "step: 33935, loss: 0.0\n",
      "step: 33936, loss: 0.0\n",
      "step: 33937, loss: 0.0\n",
      "step: 33938, loss: 2.38418573772492e-09\n",
      "step: 33939, loss: 0.0\n",
      "step: 33940, loss: 0.0\n",
      "step: 33941, loss: 0.0\n",
      "step: 33942, loss: 0.0\n",
      "step: 33943, loss: 3.075581958000839e-07\n",
      "step: 33944, loss: 0.0\n",
      "step: 33945, loss: 7.15255632499634e-09\n",
      "step: 33946, loss: 0.07784497737884521\n",
      "step: 33947, loss: 0.0\n",
      "step: 33948, loss: 1.668929350273629e-08\n",
      "step: 33949, loss: 2.38418573772492e-09\n",
      "step: 33950, loss: 2.38418573772492e-09\n",
      "step: 33951, loss: 4.76837103136063e-09\n",
      "step: 33952, loss: 1.192092646817855e-08\n",
      "step: 33953, loss: 0.04189469665288925\n",
      "step: 33954, loss: 0.0\n",
      "step: 33955, loss: 0.0\n",
      "step: 33956, loss: 2.38418573772492e-09\n",
      "step: 33957, loss: 1.668929350273629e-08\n",
      "step: 33958, loss: 0.0\n",
      "step: 33959, loss: 2.38418573772492e-09\n",
      "step: 33960, loss: 1.192092824453539e-08\n",
      "step: 33961, loss: 0.0\n",
      "step: 33962, loss: 0.0\n",
      "step: 33963, loss: 0.0\n",
      "step: 33964, loss: 1.502031352629274e-07\n",
      "step: 33965, loss: 0.0\n",
      "step: 33966, loss: 2.38418573772492e-09\n",
      "step: 33967, loss: 1.668929527909313e-08\n",
      "step: 33968, loss: 0.0\n",
      "step: 33969, loss: 0.0\n",
      "step: 33970, loss: 0.0\n",
      "step: 33971, loss: 2.124196726072114e-06\n",
      "step: 33972, loss: 4.267666611212917e-07\n",
      "step: 33973, loss: 0.0\n",
      "step: 33974, loss: 0.0\n",
      "step: 33975, loss: 1.085478925233474e-05\n",
      "step: 33976, loss: 7.15255632499634e-09\n",
      "step: 33977, loss: 2.38418573772492e-09\n",
      "step: 33978, loss: 5.9604580826544407e-08\n",
      "step: 33979, loss: 0.0\n",
      "step: 33980, loss: 2.38418573772492e-09\n",
      "step: 33981, loss: 0.0\n",
      "step: 33982, loss: 0.0\n",
      "step: 33983, loss: 0.0\n",
      "step: 33984, loss: 0.0\n",
      "step: 33985, loss: 0.0\n",
      "step: 33986, loss: 1.9073478796372e-08\n",
      "step: 33987, loss: 0.0\n",
      "step: 33988, loss: 2.38418573772492e-09\n",
      "step: 33989, loss: 2.38418573772492e-09\n",
      "step: 33990, loss: 2.38418573772492e-09\n",
      "step: 33991, loss: 5.483619602841827e-08\n",
      "step: 33992, loss: 0.0\n",
      "step: 33993, loss: 9.53674117454284e-09\n",
      "step: 33994, loss: 0.0\n",
      "step: 33995, loss: 2.38418573772492e-09\n",
      "step: 33996, loss: 2.8610216418201162e-08\n",
      "step: 33997, loss: 1.430510998545742e-08\n",
      "step: 33998, loss: 1.0251982729414522e-07\n",
      "step: 33999, loss: 8.749776156946609e-07\n",
      "step: 34000, loss: 0.0\n",
      "step: 34001, loss: 0.0\n",
      "step: 34002, loss: 1.430511264999268e-08\n",
      "step: 34003, loss: 7.013044523773715e-06\n",
      "step: 34004, loss: 0.0\n",
      "step: 34005, loss: 2.38418573772492e-09\n",
      "step: 34006, loss: 9.131223350777873e-07\n",
      "step: 34007, loss: 0.0\n",
      "step: 34008, loss: 6.437291943939272e-08\n",
      "step: 34009, loss: 2.3841844054572903e-08\n",
      "step: 34010, loss: 0.0\n",
      "step: 34011, loss: 2.38418573772492e-09\n",
      "step: 34012, loss: 0.0\n",
      "step: 34013, loss: 0.0\n",
      "step: 34014, loss: 9.53674117454284e-09\n",
      "step: 34015, loss: 0.0\n",
      "step: 34016, loss: 0.0\n",
      "step: 34017, loss: 0.11076188087463379\n",
      "step: 34018, loss: 2.38418573772492e-09\n",
      "step: 34019, loss: 4.148440382323315e-07\n",
      "step: 34020, loss: 0.0\n",
      "step: 34021, loss: 0.0\n",
      "step: 34022, loss: 7.15255632499634e-09\n",
      "step: 34023, loss: 0.0\n",
      "step: 34024, loss: 2.572371158748865e-06\n",
      "step: 34025, loss: 2.217280723471049e-07\n",
      "step: 34026, loss: 0.0\n",
      "step: 34027, loss: 2.38418573772492e-09\n",
      "step: 34028, loss: 0.0\n",
      "step: 34029, loss: 0.0\n",
      "step: 34030, loss: 0.0\n",
      "step: 34031, loss: 1.907347702001516e-08\n",
      "step: 34032, loss: 0.0\n",
      "step: 34033, loss: 0.08065489679574966\n",
      "step: 34034, loss: 2.38418573772492e-09\n",
      "step: 34035, loss: 0.0\n",
      "step: 34036, loss: 0.0\n",
      "step: 34037, loss: 2.145766586636455e-08\n",
      "step: 34038, loss: 0.0\n",
      "step: 34039, loss: 9.53674117454284e-09\n",
      "step: 34040, loss: 3.0994389277338996e-08\n",
      "step: 34041, loss: 0.0\n",
      "step: 34042, loss: 0.0\n",
      "step: 34043, loss: 4.76837103136063e-09\n",
      "step: 34044, loss: 1.0967224284286203e-07\n",
      "step: 34045, loss: 0.0\n",
      "step: 34046, loss: 0.0\n",
      "step: 34047, loss: 0.0\n",
      "step: 34048, loss: 0.0\n",
      "step: 34049, loss: 0.0\n",
      "step: 34050, loss: 0.0\n",
      "step: 34051, loss: 5.006783965200157e-08\n",
      "step: 34052, loss: 0.0\n",
      "step: 34053, loss: 0.0\n",
      "step: 34054, loss: 2.38418573772492e-09\n",
      "step: 34055, loss: 1.668929527909313e-08\n",
      "step: 34056, loss: 0.15103499591350555\n",
      "step: 34057, loss: 2.38418573772492e-09\n",
      "step: 34058, loss: 0.0\n",
      "step: 34059, loss: 0.0\n",
      "step: 34060, loss: 4.982917403140164e-07\n",
      "step: 34061, loss: 4.76837103136063e-09\n",
      "step: 34062, loss: 0.0\n",
      "step: 34063, loss: 0.04281385987997055\n",
      "step: 34064, loss: 0.0\n",
      "step: 34065, loss: 9.77514886812969e-08\n",
      "step: 34066, loss: 2.38418573772492e-09\n",
      "step: 34067, loss: 0.0\n",
      "step: 34068, loss: 2.8610209312773804e-08\n",
      "step: 34069, loss: 1.4781898016735795e-07\n",
      "step: 34070, loss: 0.0\n",
      "step: 34071, loss: 2.3708802473265678e-05\n",
      "step: 34072, loss: 1.3180254427425098e-05\n",
      "step: 34073, loss: 0.0\n",
      "step: 34074, loss: 1.59739812488624e-07\n",
      "step: 34075, loss: 0.0\n",
      "step: 34076, loss: 0.03198859840631485\n",
      "step: 34077, loss: 3.0994389277338996e-08\n",
      "step: 34078, loss: 2.38418573772492e-09\n",
      "step: 34079, loss: 7.15255632499634e-09\n",
      "step: 34080, loss: 0.0\n",
      "step: 34081, loss: 7.15255632499634e-09\n",
      "step: 34082, loss: 7.15255676908555e-09\n",
      "step: 34083, loss: 0.0\n",
      "step: 34084, loss: 0.0\n",
      "step: 34085, loss: 0.0\n",
      "step: 34086, loss: 9.53674117454284e-09\n",
      "step: 34087, loss: 8.583050714605633e-08\n",
      "step: 34088, loss: 0.0002370935253566131\n",
      "step: 34089, loss: 4.308232746552676e-05\n",
      "step: 34090, loss: 8.845133834256558e-07\n",
      "step: 34091, loss: 2.145766231365087e-08\n",
      "step: 34092, loss: 0.0\n",
      "step: 34093, loss: 1.668929883180681e-08\n",
      "step: 34094, loss: 0.0\n",
      "step: 34095, loss: 5.960378075542394e-07\n",
      "step: 34096, loss: 0.0\n",
      "step: 34097, loss: 0.0\n",
      "step: 34098, loss: 7.15255676908555e-09\n",
      "step: 34099, loss: 1.1205654004697863e-07\n",
      "step: 34100, loss: 7.629383702578707e-08\n",
      "step: 34101, loss: 2.38418573772492e-09\n",
      "step: 34102, loss: 0.0\n",
      "step: 34103, loss: 0.0\n",
      "step: 34104, loss: 2.2505448669107864e-06\n",
      "step: 34105, loss: 2.38418573772492e-09\n",
      "step: 34106, loss: 0.0\n",
      "step: 34107, loss: 0.0\n",
      "step: 34108, loss: 0.0\n",
      "step: 34109, loss: 0.0\n",
      "step: 34110, loss: 4.76837103136063e-09\n",
      "step: 34111, loss: 8.272970717371209e-07\n",
      "step: 34112, loss: 5.7736642702366225e-06\n",
      "step: 34113, loss: 0.0\n",
      "step: 34114, loss: 7.247806479426799e-07\n",
      "step: 34115, loss: 1.430510998545742e-08\n",
      "step: 34116, loss: 2.622603290092229e-08\n",
      "step: 34117, loss: 4.76837147544984e-09\n",
      "step: 34118, loss: 5.22068512509577e-06\n",
      "step: 34119, loss: 0.0\n",
      "step: 34120, loss: 0.0\n",
      "step: 34121, loss: 1.8119730782473198e-07\n",
      "step: 34122, loss: 6.508720957754122e-07\n",
      "step: 34123, loss: 1.430510998545742e-08\n",
      "step: 34124, loss: 7.86779779105018e-08\n",
      "step: 34125, loss: 3.0994396382766354e-08\n",
      "step: 34126, loss: 0.0\n",
      "step: 34127, loss: 6.437293365024743e-08\n",
      "step: 34128, loss: 0.0\n",
      "step: 34129, loss: 0.0\n",
      "step: 34130, loss: 1.192092558000013e-08\n",
      "step: 34131, loss: 0.1659175306558609\n",
      "step: 34132, loss: 1.7642902605530253e-07\n",
      "step: 34133, loss: 2.38418573772492e-09\n",
      "step: 34134, loss: 3.814693627646193e-08\n",
      "step: 34135, loss: 9.53674117454284e-09\n",
      "step: 34136, loss: 2.6295840598322684e-06\n",
      "step: 34137, loss: 0.0\n",
      "step: 34138, loss: 0.018115846440196037\n",
      "step: 34139, loss: 5.555079951591324e-07\n",
      "step: 34140, loss: 0.0\n",
      "step: 34141, loss: 2.38418573772492e-09\n",
      "step: 34142, loss: 0.0\n",
      "step: 34143, loss: 0.0011177132837474346\n",
      "step: 34144, loss: 5.006783965200157e-08\n",
      "step: 34145, loss: 4.76837147544984e-09\n",
      "step: 34146, loss: 5.960457727383073e-08\n",
      "step: 34147, loss: 0.0\n",
      "step: 34148, loss: 4.76837103136063e-09\n",
      "step: 34149, loss: 2.38418573772492e-09\n",
      "step: 34150, loss: 0.0\n",
      "step: 34151, loss: 9.53674117454284e-09\n",
      "step: 34152, loss: 0.0\n",
      "step: 34153, loss: 0.07401861995458603\n",
      "step: 34154, loss: 0.0\n",
      "step: 34155, loss: 1.192092558000013e-08\n",
      "step: 34156, loss: 0.0\n",
      "step: 34157, loss: 0.0\n",
      "step: 34158, loss: 0.0\n",
      "step: 34159, loss: 0.0\n",
      "step: 34160, loss: 2.38418573772492e-09\n",
      "step: 34161, loss: 2.145766764272139e-08\n",
      "step: 34162, loss: 0.0\n",
      "step: 34163, loss: 0.0\n",
      "step: 34164, loss: 0.0\n",
      "step: 34165, loss: 3.337857279461787e-08\n",
      "step: 34166, loss: 2.38418573772492e-09\n",
      "step: 34167, loss: 0.0\n",
      "step: 34168, loss: 0.0\n",
      "step: 34169, loss: 0.0\n",
      "step: 34170, loss: 0.0\n",
      "step: 34171, loss: 0.0\n",
      "step: 34172, loss: 2.38418573772492e-09\n",
      "step: 34173, loss: 2.38418573772492e-09\n",
      "step: 34174, loss: 1.668929350273629e-08\n",
      "step: 34175, loss: 0.0\n",
      "step: 34176, loss: 4.5222900553199e-06\n",
      "step: 34177, loss: 0.0\n",
      "step: 34178, loss: 0.0\n",
      "step: 34179, loss: 5.006783965200157e-08\n",
      "step: 34180, loss: 0.0002573079545982182\n",
      "step: 34181, loss: 0.0\n",
      "step: 34182, loss: 0.0\n",
      "step: 34183, loss: 7.15255676908555e-09\n",
      "step: 34184, loss: 9.53674117454284e-09\n",
      "step: 34185, loss: 0.0\n",
      "step: 34186, loss: 0.0\n",
      "step: 34187, loss: 2.38418573772492e-09\n",
      "step: 34188, loss: 4.76837103136063e-09\n",
      "step: 34189, loss: 0.0\n",
      "step: 34190, loss: 0.0\n",
      "step: 34191, loss: 2.3126467851852794e-07\n",
      "step: 34192, loss: 3.7908193917246535e-07\n",
      "step: 34193, loss: 0.0\n",
      "step: 34194, loss: 2.38418573772492e-09\n",
      "step: 34195, loss: 7.15255632499634e-09\n",
      "step: 34196, loss: 0.0\n",
      "step: 34197, loss: 0.0\n",
      "step: 34198, loss: 0.06197309121489525\n",
      "step: 34199, loss: 0.0\n",
      "step: 34200, loss: 0.0\n",
      "step: 34201, loss: 2.38418573772492e-09\n",
      "step: 34202, loss: 0.0\n",
      "step: 34203, loss: 0.0\n",
      "step: 34204, loss: 0.0\n",
      "step: 34205, loss: 0.0\n",
      "step: 34206, loss: 0.0\n",
      "step: 34207, loss: 0.0\n",
      "step: 34208, loss: 2.38418573772492e-09\n",
      "step: 34209, loss: 2.38418573772492e-09\n",
      "step: 34210, loss: 0.0\n",
      "step: 34211, loss: 0.0\n",
      "step: 34212, loss: 0.0\n",
      "step: 34213, loss: 0.0\n",
      "step: 34214, loss: 0.0\n",
      "step: 34215, loss: 0.0\n",
      "step: 34216, loss: 1.430510998545742e-08\n",
      "step: 34217, loss: 0.0\n",
      "step: 34218, loss: 0.0\n",
      "step: 34219, loss: 4.76837147544984e-09\n",
      "step: 34220, loss: 0.0\n",
      "step: 34221, loss: 7.15255632499634e-09\n",
      "step: 34222, loss: 8.552629878977314e-06\n",
      "step: 34223, loss: 0.0\n",
      "step: 34224, loss: 0.0\n",
      "step: 34225, loss: 0.0\n",
      "step: 34226, loss: 0.05426585301756859\n",
      "step: 34227, loss: 1.192092558000013e-08\n",
      "step: 34228, loss: 6.198873592211385e-08\n",
      "step: 34229, loss: 0.0\n",
      "step: 34230, loss: 0.0\n",
      "step: 34231, loss: 9.53674206272126e-09\n",
      "step: 34232, loss: 0.0\n",
      "step: 34233, loss: 4.76837103136063e-09\n",
      "step: 34234, loss: 7.15255676908555e-09\n",
      "step: 34235, loss: 2.38418573772492e-09\n",
      "step: 34236, loss: 3.814693982917561e-08\n",
      "step: 34237, loss: 2.38418573772492e-09\n",
      "step: 34238, loss: 2.38418573772492e-09\n",
      "step: 34239, loss: 0.0\n",
      "step: 34240, loss: 0.0\n",
      "step: 34241, loss: 0.0\n",
      "step: 34242, loss: 2.6226029348208613e-08\n",
      "step: 34243, loss: 4.76837103136063e-09\n",
      "step: 34244, loss: 0.0\n",
      "step: 34245, loss: 4.76837147544984e-09\n",
      "step: 34246, loss: 2.8610209312773804e-08\n",
      "step: 34247, loss: 1.335141348590696e-07\n",
      "step: 34248, loss: 0.05775328725576401\n",
      "step: 34249, loss: 0.0\n",
      "step: 34250, loss: 0.0\n",
      "step: 34251, loss: 0.0\n",
      "step: 34252, loss: 0.0\n",
      "step: 34253, loss: 0.0\n",
      "step: 34254, loss: 4.76837147544984e-09\n",
      "step: 34255, loss: 2.38418573772492e-09\n",
      "step: 34256, loss: 0.0\n",
      "step: 34257, loss: 0.0\n",
      "step: 34258, loss: 4.76837103136063e-09\n",
      "step: 34259, loss: 0.0\n",
      "step: 34260, loss: 0.0\n",
      "step: 34261, loss: 0.0\n",
      "step: 34262, loss: 0.0\n",
      "step: 34263, loss: 0.0\n",
      "step: 34264, loss: 2.3841845830929742e-08\n",
      "step: 34265, loss: 3.0994389277338996e-08\n",
      "step: 34266, loss: 0.0\n",
      "step: 34267, loss: 4.76837103136063e-09\n",
      "step: 34268, loss: 4.76837103136063e-09\n",
      "step: 34269, loss: 0.0\n",
      "step: 34270, loss: 2.8610209312773804e-08\n",
      "step: 34271, loss: 2.38418573772492e-09\n",
      "step: 34272, loss: 0.0\n",
      "step: 34273, loss: 0.0\n",
      "step: 34274, loss: 1.0013562246058427e-07\n",
      "step: 34275, loss: 0.0\n",
      "step: 34276, loss: 0.0\n",
      "step: 34277, loss: 0.0\n",
      "step: 34278, loss: 0.0\n",
      "step: 34279, loss: 7.15255632499634e-09\n",
      "step: 34280, loss: 0.0\n",
      "step: 34281, loss: 0.0\n",
      "step: 34282, loss: 0.05871429294347763\n",
      "step: 34283, loss: 0.0\n",
      "step: 34284, loss: 0.0710882917046547\n",
      "step: 34285, loss: 0.0\n",
      "step: 34286, loss: 0.0\n",
      "step: 34287, loss: 9.059886707518672e-08\n",
      "step: 34288, loss: 0.052470363676548004\n",
      "step: 34289, loss: 0.0\n",
      "step: 34290, loss: 0.0\n",
      "step: 34291, loss: 0.0\n",
      "step: 34292, loss: 0.0\n",
      "step: 34293, loss: 2.38418573772492e-09\n",
      "step: 34294, loss: 0.0\n",
      "step: 34295, loss: 0.0\n",
      "step: 34296, loss: 4.7683659687436375e-08\n",
      "step: 34297, loss: 7.15255632499634e-09\n",
      "step: 34298, loss: 0.0\n",
      "step: 34299, loss: 0.0\n",
      "step: 34300, loss: 0.0\n",
      "step: 34301, loss: 0.0\n",
      "step: 34302, loss: 4.76837103136063e-09\n",
      "step: 34303, loss: 2.38418573772492e-09\n",
      "step: 34304, loss: 0.12991242110729218\n",
      "step: 34305, loss: 0.0\n",
      "step: 34306, loss: 0.0\n",
      "step: 34307, loss: 0.0\n",
      "step: 34308, loss: 0.0\n",
      "step: 34309, loss: 0.0\n",
      "step: 34310, loss: 0.22600138187408447\n",
      "step: 34311, loss: 0.0\n",
      "step: 34312, loss: 0.0\n",
      "step: 34313, loss: 0.0\n",
      "step: 34314, loss: 2.45569736989637e-07\n",
      "step: 34315, loss: 0.2131713479757309\n",
      "step: 34316, loss: 7.15255632499634e-09\n",
      "step: 34317, loss: 0.0\n",
      "step: 34318, loss: 0.0\n",
      "step: 34319, loss: 0.0\n",
      "step: 34320, loss: 9.53674117454284e-09\n",
      "step: 34321, loss: 0.0\n",
      "step: 34322, loss: 1.2397727289226168e-07\n",
      "step: 34323, loss: 0.0\n",
      "step: 34324, loss: 2.38418573772492e-09\n",
      "step: 34325, loss: 4.76837147544984e-09\n",
      "step: 34326, loss: 0.0\n",
      "step: 34327, loss: 7.15255632499634e-09\n",
      "step: 34328, loss: 0.0\n",
      "step: 34329, loss: 0.0\n",
      "step: 34330, loss: 0.0\n",
      "step: 34331, loss: 9.53674117454284e-09\n",
      "step: 34332, loss: 0.0\n",
      "step: 34333, loss: 2.38418573772492e-09\n",
      "step: 34334, loss: 0.05666865035891533\n",
      "step: 34335, loss: 0.0\n",
      "step: 34336, loss: 0.0\n",
      "step: 34337, loss: 3.576277052275145e-08\n",
      "step: 34338, loss: 0.0\n",
      "step: 34339, loss: 0.0\n",
      "step: 34340, loss: 6.675709585124423e-08\n",
      "step: 34341, loss: 0.0\n",
      "step: 34342, loss: 0.0\n",
      "step: 34343, loss: 9.784692338143941e-06\n",
      "step: 34344, loss: 4.76837147544984e-09\n",
      "step: 34345, loss: 0.0\n",
      "step: 34346, loss: 7.629380149865028e-08\n",
      "step: 34347, loss: 0.0\n",
      "step: 34348, loss: 2.384184938364342e-08\n",
      "step: 34349, loss: 0.0\n",
      "step: 34350, loss: 3.218625010958931e-07\n",
      "step: 34351, loss: 0.0\n",
      "step: 34352, loss: 9.313223081619526e-09\n",
      "step: 34353, loss: 2.622604000634965e-08\n",
      "step: 34354, loss: 0.0\n",
      "step: 34355, loss: 2.145766231365087e-08\n",
      "step: 34356, loss: 0.0\n",
      "step: 34357, loss: 4.76837103136063e-09\n",
      "step: 34358, loss: 0.0\n",
      "step: 34359, loss: 2.38418573772492e-09\n",
      "step: 34360, loss: 0.0\n",
      "step: 34361, loss: 0.0\n",
      "step: 34362, loss: 3.0994389277338996e-08\n",
      "step: 34363, loss: 2.38418573772492e-09\n",
      "step: 34364, loss: 0.0\n",
      "step: 34365, loss: 0.0\n",
      "step: 34366, loss: 0.0\n",
      "step: 34367, loss: 0.0\n",
      "step: 34368, loss: 0.09591402858495712\n",
      "step: 34369, loss: 0.0\n",
      "step: 34370, loss: 0.0\n",
      "step: 34371, loss: 2.38418573772492e-09\n",
      "step: 34372, loss: 9.53674117454284e-09\n",
      "step: 34373, loss: 2.38418573772492e-09\n",
      "step: 34374, loss: 2.8610209312773804e-08\n",
      "step: 34375, loss: 0.0\n",
      "step: 34376, loss: 0.0\n",
      "step: 34377, loss: 0.0\n",
      "step: 34378, loss: 4.76837103136063e-09\n",
      "step: 34379, loss: 0.0\n",
      "step: 34380, loss: 9.53674117454284e-09\n",
      "step: 34381, loss: 2.38418573772492e-09\n",
      "step: 34382, loss: 0.0\n",
      "step: 34383, loss: 0.1585410088300705\n",
      "step: 34384, loss: 0.0\n",
      "step: 34385, loss: 2.8610216418201162e-08\n",
      "step: 34386, loss: 7.15255632499634e-09\n",
      "step: 34387, loss: 1.192092558000013e-08\n",
      "step: 34388, loss: 1.192092558000013e-08\n",
      "step: 34389, loss: 4.76837103136063e-09\n",
      "step: 34390, loss: 0.0\n",
      "step: 34391, loss: 0.00012574635911732912\n",
      "step: 34392, loss: 4.76837103136063e-09\n",
      "step: 34393, loss: 0.0\n",
      "step: 34394, loss: 0.0009008808992803097\n",
      "step: 34395, loss: 2.8610209312773804e-08\n",
      "step: 34396, loss: 0.0002640548918861896\n",
      "step: 34397, loss: 0.0\n",
      "step: 34398, loss: 6.914127226309574e-08\n",
      "step: 34399, loss: 0.0\n",
      "step: 34400, loss: 0.0\n",
      "step: 34401, loss: 0.0\n",
      "step: 34402, loss: 2.38418573772492e-09\n",
      "step: 34403, loss: 0.0\n",
      "step: 34404, loss: 0.0\n",
      "step: 34405, loss: 7.15255632499634e-09\n",
      "step: 34406, loss: 1.192092646817855e-08\n",
      "step: 34407, loss: 4.76837103136063e-09\n",
      "step: 34408, loss: 0.0\n",
      "step: 34409, loss: 4.76837103136063e-09\n",
      "step: 34410, loss: 0.0\n",
      "step: 34411, loss: 2.38418573772492e-09\n",
      "step: 34412, loss: 0.0\n",
      "step: 34413, loss: 2.622604000634965e-08\n",
      "step: 34414, loss: 9.059886707518672e-08\n",
      "step: 34415, loss: 0.0\n",
      "step: 34416, loss: 0.0\n",
      "step: 34417, loss: 0.0\n",
      "step: 34418, loss: 1.001355514063107e-07\n",
      "step: 34419, loss: 7.15255632499634e-09\n",
      "step: 34420, loss: 2.38418573772492e-09\n",
      "step: 34421, loss: 4.76837103136063e-09\n",
      "step: 34422, loss: 0.0\n",
      "step: 34423, loss: 2.38418573772492e-09\n",
      "step: 34424, loss: 0.0\n",
      "step: 34425, loss: 0.0\n",
      "step: 34426, loss: 0.0\n",
      "step: 34427, loss: 0.013629179447889328\n",
      "step: 34428, loss: 0.0\n",
      "step: 34429, loss: 0.0\n",
      "step: 34430, loss: 0.000530058634467423\n",
      "step: 34431, loss: 2.38418573772492e-09\n",
      "step: 34432, loss: 9.53674117454284e-09\n",
      "step: 34433, loss: 2.38418573772492e-09\n",
      "step: 34434, loss: 1.1682475786756186e-07\n",
      "step: 34435, loss: 7.15255676908555e-09\n",
      "step: 34436, loss: 2.38418573772492e-09\n",
      "step: 34437, loss: 0.0\n",
      "step: 34438, loss: 4.76837103136063e-09\n",
      "step: 34439, loss: 1.4305084050647565e-07\n",
      "step: 34440, loss: 1.192092558000013e-08\n",
      "step: 34441, loss: 0.03250233083963394\n",
      "step: 34442, loss: 0.0\n",
      "step: 34443, loss: 4.76837103136063e-09\n",
      "step: 34444, loss: 0.0\n",
      "step: 34445, loss: 0.17081479728221893\n",
      "step: 34446, loss: 1.1920894138484073e-07\n",
      "step: 34447, loss: 0.0\n",
      "step: 34448, loss: 0.0\n",
      "step: 34449, loss: 0.0\n",
      "step: 34450, loss: 0.0\n",
      "step: 34451, loss: 0.0\n",
      "step: 34452, loss: 1.668929527909313e-08\n",
      "step: 34453, loss: 0.0\n",
      "step: 34454, loss: 4.76837103136063e-09\n",
      "step: 34455, loss: 7.15255632499634e-09\n",
      "step: 34456, loss: 0.0\n",
      "step: 34457, loss: 2.38418573772492e-09\n",
      "step: 34458, loss: 0.0\n",
      "step: 34459, loss: 2.5987495178014797e-07\n",
      "step: 34460, loss: 0.0\n",
      "step: 34461, loss: 0.0\n",
      "step: 34462, loss: 2.38418573772492e-09\n",
      "step: 34463, loss: 2.38418573772492e-09\n",
      "step: 34464, loss: 4.76837103136063e-09\n",
      "step: 34465, loss: 2.38418573772492e-09\n",
      "step: 34466, loss: 0.0\n",
      "step: 34467, loss: 0.0\n",
      "step: 34468, loss: 5.006783965200157e-08\n",
      "step: 34469, loss: 0.0\n",
      "step: 34470, loss: 0.0\n",
      "step: 34471, loss: 2.38418573772492e-09\n",
      "step: 34472, loss: 2.38418573772492e-09\n",
      "step: 34473, loss: 4.76837103136063e-09\n",
      "step: 34474, loss: 2.38418573772492e-09\n",
      "step: 34475, loss: 2.38418573772492e-09\n",
      "step: 34476, loss: 1.3113003660691902e-07\n",
      "step: 34477, loss: 0.0\n",
      "step: 34478, loss: 0.0\n",
      "step: 34479, loss: 1.192092646817855e-08\n",
      "step: 34480, loss: 2.38418573772492e-09\n",
      "step: 34481, loss: 0.1281541883945465\n",
      "step: 34482, loss: 4.5299060502657085e-07\n",
      "step: 34483, loss: 0.0\n",
      "step: 34484, loss: 2.38418573772492e-09\n",
      "step: 34485, loss: 0.0\n",
      "step: 34486, loss: 3.337857279461787e-08\n",
      "step: 34487, loss: 2.8610216418201162e-08\n",
      "step: 34488, loss: 0.0\n",
      "step: 34489, loss: 0.0\n",
      "step: 34490, loss: 0.0\n",
      "step: 34491, loss: 0.0\n",
      "step: 34492, loss: 0.0\n",
      "step: 34493, loss: 0.0\n",
      "step: 34494, loss: 0.0\n",
      "step: 34495, loss: 0.0\n",
      "step: 34496, loss: 0.0\n",
      "step: 34497, loss: 5.006785741556996e-08\n",
      "step: 34498, loss: 4.76837147544984e-09\n",
      "step: 34499, loss: 0.0\n",
      "step: 34500, loss: 4.76837147544984e-09\n",
      "step: 34501, loss: 0.0\n",
      "step: 34502, loss: 2.38418573772492e-09\n",
      "step: 34503, loss: 4.76837103136063e-09\n",
      "step: 34504, loss: 0.0\n",
      "step: 34505, loss: 0.0\n",
      "step: 34506, loss: 0.0\n",
      "step: 34507, loss: 0.0\n",
      "step: 34508, loss: 0.0\n",
      "step: 34509, loss: 0.0\n",
      "step: 34510, loss: 0.0\n",
      "step: 34511, loss: 0.0\n",
      "step: 34512, loss: 0.0\n",
      "step: 34513, loss: 0.0\n",
      "step: 34514, loss: 4.76837103136063e-09\n",
      "step: 34515, loss: 0.0\n",
      "step: 34516, loss: 0.0\n",
      "step: 34517, loss: 0.0\n",
      "step: 34518, loss: 0.0\n",
      "step: 34519, loss: 6.803309133829316e-06\n",
      "step: 34520, loss: 0.0\n",
      "step: 34521, loss: 0.0\n",
      "step: 34522, loss: 1.668929350273629e-08\n",
      "step: 34523, loss: 2.6226025795494934e-08\n",
      "step: 34524, loss: 0.0\n",
      "step: 34525, loss: 0.0\n",
      "step: 34526, loss: 0.0\n",
      "step: 34527, loss: 0.0\n",
      "step: 34528, loss: 2.38418573772492e-09\n",
      "step: 34529, loss: 4.76837103136063e-09\n",
      "step: 34530, loss: 0.16143657267093658\n",
      "step: 34531, loss: 1.276800685445778e-05\n",
      "step: 34532, loss: 0.0\n",
      "step: 34533, loss: 0.0\n",
      "step: 34534, loss: 6.079582703932829e-07\n",
      "step: 34535, loss: 4.76837103136063e-09\n",
      "step: 34536, loss: 0.0\n",
      "step: 34537, loss: 0.06287650763988495\n",
      "step: 34538, loss: 0.0\n",
      "step: 34539, loss: 0.0\n",
      "step: 34540, loss: 0.0\n",
      "step: 34541, loss: 0.0\n",
      "step: 34542, loss: 0.0\n",
      "step: 34543, loss: 7.15255632499634e-09\n",
      "step: 34544, loss: 0.0\n",
      "step: 34545, loss: 0.0\n",
      "step: 34546, loss: 0.0\n",
      "step: 34547, loss: 0.0\n",
      "step: 34548, loss: 0.0\n",
      "step: 34549, loss: 0.0\n",
      "step: 34550, loss: 0.0\n",
      "step: 34551, loss: 4.529901787009294e-07\n",
      "step: 34552, loss: 1.430510998545742e-08\n",
      "step: 34553, loss: 5.0067846757428924e-08\n",
      "step: 34554, loss: 0.07962895184755325\n",
      "step: 34555, loss: 1.1682477918384393e-07\n",
      "step: 34556, loss: 2.38418573772492e-09\n",
      "step: 34557, loss: 2.38418573772492e-09\n",
      "step: 34558, loss: 9.53674117454284e-09\n",
      "step: 34559, loss: 0.10831523686647415\n",
      "step: 34560, loss: 0.0\n",
      "step: 34561, loss: 3.0994389277338996e-08\n",
      "step: 34562, loss: 0.05160072445869446\n",
      "step: 34563, loss: 0.05056999623775482\n",
      "step: 34564, loss: 2.38418573772492e-09\n",
      "step: 34565, loss: 0.0\n",
      "step: 34566, loss: 0.0\n",
      "step: 34567, loss: 1.1444075198596693e-07\n",
      "step: 34568, loss: 2.38418573772492e-09\n",
      "step: 34569, loss: 0.0\n",
      "step: 34570, loss: 4.76837103136063e-09\n",
      "step: 34571, loss: 0.0\n",
      "step: 34572, loss: 7.15255632499634e-09\n",
      "step: 34573, loss: 2.145766231365087e-08\n",
      "step: 34574, loss: 2.38418573772492e-09\n",
      "step: 34575, loss: 0.0\n",
      "step: 34576, loss: 0.0\n",
      "step: 34577, loss: 9.53674117454284e-09\n",
      "step: 34578, loss: 0.0\n",
      "step: 34579, loss: 0.0\n",
      "step: 34580, loss: 2.38418573772492e-09\n",
      "step: 34581, loss: 2.38418573772492e-09\n",
      "step: 34582, loss: 0.0\n",
      "step: 34583, loss: 2.38418573772492e-09\n",
      "step: 34584, loss: 0.0\n",
      "step: 34585, loss: 2.38418573772492e-09\n",
      "step: 34586, loss: 9.53674117454284e-09\n",
      "step: 34587, loss: 2.38418573772492e-09\n",
      "step: 34588, loss: 0.0\n",
      "step: 34589, loss: 4.76837147544984e-09\n",
      "step: 34590, loss: 0.0\n",
      "step: 34591, loss: 0.0\n",
      "step: 34592, loss: 7.15255632499634e-09\n",
      "step: 34593, loss: 0.0\n",
      "step: 34594, loss: 0.0\n",
      "step: 34595, loss: 0.0\n",
      "step: 34596, loss: 4.76837103136063e-09\n",
      "step: 34597, loss: 0.0\n",
      "step: 34598, loss: 2.38418573772492e-09\n",
      "step: 34599, loss: 2.727322907958296e-06\n",
      "step: 34600, loss: 8.439928933512419e-07\n",
      "step: 34601, loss: 1.668929350273629e-08\n",
      "step: 34602, loss: 2.1457660537294032e-08\n",
      "step: 34603, loss: 0.0\n",
      "step: 34604, loss: 0.0\n",
      "step: 34605, loss: 0.0\n",
      "step: 34606, loss: 0.12264091521501541\n",
      "step: 34607, loss: 2.38418573772492e-09\n",
      "step: 34608, loss: 4.76837103136063e-09\n",
      "step: 34609, loss: 1.192092558000013e-08\n",
      "step: 34610, loss: 2.8610209312773804e-08\n",
      "step: 34611, loss: 0.0\n",
      "step: 34612, loss: 9.96565404420835e-07\n",
      "step: 34613, loss: 2.38418573772492e-09\n",
      "step: 34614, loss: 0.0\n",
      "step: 34615, loss: 0.0\n",
      "step: 34616, loss: 0.0\n",
      "step: 34617, loss: 0.10637812316417694\n",
      "step: 34618, loss: 1.9073478796372e-08\n",
      "step: 34619, loss: 0.0\n",
      "step: 34620, loss: 3.0994389277338996e-08\n",
      "step: 34621, loss: 0.0\n",
      "step: 34622, loss: 0.0\n",
      "step: 34623, loss: 0.0\n",
      "step: 34624, loss: 0.0\n",
      "step: 34625, loss: 1.430511264999268e-08\n",
      "step: 34626, loss: 0.0\n",
      "step: 34627, loss: 2.38418573772492e-09\n",
      "step: 34628, loss: 0.0\n",
      "step: 34629, loss: 0.20924310386180878\n",
      "step: 34630, loss: 0.0\n",
      "step: 34631, loss: 1.103847580452566e-06\n",
      "step: 34632, loss: 4.76837147544984e-09\n",
      "step: 34633, loss: 0.0\n",
      "step: 34634, loss: 0.0\n",
      "step: 34635, loss: 0.0\n",
      "step: 34636, loss: 0.0\n",
      "step: 34637, loss: 0.0\n",
      "step: 34638, loss: 2.8610209312773804e-08\n",
      "step: 34639, loss: 0.08698626607656479\n",
      "step: 34640, loss: 0.0\n",
      "step: 34641, loss: 0.0\n",
      "step: 34642, loss: 4.76837147544984e-09\n",
      "step: 34643, loss: 0.0\n",
      "step: 34644, loss: 0.0\n",
      "step: 34645, loss: 0.0\n",
      "step: 34646, loss: 2.38418573772492e-09\n",
      "step: 34647, loss: 2.38418573772492e-09\n",
      "step: 34648, loss: 0.0\n",
      "step: 34649, loss: 0.0\n",
      "step: 34650, loss: 0.0\n",
      "step: 34651, loss: 0.0\n",
      "step: 34652, loss: 9.53674117454284e-09\n",
      "step: 34653, loss: 0.0\n",
      "step: 34654, loss: 0.0\n",
      "step: 34655, loss: 7.15255676908555e-09\n",
      "step: 34656, loss: 0.0\n",
      "step: 34657, loss: 0.0\n",
      "step: 34658, loss: 0.0\n",
      "step: 34659, loss: 0.0\n",
      "step: 34660, loss: 2.38418573772492e-09\n",
      "step: 34661, loss: 0.0\n",
      "step: 34662, loss: 4.76837103136063e-09\n",
      "step: 34663, loss: 0.0\n",
      "step: 34664, loss: 1.668929350273629e-08\n",
      "step: 34665, loss: 0.0\n",
      "step: 34666, loss: 0.0\n",
      "step: 34667, loss: 0.0\n",
      "step: 34668, loss: 0.0\n",
      "step: 34669, loss: 0.0\n",
      "step: 34670, loss: 0.021659500896930695\n",
      "step: 34671, loss: 2.38418573772492e-09\n",
      "step: 34672, loss: 0.0\n",
      "step: 34673, loss: 0.0\n",
      "step: 34674, loss: 1.907347702001516e-08\n",
      "step: 34675, loss: 0.0\n",
      "step: 34676, loss: 0.0\n",
      "step: 34677, loss: 0.022974329069256783\n",
      "step: 34678, loss: 0.0\n",
      "step: 34679, loss: 0.0\n",
      "step: 34680, loss: 0.0\n",
      "step: 34681, loss: 0.0\n",
      "step: 34682, loss: 1.668929350273629e-08\n",
      "step: 34683, loss: 0.0\n",
      "step: 34684, loss: 0.000813747406937182\n",
      "step: 34685, loss: 0.04022552818059921\n",
      "step: 34686, loss: 0.0\n",
      "step: 34687, loss: 0.0\n",
      "step: 34688, loss: 0.0\n",
      "step: 34689, loss: 0.0\n",
      "step: 34690, loss: 0.0\n",
      "step: 34691, loss: 0.0\n",
      "step: 34692, loss: 0.13836151361465454\n",
      "step: 34693, loss: 2.2172804392539547e-07\n",
      "step: 34694, loss: 0.03145579248666763\n",
      "step: 34695, loss: 4.053111624102712e-08\n",
      "step: 34696, loss: 0.0\n",
      "step: 34697, loss: 0.0\n",
      "step: 34698, loss: 0.0\n",
      "step: 34699, loss: 1.907348234908568e-08\n",
      "step: 34700, loss: 0.0\n",
      "step: 34701, loss: 0.0\n",
      "step: 34702, loss: 2.38418573772492e-09\n",
      "step: 34703, loss: 0.0\n",
      "step: 34704, loss: 2.38418573772492e-09\n",
      "step: 34705, loss: 0.0\n",
      "step: 34706, loss: 0.0\n",
      "step: 34707, loss: 4.76837103136063e-09\n",
      "step: 34708, loss: 8.821468355790785e-08\n",
      "step: 34709, loss: 0.0\n",
      "step: 34710, loss: 0.09644954651594162\n",
      "step: 34711, loss: 0.0\n",
      "step: 34712, loss: 0.0\n",
      "step: 34713, loss: 0.0690242126584053\n",
      "step: 34714, loss: 0.0\n",
      "step: 34715, loss: 0.0\n",
      "step: 34716, loss: 0.0\n",
      "step: 34717, loss: 0.0005400353111326694\n",
      "step: 34718, loss: 0.0\n",
      "step: 34719, loss: 2.837160764102009e-07\n",
      "step: 34720, loss: 7.390963929765348e-08\n",
      "step: 34721, loss: 0.0\n",
      "step: 34722, loss: 0.0\n",
      "step: 34723, loss: 2.38418573772492e-09\n",
      "step: 34724, loss: 0.0\n",
      "step: 34725, loss: 0.0\n",
      "step: 34726, loss: 0.0\n",
      "step: 34727, loss: 0.0\n",
      "step: 34728, loss: 0.0\n",
      "step: 34729, loss: 2.38418573772492e-09\n",
      "step: 34730, loss: 0.0\n",
      "step: 34731, loss: 0.0\n",
      "step: 34732, loss: 0.0\n",
      "step: 34733, loss: 0.0\n",
      "step: 34734, loss: 2.217283565641992e-07\n",
      "step: 34735, loss: 2.38418573772492e-09\n",
      "step: 34736, loss: 0.0\n",
      "step: 34737, loss: 2.38418573772492e-09\n",
      "step: 34738, loss: 0.0\n",
      "step: 34739, loss: 7.15255632499634e-09\n",
      "step: 34740, loss: 2.38418573772492e-09\n",
      "step: 34741, loss: 2.38418573772492e-09\n",
      "step: 34742, loss: 4.76837103136063e-09\n",
      "step: 34743, loss: 0.0\n",
      "step: 34744, loss: 0.0\n",
      "step: 34745, loss: 2.38418573772492e-09\n",
      "step: 34746, loss: 0.0\n",
      "step: 34747, loss: 0.0\n",
      "step: 34748, loss: 0.0\n",
      "step: 34749, loss: 0.06935488432645798\n",
      "step: 34750, loss: 0.0\n",
      "step: 34751, loss: 9.298302927618352e-08\n",
      "step: 34752, loss: 2.38418573772492e-09\n",
      "step: 34753, loss: 0.0\n",
      "step: 34754, loss: 0.05054609850049019\n",
      "step: 34755, loss: 0.0\n",
      "step: 34756, loss: 0.0\n",
      "step: 34757, loss: 2.956368518880481e-07\n",
      "step: 34758, loss: 0.0\n",
      "step: 34759, loss: 0.0\n",
      "step: 34760, loss: 0.12125843018293381\n",
      "step: 34761, loss: 0.0\n",
      "step: 34762, loss: 0.0\n",
      "step: 34763, loss: 3.337857279461787e-08\n",
      "step: 34764, loss: 0.0\n",
      "step: 34765, loss: 2.38418573772492e-09\n",
      "step: 34766, loss: 9.53674117454284e-09\n",
      "step: 34767, loss: 0.05350591987371445\n",
      "step: 34768, loss: 2.38418573772492e-09\n",
      "step: 34769, loss: 0.0\n",
      "step: 34770, loss: 0.0\n",
      "step: 34771, loss: 0.0\n",
      "step: 34772, loss: 2.38418573772492e-09\n",
      "step: 34773, loss: 1.192092558000013e-08\n",
      "step: 34774, loss: 0.0\n",
      "step: 34775, loss: 0.0\n",
      "step: 34776, loss: 2.38418573772492e-09\n",
      "step: 34777, loss: 0.0\n",
      "step: 34778, loss: 2.38418573772492e-09\n",
      "step: 34779, loss: 2.38418573772492e-09\n",
      "step: 34780, loss: 0.0\n",
      "step: 34781, loss: 2.38418573772492e-09\n",
      "step: 34782, loss: 1.192092558000013e-08\n",
      "step: 34783, loss: 2.384184938364342e-08\n",
      "step: 34784, loss: 7.15255632499634e-09\n",
      "step: 34785, loss: 2.38418573772492e-09\n",
      "step: 34786, loss: 0.0\n",
      "step: 34787, loss: 0.0\n",
      "step: 34788, loss: 1.0013562246058427e-07\n",
      "step: 34789, loss: 0.0\n",
      "step: 34790, loss: 0.0\n",
      "step: 34791, loss: 0.0\n",
      "step: 34792, loss: 4.76837103136063e-09\n",
      "step: 34793, loss: 0.0\n",
      "step: 34794, loss: 5.095906453789212e-05\n",
      "step: 34795, loss: 0.0\n",
      "step: 34796, loss: 2.38418573772492e-09\n",
      "step: 34797, loss: 0.07246261090040207\n",
      "step: 34798, loss: 0.0\n",
      "step: 34799, loss: 9.53674117454284e-09\n",
      "step: 34800, loss: 0.0\n",
      "step: 34801, loss: 0.0\n",
      "step: 34802, loss: 7.15255676908555e-09\n",
      "step: 34803, loss: 0.0\n",
      "step: 34804, loss: 0.16367094218730927\n",
      "step: 34805, loss: 4.76837103136063e-09\n",
      "step: 34806, loss: 0.0\n",
      "step: 34807, loss: 0.0\n",
      "step: 34808, loss: 0.0\n",
      "step: 34809, loss: 0.0\n",
      "step: 34810, loss: 0.0\n",
      "step: 34811, loss: 0.0\n",
      "step: 34812, loss: 0.0\n",
      "step: 34813, loss: 2.50085508923803e-06\n",
      "step: 34814, loss: 0.0\n",
      "step: 34815, loss: 1.2159310358583753e-07\n",
      "step: 34816, loss: 0.0\n",
      "step: 34817, loss: 2.6226029348208613e-08\n",
      "step: 34818, loss: 0.0\n",
      "step: 34819, loss: 0.0\n",
      "step: 34820, loss: 0.0\n",
      "step: 34821, loss: 9.53674117454284e-09\n",
      "step: 34822, loss: 4.76837103136063e-09\n",
      "step: 34823, loss: 0.0\n",
      "step: 34824, loss: 7.15255676908555e-09\n",
      "step: 34825, loss: 2.8610209312773804e-08\n",
      "step: 34826, loss: 0.0\n",
      "step: 34827, loss: 0.0\n",
      "step: 34828, loss: 4.76837103136063e-09\n",
      "step: 34829, loss: 0.0\n",
      "step: 34830, loss: 2.38418573772492e-09\n",
      "step: 34831, loss: 0.0\n",
      "step: 34832, loss: 0.0\n",
      "step: 34833, loss: 2.3841844054572903e-08\n",
      "step: 34834, loss: 3.5762759864610416e-08\n",
      "step: 34835, loss: 0.0\n",
      "step: 34836, loss: 0.0\n",
      "step: 34837, loss: 0.0\n",
      "step: 34838, loss: 0.0\n",
      "step: 34839, loss: 0.0\n",
      "step: 34840, loss: 0.0016704612644389272\n",
      "step: 34841, loss: 0.0\n",
      "step: 34842, loss: 0.0\n",
      "step: 34843, loss: 2.38418573772492e-09\n",
      "step: 34844, loss: 2.38418573772492e-09\n",
      "step: 34845, loss: 0.0\n",
      "step: 34846, loss: 0.0\n",
      "step: 34847, loss: 0.0\n",
      "step: 34848, loss: 0.0\n",
      "step: 34849, loss: 2.38418573772492e-09\n",
      "step: 34850, loss: 0.03782907873392105\n",
      "step: 34851, loss: 0.0\n",
      "step: 34852, loss: 0.00010720556019805372\n",
      "step: 34853, loss: 2.38418573772492e-09\n",
      "step: 34854, loss: 2.8610211089130644e-08\n",
      "step: 34855, loss: 4.76837147544984e-09\n",
      "step: 34856, loss: 0.0\n",
      "step: 34857, loss: 1.4305064155450964e-07\n",
      "step: 34858, loss: 0.0\n",
      "step: 34859, loss: 1.668929350273629e-08\n",
      "step: 34860, loss: 0.0\n",
      "step: 34861, loss: 0.0\n",
      "step: 34862, loss: 0.0\n",
      "step: 34863, loss: 0.0\n",
      "step: 34864, loss: 0.0\n",
      "step: 34865, loss: 2.38418573772492e-09\n",
      "step: 34866, loss: 2.38418573772492e-09\n",
      "step: 34867, loss: 3.0994396382766354e-08\n",
      "step: 34868, loss: 0.0\n",
      "step: 34869, loss: 0.0\n",
      "step: 34870, loss: 9.53674117454284e-09\n",
      "step: 34871, loss: 8.106215432235331e-08\n",
      "step: 34872, loss: 4.76837103136063e-09\n",
      "step: 34873, loss: 0.0018431340577080846\n",
      "step: 34874, loss: 1.668929883180681e-08\n",
      "step: 34875, loss: 4.410694884882105e-07\n",
      "step: 34876, loss: 8.821468355790785e-08\n",
      "step: 34877, loss: 3.337857279461787e-08\n",
      "step: 34878, loss: 2.38418573772492e-09\n",
      "step: 34879, loss: 0.0\n",
      "step: 34880, loss: 0.0\n",
      "step: 34881, loss: 4.76837103136063e-09\n",
      "step: 34882, loss: 7.15255632499634e-09\n",
      "step: 34883, loss: 1.382822887308066e-07\n",
      "step: 34884, loss: 0.0\n",
      "step: 34885, loss: 1.430510998545742e-08\n",
      "step: 34886, loss: 0.0\n",
      "step: 34887, loss: 2.38418573772492e-09\n",
      "step: 34888, loss: 0.0\n",
      "step: 34889, loss: 4.76837103136063e-09\n",
      "step: 34890, loss: 0.0\n",
      "step: 34891, loss: 2.38418573772492e-09\n",
      "step: 34892, loss: 2.38418573772492e-09\n",
      "step: 34893, loss: 2.38418573772492e-09\n",
      "step: 34894, loss: 2.38418573772492e-09\n",
      "step: 34895, loss: 2.38418573772492e-09\n",
      "step: 34896, loss: 0.0\n",
      "step: 34897, loss: 0.0\n",
      "step: 34898, loss: 0.0\n",
      "step: 34899, loss: 0.0\n",
      "step: 34900, loss: 0.0\n",
      "step: 34901, loss: 4.768366679286373e-08\n",
      "step: 34902, loss: 0.0\n",
      "step: 34903, loss: 3.504726464598207e-07\n",
      "step: 34904, loss: 4.76837147544984e-09\n",
      "step: 34905, loss: 0.0003628306440077722\n",
      "step: 34906, loss: 0.0\n",
      "step: 34907, loss: 0.0\n",
      "step: 34908, loss: 0.0\n",
      "step: 34909, loss: 2.243410563096404e-06\n",
      "step: 34910, loss: 4.76837103136063e-09\n",
      "step: 34911, loss: 0.0\n",
      "step: 34912, loss: 0.0\n",
      "step: 34913, loss: 0.0\n",
      "step: 34914, loss: 0.0\n",
      "step: 34915, loss: 0.0\n",
      "step: 34916, loss: 0.0\n",
      "step: 34917, loss: 0.0\n",
      "step: 34918, loss: 0.03995199128985405\n",
      "step: 34919, loss: 2.38418573772492e-09\n",
      "step: 34920, loss: 0.0\n",
      "step: 34921, loss: 0.0\n",
      "step: 34922, loss: 0.0\n",
      "step: 34923, loss: 0.0\n",
      "step: 34924, loss: 0.0\n",
      "step: 34925, loss: 0.1448424756526947\n",
      "step: 34926, loss: 2.38418573772492e-09\n",
      "step: 34927, loss: 6.675709585124423e-08\n",
      "step: 34928, loss: 0.06387989223003387\n",
      "step: 34929, loss: 0.0\n",
      "step: 34930, loss: 0.0\n",
      "step: 34931, loss: 0.0\n",
      "step: 34932, loss: 0.0\n",
      "step: 34933, loss: 0.0\n",
      "step: 34934, loss: 0.0\n",
      "step: 34935, loss: 0.0\n",
      "step: 34936, loss: 2.653423052834114e-06\n",
      "step: 34937, loss: 0.0\n",
      "step: 34938, loss: 0.0\n",
      "step: 34939, loss: 0.0\n",
      "step: 34940, loss: 1.430510998545742e-08\n",
      "step: 34941, loss: 0.0\n",
      "step: 34942, loss: 0.0\n",
      "step: 34943, loss: 0.0\n",
      "step: 34944, loss: 0.0\n",
      "step: 34945, loss: 0.0\n",
      "step: 34946, loss: 0.0\n",
      "step: 34947, loss: 7.15255676908555e-09\n",
      "step: 34948, loss: 0.0\n",
      "step: 34949, loss: 0.0\n",
      "step: 34950, loss: 0.0\n",
      "step: 34951, loss: 4.76837103136063e-09\n",
      "step: 34952, loss: 1.668929705544997e-08\n",
      "step: 34953, loss: 0.0\n",
      "step: 34954, loss: 0.0\n",
      "step: 34955, loss: 0.0\n",
      "step: 34956, loss: 0.05417898669838905\n",
      "step: 34957, loss: 1.6450205748697044e-06\n",
      "step: 34958, loss: 0.0\n",
      "step: 34959, loss: 0.0\n",
      "step: 34960, loss: 3.621250698415679e-06\n",
      "step: 34961, loss: 0.0\n",
      "step: 34962, loss: 2.38418573772492e-09\n",
      "step: 34963, loss: 3.0994396382766354e-08\n",
      "step: 34964, loss: 0.0\n",
      "step: 34965, loss: 4.76837103136063e-09\n",
      "step: 34966, loss: 2.2411221323181962e-07\n",
      "step: 34967, loss: 0.0\n",
      "step: 34968, loss: 0.0\n",
      "step: 34969, loss: 0.0\n",
      "step: 34970, loss: 0.02141747623682022\n",
      "step: 34971, loss: 0.0\n",
      "step: 34972, loss: 2.38418573772492e-09\n",
      "step: 34973, loss: 9.0598852864332e-08\n",
      "step: 34974, loss: 1.668929350273629e-08\n",
      "step: 34975, loss: 2.38418573772492e-09\n",
      "step: 34976, loss: 1.073787370842183e-05\n",
      "step: 34977, loss: 0.028484592214226723\n",
      "step: 34978, loss: 0.0\n",
      "step: 34979, loss: 0.09157335013151169\n",
      "step: 34980, loss: 0.0\n",
      "step: 34981, loss: 4.76837103136063e-09\n",
      "step: 34982, loss: 0.0\n",
      "step: 34983, loss: 2.38418573772492e-09\n",
      "step: 34984, loss: 0.0\n",
      "step: 34985, loss: 0.0\n",
      "step: 34986, loss: 0.0\n",
      "step: 34987, loss: 0.0\n",
      "step: 34988, loss: 9.53674117454284e-09\n",
      "step: 34989, loss: 2.38418573772492e-09\n",
      "step: 34990, loss: 4.76837103136063e-09\n",
      "step: 34991, loss: 2.38418573772492e-09\n",
      "step: 34992, loss: 0.0\n",
      "step: 34993, loss: 0.0\n",
      "step: 34994, loss: 0.0\n",
      "step: 34995, loss: 0.0859377384185791\n",
      "step: 34996, loss: 0.0\n",
      "step: 34997, loss: 0.0\n",
      "step: 34998, loss: 1.2397731552482583e-07\n",
      "step: 34999, loss: 0.0\n",
      "step: 35000, loss: 1.907347702001516e-08\n",
      "step: 35001, loss: 0.0\n",
      "step: 35002, loss: 2.5987455387621594e-07\n",
      "step: 35003, loss: 0.0\n",
      "step: 35004, loss: 0.0\n",
      "step: 35005, loss: 2.38418573772492e-09\n",
      "step: 35006, loss: 0.0\n",
      "step: 35007, loss: 0.0\n",
      "step: 35008, loss: 2.38418573772492e-09\n",
      "step: 35009, loss: 0.0\n",
      "step: 35010, loss: 0.0\n",
      "step: 35011, loss: 0.0\n",
      "step: 35012, loss: 1.430511176181426e-08\n",
      "step: 35013, loss: 0.0\n",
      "step: 35014, loss: 0.0\n",
      "step: 35015, loss: 0.0\n",
      "step: 35016, loss: 3.123259375570342e-07\n",
      "step: 35017, loss: 0.0\n",
      "step: 35018, loss: 4.76837103136063e-09\n",
      "step: 35019, loss: 4.76837147544984e-09\n",
      "step: 35020, loss: 0.0\n",
      "step: 35021, loss: 0.0\n",
      "step: 35022, loss: 8.342914952663705e-06\n",
      "step: 35023, loss: 0.15940198302268982\n",
      "step: 35024, loss: 0.0\n",
      "step: 35025, loss: 6.170519191073254e-05\n",
      "step: 35026, loss: 4.76837103136063e-09\n",
      "step: 35027, loss: 0.0\n",
      "step: 35028, loss: 0.0\n",
      "step: 35029, loss: 1.907347702001516e-08\n",
      "step: 35030, loss: 0.0\n",
      "step: 35031, loss: 0.0\n",
      "step: 35032, loss: 0.0\n",
      "step: 35033, loss: 0.0\n",
      "step: 35034, loss: 0.07001788169145584\n",
      "step: 35035, loss: 9.536721279346239e-08\n",
      "step: 35036, loss: 2.38418573772492e-09\n",
      "step: 35037, loss: 0.0\n",
      "step: 35038, loss: 0.0\n",
      "step: 35039, loss: 0.0\n",
      "step: 35040, loss: 2.38418573772492e-09\n",
      "step: 35041, loss: 0.0\n",
      "step: 35042, loss: 0.0\n",
      "step: 35043, loss: 0.0\n",
      "step: 35044, loss: 0.0\n",
      "step: 35045, loss: 6.1740884120808914e-06\n",
      "step: 35046, loss: 2.38418573772492e-09\n",
      "step: 35047, loss: 0.0\n",
      "step: 35048, loss: 0.0\n",
      "step: 35049, loss: 0.0\n",
      "step: 35050, loss: 2.38418573772492e-09\n",
      "step: 35051, loss: 0.0\n",
      "step: 35052, loss: 4.76837103136063e-09\n",
      "step: 35053, loss: 0.0\n",
      "step: 35054, loss: 0.0\n",
      "step: 35055, loss: 1.4305064155450964e-07\n",
      "step: 35056, loss: 0.026908474043011665\n",
      "step: 35057, loss: 0.0\n",
      "step: 35058, loss: 0.0\n",
      "step: 35059, loss: 2.38418573772492e-09\n",
      "step: 35060, loss: 2.38418573772492e-09\n",
      "step: 35061, loss: 2.38418573772492e-09\n",
      "step: 35062, loss: 0.044373951852321625\n",
      "step: 35063, loss: 0.0\n",
      "step: 35064, loss: 4.52994939337259e-08\n",
      "step: 35065, loss: 4.76837103136063e-09\n",
      "step: 35066, loss: 0.0\n",
      "step: 35067, loss: 0.19184532761573792\n",
      "step: 35068, loss: 1.5898582205409184e-05\n",
      "step: 35069, loss: 4.76837103136063e-09\n",
      "step: 35070, loss: 0.0\n",
      "step: 35071, loss: 0.0\n",
      "step: 35072, loss: 2.38418573772492e-09\n",
      "step: 35073, loss: 9.53674117454284e-09\n",
      "step: 35074, loss: 0.0\n",
      "step: 35075, loss: 6.437291233396536e-08\n",
      "step: 35076, loss: 0.0\n",
      "step: 35077, loss: 0.06571421027183533\n",
      "step: 35078, loss: 0.0\n",
      "step: 35079, loss: 0.0\n",
      "step: 35080, loss: 0.0\n",
      "step: 35081, loss: 0.0\n",
      "step: 35082, loss: 0.055808525532484055\n",
      "step: 35083, loss: 0.0\n",
      "step: 35084, loss: 0.0\n",
      "step: 35085, loss: 0.0\n",
      "step: 35086, loss: 0.0\n",
      "step: 35087, loss: 7.15255632499634e-09\n",
      "step: 35088, loss: 0.0\n",
      "step: 35089, loss: 0.0\n",
      "step: 35090, loss: 0.0\n",
      "step: 35091, loss: 2.38418573772492e-09\n",
      "step: 35092, loss: 2.6226029348208613e-08\n",
      "step: 35093, loss: 0.0\n",
      "step: 35094, loss: 0.0\n",
      "step: 35095, loss: 0.0\n",
      "step: 35096, loss: 0.0\n",
      "step: 35097, loss: 0.0\n",
      "step: 35098, loss: 0.0\n",
      "step: 35099, loss: 0.0\n",
      "step: 35100, loss: 0.005039668641984463\n",
      "step: 35101, loss: 0.0\n",
      "step: 35102, loss: 0.0\n",
      "step: 35103, loss: 0.0\n",
      "step: 35104, loss: 0.0\n",
      "step: 35105, loss: 0.0\n",
      "step: 35106, loss: 0.0\n",
      "step: 35107, loss: 0.0\n",
      "step: 35108, loss: 0.005086301360279322\n",
      "step: 35109, loss: 2.38418573772492e-09\n",
      "step: 35110, loss: 0.0\n",
      "step: 35111, loss: 0.0\n",
      "step: 35112, loss: 0.0\n",
      "step: 35113, loss: 0.0\n",
      "step: 35114, loss: 0.0\n",
      "step: 35115, loss: 7.15255632499634e-09\n",
      "step: 35116, loss: 0.0\n",
      "step: 35117, loss: 1.430510998545742e-08\n",
      "step: 35118, loss: 0.0\n",
      "step: 35119, loss: 0.0\n",
      "step: 35120, loss: 2.3841845830929742e-08\n",
      "step: 35121, loss: 2.38418573772492e-09\n",
      "step: 35122, loss: 0.0\n",
      "step: 35123, loss: 0.0\n",
      "step: 35124, loss: 0.0\n",
      "step: 35125, loss: 0.0\n",
      "step: 35126, loss: 0.0\n",
      "step: 35127, loss: 0.0\n",
      "step: 35128, loss: 0.0\n",
      "step: 35129, loss: 0.0\n",
      "step: 35130, loss: 0.06964364647865295\n",
      "step: 35131, loss: 0.0\n",
      "step: 35132, loss: 0.0\n",
      "step: 35133, loss: 0.0\n",
      "step: 35134, loss: 0.0\n",
      "step: 35135, loss: 2.38418573772492e-09\n",
      "step: 35136, loss: 0.07897577434778214\n",
      "step: 35137, loss: 0.0\n",
      "step: 35138, loss: 4.76837103136063e-09\n",
      "step: 35139, loss: 4.76837103136063e-09\n",
      "step: 35140, loss: 0.0\n",
      "step: 35141, loss: 0.0\n",
      "step: 35142, loss: 0.0\n",
      "step: 35143, loss: 2.38418573772492e-09\n",
      "step: 35144, loss: 4.76837103136063e-09\n",
      "step: 35145, loss: 0.0\n",
      "step: 35146, loss: 2.38418573772492e-09\n",
      "step: 35147, loss: 0.0\n",
      "step: 35148, loss: 0.0\n",
      "step: 35149, loss: 0.0\n",
      "step: 35150, loss: 2.1457660537294032e-08\n",
      "step: 35151, loss: 0.0\n",
      "step: 35152, loss: 5.0067846757428924e-08\n",
      "step: 35153, loss: 0.027745245024561882\n",
      "step: 35154, loss: 0.0\n",
      "step: 35155, loss: 2.002904875553213e-05\n",
      "step: 35156, loss: 2.7894833465325064e-07\n",
      "step: 35157, loss: 0.0\n",
      "step: 35158, loss: 0.0\n",
      "step: 35159, loss: 0.0\n",
      "step: 35160, loss: 9.53674206272126e-09\n",
      "step: 35161, loss: 2.38418573772492e-09\n",
      "step: 35162, loss: 0.0\n",
      "step: 35163, loss: 2.38418573772492e-09\n",
      "step: 35164, loss: 0.0\n",
      "step: 35165, loss: 0.0\n",
      "step: 35166, loss: 0.0\n",
      "step: 35167, loss: 0.0\n",
      "step: 35168, loss: 0.0\n",
      "step: 35169, loss: 0.0\n",
      "step: 35170, loss: 9.53674117454284e-09\n",
      "step: 35171, loss: 0.06735814362764359\n",
      "step: 35172, loss: 2.384184938364342e-08\n",
      "step: 35173, loss: 0.0\n",
      "step: 35174, loss: 0.0\n",
      "step: 35175, loss: 4.76837103136063e-09\n",
      "step: 35176, loss: 0.0\n",
      "step: 35177, loss: 0.0\n",
      "step: 35178, loss: 0.0\n",
      "step: 35179, loss: 0.0\n",
      "step: 35180, loss: 1.2874582466793072e-07\n",
      "step: 35181, loss: 0.0\n",
      "step: 35182, loss: 2.38418573772492e-09\n",
      "step: 35183, loss: 0.0\n",
      "step: 35184, loss: 1.430510998545742e-08\n",
      "step: 35185, loss: 0.0\n",
      "step: 35186, loss: 7.15255632499634e-09\n",
      "step: 35187, loss: 2.38418573772492e-09\n",
      "step: 35188, loss: 0.0\n",
      "step: 35189, loss: 0.0\n",
      "step: 35190, loss: 0.0\n",
      "step: 35191, loss: 0.0\n",
      "step: 35192, loss: 1.668929350273629e-08\n",
      "step: 35193, loss: 0.0\n",
      "step: 35194, loss: 0.07461431622505188\n",
      "step: 35195, loss: 0.0\n",
      "step: 35196, loss: 0.0\n",
      "step: 35197, loss: 0.0\n",
      "step: 35198, loss: 0.0\n",
      "step: 35199, loss: 0.0\n",
      "step: 35200, loss: 0.0\n",
      "step: 35201, loss: 0.0\n",
      "step: 35202, loss: 0.0\n",
      "step: 35203, loss: 0.0\n",
      "step: 35204, loss: 0.0\n",
      "step: 35205, loss: 2.38418573772492e-09\n",
      "step: 35206, loss: 0.0\n",
      "step: 35207, loss: 0.0\n",
      "step: 35208, loss: 2.6226025795494934e-08\n",
      "step: 35209, loss: 2.38418573772492e-09\n",
      "step: 35210, loss: 0.0\n",
      "step: 35211, loss: 2.38418573772492e-09\n",
      "step: 35212, loss: 0.0\n",
      "step: 35213, loss: 0.0\n",
      "step: 35214, loss: 0.0\n",
      "step: 35215, loss: 7.15255632499634e-09\n",
      "step: 35216, loss: 0.0\n",
      "step: 35217, loss: 0.0\n",
      "step: 35218, loss: 0.0\n",
      "step: 35219, loss: 0.0\n",
      "step: 35220, loss: 0.0\n",
      "step: 35221, loss: 0.0\n",
      "step: 35222, loss: 0.0\n",
      "step: 35223, loss: 0.0\n",
      "step: 35224, loss: 9.53674206272126e-09\n",
      "step: 35225, loss: 0.04604475945234299\n",
      "step: 35226, loss: 0.0\n",
      "step: 35227, loss: 0.0\n",
      "step: 35228, loss: 0.0\n",
      "step: 35229, loss: 1.1920908349338788e-07\n",
      "step: 35230, loss: 0.0\n",
      "step: 35231, loss: 0.0\n",
      "step: 35232, loss: 0.0\n",
      "step: 35233, loss: 0.0\n",
      "step: 35234, loss: 0.0\n",
      "step: 35235, loss: 7.15255632499634e-09\n",
      "step: 35236, loss: 0.0\n",
      "step: 35237, loss: 0.0\n",
      "step: 35238, loss: 1.192092558000013e-08\n",
      "step: 35239, loss: 0.0\n",
      "step: 35240, loss: 0.06979792565107346\n",
      "step: 35241, loss: 0.0\n",
      "step: 35242, loss: 0.0\n",
      "step: 35243, loss: 0.0\n",
      "step: 35244, loss: 0.0\n",
      "step: 35245, loss: 2.38418573772492e-09\n",
      "step: 35246, loss: 4.76837103136063e-09\n",
      "step: 35247, loss: 2.384185116000026e-08\n",
      "step: 35248, loss: 0.0\n",
      "step: 35249, loss: 1.668929705544997e-08\n",
      "step: 35250, loss: 0.0\n",
      "step: 35251, loss: 0.0\n",
      "step: 35252, loss: 1.668929350273629e-08\n",
      "step: 35253, loss: 0.0\n",
      "step: 35254, loss: 2.38418573772492e-09\n",
      "step: 35255, loss: 0.0\n",
      "step: 35256, loss: 0.0014915347564965487\n",
      "step: 35257, loss: 0.0\n",
      "step: 35258, loss: 3.5762759864610416e-08\n",
      "step: 35259, loss: 0.0\n",
      "step: 35260, loss: 0.0\n",
      "step: 35261, loss: 0.0\n",
      "step: 35262, loss: 7.15255632499634e-09\n",
      "step: 35263, loss: 2.38418573772492e-09\n",
      "step: 35264, loss: 0.06767734885215759\n",
      "step: 35265, loss: 0.0\n",
      "step: 35266, loss: 0.0\n",
      "step: 35267, loss: 9.53674117454284e-09\n",
      "step: 35268, loss: 4.76837147544984e-09\n",
      "step: 35269, loss: 1.192092558000013e-08\n",
      "step: 35270, loss: 0.0\n",
      "step: 35271, loss: 0.0\n",
      "step: 35272, loss: 2.38418573772492e-09\n",
      "step: 35273, loss: 0.0\n",
      "step: 35274, loss: 0.0\n",
      "step: 35275, loss: 0.0\n",
      "step: 35276, loss: 0.0\n",
      "step: 35277, loss: 0.0\n",
      "step: 35278, loss: 9.53674117454284e-09\n",
      "step: 35279, loss: 0.0\n",
      "step: 35280, loss: 0.0\n",
      "step: 35281, loss: 0.0\n",
      "step: 35282, loss: 0.0\n",
      "step: 35283, loss: 0.0\n",
      "step: 35284, loss: 0.0\n",
      "step: 35285, loss: 0.004397328477352858\n",
      "step: 35286, loss: 0.0\n",
      "step: 35287, loss: 0.0\n",
      "step: 35288, loss: 0.3110730051994324\n",
      "step: 35289, loss: 0.0\n",
      "step: 35290, loss: 0.0\n",
      "step: 35291, loss: 0.0\n",
      "step: 35292, loss: 0.0\n",
      "step: 35293, loss: 0.0\n",
      "step: 35294, loss: 0.0\n",
      "step: 35295, loss: 2.38418573772492e-09\n",
      "step: 35296, loss: 2.38418573772492e-09\n",
      "step: 35297, loss: 0.0\n",
      "step: 35298, loss: 0.0\n",
      "step: 35299, loss: 0.0\n",
      "step: 35300, loss: 0.0\n",
      "step: 35301, loss: 0.0\n",
      "step: 35302, loss: 2.38418573772492e-09\n",
      "step: 35303, loss: 0.0\n",
      "step: 35304, loss: 2.38418573772492e-09\n",
      "step: 35305, loss: 0.0\n",
      "step: 35306, loss: 0.0\n",
      "step: 35307, loss: 0.0\n",
      "step: 35308, loss: 0.0\n",
      "step: 35309, loss: 1.192092558000013e-08\n",
      "step: 35310, loss: 0.0\n",
      "step: 35311, loss: 0.0733562782406807\n",
      "step: 35312, loss: 6.127263532107463e-07\n",
      "step: 35313, loss: 0.0\n",
      "step: 35314, loss: 4.76837103136063e-09\n",
      "step: 35315, loss: 7.15255632499634e-09\n",
      "step: 35316, loss: 4.76837103136063e-09\n",
      "step: 35317, loss: 2.3841845830929742e-08\n",
      "step: 35318, loss: 1.668929350273629e-08\n",
      "step: 35319, loss: 0.0\n",
      "step: 35320, loss: 0.0\n",
      "step: 35321, loss: 1.192092646817855e-08\n",
      "step: 35322, loss: 0.0\n",
      "step: 35323, loss: 4.76837147544984e-09\n",
      "step: 35324, loss: 5.722005198549596e-07\n",
      "step: 35325, loss: 0.0\n",
      "step: 35326, loss: 6.437291233396536e-08\n",
      "step: 35327, loss: 0.0\n",
      "step: 35328, loss: 0.0\n",
      "step: 35329, loss: 1.192092558000013e-08\n",
      "step: 35330, loss: 0.18777123093605042\n",
      "step: 35331, loss: 0.0\n",
      "step: 35332, loss: 0.0\n",
      "step: 35333, loss: 0.001128109055571258\n",
      "step: 35334, loss: 3.814695404003032e-08\n",
      "step: 35335, loss: 0.27690747380256653\n",
      "step: 35336, loss: 2.38418573772492e-09\n",
      "step: 35337, loss: 3.821772406809032e-05\n",
      "step: 35338, loss: 0.0\n",
      "step: 35339, loss: 0.0\n",
      "step: 35340, loss: 0.0\n",
      "step: 35341, loss: 1.144405885611377e-07\n",
      "step: 35342, loss: 1.668929350273629e-08\n",
      "step: 35343, loss: 1.1474177881609648e-05\n",
      "step: 35344, loss: 0.0\n",
      "step: 35345, loss: 0.0\n",
      "step: 35346, loss: 0.0\n",
      "step: 35347, loss: 0.0\n",
      "step: 35348, loss: 0.0\n",
      "step: 35349, loss: 0.0\n",
      "step: 35350, loss: 0.0\n",
      "step: 35351, loss: 0.0\n",
      "step: 35352, loss: 0.0\n",
      "step: 35353, loss: 0.0\n",
      "step: 35354, loss: 0.0\n",
      "step: 35355, loss: 2.38418573772492e-09\n",
      "step: 35356, loss: 0.0\n",
      "step: 35357, loss: 0.0\n",
      "step: 35358, loss: 4.76837103136063e-09\n",
      "step: 35359, loss: 7.15255632499634e-09\n",
      "step: 35360, loss: 2.38418573772492e-09\n",
      "step: 35361, loss: 0.0\n",
      "step: 35362, loss: 2.38418573772492e-09\n",
      "step: 35363, loss: 0.0\n",
      "step: 35364, loss: 0.0\n",
      "step: 35365, loss: 2.38418573772492e-09\n",
      "step: 35366, loss: 0.0\n",
      "step: 35367, loss: 0.0\n",
      "step: 35368, loss: 2.38418573772492e-09\n",
      "step: 35369, loss: 0.0\n",
      "step: 35370, loss: 0.0\n",
      "step: 35371, loss: 2.38418573772492e-09\n",
      "step: 35372, loss: 0.0\n",
      "step: 35373, loss: 1.192092558000013e-08\n",
      "step: 35374, loss: 1.59739812488624e-07\n",
      "step: 35375, loss: 0.0\n",
      "step: 35376, loss: 1.907347702001516e-08\n",
      "step: 35377, loss: 0.0\n",
      "step: 35378, loss: 0.0\n",
      "step: 35379, loss: 0.057342950254678726\n",
      "step: 35380, loss: 0.0\n",
      "step: 35381, loss: 0.0\n",
      "step: 35382, loss: 0.17206686735153198\n",
      "step: 35383, loss: 8.010704277694458e-07\n",
      "step: 35384, loss: 0.0\n",
      "step: 35385, loss: 0.0\n",
      "step: 35386, loss: 4.291529975830599e-08\n",
      "step: 35387, loss: 0.0\n",
      "step: 35388, loss: 3.43314022757113e-05\n",
      "step: 35389, loss: 0.0\n",
      "step: 35390, loss: 5.2452033827421474e-08\n",
      "step: 35391, loss: 0.0\n",
      "step: 35392, loss: 4.76837147544984e-09\n",
      "step: 35393, loss: 3.337858700547258e-08\n",
      "step: 35394, loss: 0.0\n",
      "step: 35395, loss: 3.814693627646193e-08\n",
      "step: 35396, loss: 7.15255676908555e-09\n",
      "step: 35397, loss: 0.0\n",
      "step: 35398, loss: 4.76837103136063e-09\n",
      "step: 35399, loss: 0.0\n",
      "step: 35400, loss: 0.0\n",
      "step: 35401, loss: 0.0\n",
      "step: 35402, loss: 0.0\n",
      "step: 35403, loss: 0.0\n",
      "step: 35404, loss: 0.0\n",
      "step: 35405, loss: 2.3841844054572903e-08\n",
      "step: 35406, loss: 0.0\n",
      "step: 35407, loss: 0.0\n",
      "step: 35408, loss: 0.0\n",
      "step: 35409, loss: 0.0\n",
      "step: 35410, loss: 0.0\n",
      "step: 35411, loss: 0.0\n",
      "step: 35412, loss: 2.38418573772492e-09\n",
      "step: 35413, loss: 0.0\n",
      "step: 35414, loss: 0.0\n",
      "step: 35415, loss: 0.0\n",
      "step: 35416, loss: 0.0\n",
      "step: 35417, loss: 0.0\n",
      "step: 35418, loss: 0.03767053782939911\n",
      "step: 35419, loss: 0.0\n",
      "step: 35420, loss: 0.008571991696953773\n",
      "step: 35421, loss: 0.0\n",
      "step: 35422, loss: 4.76837103136063e-09\n",
      "step: 35423, loss: 0.0\n",
      "step: 35424, loss: 4.76837103136063e-09\n",
      "step: 35425, loss: 0.0\n",
      "step: 35426, loss: 2.6226025795494934e-08\n",
      "step: 35427, loss: 4.458384808003757e-07\n",
      "step: 35428, loss: 2.047910584224155e-06\n",
      "step: 35429, loss: 0.0\n",
      "step: 35430, loss: 2.38418573772492e-09\n",
      "step: 35431, loss: 0.0\n",
      "step: 35432, loss: 0.0\n",
      "step: 35433, loss: 1.385163955092139e-06\n",
      "step: 35434, loss: 0.0\n",
      "step: 35435, loss: 0.0\n",
      "step: 35436, loss: 2.1457660537294032e-08\n",
      "step: 35437, loss: 0.0\n",
      "step: 35438, loss: 2.38418573772492e-09\n",
      "step: 35439, loss: 0.0\n",
      "step: 35440, loss: 2.38418573772492e-09\n",
      "step: 35441, loss: 0.0\n",
      "step: 35442, loss: 0.04154297336935997\n",
      "step: 35443, loss: 0.0\n",
      "step: 35444, loss: 7.15255676908555e-09\n",
      "step: 35445, loss: 2.38418573772492e-09\n",
      "step: 35446, loss: 2.4475373720633797e-05\n",
      "step: 35447, loss: 0.0\n",
      "step: 35448, loss: 4.7439058107556775e-05\n",
      "step: 35449, loss: 0.0\n",
      "step: 35450, loss: 2.38418573772492e-09\n",
      "step: 35451, loss: 7.15255676908555e-09\n",
      "step: 35452, loss: 9.53674117454284e-09\n",
      "step: 35453, loss: 4.76837103136063e-09\n",
      "step: 35454, loss: 0.0\n",
      "step: 35455, loss: 0.07173878699541092\n",
      "step: 35456, loss: 1.8834984416571388e-07\n",
      "step: 35457, loss: 2.38418573772492e-09\n",
      "step: 35458, loss: 4.76837103136063e-09\n",
      "step: 35459, loss: 0.0\n",
      "step: 35460, loss: 0.0\n",
      "step: 35461, loss: 0.0\n",
      "step: 35462, loss: 0.0\n",
      "step: 35463, loss: 4.76837103136063e-09\n",
      "step: 35464, loss: 0.0\n",
      "step: 35465, loss: 0.0\n",
      "step: 35466, loss: 0.0\n",
      "step: 35467, loss: 2.38418573772492e-09\n",
      "step: 35468, loss: 0.0\n",
      "step: 35469, loss: 0.1371484398841858\n",
      "step: 35470, loss: 0.0\n",
      "step: 35471, loss: 0.0\n",
      "step: 35472, loss: 0.0\n",
      "step: 35473, loss: 4.768366679286373e-08\n",
      "step: 35474, loss: 0.0\n",
      "step: 35475, loss: 4.76837103136063e-09\n",
      "step: 35476, loss: 0.0\n",
      "step: 35477, loss: 0.0\n",
      "step: 35478, loss: 0.0\n",
      "step: 35479, loss: 2.38418573772492e-09\n",
      "step: 35480, loss: 0.0\n",
      "step: 35481, loss: 0.0\n",
      "step: 35482, loss: 5.587935003603661e-09\n",
      "step: 35483, loss: 1.615348992345389e-05\n",
      "step: 35484, loss: 0.0\n",
      "step: 35485, loss: 0.0\n",
      "step: 35486, loss: 0.0\n",
      "step: 35487, loss: 0.0\n",
      "step: 35488, loss: 2.38418573772492e-09\n",
      "step: 35489, loss: 0.0\n",
      "step: 35490, loss: 0.0\n",
      "step: 35491, loss: 0.0\n",
      "step: 35492, loss: 0.0017383333761245012\n",
      "step: 35493, loss: 0.0\n",
      "step: 35494, loss: 2.38418573772492e-09\n",
      "step: 35495, loss: 0.0\n",
      "step: 35496, loss: 0.0\n",
      "step: 35497, loss: 0.0\n",
      "step: 35498, loss: 0.0\n",
      "step: 35499, loss: 0.07109349966049194\n",
      "step: 35500, loss: 0.0\n",
      "step: 35501, loss: 0.0\n",
      "step: 35502, loss: 0.0\n",
      "step: 35503, loss: 0.0\n",
      "step: 35504, loss: 0.0\n",
      "step: 35505, loss: 5.555075972552004e-07\n",
      "step: 35506, loss: 0.0\n",
      "step: 35507, loss: 0.0\n",
      "step: 35508, loss: 4.76837103136063e-09\n",
      "step: 35509, loss: 7.15255676908555e-09\n",
      "step: 35510, loss: 1.907347702001516e-08\n",
      "step: 35511, loss: 0.0\n",
      "step: 35512, loss: 0.0\n",
      "step: 35513, loss: 0.0\n",
      "step: 35514, loss: 0.0\n",
      "step: 35515, loss: 0.0037134105805307627\n",
      "step: 35516, loss: 0.0\n",
      "step: 35517, loss: 4.76837103136063e-09\n",
      "step: 35518, loss: 0.0\n",
      "step: 35519, loss: 0.0\n",
      "step: 35520, loss: 1.001355514063107e-07\n",
      "step: 35521, loss: 7.15255676908555e-09\n",
      "step: 35522, loss: 9.53674117454284e-09\n",
      "step: 35523, loss: 0.0\n",
      "step: 35524, loss: 0.0\n",
      "step: 35525, loss: 1.192092646817855e-08\n",
      "step: 35526, loss: 1.430511176181426e-08\n",
      "step: 35527, loss: 0.0\n",
      "step: 35528, loss: 0.0\n",
      "step: 35529, loss: 0.0\n",
      "step: 35530, loss: 0.0\n",
      "step: 35531, loss: 0.0\n",
      "step: 35532, loss: 2.8610216418201162e-08\n",
      "step: 35533, loss: 0.0\n",
      "step: 35534, loss: 0.06220240145921707\n",
      "step: 35535, loss: 0.05842308700084686\n",
      "step: 35536, loss: 0.0\n",
      "step: 35537, loss: 4.76837147544984e-09\n",
      "step: 35538, loss: 0.0\n",
      "step: 35539, loss: 0.0\n",
      "step: 35540, loss: 0.0\n",
      "step: 35541, loss: 0.0\n",
      "step: 35542, loss: 0.0\n",
      "step: 35543, loss: 7.15255632499634e-09\n",
      "step: 35544, loss: 0.0\n",
      "step: 35545, loss: 2.38418573772492e-09\n",
      "step: 35546, loss: 0.0\n",
      "step: 35547, loss: 0.0\n",
      "step: 35548, loss: 2.38418573772492e-09\n",
      "step: 35549, loss: 2.38418573772492e-09\n",
      "step: 35550, loss: 2.38418573772492e-09\n",
      "step: 35551, loss: 2.38418573772492e-09\n",
      "step: 35552, loss: 0.0\n",
      "step: 35553, loss: 0.15379776060581207\n",
      "step: 35554, loss: 0.0\n",
      "step: 35555, loss: 0.0\n",
      "step: 35556, loss: 0.0\n",
      "step: 35557, loss: 2.38418573772492e-09\n",
      "step: 35558, loss: 1.192092558000013e-08\n",
      "step: 35559, loss: 0.0\n",
      "step: 35560, loss: 0.0\n",
      "step: 35561, loss: 8.868976806297724e-07\n",
      "step: 35562, loss: 0.0\n",
      "step: 35563, loss: 2.38418573772492e-09\n",
      "step: 35564, loss: 2.884844150230492e-07\n",
      "step: 35565, loss: 0.0\n",
      "step: 35566, loss: 7.15255676908555e-09\n",
      "step: 35567, loss: 0.0\n",
      "step: 35568, loss: 1.907347702001516e-08\n",
      "step: 35569, loss: 4.76837147544984e-09\n",
      "step: 35570, loss: 0.0\n",
      "step: 35571, loss: 2.38418573772492e-09\n",
      "step: 35572, loss: 0.0\n",
      "step: 35573, loss: 0.0\n",
      "step: 35574, loss: 2.38418573772492e-09\n",
      "step: 35575, loss: 4.005392213457526e-07\n",
      "step: 35576, loss: 4.76837103136063e-09\n",
      "step: 35577, loss: 0.0\n",
      "step: 35578, loss: 0.0\n",
      "step: 35579, loss: 0.0\n",
      "step: 35580, loss: 0.0\n",
      "step: 35581, loss: 0.0\n",
      "step: 35582, loss: 0.0\n",
      "step: 35583, loss: 1.192092646817855e-08\n",
      "step: 35584, loss: 0.0\n",
      "step: 35585, loss: 0.0\n",
      "step: 35586, loss: 0.0\n",
      "step: 35587, loss: 0.0\n",
      "step: 35588, loss: 0.0\n",
      "step: 35589, loss: 0.0\n",
      "step: 35590, loss: 0.0\n",
      "step: 35591, loss: 0.01306374091655016\n",
      "step: 35592, loss: 2.38418573772492e-09\n",
      "step: 35593, loss: 2.38418573772492e-09\n",
      "step: 35594, loss: 0.02255583554506302\n",
      "step: 35595, loss: 0.0\n",
      "step: 35596, loss: 4.053111624102712e-08\n",
      "step: 35597, loss: 0.0\n",
      "step: 35598, loss: 0.0\n",
      "step: 35599, loss: 1.2874561150511e-07\n",
      "step: 35600, loss: 4.76837103136063e-09\n",
      "step: 35601, loss: 0.0\n",
      "step: 35602, loss: 0.0\n",
      "step: 35603, loss: 0.0\n",
      "step: 35604, loss: 0.0\n",
      "step: 35605, loss: 0.0\n",
      "step: 35606, loss: 0.0\n",
      "step: 35607, loss: 0.0\n",
      "step: 35608, loss: 2.38418573772492e-09\n",
      "step: 35609, loss: 0.0\n",
      "step: 35610, loss: 0.0\n",
      "step: 35611, loss: 0.0\n",
      "step: 35612, loss: 2.38418573772492e-09\n",
      "step: 35613, loss: 0.0\n",
      "step: 35614, loss: 3.576275631189674e-08\n",
      "step: 35615, loss: 0.0\n",
      "step: 35616, loss: 0.0\n",
      "step: 35617, loss: 2.1457660537294032e-08\n",
      "step: 35618, loss: 0.0\n",
      "step: 35619, loss: 0.0\n",
      "step: 35620, loss: 0.0\n",
      "step: 35621, loss: 0.0\n",
      "step: 35622, loss: 0.0\n",
      "step: 35623, loss: 0.0\n",
      "step: 35624, loss: 0.0\n",
      "step: 35625, loss: 1.406664864589402e-07\n",
      "step: 35626, loss: 0.1696833223104477\n",
      "step: 35627, loss: 1.6450815110147232e-07\n",
      "step: 35628, loss: 4.76837103136063e-09\n",
      "step: 35629, loss: 0.03219746798276901\n",
      "step: 35630, loss: 6.198873592211385e-08\n",
      "step: 35631, loss: 0.0\n",
      "step: 35632, loss: 0.0\n",
      "step: 35633, loss: 0.0\n",
      "step: 35634, loss: 2.38418573772492e-09\n",
      "step: 35635, loss: 0.0\n",
      "step: 35636, loss: 9.146031516138464e-06\n",
      "step: 35637, loss: 1.668929350273629e-08\n",
      "step: 35638, loss: 0.0\n",
      "step: 35639, loss: 0.0\n",
      "step: 35640, loss: 0.0\n",
      "step: 35641, loss: 2.6176744540862273e-06\n",
      "step: 35642, loss: 0.0\n",
      "step: 35643, loss: 0.0\n",
      "step: 35644, loss: 0.12710623443126678\n",
      "step: 35645, loss: 4.7683659687436375e-08\n",
      "step: 35646, loss: 0.0\n",
      "step: 35647, loss: 0.0\n",
      "step: 35648, loss: 2.38418573772492e-09\n",
      "step: 35649, loss: 0.0\n",
      "step: 35650, loss: 0.0\n",
      "step: 35651, loss: 7.15255632499634e-09\n",
      "step: 35652, loss: 0.071104496717453\n",
      "step: 35653, loss: 0.0\n",
      "step: 35654, loss: 3.337857279461787e-08\n",
      "step: 35655, loss: 0.0\n",
      "step: 35656, loss: 0.0\n",
      "step: 35657, loss: 3.576275631189674e-08\n",
      "step: 35658, loss: 0.0\n",
      "step: 35659, loss: 0.0\n",
      "step: 35660, loss: 0.07308848202228546\n",
      "step: 35661, loss: 0.0\n",
      "step: 35662, loss: 2.38418573772492e-09\n",
      "step: 35663, loss: 1.5830366919544758e-06\n",
      "step: 35664, loss: 0.0\n",
      "step: 35665, loss: 0.0\n",
      "step: 35666, loss: 0.0\n",
      "step: 35667, loss: 1.4305064155450964e-07\n",
      "step: 35668, loss: 0.0\n",
      "step: 35669, loss: 0.0\n",
      "step: 35670, loss: 0.0\n",
      "step: 35671, loss: 0.0\n",
      "step: 35672, loss: 0.0\n",
      "step: 35673, loss: 0.0\n",
      "step: 35674, loss: 0.0\n",
      "step: 35675, loss: 0.0\n",
      "step: 35676, loss: 0.0\n",
      "step: 35677, loss: 0.041447173804044724\n",
      "step: 35678, loss: 0.0\n",
      "step: 35679, loss: 2.38418573772492e-09\n",
      "step: 35680, loss: 0.0\n",
      "step: 35681, loss: 0.0\n",
      "step: 35682, loss: 0.0\n",
      "step: 35683, loss: 0.0\n",
      "step: 35684, loss: 2.38418573772492e-09\n",
      "step: 35685, loss: 0.0\n",
      "step: 35686, loss: 0.0\n",
      "step: 35687, loss: 2.3841844054572903e-08\n",
      "step: 35688, loss: 0.0\n",
      "step: 35689, loss: 5.722037599298346e-08\n",
      "step: 35690, loss: 0.0\n",
      "step: 35691, loss: 0.0\n",
      "step: 35692, loss: 0.0\n",
      "step: 35693, loss: 4.76837103136063e-09\n",
      "step: 35694, loss: 0.0\n",
      "step: 35695, loss: 2.38418573772492e-09\n",
      "step: 35696, loss: 0.0\n",
      "step: 35697, loss: 0.052319131791591644\n",
      "step: 35698, loss: 4.76837103136063e-09\n",
      "step: 35699, loss: 0.0\n",
      "step: 35700, loss: 0.0\n",
      "step: 35701, loss: 0.0\n",
      "step: 35702, loss: 0.0\n",
      "step: 35703, loss: 0.0\n",
      "step: 35704, loss: 4.76837103136063e-09\n",
      "step: 35705, loss: 2.38418573772492e-09\n",
      "step: 35706, loss: 0.0\n",
      "step: 35707, loss: 7.15255632499634e-09\n",
      "step: 35708, loss: 1.8626450382086546e-09\n",
      "step: 35709, loss: 0.0\n",
      "step: 35710, loss: 0.0\n",
      "step: 35711, loss: 2.38418573772492e-09\n",
      "step: 35712, loss: 4.76837103136063e-09\n",
      "step: 35713, loss: 0.0\n",
      "step: 35714, loss: 0.0\n",
      "step: 35715, loss: 0.0\n",
      "step: 35716, loss: 1.4183259736455511e-05\n",
      "step: 35717, loss: 0.0\n",
      "step: 35718, loss: 4.76837103136063e-09\n",
      "step: 35719, loss: 0.0\n",
      "step: 35720, loss: 0.0\n",
      "step: 35721, loss: 0.0\n",
      "step: 35722, loss: 0.0\n",
      "step: 35723, loss: 0.0\n",
      "step: 35724, loss: 0.0\n",
      "step: 35725, loss: 0.0\n",
      "step: 35726, loss: 2.38418573772492e-09\n",
      "step: 35727, loss: 0.0\n",
      "step: 35728, loss: 0.0\n",
      "step: 35729, loss: 0.0\n",
      "step: 35730, loss: 0.0\n",
      "step: 35731, loss: 1.192092646817855e-08\n",
      "step: 35732, loss: 0.0\n",
      "step: 35733, loss: 0.0\n",
      "step: 35734, loss: 0.0\n",
      "step: 35735, loss: 7.843822231734521e-07\n",
      "step: 35736, loss: 7.15255632499634e-09\n",
      "step: 35737, loss: 9.53674117454284e-09\n",
      "step: 35738, loss: 0.0\n",
      "step: 35739, loss: 0.0\n",
      "step: 35740, loss: 0.0\n",
      "step: 35741, loss: 0.0\n",
      "step: 35742, loss: 0.0\n",
      "step: 35743, loss: 0.0\n",
      "step: 35744, loss: 0.0\n",
      "step: 35745, loss: 0.0\n",
      "step: 35746, loss: 0.0\n",
      "step: 35747, loss: 0.0\n",
      "step: 35748, loss: 0.0\n",
      "step: 35749, loss: 0.0\n",
      "step: 35750, loss: 0.0\n",
      "step: 35751, loss: 0.0\n",
      "step: 35752, loss: 0.0\n",
      "step: 35753, loss: 0.0\n",
      "step: 35754, loss: 0.0\n",
      "step: 35755, loss: 0.0\n",
      "step: 35756, loss: 0.0\n",
      "step: 35757, loss: 4.76837103136063e-09\n",
      "step: 35758, loss: 7.79716265242314e-06\n",
      "step: 35759, loss: 2.7894779464077146e-07\n",
      "step: 35760, loss: 0.0\n",
      "step: 35761, loss: 7.15255632499634e-09\n",
      "step: 35762, loss: 0.0\n",
      "step: 35763, loss: 0.0\n",
      "step: 35764, loss: 0.0\n",
      "step: 35765, loss: 0.0\n",
      "step: 35766, loss: 7.15255632499634e-09\n",
      "step: 35767, loss: 0.0\n",
      "step: 35768, loss: 7.15255632499634e-09\n",
      "step: 35769, loss: 0.0\n",
      "step: 35770, loss: 2.3841844054572903e-08\n",
      "step: 35771, loss: 0.0\n",
      "step: 35772, loss: 0.0\n",
      "step: 35773, loss: 1.430510998545742e-08\n",
      "step: 35774, loss: 7.15255632499634e-09\n",
      "step: 35775, loss: 0.1572575867176056\n",
      "step: 35776, loss: 0.0\n",
      "step: 35777, loss: 2.38418573772492e-09\n",
      "step: 35778, loss: 2.38418573772492e-09\n",
      "step: 35779, loss: 0.0\n",
      "step: 35780, loss: 0.0\n",
      "step: 35781, loss: 0.0\n",
      "step: 35782, loss: 0.0\n",
      "step: 35783, loss: 2.38418573772492e-09\n",
      "step: 35784, loss: 0.0\n",
      "step: 35785, loss: 0.0\n",
      "step: 35786, loss: 0.0\n",
      "step: 35787, loss: 0.0\n",
      "step: 35788, loss: 0.00018067756900563836\n",
      "step: 35789, loss: 0.09133520722389221\n",
      "step: 35790, loss: 2.38418573772492e-09\n",
      "step: 35791, loss: 2.38418573772492e-09\n",
      "step: 35792, loss: 1.907348234908568e-08\n",
      "step: 35793, loss: 0.0\n",
      "step: 35794, loss: 0.0\n",
      "step: 35795, loss: 0.0\n",
      "step: 35796, loss: 0.0\n",
      "step: 35797, loss: 0.0\n",
      "step: 35798, loss: 0.0\n",
      "step: 35799, loss: 2.38418573772492e-09\n",
      "step: 35800, loss: 0.0\n",
      "step: 35801, loss: 0.0\n",
      "step: 35802, loss: 0.0\n",
      "step: 35803, loss: 4.291530686373335e-08\n",
      "step: 35804, loss: 2.38418573772492e-09\n",
      "step: 35805, loss: 0.0\n",
      "step: 35806, loss: 0.0003644975950010121\n",
      "step: 35807, loss: 2.38418573772492e-09\n",
      "step: 35808, loss: 1.192092558000013e-08\n",
      "step: 35809, loss: 2.38418573772492e-09\n",
      "step: 35810, loss: 1.621240102167576e-07\n",
      "step: 35811, loss: 0.0\n",
      "step: 35812, loss: 0.0\n",
      "step: 35813, loss: 7.15255676908555e-09\n",
      "step: 35814, loss: 0.0\n",
      "step: 35815, loss: 0.0\n",
      "step: 35816, loss: 4.76837147544984e-09\n",
      "step: 35817, loss: 1.668929527909313e-08\n",
      "step: 35818, loss: 9.53674117454284e-09\n",
      "step: 35819, loss: 0.0\n",
      "step: 35820, loss: 0.0\n",
      "step: 35821, loss: 0.0\n",
      "step: 35822, loss: 0.0\n",
      "step: 35823, loss: 0.0\n",
      "step: 35824, loss: 4.76837103136063e-09\n",
      "step: 35825, loss: 4.76837103136063e-09\n",
      "step: 35826, loss: 0.0\n",
      "step: 35827, loss: 0.0\n",
      "step: 35828, loss: 7.15255632499634e-09\n",
      "step: 35829, loss: 3.5762763417324095e-08\n",
      "step: 35830, loss: 2.38418573772492e-09\n",
      "step: 35831, loss: 2.38418573772492e-09\n",
      "step: 35832, loss: 0.0\n",
      "step: 35833, loss: 0.0\n",
      "step: 35834, loss: 0.0007286467589437962\n",
      "step: 35835, loss: 7.15255676908555e-09\n",
      "step: 35836, loss: 0.0\n",
      "step: 35837, loss: 2.6226025795494934e-08\n",
      "step: 35838, loss: 2.3841845830929742e-08\n",
      "step: 35839, loss: 0.0\n",
      "step: 35840, loss: 0.0\n",
      "step: 35841, loss: 0.0\n",
      "step: 35842, loss: 0.0\n",
      "step: 35843, loss: 2.38418573772492e-09\n",
      "step: 35844, loss: 0.0\n",
      "step: 35845, loss: 0.0\n",
      "step: 35846, loss: 2.38418573772492e-09\n",
      "step: 35847, loss: 0.041172485798597336\n",
      "step: 35848, loss: 0.0\n",
      "step: 35849, loss: 2.38418573772492e-09\n",
      "step: 35850, loss: 0.0\n",
      "step: 35851, loss: 0.0\n",
      "step: 35852, loss: 0.0\n",
      "step: 35853, loss: 2.38418573772492e-09\n",
      "step: 35854, loss: 0.0\n",
      "step: 35855, loss: 0.0\n",
      "step: 35856, loss: 1.907348234908568e-08\n",
      "step: 35857, loss: 0.09055066853761673\n",
      "step: 35858, loss: 3.337857279461787e-08\n",
      "step: 35859, loss: 7.15255632499634e-09\n",
      "step: 35860, loss: 4.76837103136063e-09\n",
      "step: 35861, loss: 1.430510998545742e-08\n",
      "step: 35862, loss: 0.0\n",
      "step: 35863, loss: 0.0\n",
      "step: 35864, loss: 0.0\n",
      "step: 35865, loss: 0.0\n",
      "step: 35866, loss: 2.38418573772492e-09\n",
      "step: 35867, loss: 2.38418573772492e-09\n",
      "step: 35868, loss: 0.06759013235569\n",
      "step: 35869, loss: 0.0\n",
      "step: 35870, loss: 0.0\n",
      "step: 35871, loss: 1.4757432836631779e-05\n",
      "step: 35872, loss: 0.0\n",
      "step: 35873, loss: 0.0\n",
      "step: 35874, loss: 0.0\n",
      "step: 35875, loss: 3.814693627646193e-08\n",
      "step: 35876, loss: 0.0\n",
      "step: 35877, loss: 2.38418573772492e-09\n",
      "step: 35878, loss: 2.38418573772492e-09\n",
      "step: 35879, loss: 0.0\n",
      "step: 35880, loss: 2.38418573772492e-09\n",
      "step: 35881, loss: 0.0\n",
      "step: 35882, loss: 0.0\n",
      "step: 35883, loss: 0.0016634330386295915\n",
      "step: 35884, loss: 0.06491517275571823\n",
      "step: 35885, loss: 0.0\n",
      "step: 35886, loss: 0.0\n",
      "step: 35887, loss: 0.0\n",
      "step: 35888, loss: 0.0\n",
      "step: 35889, loss: 0.0\n",
      "step: 35890, loss: 0.0\n",
      "step: 35891, loss: 0.0\n",
      "step: 35892, loss: 0.0\n",
      "step: 35893, loss: 0.0\n",
      "step: 35894, loss: 0.05474274978041649\n",
      "step: 35895, loss: 0.0\n",
      "step: 35896, loss: 0.0\n",
      "step: 35897, loss: 1.192092646817855e-08\n",
      "step: 35898, loss: 7.15255676908555e-09\n",
      "step: 35899, loss: 0.0\n",
      "step: 35900, loss: 0.0\n",
      "step: 35901, loss: 0.0\n",
      "step: 35902, loss: 0.0\n",
      "step: 35903, loss: 0.0\n",
      "step: 35904, loss: 0.0\n",
      "step: 35905, loss: 0.0\n",
      "step: 35906, loss: 0.06546571850776672\n",
      "step: 35907, loss: 2.38418573772492e-09\n",
      "step: 35908, loss: 0.0\n",
      "step: 35909, loss: 0.0\n",
      "step: 35910, loss: 0.0\n",
      "step: 35911, loss: 0.0\n",
      "step: 35912, loss: 1.3351396432881302e-07\n",
      "step: 35913, loss: 2.38418573772492e-09\n",
      "step: 35914, loss: 0.0766361653804779\n",
      "step: 35915, loss: 0.07198466360569\n",
      "step: 35916, loss: 1.430510998545742e-08\n",
      "step: 35917, loss: 1.192092646817855e-08\n",
      "step: 35918, loss: 0.0\n",
      "step: 35919, loss: 0.14537504315376282\n",
      "step: 35920, loss: 0.0\n",
      "step: 35921, loss: 1.0013556561716541e-07\n",
      "step: 35922, loss: 0.0\n",
      "step: 35923, loss: 0.0\n",
      "step: 35924, loss: 0.0\n",
      "step: 35925, loss: 1.192092558000013e-08\n",
      "step: 35926, loss: 0.0\n",
      "step: 35927, loss: 0.0\n",
      "step: 35928, loss: 0.0\n",
      "step: 35929, loss: 0.0\n",
      "step: 35930, loss: 0.0\n",
      "step: 35931, loss: 0.0\n",
      "step: 35932, loss: 0.0\n",
      "step: 35933, loss: 0.0\n",
      "step: 35934, loss: 0.0\n",
      "step: 35935, loss: 2.38418573772492e-09\n",
      "step: 35936, loss: 0.0\n",
      "step: 35937, loss: 2.38418573772492e-09\n",
      "step: 35938, loss: 0.0\n",
      "step: 35939, loss: 0.0\n",
      "step: 35940, loss: 0.0\n",
      "step: 35941, loss: 7.15255632499634e-09\n",
      "step: 35942, loss: 0.0\n",
      "step: 35943, loss: 0.0\n",
      "step: 35944, loss: 4.76837103136063e-09\n",
      "step: 35945, loss: 1.2159311779669224e-07\n",
      "step: 35946, loss: 0.0\n",
      "step: 35947, loss: 1.192092558000013e-08\n",
      "step: 35948, loss: 0.0\n",
      "step: 35949, loss: 0.0\n",
      "step: 35950, loss: 0.0\n",
      "step: 35951, loss: 0.0\n",
      "step: 35952, loss: 0.06003453582525253\n",
      "step: 35953, loss: 0.0\n",
      "step: 35954, loss: 0.054734326899051666\n",
      "step: 35955, loss: 0.0\n",
      "step: 35956, loss: 1.430510998545742e-08\n",
      "step: 35957, loss: 0.0\n",
      "step: 35958, loss: 0.0\n",
      "step: 35959, loss: 0.0\n",
      "step: 35960, loss: 2.38418573772492e-09\n",
      "step: 35961, loss: 2.38418573772492e-09\n",
      "step: 35962, loss: 0.05625670403242111\n",
      "step: 35963, loss: 0.0\n",
      "step: 35964, loss: 0.0\n",
      "step: 35965, loss: 1.6927647550346592e-07\n",
      "step: 35966, loss: 0.0\n",
      "step: 35967, loss: 0.0\n",
      "step: 35968, loss: 0.0\n",
      "step: 35969, loss: 0.0\n",
      "step: 35970, loss: 1.430511176181426e-08\n",
      "step: 35971, loss: 0.0\n",
      "step: 35972, loss: 0.06675530225038528\n",
      "step: 35973, loss: 0.0\n",
      "step: 35974, loss: 2.38418573772492e-09\n",
      "step: 35975, loss: 2.38418573772492e-09\n",
      "step: 35976, loss: 2.5987455387621594e-07\n",
      "step: 35977, loss: 0.0\n",
      "step: 35978, loss: 0.0\n",
      "step: 35979, loss: 0.0\n",
      "step: 35980, loss: 4.76837103136063e-09\n",
      "step: 35981, loss: 4.76837103136063e-09\n",
      "step: 35982, loss: 4.76837103136063e-09\n",
      "step: 35983, loss: 4.76837147544984e-09\n",
      "step: 35984, loss: 0.0\n",
      "step: 35985, loss: 2.38418573772492e-09\n",
      "step: 35986, loss: 0.0\n",
      "step: 35987, loss: 0.0\n",
      "step: 35988, loss: 0.0\n",
      "step: 35989, loss: 0.0\n",
      "step: 35990, loss: 0.0\n",
      "step: 35991, loss: 0.0\n",
      "step: 35992, loss: 0.0\n",
      "step: 35993, loss: 0.0\n",
      "step: 35994, loss: 0.0\n",
      "step: 35995, loss: 1.430510998545742e-08\n",
      "step: 35996, loss: 0.0\n",
      "step: 35997, loss: 0.0\n",
      "step: 35998, loss: 1.192092558000013e-08\n",
      "step: 35999, loss: 0.0\n",
      "step: 36000, loss: 0.0\n",
      "Epoch: 1\n",
      "Accuracy on test data: 0.0525756771109931\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "def save_parameters(name,value,epoch):\n",
    "  filename = name.replace(':','-colon-') + '-epoch-{}.txt'.format(epoch)\n",
    "  if len(value.shape) == 1:\n",
    "    string_form = ','.join([str(number) for number in value])\n",
    "  else:\n",
    "    string_form = '\\n'.join([','.join([str(number) for number in value[row]]) for row in range(value.shape[0])])\n",
    "  with open('D:/E/CUONG/machine learning/20news-bydate/saved-paras/' + filename,'w') as f:\n",
    "      f.write(string_form)\n",
    "def restore_parameters(name, epoch):\n",
    "  filename = name.replace(':','-colon-') + '-epoch-{}.txt'.format(epoch)\n",
    "  with open('D:/E/CUONG/machine learning/20news-bydate/saved-paras/' + filename) as f:\n",
    "    lines = f.read().splitlines()\n",
    "    if len(lines) == 1:\n",
    "      value = [float(number) for number in lines[0].split(',')]\n",
    "    else:\n",
    "      value = [[float(number) for number in lines[row].split(',')] for row in range(len(lines))]\n",
    "  return value\n",
    "class MLP:\n",
    "  def __init__(self, vocab_size, hidden_size):\n",
    "    self._vocab_size = vocab_size\n",
    "    self._hidden_size = hidden_size\n",
    "  def build_graph(self):\n",
    "    NUM_CLASSES = 100\n",
    "    self._X = tf.placeholder(tf.float32, shape = [None, self._vocab_size])\n",
    "    self._real_Y = tf.placeholder(tf.int32, shape = [None, ])\n",
    "\n",
    "    weights_1 = tf.get_variable(name = 'weights_input_hidden', shape = (self._vocab_size, self._hidden_size), initializer = tf.random_normal_initializer(seed = 2018))\n",
    "    biases_1 = tf.get_variable(name = 'biases_input_hidden', shape = (self._hidden_size), initializer = tf.random_normal_initializer(seed = 2018))\n",
    "    weights_2 = tf.get_variable(name = 'weights_hidden_output', shape = (self._hidden_size, NUM_CLASSES), initializer = tf.random_normal_initializer(seed = 2018))\n",
    "    biases_2 = tf.get_variable(name = 'biases_hidden_output', shape = (NUM_CLASSES), initializer = tf.random_normal_initializer(seed = 2018))\n",
    "    hidden = tf.matmul(self._X, weights_1) + biases_1\n",
    "    hidden = tf.sigmoid(hidden)\n",
    "    logits = tf.matmul(hidden, weights_2) + biases_2\n",
    "    labels_one_hot = tf.one_hot(indices=self._real_Y, depth=NUM_CLASSES, dtype=tf.float32)\n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(labels=labels_one_hot, logits=logits)\n",
    "    loss = tf.reduce_mean(loss)\n",
    "    probs = tf.nn.softmax(logits)\n",
    "    predicted_labels = tf.argmax(probs, axis = 1)\n",
    "    predicted_labels = tf.squeeze(predicted_labels)\n",
    "    return predicted_labels, loss\n",
    "  def trainer(self, loss, learning_rate):\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "    return train_op\n",
    "with open('D:/E/CUONG/machine learning/20news-bydate/words_idfs.txt') as f:\n",
    "    vocab_size = len(f.read().splitlines())\n",
    "    mlp = MLP(vocab_size = vocab_size, hidden_size = 50)\n",
    "    predicted_labels, loss = mlp.build_graph()\n",
    "    train_op = mlp.trainer(loss = loss, learning_rate = 0.1)\n",
    "\n",
    "\n",
    "class DataReader:\n",
    "  def __init__(self, data_path, batch_size, vocab_size):\n",
    "    self._batch_size = batch_size\n",
    "    with open(data_path) as f:\n",
    "      d_lines = f.read().splitlines()\n",
    "    self._data = []\n",
    "    self._labels =[]\n",
    "    for data_id, line in enumerate(d_lines):\n",
    "      vector = [0.0 for _ in range(vocab_size)]\n",
    "      features = line.split('<fff>')\n",
    "      label, doc_id = int(features[0]), int(features[1])\n",
    "      tokens = features[2].split()\n",
    "      for token in tokens:\n",
    "        index, value = int(token.split(':')[0]), float(token.split(':')[1])\n",
    "        vector[index] = value\n",
    "      self._data.append(vector)\n",
    "      self._labels.append(label)\n",
    "    self._data = np.array(self._data)\n",
    "    self._labels = np.array(self._labels)\n",
    "    self._num_epoch = 0\n",
    "    self._batch_id = 0\n",
    "  def next_batch(self):\n",
    "    start = self._batch_id * self._batch_size\n",
    "    end = start + self._batch_size\n",
    "    self._batch_id += 1\n",
    "    if end + self._batch_size > len(self._data):\n",
    "      end = len(self._data)\n",
    "      self._num_epoch += 1\n",
    "      self._batch_id = 0\n",
    "      indices = list(range(len(self._data)))\n",
    "      random.seed(2018)\n",
    "      random.shuffle(indices)\n",
    "      self._data, self._labels = self._data[indices], self._labels[indices]\n",
    "    return self._data[start:end], self._labels[start:end]\n",
    "def load_dataset():\n",
    "    train_data_reader = DataReader(data_path = 'D:/E/CUONG/machine learning/20news-bydate/20news-train-tfidf.txt',batch_size = 50, vocab_size = vocab_size)\n",
    "    test_data_reader = DataReader(data_path = 'D:/E/CUONG/machine learning/20news-bydate/20news-test-tfidf.txt',batch_size = 50, vocab_size = vocab_size)\n",
    "    return train_data_reader, test_data_reader    \n",
    "\n",
    "  \n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    train_data_reader, test_data_reader = load_dataset()\n",
    "    step, MAX_STEP = 0, 36000\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    while step < MAX_STEP:\n",
    "      train_data, train_labels = train_data_reader.next_batch()\n",
    "      plabels_eval, loss_eval, _ = sess.run([predicted_labels, loss, train_op], feed_dict={mlp._X: train_data, mlp._real_Y: train_labels})\n",
    "      \n",
    "\n",
    "\n",
    "      step += 1\n",
    "      print('step: {}, loss: {}'.format(step, loss_eval))\n",
    "    trainable_variables = tf.trainable_variables()\n",
    "    \n",
    "    for variable in trainable_variables:\n",
    "        #print(variable.name)\n",
    "        #print(variable.eval())\n",
    "        save_parameters(name = variable.name, value = variable.eval(), epoch = train_data_reader._num_epoch)\n",
    "    test_data_reader = DataReader(data_path = 'D:/E/CUONG/machine learning/20news-bydate/20news-test-tfidf.txt',batch_size = 50, vocab_size = vocab_size)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        epoch = 0\n",
    "        for variable in trainable_variables:\n",
    "            saved_value = restore_parameters(variable.name, epoch)\n",
    "            assign_op = variable.assign(saved_value)\n",
    "            sess.run(assign_op)\n",
    "        num_true_preds = 0\n",
    "        while True:\n",
    "            test_data, test_labels = test_data_reader.next_batch()\n",
    "            test_plabels_eval = sess.run(predicted_labels, feed_dict = {mlp._X: test_data, mlp._real_Y:test_labels})\n",
    "            matches = np.equal(test_plabels_eval, test_labels)\n",
    "            num_true_preds += np.sum(matches.astype(float))\n",
    "            if test_data_reader._batch_id == 0:\n",
    "                break\n",
    "        print('Epoch:',test_data_reader._num_epoch)\n",
    "        print('Accuracy on test data:', num_true_preds / len(test_data_reader._data))\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: 0-alt.atheism\n",
      "Processing: 1-comp.graphics\n",
      "Processing: 2-comp.os.ms-windows.misc\n",
      "Processing: 3-comp.sys.ibm.pc.hardware\n",
      "Processing: 4-comp.sys.mac.hardware\n",
      "Processing: 5-comp.windows.x\n",
      "Processing: 6-misc.forsale\n",
      "Processing: 7-rec.autos\n",
      "Processing: 8-rec.motorcycles\n",
      "Processing: 9-rec.sport.baseball\n",
      "Processing: 10-rec.sport.hockey\n",
      "Processing: 11-sci.crypt\n",
      "Processing: 12-sci.electronics\n",
      "Processing: 13-sci.med\n",
      "Processing: 14-sci.space\n",
      "Processing: 15-soc.religion.christian\n",
      "Processing: 16-talk.politics.guns\n",
      "Processing: 17-talk.politics.mideast\n",
      "Processing: 18-talk.politics.misc\n",
      "Processing: 19-talk.religion.misc\n",
      "Processing: 0-alt.atheism\n",
      "Processing: 1-comp.graphics\n",
      "Processing: 2-comp.os.ms-windows.misc\n",
      "Processing: 3-comp.sys.ibm.pc.hardware\n",
      "Processing: 4-comp.sys.mac.hardware\n",
      "Processing: 5-comp.windows.x\n",
      "Processing: 6-misc.forsale\n",
      "Processing: 7-rec.autos\n",
      "Processing: 8-rec.motorcycles\n",
      "Processing: 9-rec.sport.baseball\n",
      "Processing: 10-rec.sport.hockey\n",
      "Processing: 11-sci.crypt\n",
      "Processing: 12-sci.electronics\n",
      "Processing: 13-sci.med\n",
      "Processing: 14-sci.space\n",
      "Processing: 15-soc.religion.christian\n",
      "Processing: 16-talk.politics.guns\n",
      "Processing: 17-talk.politics.mideast\n",
      "Processing: 18-talk.politics.misc\n",
      "Processing: 19-talk.religion.misc\n"
     ]
    }
   ],
   "source": [
    "def gen_data_and_vocab():\n",
    "    def collect_data_from(parent_path, newsgroup_list, word_count = None):\n",
    "        data = []\n",
    "        for group_id, newsgroup in enumerate(newsgroup_list):\n",
    "            dir_path = parent_path + '/' + newsgroup +'/'\n",
    "            files = [(filename, dir_path + filename) for filename in listdir(dir_path) if isfile(dir_path+filename)]\n",
    "            files.sort()\n",
    "            label = group_id\n",
    "            print('Processing: {}-{}'.format(group_id, newsgroup))\n",
    "            for filename, filepath in files:\n",
    "                with open(filepath) as f:\n",
    "                    text = f.read().lower()\n",
    "                    words = re.split('\\W+', text)\n",
    "                    if word_count is not None:\n",
    "                        for word in words:\n",
    "                            word_count[word] +=1\n",
    "                    content = ' '.join(words)\n",
    "                    assert len(content.splitlines()) == 1\n",
    "                    data.append(str(label)+ '<fff>' + filename +'<fff>' + content)\n",
    "        return data\n",
    "    word_count = defaultdict(int)\n",
    "    path = 'D:/E/CUONG/machine learning/20news-bydate/'\n",
    "    parts = [path+ dir_name + '/' for dir_name in listdir(path) if not isfile(path + dir_name)]\n",
    "    train_path, test_path = (parts[0], parts[1]) if 'train' in parts[0] else (parts[1], parts[0])\n",
    "    newsgroup_list = [newsgroup for newsgroup in listdir(train_path)]\n",
    "    newsgroup_list.sort()\n",
    "    train_data = collect_data_from(parent_path = train_path, newsgroup_list = newsgroup_list, word_count = word_count)\n",
    "    vocab = [word for word,freq in zip(word_count.keys(), word_count.values()) if freq >10]\n",
    "    vocab.sort()\n",
    "    with open('D:/E/CUONG/machine learning/w2v/vocab-raw.txt', 'w') as f:\n",
    "        f.write('\\n'.join(vocab))\n",
    "    test_data = collect_data_from(parent_path = test_path, newsgroup_list = newsgroup_list)\n",
    "    with open('D:/E/CUONG/machine learning/w2v/20news-train-raw.txt', 'w') as f:\n",
    "        f.write('\\n'.join(train_data))\n",
    "    with open('D:/E/CUONG/machine learning/w2v/20news-test-raw.txt', 'w') as f:\n",
    "        f.write('\\n'.join(test_data))\n",
    "\n",
    "    \n",
    "gen_data_and_vocab()                         \n",
    "                                                                                                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_data(data_path, vocab_path):\n",
    "    with open(vocab_path) as f:\n",
    "        vocab = dict([(word, word_ID + 2) for word_ID, word in enumerate(f.read().splitlines())])\n",
    "    with open(data_path) as f:\n",
    "        documents = [(line.split('<fff>')[0], line.split('<fff>')[1], line.split('<fff>')[2]) for line in f.read().splitlines()]\n",
    "    encoded_data = []\n",
    "    for document in documents:\n",
    "        label, doc_id, text = document\n",
    "        words = text.split()[:500]\n",
    "        sentence_length = len(words)\n",
    "        encoded_text =[]\n",
    "        for word in words:\n",
    "            if word in vocab:\n",
    "                encoded_text.append(str(vocab[word]))\n",
    "            else:\n",
    "                encoded_text.append(str(1))\n",
    "        if len(words) < 500:\n",
    "            num_padding = 500 - len(words)\n",
    "            for _ in range(num_padding):\n",
    "                encoded_text.append(str(0))\n",
    "        encoded_data.append(str(label) + '<fff>' + str(doc_id) +'<fff>' + str(sentence_length)+'<fff>'+ ' '.join(encoded_text))\n",
    "    dir_name = '/'.join(data_path.split('/')[:-1])\n",
    "    file_name ='-'.join(data_path.split('/')[-1].split('-')[:-1]) + '-encoded.txt'\n",
    "    with open(dir_name + '/' + file_name, 'w') as f:\n",
    "        f.write('\\n'.join(encoded_data))\n",
    "data_path = 'D:/E/CUONG/machine learning/20news-bydate/20news-test-processed.txt'\n",
    "vocab_path = 'D:/E/CUONG/machine learning/w2v/vocab-raw.txt'\n",
    "encode_data(data_path,vocab_path)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Admin\\AppData\\Local\\Temp/ipykernel_19628/775199960.py:71: static_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell, unroll=True)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From C:\\Users\\Admin\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\layers\\legacy_rnn\\rnn_cell_impl.py:979: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\layers\\legacy_rnn\\rnn_cell_impl.py:901: UserWarning: `tf.nn.rnn_cell.LSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\n",
      "  warnings.warn(\"`tf.nn.rnn_cell.LSTMCell` is deprecated and will be \"\n",
      "C:\\Users\\Admin\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\base_layer_v1.py:1684: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
      "  warnings.warn('`layer.add_variable` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11314 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "loss 0.0076287673\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "loss 0.93945414\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "loss 7.727229\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "loss 1.4979831\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "loss 1.9450568\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "loss 7.4276824\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "loss 2.4053252\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "loss 4.556147\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "loss 3.851304\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "loss 5.972891\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "loss 6.4243836\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "Epoch: 1\n",
      "Accuracy on test data: 7.819968135953266\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "loss 2.7600589\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "loss 2.3606608\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "loss 1.8744122\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "loss 1.6167533\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "loss 1.8109404\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "loss 1.6129062\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "loss 1.3825628\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "loss 1.061198\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "loss 1.0872576\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "loss 1.2219123\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "loss 1.2965479\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "Epoch: 2\n",
      "Accuracy on test data: 69.4768985661179\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "loss 0.8124956\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "loss 0.7456256\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "loss 0.7951634\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "loss 0.7798263\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "loss 0.8164633\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "loss 0.5028439\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "loss 0.6444405\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "loss 0.5532463\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "loss 0.6273195\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "loss 0.6124098\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "loss 0.51926583\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "Epoch: 3\n",
      "Accuracy on test data: 72.90228359001593\n",
      "50 500\n",
      "50 500\n",
      "loss 0.35591498\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "loss 0.2014252\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "loss 0.2959336\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "loss 0.3188107\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "loss 0.19836001\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "loss 0.1308624\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "loss 0.14158133\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "loss 0.14281899\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "loss 0.15072125\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "loss 0.19878545\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "loss 0.40166742\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "loss 0.45034683\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "Epoch: 4\n",
      "Accuracy on test data: 74.20339883165163\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "loss 0.052053604\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "loss 0.06043396\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "loss 0.1528635\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "loss 0.12251841\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "50 500\n",
      "loss 0.14144765\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "class DataReader_session_4:\n",
    "  def __init__(self, data_path, batch_size, vocab_size):\n",
    "    self._batch_size = batch_size\n",
    "    with open(data_path) as f:\n",
    "      d_lines = f.read().splitlines()\n",
    "    self._data = []\n",
    "    self._labels =[]\n",
    "    self._sentence_lengths = []\n",
    "    self._final_tokens = []\n",
    "    for data_id, line in enumerate(d_lines):\n",
    "      vector = []\n",
    "      features = line.split('<fff>')\n",
    "      label, doc_id, sentence_lengths = int(features[0]), int(features[1]), int(features[2])\n",
    "      tokens = features[3].split()\n",
    "      for token in tokens:\n",
    "        vector.append(token)\n",
    "      self._data.append(vector)\n",
    "      self._labels.append(label)\n",
    "      self._sentence_lengths.append(sentence_lengths)\n",
    "      self._final_tokens.append(tokens[-1])\n",
    "    self._data = np.array(self._data)\n",
    "    self._labels = np.array(self._labels)\n",
    "    self._sentence_lengths = np.array(self._sentence_lengths)\n",
    "    self._final_tokens = np.array(self._final_tokens)\n",
    "    self._size = len(self._data)\n",
    "    self._num_epoch = 0\n",
    "    self._batch_id = 0\n",
    "  def next_batch(self):\n",
    "    start = self._batch_id * self._batch_size\n",
    "    end = start + self._batch_size\n",
    "    self._batch_id += 1\n",
    "\n",
    "    if end + self._batch_size > len(self._data):\n",
    "      self._size = end\n",
    "      end = len(self._data)\n",
    "      start = end - self._batch_size\n",
    "      self._num_epoch += 1\n",
    "      self._batch_id = 0\n",
    "      indices = list(range(len(self._data)))\n",
    "      random.seed(2021)\n",
    "      random.shuffle(indices)\n",
    "      self._data, self._labels, self._sentence_lengths, self._final_tokens = self._data[indices], self._labels[indices], self._sentence_lengths[indices], self._final_tokens[indices]\n",
    "\n",
    "    return self._data[start:end], self._labels[start:end], self._sentence_lengths[start:end], self._final_tokens[start:end]\n",
    "class RNN:\n",
    "    def __init__(self,vocab_size, embedding_size, lstm_size,batch_size):\n",
    "        self._vocab_size = vocab_size\n",
    "        self._embedding_size = embedding_size\n",
    "        self._lstm_size = lstm_size\n",
    "        self._batch_size = batch_size\n",
    "        \n",
    "        self._data = tf.placeholder(tf.int32, shape = [batch_size,500])\n",
    "        self._labels = tf.placeholder(tf.int32, shape = [batch_size, ])\n",
    "        self._sentence_lengths = tf.placeholder(tf.int32,shape = [batch_size, ])\n",
    "        self._final_tokens = tf.placeholder(tf.int32, shape = [batch_size, ])\n",
    "    def embedding_layer(self,indices):\n",
    "        pretrained_vectors =[]\n",
    "        pretrained_vectors.append(np.zeros(self._embedding_size))\n",
    "        np.random.seed(2018)\n",
    "        for _ in range(self._vocab_size + 1):\n",
    "            pretrained_vectors.append(np.random.normal(loc = 0., scale = 1., size = self._embedding_size))\n",
    "        pretrained_vectors = np.array(pretrained_vectors)\n",
    "        self._embedding_matrix = tf.get_variable(name = 'embedding', shape = (self._vocab_size + 2, self._embedding_size),initializer = tf.constant_initializer(pretrained_vectors))\n",
    "        return tf.nn.embedding_lookup(self._embedding_matrix, indices)\n",
    "    def LSTM_layer(self, embeddings):\n",
    "        lstm_cell = tf.nn.rnn_cell.LSTMCell(self._lstm_size)\n",
    "        zero_state = tf.zeros(shape = (self._batch_size, self._lstm_size))\n",
    "        initial_state = tf.nn.rnn_cell.LSTMStateTuple(zero_state, zero_state)\n",
    "        lstm_inputs = tf.unstack(tf.transpose(embeddings, perm = [1,0,2]))\n",
    "        lstm_outputs, last_state = tf.nn.static_rnn(cell = lstm_cell, inputs = lstm_inputs, initial_state = initial_state, sequence_length = self._sentence_lengths)\n",
    "        lstm_outputs = tf.unstack(tf.transpose(lstm_outputs, perm = [1,0,2]))\n",
    "        lstm_outputs = tf.concat(lstm_outputs, axis =0)\n",
    "        mask = tf.sequence_mask(lengths = self._sentence_lengths, maxlen = 500, dtype = tf.float32)\n",
    "        mask = tf.concat(tf.unstack(mask, axis =0), axis =0)\n",
    "        mask = tf.expand_dims(mask, -1)\n",
    "        lstm_outputs = mask * lstm_outputs\n",
    "        lstm_outputs_split = tf.split(lstm_outputs, num_or_size_splits = self._batch_size)\n",
    "        lstm_outputs_sum = tf.reduce_sum(lstm_outputs_split,axis = 1)\n",
    "        lstm_outputs_average = lstm_outputs_sum / tf.expand_dims(tf.cast(self._sentence_lengths, tf.float32),-1)\n",
    "        return lstm_outputs_average\n",
    "    def build_graph(self):\n",
    "        NUM_CLASSES = 20\n",
    "        embeddings = self.embedding_layer(self._data)\n",
    "        lstm_outputs = self.LSTM_layer(embeddings)\n",
    "        \n",
    "        weights = tf.get_variable(name = 'final_layer_weights', shape =(self._lstm_size, NUM_CLASSES), initializer = tf.random_normal_initializer(seed = 2018))\n",
    "        biases = tf.get_variable(name = 'final_layer_biases', shape =(NUM_CLASSES), initializer = tf.random_normal_initializer(seed = 2018))\n",
    "        logits = tf.matmul(lstm_outputs, weights) + biases\n",
    "        logits = tf.matmul(lstm_outputs, weights) + biases\n",
    "        labels_one_hot = tf.one_hot(indices = self._labels, depth = NUM_CLASSES, dtype = tf.float32)\n",
    "        loss = tf.nn.softmax_cross_entropy_with_logits(labels=labels_one_hot, logits=logits)\n",
    "        loss = tf.reduce_mean(loss)\n",
    "        probs = tf.nn.softmax(logits)\n",
    "        predicted_labels = tf.argmax(probs, axis = 1)\n",
    "        predicted_labels = tf.squeeze(predicted_labels)\n",
    "        return predicted_labels, loss\n",
    "    def trainer(self, loss, learning_rate):\n",
    "        train_op = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "        return train_op\n",
    "def train_and_evaluate_RNN():\n",
    "        with open('D:/E/CUONG/machine learning/w2v/vocab-raw.txt') as f:\n",
    "            vocab_size = len(f.read().splitlines())\n",
    "        tf.set_random_seed(2018)\n",
    "        rnn = RNN(vocab_size = vocab_size, embedding_size = 300, lstm_size = 50, batch_size = 50)\n",
    "        predicted_labels, loss = rnn.build_graph()\n",
    "        train_op = rnn.trainer(loss = loss , learning_rate = 0.01)\n",
    "        with tf.Session() as sess:\n",
    "            train_data_reader = DataReader_session_4(data_path = 'D:/E/CUONG/machine learning/20news-bydate/20news-train-encoded.txt', batch_size = 50,vocab_size = vocab_size)\n",
    "            test_data_reader = DataReader_session_4(data_path = 'D:/E/CUONG/machine learning/20news-bydate/20news-test-encoded.txt', batch_size = 50, vocab_size = vocab_size)\n",
    "            #print(len(train_data_reader._data),len(train_data_reader._data[0]))\n",
    "            step = 0\n",
    "            MAX_STEP = 1000\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            while step < MAX_STEP:\n",
    "                next_train_batch = train_data_reader.next_batch()\n",
    "                train_data, train_labels, train_sentence_lengths, train_final_tokens = next_train_batch\n",
    "                print(len(train_data),len(train_data[0]))\n",
    "                plabels_eval, loss_eval, _ = sess.run([predicted_labels, loss, train_op], feed_dict = {rnn._data: train_data, rnn._labels: train_labels, rnn._sentence_lengths: train_sentence_lengths, rnn._final_tokens: train_final_tokens})\n",
    "                step +=1\n",
    "                if step % 20 ==0:\n",
    "                    print('loss', loss_eval)\n",
    "                if train_data_reader._batch_id ==0:\n",
    "                    num_true_preds = 0\n",
    "                    while True:\n",
    "                        next_test_batch = test_data_reader.next_batch()\n",
    "                        test_data, test_labels, test_sentence_lengths, test_final_tokens = next_test_batch\n",
    "                        test_plabels_eval = sess.run(predicted_labels, feed_dict = {rnn._data: test_data, rnn._labels: test_labels, rnn._sentence_lengths: test_sentence_lengths, rnn._final_tokens: test_final_tokens})\n",
    "                        matches = np.equal(test_plabels_eval, test_labels)\n",
    "                        num_true_preds += np.sum(matches.astype(float))\n",
    "                        if test_data_reader._batch_id == 0:\n",
    "                            break\n",
    "                    print('Epoch:', train_data_reader._num_epoch)\n",
    "                    print('Accuracy on test data:', num_true_preds * 100./len(test_data_reader._data))\n",
    "train_and_evaluate_RNN()            \n",
    "\n",
    "                                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "tensorflow_multilayer_perceptron.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
